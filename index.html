<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>FUTR3D</title>

    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;
</script>

<body>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->

    <!-- <div class="logo" style="padding-left: 120pt" align="top">
        <a href="https://github.com/metadriverse/metadrive" target="_blank">
            <img style=" width: 450pt;" src="images/metadrive.png">
        </a>
    </div> -->

    <div class="header">
        <div style="" class="title" id="lang">
            FUTR3D: A Unified Sensor Fusion Framework for 3D Detection
        </div>
    </div>
    <!-- === Title Ends === -->

    <div class="author" style="margin-top: -30pt">
        <a href="#" target="_blank">Xuanyao Chen</a><sup>*</sup>,&nbsp;
        <a href="https://tianyuanzhang.com/" target="_blank">Tianyuan Zhang</a><sup>*</sup>,&nbsp;
        <a href="https://people.csail.mit.edu/yuewang/" target="_blank">Yue Wang</a><sup>3</sup>,&nbsp;
        <a href="https://scholar.google.com.hk/citations?hl=en&user=nUyTDosAAAAJ">Yilun Wang</a><sup>4</sup>,&nbsp;
        <a href="https://hangzhaomit.github.io/">Hang Zhao</a><sup>5</sup>&nbsp;
    </div>

    <div class="institution">
        <div><sup>1</sup>Fudan University,
            <sup>2</sup>CMU,
            <sup>3</sup>MIT,
        </div>
        <div>
            <sup>4</sup>Li Auto,
            <sup>5</sup>Tsinghua University
        </div>
    </div>

    <table border="0" align="center">
        <tr>
            <td align="center" style="padding: 0pt 0 15pt 0">
                <a class="bar" href="https://tsinghua-mars-lab.github.io/FUTR3D/"><b>Webpage</b></a> |
                <a class="bar" href="#"><b>Code</b></a> |
                <a class="bar" href="https://arxiv.org/abs/2205.00613"><b>Paper</b></a>
            </td>
        </tr>
    </table>
    
    <!--<div align="center">
        <table width="100%" style="margin: 0pt 0pt; text-align: center;">
            <tr>
                <td>
                    <video style="display:block; width:100%; height:auto; "
                           autoplay="autoplay" muted loop="loop" controls playsinline>
                        <source src="https://raw.githubusercontent.com/decisionforce/archive/master/MetaDrive/metadrive_teaser.mp4"
                                type="video/mp4"/>
                    </video>
                </td>
            </tr>
        </table>
    </div> -->
    <p>
        A unified sensor fusion framework that works with arbitrary sensor combinations and performs competitively with various customized state-of-the-art models. FUTR3D can work with camera-LiDAR fusion, camera-radar fusion, camera-LiDAR-radar fusion.
    </p>

</div>
<!-- === Home Section Ends === -->




<div class="section">
    <div class="title" id="lang">Abstract</div>
    <div class="logo" style="" align="center">
        <img style="width: 700pt;" src="images/teaser.png">
    </div>
    <p>
        Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and Radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (56.8 mAP) achieves on par performance with state-of-the-art 3D detection model CenterPoint (56.6 mAP) using a 32-beam LiDAR.
    </p>
    <!-- <ul>
        <li>
            <b>Compositional</b>: It supports generating infinite scenes with various road maps and traffic settings for the
            research of generalizable RL.
        </li>
        <li>
            <b>Lightweight</b>: It is easy to install and run. It can run up to 300 FPS on a standard PC.
        </li>
        <li>
            <b>Realistic</b>: Accurate physics simulation and multiple sensory input including Lidar, RGB images, top-down
            semantic map and first-person view images. The real traffic data replay is also supported.
        </li>
    </ul> -->
 
</div>

<div class="section">
    <div class="title" id="lang">Method</div>
    <div class="logo" style="" align="center">
        <img style="width: 700pt;" src="images/FUTR3D.png">
    </div>
    <p>
        <b> Unified sensor fusion framework.</b> FUTR3D first encodes features for each modality individually, and then employs a query-based Modality-Agnostic Feature Sampler (MAFS) that works in a unified domain and extract features from different modalities. Finally, a transformer decoder operates on a set of 3D queries and performs set predictions of objects. The contributions of our work are the following:
    </p>
    <ul>
        <li><b> Uified Framework</b> Modality-Agnostic Feature Sampler, called MAFS, enables our method to operate on any sensors and their combinations in a modality agnostic way. Using 3D queries, MAFS samples and aggregates features from cameras, high-resolution LiDARs, low-resolution LiDARs, and radars. </li>
        <li><b> Effecetiveness</b>  FUTR3D performs competitively with specifically designed fusion methods across different sensor combinations.</li>  
        <li><b> Low-cost System</b> FUTR3D achieves excellent flexibility with different sensor configurations and enables low-cost perception systems for autonomous driving. On the nuScenes dataset, FUTR3D achieves 56.8 mAP with a 4-beam LiDAR and camera images, which is on a par with the state-of-the-art model using a 32-beam LiDAR.</li> 
    </ul>
    
</div>

<div class="section">
    <div class="title" id="lang">Related Projects on VCAD (Vision-Centric Autonomous Driving)</div>
    <div class="col text-center">

    <table width="100%" style="margin: 0pt 0pt; text-align: center;">
    <tr>
      <td>
      BEV Mapping<br>
      <a href="https://tsinghua-mars-lab.github.io/HDMapNet/" class="d-inline-block p-3"><img height="100"
          src="https://tsinghua-mars-lab.github.io/HDMapNet/images/scene0_car_small.gif" style="border:1px solid" data-nothumb><br>HDMapNet</a>
      </td>
      <td>
      BEV Detection<br>
        <a href="https://tsinghua-mars-lab.github.io/DETR3D/" class="d-inline-block p-3"><img height="100"
          src="images/MUTR3D_paper_thumbnail.png" style="border:1px solid"
          data-nothumb><br>DETR3D</a>
      </td>
      <td>
        BEV Fusion<br>
      <a href="https://tsinghua-mars-lab.github.io/FUTR3D/" class="d-inline-block p-3"><img height="100"
          src="images/MUTR3D_paper_thumbnail.png" style="border:1px solid"
          data-nothumb><br>FUTR3D</a>
      </td>

    <td>
        BEV Tracking<br>
      <a href="https://tsinghua-mars-lab.github.io/MUTR3D/" class="d-inline-block p-3"><img height="100"
          src="images/MUTR3D_paper_thumbnail.png" style="border:1px solid"
          data-nothumb><br>MUTR3D</a>
      </td>

    </tr>
    </table>
    </div>

    
</div>

<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">
       <div class="title" id="lang">Reference</div>
    </div>
<p>If you find our work useful in your research, please cite our paper:</p>
    <pre>
@article{chen2022futr3d,
  title={FUTR3D: A Unified Sensor Fusion Framework for 3D Detection},
  author={Chen, Xuanyao and Zhang, Tianyuan and Wang, Yue and Wang, Yilun and Zhao, Hang},
  journal={arXiv preprint arXiv:2203.10642},
  year={2022}
}
</pre>
    <!-- Adjust the frame size based on the demo (Every project differs). -->
</div>

</body>
</html>
