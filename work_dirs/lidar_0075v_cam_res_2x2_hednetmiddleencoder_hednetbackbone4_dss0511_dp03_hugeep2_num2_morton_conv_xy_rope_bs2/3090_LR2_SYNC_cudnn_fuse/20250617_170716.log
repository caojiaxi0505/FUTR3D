2025-06-17 17:07:16,990 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+3056288
spconv2.0: True
------------------------------------------------------------

2025-06-17 17:07:18,176 - mmdet - INFO - 分布式训练: True
2025-06-17 17:07:19,311 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/3090_LR2_SYNC_cudnn_fuse'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
cudnn_benchmark = True
gpu_ids = range(0, 2)

2025-06-17 17:07:19,311 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-17 17:07:20,964 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-17 17:07:21,428 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-17 17:07:21,582 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,583 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,584 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,585 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,585 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,586 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,587 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,588 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,593 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,597 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,601 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,605 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,609 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,613 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,618 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,622 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,626 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,630 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,634 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,638 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,642 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,647 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,651 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,655 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,659 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,663 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,667 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,672 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,676 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,680 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,698 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,713 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,728 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 17:07:21,769 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.D - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.conv1d.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.conv1d.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.x_proj.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.dt_proj.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.dt_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-17 17:07:21,798 - mmdet - INFO - 使用SyncBN
2025-06-17 17:07:21,828 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMamba(
                  (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                  (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                  (x_proj): Linear(in_features=512, out_features=48, bias=False)
                  (dt_proj): Linear(in_features=16, out_features=512, bias=True)
                  (out_proj): Linear(in_features=512, out_features=256, bias=False)
                  (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-17 17:07:43,484 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-17 17:07:44,625 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.A_log_h2t, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.A_log_t2h, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.D, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.in_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.conv1d.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.conv1d.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.x_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.dt_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.dt_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.lidar_guidance_proj.bias

2025-06-17 17:07:44,668 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/3090_LR2_SYNC_cudnn_fuse
2025-06-17 17:07:44,669 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-17 17:07:44,670 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-17 17:07:44,671 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/3090_LR2_SYNC_cudnn_fuse by HardDiskBackend.
2025-06-17 17:09:33,443 - mmdet - INFO - Epoch [1][50/7033]	lr: 7.973e-05, eta: 1 day, 1:20:46, time: 2.165, data_time: 0.173, memory: 17974, loss_cls: 1.0431, loss_bbox: 1.5351, d0.loss_cls: 1.1102, d0.loss_bbox: 1.6208, d1.loss_cls: 1.1158, d1.loss_bbox: 1.5602, d2.loss_cls: 1.0758, d2.loss_bbox: 1.5409, d3.loss_cls: 1.0623, d3.loss_bbox: 1.5642, d4.loss_cls: 1.0371, d4.loss_bbox: 1.5454, loss: 15.8110, grad_norm: 58.5947
2025-06-17 17:11:18,957 - mmdet - INFO - Epoch [1][100/7033]	lr: 9.307e-05, eta: 1 day, 0:59:48, time: 2.110, data_time: 0.051, memory: 17974, loss_cls: 0.8692, loss_bbox: 1.3058, d0.loss_cls: 0.9649, d0.loss_bbox: 1.4435, d1.loss_cls: 0.9465, d1.loss_bbox: 1.3705, d2.loss_cls: 0.8825, d2.loss_bbox: 1.3350, d3.loss_cls: 0.8874, d3.loss_bbox: 1.3583, d4.loss_cls: 0.8604, d4.loss_bbox: 1.2996, loss: 13.5237, grad_norm: 47.8641
2025-06-17 17:13:20,455 - mmdet - INFO - Epoch [1][150/7033]	lr: 1.064e-04, eta: 1 day, 2:06:18, time: 2.430, data_time: 0.051, memory: 17974, loss_cls: 0.7603, loss_bbox: 1.2050, d0.loss_cls: 0.9099, d0.loss_bbox: 1.3787, d1.loss_cls: 0.8426, d1.loss_bbox: 1.3040, d2.loss_cls: 0.7402, d2.loss_bbox: 1.2389, d3.loss_cls: 0.7658, d3.loss_bbox: 1.2629, d4.loss_cls: 0.7405, d4.loss_bbox: 1.1961, loss: 12.3450, grad_norm: 45.0170
2025-06-17 17:15:28,204 - mmdet - INFO - Epoch [1][200/7033]	lr: 1.197e-04, eta: 1 day, 3:00:21, time: 2.555, data_time: 0.050, memory: 17974, loss_cls: 0.5486, loss_bbox: 1.1263, d0.loss_cls: 0.7871, d0.loss_bbox: 1.3214, d1.loss_cls: 0.6997, d1.loss_bbox: 1.2372, d2.loss_cls: 0.5358, d2.loss_bbox: 1.1680, d3.loss_cls: 0.5438, d3.loss_bbox: 1.1824, d4.loss_cls: 0.5179, d4.loss_bbox: 1.1169, loss: 10.7850, grad_norm: 32.0769
2025-06-17 17:17:39,762 - mmdet - INFO - Epoch [1][250/7033]	lr: 1.331e-04, eta: 1 day, 3:42:42, time: 2.632, data_time: 0.050, memory: 17974, loss_cls: 0.4319, loss_bbox: 1.0010, d0.loss_cls: 0.6289, d0.loss_bbox: 1.2275, d1.loss_cls: 0.5348, d1.loss_bbox: 1.1268, d2.loss_cls: 0.4279, d2.loss_bbox: 1.0372, d3.loss_cls: 0.4437, d3.loss_bbox: 1.0600, d4.loss_cls: 0.4094, d4.loss_bbox: 0.9920, loss: 9.3210, grad_norm: 29.6705
2025-06-17 17:19:52,662 - mmdet - INFO - Epoch [1][300/7033]	lr: 1.464e-04, eta: 1 day, 4:13:12, time: 2.657, data_time: 0.051, memory: 17974, loss_cls: 0.3623, loss_bbox: 0.9113, d0.loss_cls: 0.5360, d0.loss_bbox: 1.1596, d1.loss_cls: 0.4262, d1.loss_bbox: 1.0533, d2.loss_cls: 0.3748, d2.loss_bbox: 0.9666, d3.loss_cls: 0.3855, d3.loss_bbox: 0.9819, d4.loss_cls: 0.3534, d4.loss_bbox: 0.8971, loss: 8.4080, grad_norm: 26.3329
2025-06-17 17:22:09,379 - mmdet - INFO - Epoch [1][350/7033]	lr: 1.597e-04, eta: 1 day, 4:42:01, time: 2.734, data_time: 0.053, memory: 17974, loss_cls: 0.3076, loss_bbox: 0.8672, d0.loss_cls: 0.4577, d0.loss_bbox: 1.1199, d1.loss_cls: 0.3551, d1.loss_bbox: 1.0183, d2.loss_cls: 0.3227, d2.loss_bbox: 0.9399, d3.loss_cls: 0.3281, d3.loss_bbox: 0.9491, d4.loss_cls: 0.3027, d4.loss_bbox: 0.8458, loss: 7.8140, grad_norm: 30.7007
2025-06-17 17:24:30,455 - mmdet - INFO - Epoch [1][400/7033]	lr: 1.731e-04, eta: 1 day, 5:10:43, time: 2.822, data_time: 0.052, memory: 17974, loss_cls: 0.2651, loss_bbox: 0.7990, d0.loss_cls: 0.4043, d0.loss_bbox: 1.0502, d1.loss_cls: 0.3216, d1.loss_bbox: 0.9581, d2.loss_cls: 0.2881, d2.loss_bbox: 0.8854, d3.loss_cls: 0.2927, d3.loss_bbox: 0.8960, d4.loss_cls: 0.2657, d4.loss_bbox: 0.7786, loss: 7.2047, grad_norm: 38.3369
2025-06-17 17:26:50,148 - mmdet - INFO - Epoch [1][450/7033]	lr: 1.864e-04, eta: 1 day, 5:30:19, time: 2.794, data_time: 0.046, memory: 17974, loss_cls: 0.2499, loss_bbox: 0.7383, d0.loss_cls: 0.3741, d0.loss_bbox: 0.9840, d1.loss_cls: 0.3013, d1.loss_bbox: 0.8836, d2.loss_cls: 0.2701, d2.loss_bbox: 0.8199, d3.loss_cls: 0.2744, d3.loss_bbox: 0.8240, d4.loss_cls: 0.2488, d4.loss_bbox: 0.7180, loss: 6.6864, grad_norm: 25.6788
2025-06-17 17:29:10,032 - mmdet - INFO - Epoch [1][500/7033]	lr: 1.997e-04, eta: 1 day, 5:45:48, time: 2.798, data_time: 0.051, memory: 17974, loss_cls: 0.2204, loss_bbox: 0.6619, d0.loss_cls: 0.3399, d0.loss_bbox: 0.9356, d1.loss_cls: 0.2716, d1.loss_bbox: 0.8368, d2.loss_cls: 0.2442, d2.loss_bbox: 0.7728, d3.loss_cls: 0.2415, d3.loss_bbox: 0.7838, d4.loss_cls: 0.2211, d4.loss_bbox: 0.6556, loss: 6.1851, grad_norm: 28.7778
2025-06-17 17:31:30,166 - mmdet - INFO - Epoch [1][550/7033]	lr: 2.000e-04, eta: 1 day, 5:58:22, time: 2.803, data_time: 0.052, memory: 17974, loss_cls: 0.1923, loss_bbox: 0.6181, d0.loss_cls: 0.2965, d0.loss_bbox: 0.8591, d1.loss_cls: 0.2363, d1.loss_bbox: 0.7684, d2.loss_cls: 0.2124, d2.loss_bbox: 0.7049, d3.loss_cls: 0.2089, d3.loss_bbox: 0.7137, d4.loss_cls: 0.1888, d4.loss_bbox: 0.6099, loss: 5.6093, grad_norm: 29.7427
2025-06-17 17:33:55,931 - mmdet - INFO - Epoch [1][600/7033]	lr: 2.000e-04, eta: 1 day, 6:14:57, time: 2.915, data_time: 0.049, memory: 17974, loss_cls: 0.1784, loss_bbox: 0.5759, d0.loss_cls: 0.2850, d0.loss_bbox: 0.8491, d1.loss_cls: 0.2215, d1.loss_bbox: 0.7495, d2.loss_cls: 0.1984, d2.loss_bbox: 0.6911, d3.loss_cls: 0.1986, d3.loss_bbox: 0.6802, d4.loss_cls: 0.1824, d4.loss_bbox: 0.5857, loss: 5.3958, grad_norm: 48.5997
2025-06-17 17:36:18,227 - mmdet - INFO - Epoch [1][650/7033]	lr: 2.000e-04, eta: 1 day, 6:24:55, time: 2.846, data_time: 0.051, memory: 17974, loss_cls: 0.1699, loss_bbox: 0.5333, d0.loss_cls: 0.2806, d0.loss_bbox: 0.8202, d1.loss_cls: 0.2257, d1.loss_bbox: 0.7142, d2.loss_cls: 0.1941, d2.loss_bbox: 0.6539, d3.loss_cls: 0.1903, d3.loss_bbox: 0.6250, d4.loss_cls: 0.1695, d4.loss_bbox: 0.5542, loss: 5.1309, grad_norm: 35.8611
2025-06-17 17:38:42,015 - mmdet - INFO - Epoch [1][700/7033]	lr: 2.000e-04, eta: 1 day, 6:34:36, time: 2.876, data_time: 0.050, memory: 17974, loss_cls: 0.1801, loss_bbox: 0.5069, d0.loss_cls: 0.2689, d0.loss_bbox: 0.7993, d1.loss_cls: 0.2244, d1.loss_bbox: 0.6786, d2.loss_cls: 0.2023, d2.loss_bbox: 0.6099, d3.loss_cls: 0.1976, d3.loss_bbox: 0.5754, d4.loss_cls: 0.1775, d4.loss_bbox: 0.5287, loss: 4.9496, grad_norm: 25.4619
2025-06-17 17:41:07,819 - mmdet - INFO - Epoch [1][750/7033]	lr: 2.000e-04, eta: 1 day, 6:44:30, time: 2.916, data_time: 0.049, memory: 17974, loss_cls: 0.1600, loss_bbox: 0.4932, d0.loss_cls: 0.2563, d0.loss_bbox: 0.8030, d1.loss_cls: 0.2060, d1.loss_bbox: 0.6580, d2.loss_cls: 0.1811, d2.loss_bbox: 0.5855, d3.loss_cls: 0.1820, d3.loss_bbox: 0.5673, d4.loss_cls: 0.1630, d4.loss_bbox: 0.5142, loss: 4.7696, grad_norm: 29.4172
2025-06-17 17:43:31,240 - mmdet - INFO - Epoch [1][800/7033]	lr: 2.000e-04, eta: 1 day, 6:50:51, time: 2.869, data_time: 0.047, memory: 17974, loss_cls: 0.1584, loss_bbox: 0.4624, d0.loss_cls: 0.2520, d0.loss_bbox: 0.7748, d1.loss_cls: 0.2013, d1.loss_bbox: 0.6110, d2.loss_cls: 0.1809, d2.loss_bbox: 0.5416, d3.loss_cls: 0.1777, d3.loss_bbox: 0.5332, d4.loss_cls: 0.1589, d4.loss_bbox: 0.4971, loss: 4.5493, grad_norm: 40.8883
2025-06-17 17:45:54,241 - mmdet - INFO - Epoch [1][850/7033]	lr: 2.000e-04, eta: 1 day, 6:55:47, time: 2.859, data_time: 0.051, memory: 17974, loss_cls: 0.1565, loss_bbox: 0.4304, d0.loss_cls: 0.2499, d0.loss_bbox: 0.7654, d1.loss_cls: 0.2056, d1.loss_bbox: 0.5873, d2.loss_cls: 0.1773, d2.loss_bbox: 0.5269, d3.loss_cls: 0.1708, d3.loss_bbox: 0.4921, d4.loss_cls: 0.1545, d4.loss_bbox: 0.4687, loss: 4.3855, grad_norm: 48.1393
2025-06-17 17:48:16,963 - mmdet - INFO - Epoch [1][900/7033]	lr: 2.000e-04, eta: 1 day, 6:59:43, time: 2.855, data_time: 0.050, memory: 17974, loss_cls: 0.1516, loss_bbox: 0.4068, d0.loss_cls: 0.2469, d0.loss_bbox: 0.7536, d1.loss_cls: 0.1954, d1.loss_bbox: 0.5556, d2.loss_cls: 0.1719, d2.loss_bbox: 0.4977, d3.loss_cls: 0.1704, d3.loss_bbox: 0.4631, d4.loss_cls: 0.1531, d4.loss_bbox: 0.4345, loss: 4.2005, grad_norm: 29.6179
2025-06-17 17:50:38,544 - mmdet - INFO - Epoch [1][950/7033]	lr: 2.000e-04, eta: 1 day, 7:02:09, time: 2.831, data_time: 0.049, memory: 17974, loss_cls: 0.1365, loss_bbox: 0.3849, d0.loss_cls: 0.2289, d0.loss_bbox: 0.7357, d1.loss_cls: 0.1761, d1.loss_bbox: 0.5283, d2.loss_cls: 0.1520, d2.loss_bbox: 0.4575, d3.loss_cls: 0.1485, d3.loss_bbox: 0.4252, d4.loss_cls: 0.1343, d4.loss_bbox: 0.4023, loss: 3.9101, grad_norm: 30.8617
2025-06-17 17:53:01,108 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 17:53:01,109 - mmdet - INFO - Epoch [1][1000/7033]	lr: 2.000e-04, eta: 1 day, 7:04:48, time: 2.852, data_time: 0.053, memory: 17974, loss_cls: 0.1302, loss_bbox: 0.3579, d0.loss_cls: 0.2249, d0.loss_bbox: 0.7026, d1.loss_cls: 0.1728, d1.loss_bbox: 0.4937, d2.loss_cls: 0.1510, d2.loss_bbox: 0.4044, d3.loss_cls: 0.1447, d3.loss_bbox: 0.3940, d4.loss_cls: 0.1331, d4.loss_bbox: 0.3667, loss: 3.6761, grad_norm: 45.8780
2025-06-17 17:55:25,230 - mmdet - INFO - Epoch [1][1050/7033]	lr: 2.000e-04, eta: 1 day, 7:07:59, time: 2.882, data_time: 0.047, memory: 17974, loss_cls: 0.1377, loss_bbox: 0.3674, d0.loss_cls: 0.2356, d0.loss_bbox: 0.7295, d1.loss_cls: 0.1857, d1.loss_bbox: 0.5046, d2.loss_cls: 0.1582, d2.loss_bbox: 0.4081, d3.loss_cls: 0.1504, d3.loss_bbox: 0.4023, d4.loss_cls: 0.1399, d4.loss_bbox: 0.3661, loss: 3.7856, grad_norm: 40.8430
2025-06-17 17:57:45,456 - mmdet - INFO - Epoch [1][1100/7033]	lr: 2.000e-04, eta: 1 day, 7:08:14, time: 2.805, data_time: 0.053, memory: 17974, loss_cls: 0.1500, loss_bbox: 0.3448, d0.loss_cls: 0.2435, d0.loss_bbox: 0.7062, d1.loss_cls: 0.1988, d1.loss_bbox: 0.4784, d2.loss_cls: 0.1696, d2.loss_bbox: 0.3929, d3.loss_cls: 0.1681, d3.loss_bbox: 0.3842, d4.loss_cls: 0.1493, d4.loss_bbox: 0.3425, loss: 3.7284, grad_norm: 64.3105
2025-06-17 18:00:10,592 - mmdet - INFO - Epoch [1][1150/7033]	lr: 2.000e-04, eta: 1 day, 7:11:10, time: 2.903, data_time: 0.071, memory: 17974, loss_cls: 0.1456, loss_bbox: 0.3396, d0.loss_cls: 0.2409, d0.loss_bbox: 0.6644, d1.loss_cls: 0.1941, d1.loss_bbox: 0.4601, d2.loss_cls: 0.1669, d2.loss_bbox: 0.3777, d3.loss_cls: 0.1595, d3.loss_bbox: 0.3658, d4.loss_cls: 0.1458, d4.loss_bbox: 0.3382, loss: 3.5986, grad_norm: 34.9713
2025-06-17 18:02:32,312 - mmdet - INFO - Epoch [1][1200/7033]	lr: 2.000e-04, eta: 1 day, 7:11:43, time: 2.834, data_time: 0.048, memory: 17974, loss_cls: 0.1416, loss_bbox: 0.3613, d0.loss_cls: 0.2289, d0.loss_bbox: 0.6753, d1.loss_cls: 0.1858, d1.loss_bbox: 0.4467, d2.loss_cls: 0.1601, d2.loss_bbox: 0.3818, d3.loss_cls: 0.1568, d3.loss_bbox: 0.3764, d4.loss_cls: 0.1443, d4.loss_bbox: 0.3551, loss: 3.6141, grad_norm: 38.8475
2025-06-17 18:04:55,613 - mmdet - INFO - Epoch [1][1250/7033]	lr: 2.000e-04, eta: 1 day, 7:12:53, time: 2.866, data_time: 0.051, memory: 17974, loss_cls: 0.1371, loss_bbox: 0.3421, d0.loss_cls: 0.2368, d0.loss_bbox: 0.6482, d1.loss_cls: 0.1744, d1.loss_bbox: 0.4203, d2.loss_cls: 0.1506, d2.loss_bbox: 0.3705, d3.loss_cls: 0.1489, d3.loss_bbox: 0.3578, d4.loss_cls: 0.1369, d4.loss_bbox: 0.3372, loss: 3.4608, grad_norm: 61.8812
2025-06-17 18:07:17,117 - mmdet - INFO - Epoch [1][1300/7033]	lr: 2.000e-04, eta: 1 day, 7:12:49, time: 2.829, data_time: 0.049, memory: 17974, loss_cls: 0.1278, loss_bbox: 0.3213, d0.loss_cls: 0.2174, d0.loss_bbox: 0.6203, d1.loss_cls: 0.1701, d1.loss_bbox: 0.3977, d2.loss_cls: 0.1484, d2.loss_bbox: 0.3446, d3.loss_cls: 0.1435, d3.loss_bbox: 0.3340, d4.loss_cls: 0.1289, d4.loss_bbox: 0.3151, loss: 3.2690, grad_norm: 38.7866
2025-06-17 18:09:36,300 - mmdet - INFO - Epoch [1][1350/7033]	lr: 2.000e-04, eta: 1 day, 7:11:28, time: 2.785, data_time: 0.049, memory: 17974, loss_cls: 0.1322, loss_bbox: 0.3217, d0.loss_cls: 0.2146, d0.loss_bbox: 0.6145, d1.loss_cls: 0.1701, d1.loss_bbox: 0.4001, d2.loss_cls: 0.1486, d2.loss_bbox: 0.3541, d3.loss_cls: 0.1412, d3.loss_bbox: 0.3418, d4.loss_cls: 0.1331, d4.loss_bbox: 0.3163, loss: 3.2881, grad_norm: 404.8282
2025-06-17 18:12:00,116 - mmdet - INFO - Epoch [1][1400/7033]	lr: 2.000e-04, eta: 1 day, 7:12:16, time: 2.876, data_time: 0.051, memory: 17974, loss_cls: 0.1328, loss_bbox: 0.3046, d0.loss_cls: 0.2259, d0.loss_bbox: 0.5961, d1.loss_cls: 0.1764, d1.loss_bbox: 0.3711, d2.loss_cls: 0.1496, d2.loss_bbox: 0.3307, d3.loss_cls: 0.1420, d3.loss_bbox: 0.3224, d4.loss_cls: 0.1344, d4.loss_bbox: 0.3003, loss: 3.1864, grad_norm: 51.6516
2025-06-17 18:14:22,098 - mmdet - INFO - Epoch [1][1450/7033]	lr: 2.000e-04, eta: 1 day, 7:12:00, time: 2.840, data_time: 0.049, memory: 17974, loss_cls: 0.1216, loss_bbox: 0.3092, d0.loss_cls: 0.2068, d0.loss_bbox: 0.5951, d1.loss_cls: 0.1570, d1.loss_bbox: 0.3705, d2.loss_cls: 0.1337, d2.loss_bbox: 0.3313, d3.loss_cls: 0.1298, d3.loss_bbox: 0.3200, d4.loss_cls: 0.1227, d4.loss_bbox: 0.3022, loss: 3.1000, grad_norm: 67.2643
2025-06-17 18:16:45,261 - mmdet - INFO - Epoch [1][1500/7033]	lr: 2.000e-04, eta: 1 day, 7:12:07, time: 2.863, data_time: 0.047, memory: 17974, loss_cls: 0.1142, loss_bbox: 0.3010, d0.loss_cls: 0.2112, d0.loss_bbox: 0.5722, d1.loss_cls: 0.1578, d1.loss_bbox: 0.3657, d2.loss_cls: 0.1318, d2.loss_bbox: 0.3275, d3.loss_cls: 0.1258, d3.loss_bbox: 0.3162, d4.loss_cls: 0.1162, d4.loss_bbox: 0.2984, loss: 3.0381, grad_norm: 26.5795
2025-06-17 18:19:04,793 - mmdet - INFO - Epoch [1][1550/7033]	lr: 2.000e-04, eta: 1 day, 7:10:29, time: 2.791, data_time: 0.051, memory: 17974, loss_cls: 0.1780, loss_bbox: 0.3133, d0.loss_cls: 0.3209, d0.loss_bbox: 0.6556, d1.loss_cls: 0.2351, d1.loss_bbox: 0.4027, d2.loss_cls: 0.2019, d2.loss_bbox: 0.3500, d3.loss_cls: 0.1976, d3.loss_bbox: 0.3345, d4.loss_cls: 0.1802, d4.loss_bbox: 0.3111, loss: 3.6809, grad_norm: 34.9473
2025-06-17 18:21:26,653 - mmdet - INFO - Epoch [1][1600/7033]	lr: 2.000e-04, eta: 1 day, 7:09:47, time: 2.837, data_time: 0.051, memory: 17974, loss_cls: 0.1319, loss_bbox: 0.3124, d0.loss_cls: 0.2159, d0.loss_bbox: 0.5646, d1.loss_cls: 0.1649, d1.loss_bbox: 0.3725, d2.loss_cls: 0.1482, d2.loss_bbox: 0.3366, d3.loss_cls: 0.1421, d3.loss_bbox: 0.3273, d4.loss_cls: 0.1305, d4.loss_bbox: 0.3126, loss: 3.1595, grad_norm: 39.1292
2025-06-17 18:23:48,193 - mmdet - INFO - Epoch [1][1650/7033]	lr: 2.000e-04, eta: 1 day, 7:08:52, time: 2.831, data_time: 0.049, memory: 17974, loss_cls: 0.1182, loss_bbox: 0.2931, d0.loss_cls: 0.2121, d0.loss_bbox: 0.5186, d1.loss_cls: 0.1562, d1.loss_bbox: 0.3447, d2.loss_cls: 0.1356, d2.loss_bbox: 0.3109, d3.loss_cls: 0.1302, d3.loss_bbox: 0.3036, d4.loss_cls: 0.1199, d4.loss_bbox: 0.2894, loss: 2.9325, grad_norm: 60.9504
2025-06-17 18:26:06,109 - mmdet - INFO - Epoch [1][1700/7033]	lr: 2.000e-04, eta: 1 day, 7:06:25, time: 2.758, data_time: 0.051, memory: 17974, loss_cls: 0.1182, loss_bbox: 0.3063, d0.loss_cls: 0.2156, d0.loss_bbox: 0.5324, d1.loss_cls: 0.1624, d1.loss_bbox: 0.3628, d2.loss_cls: 0.1360, d2.loss_bbox: 0.3285, d3.loss_cls: 0.1289, d3.loss_bbox: 0.3202, d4.loss_cls: 0.1188, d4.loss_bbox: 0.3030, loss: 3.0331, grad_norm: 36.2468
2025-06-17 18:28:55,116 - mmdet - INFO - Epoch [1][1750/7033]	lr: 2.000e-04, eta: 1 day, 7:15:57, time: 3.380, data_time: 0.468, memory: 17974, loss_cls: 0.1222, loss_bbox: 0.2909, d0.loss_cls: 0.2075, d0.loss_bbox: 0.5079, d1.loss_cls: 0.1530, d1.loss_bbox: 0.3497, d2.loss_cls: 0.1359, d2.loss_bbox: 0.3144, d3.loss_cls: 0.1297, d3.loss_bbox: 0.3046, d4.loss_cls: 0.1194, d4.loss_bbox: 0.2917, loss: 2.9268, grad_norm: 50.7691
2025-06-17 18:31:30,205 - mmdet - INFO - Epoch [1][1800/7033]	lr: 2.000e-04, eta: 1 day, 7:19:37, time: 3.102, data_time: 0.442, memory: 17974, loss_cls: 0.1162, loss_bbox: 0.2979, d0.loss_cls: 0.2056, d0.loss_bbox: 0.5017, d1.loss_cls: 0.1471, d1.loss_bbox: 0.3570, d2.loss_cls: 0.1336, d2.loss_bbox: 0.3248, d3.loss_cls: 0.1245, d3.loss_bbox: 0.3132, d4.loss_cls: 0.1156, d4.loss_bbox: 0.2959, loss: 2.9331, grad_norm: 36.9206
2025-06-17 18:35:10,400 - mmdet - INFO - Epoch [1][1850/7033]	lr: 2.000e-04, eta: 1 day, 7:46:35, time: 4.404, data_time: 1.755, memory: 17974, loss_cls: 0.1120, loss_bbox: 0.2825, d0.loss_cls: 0.1987, d0.loss_bbox: 0.4666, d1.loss_cls: 0.1447, d1.loss_bbox: 0.3300, d2.loss_cls: 0.1237, d2.loss_bbox: 0.3036, d3.loss_cls: 0.1177, d3.loss_bbox: 0.2939, d4.loss_cls: 0.1122, d4.loss_bbox: 0.2816, loss: 2.7672, grad_norm: 36.4253
2025-06-17 18:37:08,137 - mmdet - INFO - Epoch [1][1900/7033]	lr: 2.000e-04, eta: 1 day, 7:35:44, time: 2.355, data_time: 0.057, memory: 17974, loss_cls: 0.1110, loss_bbox: 0.2747, d0.loss_cls: 0.2000, d0.loss_bbox: 0.4611, d1.loss_cls: 0.1479, d1.loss_bbox: 0.3224, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2965, d3.loss_cls: 0.1189, d3.loss_bbox: 0.2864, d4.loss_cls: 0.1100, d4.loss_bbox: 0.2724, loss: 2.7249, grad_norm: 320.5014
2025-06-17 18:39:21,686 - mmdet - INFO - Epoch [1][1950/7033]	lr: 2.000e-04, eta: 1 day, 7:30:46, time: 2.670, data_time: 0.053, memory: 17974, loss_cls: 0.1102, loss_bbox: 0.2920, d0.loss_cls: 0.2012, d0.loss_bbox: 0.4764, d1.loss_cls: 0.1483, d1.loss_bbox: 0.3362, d2.loss_cls: 0.1286, d2.loss_bbox: 0.3089, d3.loss_cls: 0.1198, d3.loss_bbox: 0.3009, d4.loss_cls: 0.1106, d4.loss_bbox: 0.2890, loss: 2.8221, grad_norm: 38.0175
2025-06-17 18:41:35,279 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 18:41:35,280 - mmdet - INFO - Epoch [1][2000/7033]	lr: 2.000e-04, eta: 1 day, 7:25:58, time: 2.672, data_time: 0.054, memory: 17974, loss_cls: 0.1327, loss_bbox: 0.3044, d0.loss_cls: 0.2319, d0.loss_bbox: 0.4995, d1.loss_cls: 0.1641, d1.loss_bbox: 0.3556, d2.loss_cls: 0.1428, d2.loss_bbox: 0.3249, d3.loss_cls: 0.1370, d3.loss_bbox: 0.3182, d4.loss_cls: 0.1298, d4.loss_bbox: 0.3029, loss: 3.0437, grad_norm: 100.8274
2025-06-17 18:43:50,807 - mmdet - INFO - Epoch [1][2050/7033]	lr: 2.000e-04, eta: 1 day, 7:21:55, time: 2.711, data_time: 0.050, memory: 17974, loss_cls: 0.1202, loss_bbox: 0.2923, d0.loss_cls: 0.2138, d0.loss_bbox: 0.4648, d1.loss_cls: 0.1548, d1.loss_bbox: 0.3372, d2.loss_cls: 0.1345, d2.loss_bbox: 0.3097, d3.loss_cls: 0.1264, d3.loss_bbox: 0.3023, d4.loss_cls: 0.1199, d4.loss_bbox: 0.2888, loss: 2.8648, grad_norm: 30.4527
2025-06-17 18:46:09,674 - mmdet - INFO - Epoch [1][2100/7033]	lr: 2.000e-04, eta: 1 day, 7:19:01, time: 2.777, data_time: 0.057, memory: 17974, loss_cls: 0.1132, loss_bbox: 0.2892, d0.loss_cls: 0.2028, d0.loss_bbox: 0.4594, d1.loss_cls: 0.1490, d1.loss_bbox: 0.3338, d2.loss_cls: 0.1275, d2.loss_bbox: 0.3092, d3.loss_cls: 0.1243, d3.loss_bbox: 0.3022, d4.loss_cls: 0.1114, d4.loss_bbox: 0.2877, loss: 2.8097, grad_norm: 35.6749
2025-06-17 18:48:24,773 - mmdet - INFO - Epoch [1][2150/7033]	lr: 2.000e-04, eta: 1 day, 7:14:57, time: 2.702, data_time: 0.057, memory: 17974, loss_cls: 0.1176, loss_bbox: 0.2843, d0.loss_cls: 0.2103, d0.loss_bbox: 0.4471, d1.loss_cls: 0.1512, d1.loss_bbox: 0.3252, d2.loss_cls: 0.1289, d2.loss_bbox: 0.3026, d3.loss_cls: 0.1261, d3.loss_bbox: 0.2910, d4.loss_cls: 0.1180, d4.loss_bbox: 0.2803, loss: 2.7825, grad_norm: 47.0146
2025-06-17 18:50:43,006 - mmdet - INFO - Epoch [1][2200/7033]	lr: 2.000e-04, eta: 1 day, 7:11:56, time: 2.764, data_time: 0.078, memory: 17974, loss_cls: 0.1130, loss_bbox: 0.2839, d0.loss_cls: 0.1990, d0.loss_bbox: 0.4495, d1.loss_cls: 0.1447, d1.loss_bbox: 0.3305, d2.loss_cls: 0.1267, d2.loss_bbox: 0.2995, d3.loss_cls: 0.1211, d3.loss_bbox: 0.2928, d4.loss_cls: 0.1127, d4.loss_bbox: 0.2799, loss: 2.7533, grad_norm: 83.3289
2025-06-17 18:53:00,293 - mmdet - INFO - Epoch [1][2250/7033]	lr: 2.000e-04, eta: 1 day, 7:08:41, time: 2.746, data_time: 0.077, memory: 17974, loss_cls: 0.1096, loss_bbox: 0.2827, d0.loss_cls: 0.1999, d0.loss_bbox: 0.4406, d1.loss_cls: 0.1464, d1.loss_bbox: 0.3269, d2.loss_cls: 0.1220, d2.loss_bbox: 0.3013, d3.loss_cls: 0.1193, d3.loss_bbox: 0.2900, d4.loss_cls: 0.1109, d4.loss_bbox: 0.2796, loss: 2.7293, grad_norm: 100.0877
2025-06-17 18:55:17,278 - mmdet - INFO - Epoch [1][2300/7033]	lr: 2.000e-04, eta: 1 day, 7:05:23, time: 2.740, data_time: 0.056, memory: 17974, loss_cls: 0.1127, loss_bbox: 0.2840, d0.loss_cls: 0.2035, d0.loss_bbox: 0.4284, d1.loss_cls: 0.1432, d1.loss_bbox: 0.3160, d2.loss_cls: 0.1204, d2.loss_bbox: 0.2988, d3.loss_cls: 0.1169, d3.loss_bbox: 0.2910, d4.loss_cls: 0.1119, d4.loss_bbox: 0.2816, loss: 2.7083, grad_norm: 28.3478
2025-06-17 18:57:36,211 - mmdet - INFO - Epoch [1][2350/7033]	lr: 2.000e-04, eta: 1 day, 7:02:40, time: 2.779, data_time: 0.053, memory: 17974, loss_cls: 0.1077, loss_bbox: 0.2918, d0.loss_cls: 0.1978, d0.loss_bbox: 0.4528, d1.loss_cls: 0.1425, d1.loss_bbox: 0.3329, d2.loss_cls: 0.1204, d2.loss_bbox: 0.3065, d3.loss_cls: 0.1144, d3.loss_bbox: 0.3000, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2883, loss: 2.7617, grad_norm: 27.2450
2025-06-17 18:59:54,350 - mmdet - INFO - Epoch [1][2400/7033]	lr: 2.000e-04, eta: 1 day, 6:59:45, time: 2.763, data_time: 0.055, memory: 17974, loss_cls: 0.1079, loss_bbox: 0.2787, d0.loss_cls: 0.1993, d0.loss_bbox: 0.4406, d1.loss_cls: 0.1408, d1.loss_bbox: 0.3263, d2.loss_cls: 0.1209, d2.loss_bbox: 0.2982, d3.loss_cls: 0.1151, d3.loss_bbox: 0.2888, d4.loss_cls: 0.1075, d4.loss_bbox: 0.2768, loss: 2.7009, grad_norm: 33.1272
2025-06-17 19:02:09,416 - mmdet - INFO - Epoch [1][2450/7033]	lr: 2.000e-04, eta: 1 day, 6:56:01, time: 2.701, data_time: 0.050, memory: 17974, loss_cls: 0.1114, loss_bbox: 0.2782, d0.loss_cls: 0.2047, d0.loss_bbox: 0.4393, d1.loss_cls: 0.1449, d1.loss_bbox: 0.3184, d2.loss_cls: 0.1226, d2.loss_bbox: 0.2943, d3.loss_cls: 0.1195, d3.loss_bbox: 0.2844, d4.loss_cls: 0.1109, d4.loss_bbox: 0.2750, loss: 2.7035, grad_norm: 83.9151
2025-06-17 19:04:26,335 - mmdet - INFO - Epoch [1][2500/7033]	lr: 2.000e-04, eta: 1 day, 6:52:51, time: 2.739, data_time: 0.056, memory: 17974, loss_cls: 0.1038, loss_bbox: 0.2754, d0.loss_cls: 0.1992, d0.loss_bbox: 0.4399, d1.loss_cls: 0.1456, d1.loss_bbox: 0.3157, d2.loss_cls: 0.1223, d2.loss_bbox: 0.2919, d3.loss_cls: 0.1133, d3.loss_bbox: 0.2836, d4.loss_cls: 0.1047, d4.loss_bbox: 0.2752, loss: 2.6705, grad_norm: 39.7798
2025-06-17 19:06:42,852 - mmdet - INFO - Epoch [1][2550/7033]	lr: 2.000e-04, eta: 1 day, 6:49:37, time: 2.730, data_time: 0.057, memory: 17974, loss_cls: 0.1028, loss_bbox: 0.2715, d0.loss_cls: 0.1937, d0.loss_bbox: 0.4237, d1.loss_cls: 0.1383, d1.loss_bbox: 0.3145, d2.loss_cls: 0.1169, d2.loss_bbox: 0.2865, d3.loss_cls: 0.1109, d3.loss_bbox: 0.2762, d4.loss_cls: 0.1043, d4.loss_bbox: 0.2665, loss: 2.6057, grad_norm: 36.3667
2025-06-17 19:08:59,168 - mmdet - INFO - Epoch [1][2600/7033]	lr: 2.000e-04, eta: 1 day, 6:46:21, time: 2.726, data_time: 0.058, memory: 17974, loss_cls: 0.1099, loss_bbox: 0.2776, d0.loss_cls: 0.1983, d0.loss_bbox: 0.4363, d1.loss_cls: 0.1470, d1.loss_bbox: 0.3220, d2.loss_cls: 0.1242, d2.loss_bbox: 0.2948, d3.loss_cls: 0.1190, d3.loss_bbox: 0.2848, d4.loss_cls: 0.1108, d4.loss_bbox: 0.2776, loss: 2.7022, grad_norm: 47.0790
2025-06-17 19:11:17,404 - mmdet - INFO - Epoch [1][2650/7033]	lr: 2.000e-04, eta: 1 day, 6:43:37, time: 2.765, data_time: 0.051, memory: 17974, loss_cls: 0.1101, loss_bbox: 0.2909, d0.loss_cls: 0.1964, d0.loss_bbox: 0.4452, d1.loss_cls: 0.1498, d1.loss_bbox: 0.3309, d2.loss_cls: 0.1279, d2.loss_bbox: 0.3099, d3.loss_cls: 0.1201, d3.loss_bbox: 0.3002, d4.loss_cls: 0.1112, d4.loss_bbox: 0.2882, loss: 2.7810, grad_norm: 35.2521
2025-06-17 19:13:31,943 - mmdet - INFO - Epoch [1][2700/7033]	lr: 2.000e-04, eta: 1 day, 6:39:59, time: 2.691, data_time: 0.056, memory: 17974, loss_cls: 0.1096, loss_bbox: 0.2730, d0.loss_cls: 0.2062, d0.loss_bbox: 0.4280, d1.loss_cls: 0.1529, d1.loss_bbox: 0.3176, d2.loss_cls: 0.1273, d2.loss_bbox: 0.2914, d3.loss_cls: 0.1196, d3.loss_bbox: 0.2831, d4.loss_cls: 0.1094, d4.loss_bbox: 0.2731, loss: 2.6911, grad_norm: 33.4038
2025-06-17 19:15:47,470 - mmdet - INFO - Epoch [1][2750/7033]	lr: 2.000e-04, eta: 1 day, 6:36:39, time: 2.711, data_time: 0.058, memory: 17974, loss_cls: 0.1118, loss_bbox: 0.2847, d0.loss_cls: 0.2056, d0.loss_bbox: 0.4439, d1.loss_cls: 0.1499, d1.loss_bbox: 0.3330, d2.loss_cls: 0.1269, d2.loss_bbox: 0.3077, d3.loss_cls: 0.1193, d3.loss_bbox: 0.2987, d4.loss_cls: 0.1123, d4.loss_bbox: 0.2836, loss: 2.7774, grad_norm: 35.1612
2025-06-17 19:18:01,555 - mmdet - INFO - Epoch [1][2800/7033]	lr: 2.000e-04, eta: 1 day, 6:33:00, time: 2.681, data_time: 0.055, memory: 17974, loss_cls: 0.1065, loss_bbox: 0.2693, d0.loss_cls: 0.2026, d0.loss_bbox: 0.4246, d1.loss_cls: 0.1547, d1.loss_bbox: 0.3177, d2.loss_cls: 0.1228, d2.loss_bbox: 0.2895, d3.loss_cls: 0.1156, d3.loss_bbox: 0.2779, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2677, loss: 2.6559, grad_norm: 27.2524
2025-06-17 19:20:14,830 - mmdet - INFO - Epoch [1][2850/7033]	lr: 2.000e-04, eta: 1 day, 6:29:14, time: 2.666, data_time: 0.050, memory: 17974, loss_cls: 0.1071, loss_bbox: 0.2662, d0.loss_cls: 0.2001, d0.loss_bbox: 0.4138, d1.loss_cls: 0.1459, d1.loss_bbox: 0.3104, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2849, d3.loss_cls: 0.1164, d3.loss_bbox: 0.2759, d4.loss_cls: 0.1081, d4.loss_bbox: 0.2650, loss: 2.6159, grad_norm: 27.7249
2025-06-17 19:22:27,226 - mmdet - INFO - Epoch [1][2900/7033]	lr: 2.000e-04, eta: 1 day, 6:25:18, time: 2.648, data_time: 0.048, memory: 17974, loss_cls: 0.1040, loss_bbox: 0.2626, d0.loss_cls: 0.2020, d0.loss_bbox: 0.4160, d1.loss_cls: 0.1446, d1.loss_bbox: 0.3037, d2.loss_cls: 0.1171, d2.loss_bbox: 0.2803, d3.loss_cls: 0.1109, d3.loss_bbox: 0.2693, d4.loss_cls: 0.1047, d4.loss_bbox: 0.2611, loss: 2.5764, grad_norm: 43.3972
2025-06-17 19:24:39,002 - mmdet - INFO - Epoch [1][2950/7033]	lr: 2.000e-04, eta: 1 day, 6:21:18, time: 2.635, data_time: 0.059, memory: 17974, loss_cls: 0.1100, loss_bbox: 0.2770, d0.loss_cls: 0.2081, d0.loss_bbox: 0.4305, d1.loss_cls: 0.1492, d1.loss_bbox: 0.3240, d2.loss_cls: 0.1294, d2.loss_bbox: 0.2934, d3.loss_cls: 0.1220, d3.loss_bbox: 0.2843, d4.loss_cls: 0.1118, d4.loss_bbox: 0.2747, loss: 2.7144, grad_norm: 35.4209
2025-06-17 19:26:50,355 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 19:26:50,355 - mmdet - INFO - Epoch [1][3000/7033]	lr: 2.000e-04, eta: 1 day, 6:17:17, time: 2.627, data_time: 0.054, memory: 17974, loss_cls: 0.1169, loss_bbox: 0.2798, d0.loss_cls: 0.2229, d0.loss_bbox: 0.4375, d1.loss_cls: 0.1603, d1.loss_bbox: 0.3204, d2.loss_cls: 0.1342, d2.loss_bbox: 0.2928, d3.loss_cls: 0.1229, d3.loss_bbox: 0.2838, d4.loss_cls: 0.1175, d4.loss_bbox: 0.2763, loss: 2.7651, grad_norm: 24.3345
2025-06-17 19:29:03,300 - mmdet - INFO - Epoch [1][3050/7033]	lr: 2.000e-04, eta: 1 day, 6:13:39, time: 2.659, data_time: 0.051, memory: 17974, loss_cls: 0.1059, loss_bbox: 0.2774, d0.loss_cls: 0.2019, d0.loss_bbox: 0.4238, d1.loss_cls: 0.1458, d1.loss_bbox: 0.3153, d2.loss_cls: 0.1219, d2.loss_bbox: 0.2894, d3.loss_cls: 0.1144, d3.loss_bbox: 0.2808, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2729, loss: 2.6571, grad_norm: 33.2685
2025-06-17 19:31:18,052 - mmdet - INFO - Epoch [1][3100/7033]	lr: 2.000e-04, eta: 1 day, 6:10:26, time: 2.695, data_time: 0.053, memory: 17974, loss_cls: 0.1016, loss_bbox: 0.2652, d0.loss_cls: 0.1960, d0.loss_bbox: 0.4141, d1.loss_cls: 0.1377, d1.loss_bbox: 0.3101, d2.loss_cls: 0.1144, d2.loss_bbox: 0.2828, d3.loss_cls: 0.1055, d3.loss_bbox: 0.2730, d4.loss_cls: 0.1013, d4.loss_bbox: 0.2640, loss: 2.5656, grad_norm: 23.9146
2025-06-17 19:33:29,167 - mmdet - INFO - Epoch [1][3150/7033]	lr: 2.000e-04, eta: 1 day, 6:06:31, time: 2.622, data_time: 0.049, memory: 17974, loss_cls: 0.1087, loss_bbox: 0.2804, d0.loss_cls: 0.2049, d0.loss_bbox: 0.4303, d1.loss_cls: 0.1479, d1.loss_bbox: 0.3245, d2.loss_cls: 0.1240, d2.loss_bbox: 0.2994, d3.loss_cls: 0.1172, d3.loss_bbox: 0.2897, d4.loss_cls: 0.1112, d4.loss_bbox: 0.2775, loss: 2.7157, grad_norm: 21.4602
2025-06-17 19:35:41,666 - mmdet - INFO - Epoch [1][3200/7033]	lr: 2.000e-04, eta: 1 day, 6:02:55, time: 2.650, data_time: 0.052, memory: 17974, loss_cls: 0.1107, loss_bbox: 0.2747, d0.loss_cls: 0.2060, d0.loss_bbox: 0.4241, d1.loss_cls: 0.1526, d1.loss_bbox: 0.3385, d2.loss_cls: 0.1333, d2.loss_bbox: 0.3043, d3.loss_cls: 0.1224, d3.loss_bbox: 0.2874, d4.loss_cls: 0.1119, d4.loss_bbox: 0.2750, loss: 2.7409, grad_norm: 520.5369
2025-06-17 19:37:51,534 - mmdet - INFO - Epoch [1][3250/7033]	lr: 2.000e-04, eta: 1 day, 5:58:50, time: 2.597, data_time: 0.052, memory: 17974, loss_cls: 0.1062, loss_bbox: 0.2686, d0.loss_cls: 0.1989, d0.loss_bbox: 0.4187, d1.loss_cls: 0.1448, d1.loss_bbox: 0.3180, d2.loss_cls: 0.1210, d2.loss_bbox: 0.2925, d3.loss_cls: 0.1145, d3.loss_bbox: 0.2796, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2664, loss: 2.6363, grad_norm: 31.3166
2025-06-17 19:40:03,571 - mmdet - INFO - Epoch [1][3300/7033]	lr: 2.000e-04, eta: 1 day, 5:55:15, time: 2.641, data_time: 0.051, memory: 17974, loss_cls: 0.1118, loss_bbox: 0.2743, d0.loss_cls: 0.2007, d0.loss_bbox: 0.4213, d1.loss_cls: 0.1427, d1.loss_bbox: 0.3182, d2.loss_cls: 0.1297, d2.loss_bbox: 0.2940, d3.loss_cls: 0.1240, d3.loss_bbox: 0.2841, d4.loss_cls: 0.1109, d4.loss_bbox: 0.2734, loss: 2.6852, grad_norm: 23.7944
2025-06-17 19:42:13,343 - mmdet - INFO - Epoch [1][3350/7033]	lr: 2.000e-04, eta: 1 day, 5:51:16, time: 2.595, data_time: 0.053, memory: 17974, loss_cls: 0.1011, loss_bbox: 0.2741, d0.loss_cls: 0.2023, d0.loss_bbox: 0.4202, d1.loss_cls: 0.1380, d1.loss_bbox: 0.3176, d2.loss_cls: 0.1191, d2.loss_bbox: 0.2925, d3.loss_cls: 0.1123, d3.loss_bbox: 0.2821, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2713, loss: 2.6330, grad_norm: 28.4392
2025-06-17 19:44:23,117 - mmdet - INFO - Epoch [1][3400/7033]	lr: 2.000e-04, eta: 1 day, 5:47:20, time: 2.595, data_time: 0.048, memory: 17974, loss_cls: 0.1063, loss_bbox: 0.2793, d0.loss_cls: 0.2052, d0.loss_bbox: 0.4124, d1.loss_cls: 0.1429, d1.loss_bbox: 0.3105, d2.loss_cls: 0.1157, d2.loss_bbox: 0.2887, d3.loss_cls: 0.1120, d3.loss_bbox: 0.2817, d4.loss_cls: 0.1071, d4.loss_bbox: 0.2726, loss: 2.6344, grad_norm: 40.3539
2025-06-17 19:46:31,752 - mmdet - INFO - Epoch [1][3450/7033]	lr: 2.000e-04, eta: 1 day, 5:43:14, time: 2.572, data_time: 0.051, memory: 17974, loss_cls: 0.0999, loss_bbox: 0.2656, d0.loss_cls: 0.1949, d0.loss_bbox: 0.4027, d1.loss_cls: 0.1356, d1.loss_bbox: 0.3015, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2785, d3.loss_cls: 0.1114, d3.loss_bbox: 0.2690, d4.loss_cls: 0.1021, d4.loss_bbox: 0.2607, loss: 2.5382, grad_norm: 25.6173
2025-06-17 19:48:37,052 - mmdet - INFO - Epoch [1][3500/7033]	lr: 2.000e-04, eta: 1 day, 5:38:35, time: 2.506, data_time: 0.049, memory: 17974, loss_cls: 0.1062, loss_bbox: 0.2584, d0.loss_cls: 0.1957, d0.loss_bbox: 0.3937, d1.loss_cls: 0.1404, d1.loss_bbox: 0.2955, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2738, d3.loss_cls: 0.1113, d3.loss_bbox: 0.2647, d4.loss_cls: 0.1067, d4.loss_bbox: 0.2547, loss: 2.5208, grad_norm: 31.9020
2025-06-17 19:50:42,225 - mmdet - INFO - Epoch [1][3550/7033]	lr: 2.000e-04, eta: 1 day, 5:33:59, time: 2.503, data_time: 0.048, memory: 17974, loss_cls: 0.1031, loss_bbox: 0.2577, d0.loss_cls: 0.2032, d0.loss_bbox: 0.4028, d1.loss_cls: 0.1391, d1.loss_bbox: 0.2998, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2743, d3.loss_cls: 0.1103, d3.loss_bbox: 0.2651, d4.loss_cls: 0.1038, d4.loss_bbox: 0.2539, loss: 2.5308, grad_norm: 58.8117
2025-06-17 19:52:51,718 - mmdet - INFO - Epoch [1][3600/7033]	lr: 2.000e-04, eta: 1 day, 5:30:13, time: 2.590, data_time: 0.049, memory: 17974, loss_cls: 0.1017, loss_bbox: 0.2637, d0.loss_cls: 0.2029, d0.loss_bbox: 0.4053, d1.loss_cls: 0.1492, d1.loss_bbox: 0.3044, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2819, d3.loss_cls: 0.1105, d3.loss_bbox: 0.2709, d4.loss_cls: 0.1029, d4.loss_bbox: 0.2617, loss: 2.5766, grad_norm: 32.5859
2025-06-17 19:55:01,817 - mmdet - INFO - Epoch [1][3650/7033]	lr: 2.000e-04, eta: 1 day, 5:26:37, time: 2.602, data_time: 0.051, memory: 17974, loss_cls: 0.1031, loss_bbox: 0.2624, d0.loss_cls: 0.2018, d0.loss_bbox: 0.4085, d1.loss_cls: 0.1455, d1.loss_bbox: 0.3050, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2814, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2728, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2612, loss: 2.5780, grad_norm: 45.5177
2025-06-17 19:57:09,254 - mmdet - INFO - Epoch [1][3700/7033]	lr: 2.000e-04, eta: 1 day, 5:22:35, time: 2.549, data_time: 0.052, memory: 17974, loss_cls: 0.1055, loss_bbox: 0.2684, d0.loss_cls: 0.1980, d0.loss_bbox: 0.4186, d1.loss_cls: 0.1426, d1.loss_bbox: 0.3086, d2.loss_cls: 0.1192, d2.loss_bbox: 0.2845, d3.loss_cls: 0.1143, d3.loss_bbox: 0.2778, d4.loss_cls: 0.1073, d4.loss_bbox: 0.2670, loss: 2.6118, grad_norm: 43.5793
2025-06-17 19:59:19,608 - mmdet - INFO - Epoch [1][3750/7033]	lr: 2.000e-04, eta: 1 day, 5:19:05, time: 2.607, data_time: 0.051, memory: 17974, loss_cls: 0.1069, loss_bbox: 0.2634, d0.loss_cls: 0.1932, d0.loss_bbox: 0.3922, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2956, d2.loss_cls: 0.1182, d2.loss_bbox: 0.2784, d3.loss_cls: 0.1130, d3.loss_bbox: 0.2714, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2621, loss: 2.5444, grad_norm: 28.0426
2025-06-17 20:01:31,428 - mmdet - INFO - Epoch [1][3800/7033]	lr: 2.000e-04, eta: 1 day, 5:15:53, time: 2.637, data_time: 0.072, memory: 17974, loss_cls: 0.1105, loss_bbox: 0.2657, d0.loss_cls: 0.1968, d0.loss_bbox: 0.3976, d1.loss_cls: 0.1424, d1.loss_bbox: 0.3005, d2.loss_cls: 0.1247, d2.loss_bbox: 0.2788, d3.loss_cls: 0.1169, d3.loss_bbox: 0.2739, d4.loss_cls: 0.1108, d4.loss_bbox: 0.2645, loss: 2.5831, grad_norm: 36.1019
2025-06-17 20:03:42,418 - mmdet - INFO - Epoch [1][3850/7033]	lr: 2.000e-04, eta: 1 day, 5:12:34, time: 2.620, data_time: 0.050, memory: 17974, loss_cls: 0.1011, loss_bbox: 0.2691, d0.loss_cls: 0.1932, d0.loss_bbox: 0.4036, d1.loss_cls: 0.1360, d1.loss_bbox: 0.3113, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2862, d3.loss_cls: 0.1162, d3.loss_bbox: 0.2788, d4.loss_cls: 0.1016, d4.loss_bbox: 0.2671, loss: 2.5863, grad_norm: 53.9280
2025-06-17 20:05:52,075 - mmdet - INFO - Epoch [1][3900/7033]	lr: 2.000e-04, eta: 1 day, 5:09:04, time: 2.593, data_time: 0.049, memory: 17974, loss_cls: 0.1029, loss_bbox: 0.2604, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3920, d1.loss_cls: 0.1417, d1.loss_bbox: 0.3027, d2.loss_cls: 0.1200, d2.loss_bbox: 0.2791, d3.loss_cls: 0.1123, d3.loss_bbox: 0.2677, d4.loss_cls: 0.1043, d4.loss_bbox: 0.2602, loss: 2.5405, grad_norm: 40.0166
2025-06-17 20:08:04,624 - mmdet - INFO - Epoch [1][3950/7033]	lr: 2.000e-04, eta: 1 day, 5:06:04, time: 2.651, data_time: 0.050, memory: 17974, loss_cls: 0.1012, loss_bbox: 0.2717, d0.loss_cls: 0.1964, d0.loss_bbox: 0.4192, d1.loss_cls: 0.1358, d1.loss_bbox: 0.3174, d2.loss_cls: 0.1155, d2.loss_bbox: 0.2955, d3.loss_cls: 0.1134, d3.loss_bbox: 0.2813, d4.loss_cls: 0.1014, d4.loss_bbox: 0.2687, loss: 2.6176, grad_norm: 59.9339
2025-06-17 20:10:10,219 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 20:10:10,219 - mmdet - INFO - Epoch [1][4000/7033]	lr: 2.000e-04, eta: 1 day, 5:01:58, time: 2.512, data_time: 0.050, memory: 17974, loss_cls: 0.1074, loss_bbox: 0.2587, d0.loss_cls: 0.2032, d0.loss_bbox: 0.3977, d1.loss_cls: 0.1454, d1.loss_bbox: 0.2979, d2.loss_cls: 0.1239, d2.loss_bbox: 0.2759, d3.loss_cls: 0.1162, d3.loss_bbox: 0.2647, d4.loss_cls: 0.1065, d4.loss_bbox: 0.2570, loss: 2.5544, grad_norm: 129.0690
2025-06-17 20:12:14,939 - mmdet - INFO - Epoch [1][4050/7033]	lr: 2.000e-04, eta: 1 day, 4:57:48, time: 2.494, data_time: 0.052, memory: 17974, loss_cls: 0.1066, loss_bbox: 0.2599, d0.loss_cls: 0.2012, d0.loss_bbox: 0.4030, d1.loss_cls: 0.1457, d1.loss_bbox: 0.3017, d2.loss_cls: 0.1242, d2.loss_bbox: 0.2802, d3.loss_cls: 0.1174, d3.loss_bbox: 0.2695, d4.loss_cls: 0.1081, d4.loss_bbox: 0.2589, loss: 2.5765, grad_norm: 27.5179
2025-06-17 20:14:21,037 - mmdet - INFO - Epoch [1][4100/7033]	lr: 2.000e-04, eta: 1 day, 4:53:53, time: 2.522, data_time: 0.050, memory: 17974, loss_cls: 0.1022, loss_bbox: 0.2689, d0.loss_cls: 0.2000, d0.loss_bbox: 0.4037, d1.loss_cls: 0.1381, d1.loss_bbox: 0.3118, d2.loss_cls: 0.1191, d2.loss_bbox: 0.2861, d3.loss_cls: 0.1121, d3.loss_bbox: 0.2756, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2697, loss: 2.5874, grad_norm: 29.0861
2025-06-17 20:16:28,034 - mmdet - INFO - Epoch [1][4150/7033]	lr: 2.000e-04, eta: 1 day, 4:50:09, time: 2.540, data_time: 0.059, memory: 17974, loss_cls: 0.0965, loss_bbox: 0.2617, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3845, d1.loss_cls: 0.1322, d1.loss_bbox: 0.2948, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2704, d3.loss_cls: 0.1054, d3.loss_bbox: 0.2650, d4.loss_cls: 0.0985, d4.loss_bbox: 0.2584, loss: 2.4698, grad_norm: 26.3412
2025-06-17 20:18:33,047 - mmdet - INFO - Epoch [1][4200/7033]	lr: 2.000e-04, eta: 1 day, 4:46:09, time: 2.500, data_time: 0.050, memory: 17974, loss_cls: 0.0973, loss_bbox: 0.2656, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3927, d1.loss_cls: 0.1358, d1.loss_bbox: 0.3040, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2795, d3.loss_cls: 0.1091, d3.loss_bbox: 0.2737, d4.loss_cls: 0.0997, d4.loss_bbox: 0.2635, loss: 2.5286, grad_norm: 26.3204
2025-06-17 20:20:37,265 - mmdet - INFO - Epoch [1][4250/7033]	lr: 2.000e-04, eta: 1 day, 4:42:05, time: 2.484, data_time: 0.052, memory: 17974, loss_cls: 0.0965, loss_bbox: 0.2530, d0.loss_cls: 0.1911, d0.loss_bbox: 0.3785, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2865, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2654, d3.loss_cls: 0.1031, d3.loss_bbox: 0.2575, d4.loss_cls: 0.0981, d4.loss_bbox: 0.2499, loss: 2.4204, grad_norm: 46.4498
2025-06-17 20:22:41,777 - mmdet - INFO - Epoch [1][4300/7033]	lr: 2.000e-04, eta: 1 day, 4:38:07, time: 2.490, data_time: 0.054, memory: 17974, loss_cls: 0.1030, loss_bbox: 0.2679, d0.loss_cls: 0.1972, d0.loss_bbox: 0.4111, d1.loss_cls: 0.1386, d1.loss_bbox: 0.3149, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2874, d3.loss_cls: 0.1111, d3.loss_bbox: 0.2789, d4.loss_cls: 0.1037, d4.loss_bbox: 0.2672, loss: 2.5988, grad_norm: 33.4506
2025-06-17 20:24:46,091 - mmdet - INFO - Epoch [1][4350/7033]	lr: 2.000e-04, eta: 1 day, 4:34:09, time: 2.486, data_time: 0.050, memory: 17974, loss_cls: 0.0944, loss_bbox: 0.2543, d0.loss_cls: 0.1859, d0.loss_bbox: 0.3906, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2920, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2702, d3.loss_cls: 0.0993, d3.loss_bbox: 0.2627, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2543, loss: 2.4392, grad_norm: 35.0559
2025-06-17 20:26:53,098 - mmdet - INFO - Epoch [1][4400/7033]	lr: 2.000e-04, eta: 1 day, 4:30:37, time: 2.540, data_time: 0.050, memory: 17974, loss_cls: 0.0999, loss_bbox: 0.2571, d0.loss_cls: 0.1990, d0.loss_bbox: 0.4087, d1.loss_cls: 0.1423, d1.loss_bbox: 0.2966, d2.loss_cls: 0.1195, d2.loss_bbox: 0.2721, d3.loss_cls: 0.1080, d3.loss_bbox: 0.2620, d4.loss_cls: 0.1028, d4.loss_bbox: 0.2545, loss: 2.5225, grad_norm: 39.7083
2025-06-17 20:28:55,971 - mmdet - INFO - Epoch [1][4450/7033]	lr: 2.000e-04, eta: 1 day, 4:26:32, time: 2.457, data_time: 0.053, memory: 17974, loss_cls: 0.1025, loss_bbox: 0.2562, d0.loss_cls: 0.1883, d0.loss_bbox: 0.3982, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2984, d2.loss_cls: 0.1192, d2.loss_bbox: 0.2746, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2658, d4.loss_cls: 0.1033, d4.loss_bbox: 0.2560, loss: 2.5114, grad_norm: 28.8637
2025-06-17 20:30:58,617 - mmdet - INFO - Epoch [1][4500/7033]	lr: 2.000e-04, eta: 1 day, 4:22:27, time: 2.453, data_time: 0.051, memory: 17974, loss_cls: 0.0931, loss_bbox: 0.2549, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3909, d1.loss_cls: 0.1357, d1.loss_bbox: 0.2927, d2.loss_cls: 0.1120, d2.loss_bbox: 0.2678, d3.loss_cls: 0.1007, d3.loss_bbox: 0.2587, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2551, loss: 2.4452, grad_norm: 28.2309
2025-06-17 20:33:01,504 - mmdet - INFO - Epoch [1][4550/7033]	lr: 2.000e-04, eta: 1 day, 4:18:28, time: 2.458, data_time: 0.053, memory: 17974, loss_cls: 0.0999, loss_bbox: 0.2642, d0.loss_cls: 0.2029, d0.loss_bbox: 0.4019, d1.loss_cls: 0.1364, d1.loss_bbox: 0.2994, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2811, d3.loss_cls: 0.1071, d3.loss_bbox: 0.2723, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2641, loss: 2.5449, grad_norm: 297.0239
2025-06-17 20:35:04,379 - mmdet - INFO - Epoch [1][4600/7033]	lr: 2.000e-04, eta: 1 day, 4:14:30, time: 2.457, data_time: 0.055, memory: 17974, loss_cls: 0.1036, loss_bbox: 0.2713, d0.loss_cls: 0.1995, d0.loss_bbox: 0.4090, d1.loss_cls: 0.1439, d1.loss_bbox: 0.3064, d2.loss_cls: 0.1208, d2.loss_bbox: 0.2882, d3.loss_cls: 0.1155, d3.loss_bbox: 0.2787, d4.loss_cls: 0.1055, d4.loss_bbox: 0.2706, loss: 2.6131, grad_norm: 32.0087
2025-06-17 20:37:06,649 - mmdet - INFO - Epoch [1][4650/7033]	lr: 2.000e-04, eta: 1 day, 4:10:30, time: 2.445, data_time: 0.053, memory: 17974, loss_cls: 0.1045, loss_bbox: 0.2636, d0.loss_cls: 0.2008, d0.loss_bbox: 0.4084, d1.loss_cls: 0.1391, d1.loss_bbox: 0.3019, d2.loss_cls: 0.1197, d2.loss_bbox: 0.2792, d3.loss_cls: 0.1124, d3.loss_bbox: 0.2693, d4.loss_cls: 0.1043, d4.loss_bbox: 0.2600, loss: 2.5631, grad_norm: 39.5604
2025-06-17 20:39:11,461 - mmdet - INFO - Epoch [1][4700/7033]	lr: 2.000e-04, eta: 1 day, 4:06:54, time: 2.497, data_time: 0.052, memory: 17974, loss_cls: 0.0928, loss_bbox: 0.2689, d0.loss_cls: 0.1940, d0.loss_bbox: 0.4087, d1.loss_cls: 0.1303, d1.loss_bbox: 0.3055, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2847, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2738, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2664, loss: 2.5304, grad_norm: 31.9016
2025-06-17 20:41:12,380 - mmdet - INFO - Epoch [1][4750/7033]	lr: 2.000e-04, eta: 1 day, 4:02:48, time: 2.418, data_time: 0.054, memory: 17974, loss_cls: 0.0930, loss_bbox: 0.2473, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1293, d1.loss_bbox: 0.2836, d2.loss_cls: 0.1102, d2.loss_bbox: 0.2624, d3.loss_cls: 0.1029, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2461, loss: 2.3909, grad_norm: 201.4074
2025-06-17 20:43:14,639 - mmdet - INFO - Epoch [1][4800/7033]	lr: 2.000e-04, eta: 1 day, 3:58:55, time: 2.445, data_time: 0.053, memory: 17974, loss_cls: 0.1019, loss_bbox: 0.2579, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3920, d1.loss_cls: 0.1351, d1.loss_bbox: 0.2936, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2745, d3.loss_cls: 0.1065, d3.loss_bbox: 0.2650, d4.loss_cls: 0.1029, d4.loss_bbox: 0.2574, loss: 2.4943, grad_norm: 133.8257
2025-06-17 20:45:17,160 - mmdet - INFO - Epoch [1][4850/7033]	lr: 2.000e-04, eta: 1 day, 3:55:07, time: 2.450, data_time: 0.054, memory: 17974, loss_cls: 0.1086, loss_bbox: 0.2627, d0.loss_cls: 0.2030, d0.loss_bbox: 0.4009, d1.loss_cls: 0.1462, d1.loss_bbox: 0.3106, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2841, d3.loss_cls: 0.1166, d3.loss_bbox: 0.2749, d4.loss_cls: 0.1083, d4.loss_bbox: 0.2643, loss: 2.6038, grad_norm: 25.4978
2025-06-17 20:47:17,453 - mmdet - INFO - Epoch [1][4900/7033]	lr: 2.000e-04, eta: 1 day, 3:51:04, time: 2.406, data_time: 0.054, memory: 17974, loss_cls: 0.1102, loss_bbox: 0.2706, d0.loss_cls: 0.1984, d0.loss_bbox: 0.4089, d1.loss_cls: 0.1434, d1.loss_bbox: 0.3292, d2.loss_cls: 0.1248, d2.loss_bbox: 0.2948, d3.loss_cls: 0.1164, d3.loss_bbox: 0.2829, d4.loss_cls: 0.1095, d4.loss_bbox: 0.2704, loss: 2.6595, grad_norm: 23.0667
2025-06-17 20:49:19,705 - mmdet - INFO - Epoch [1][4950/7033]	lr: 2.000e-04, eta: 1 day, 3:47:18, time: 2.445, data_time: 0.053, memory: 17974, loss_cls: 0.1037, loss_bbox: 0.2607, d0.loss_cls: 0.2019, d0.loss_bbox: 0.4121, d1.loss_cls: 0.1394, d1.loss_bbox: 0.3140, d2.loss_cls: 0.1201, d2.loss_bbox: 0.2867, d3.loss_cls: 0.1146, d3.loss_bbox: 0.2741, d4.loss_cls: 0.1049, d4.loss_bbox: 0.2614, loss: 2.5936, grad_norm: 35.1779
2025-06-17 20:51:21,255 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 20:51:21,256 - mmdet - INFO - Epoch [1][5000/7033]	lr: 2.000e-04, eta: 1 day, 3:43:29, time: 2.431, data_time: 0.053, memory: 17974, loss_cls: 0.0900, loss_bbox: 0.2513, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3729, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2876, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2658, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2592, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2517, loss: 2.3909, grad_norm: 22.1825
2025-06-17 20:53:20,199 - mmdet - INFO - Epoch [1][5050/7033]	lr: 2.000e-04, eta: 1 day, 3:39:23, time: 2.379, data_time: 0.051, memory: 17974, loss_cls: 0.1029, loss_bbox: 0.2486, d0.loss_cls: 0.1913, d0.loss_bbox: 0.3823, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2868, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2683, d3.loss_cls: 0.1116, d3.loss_bbox: 0.2576, d4.loss_cls: 0.1036, d4.loss_bbox: 0.2487, loss: 2.4525, grad_norm: 25.9586
2025-06-17 20:55:18,460 - mmdet - INFO - Epoch [1][5100/7033]	lr: 2.000e-04, eta: 1 day, 3:35:14, time: 2.365, data_time: 0.054, memory: 17974, loss_cls: 0.0973, loss_bbox: 0.2636, d0.loss_cls: 0.1936, d0.loss_bbox: 0.4031, d1.loss_cls: 0.1305, d1.loss_bbox: 0.3043, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2860, d3.loss_cls: 0.1031, d3.loss_bbox: 0.2735, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2660, loss: 2.5306, grad_norm: 46.0563
2025-06-17 20:57:18,354 - mmdet - INFO - Epoch [1][5150/7033]	lr: 2.000e-04, eta: 1 day, 3:31:20, time: 2.398, data_time: 0.053, memory: 17974, loss_cls: 0.1057, loss_bbox: 0.2586, d0.loss_cls: 0.1950, d0.loss_bbox: 0.3965, d1.loss_cls: 0.1385, d1.loss_bbox: 0.2983, d2.loss_cls: 0.1190, d2.loss_bbox: 0.2752, d3.loss_cls: 0.1116, d3.loss_bbox: 0.2677, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2591, loss: 2.5323, grad_norm: 28.4067
2025-06-17 20:59:16,911 - mmdet - INFO - Epoch [1][5200/7033]	lr: 2.000e-04, eta: 1 day, 3:27:19, time: 2.371, data_time: 0.081, memory: 17974, loss_cls: 0.0967, loss_bbox: 0.2577, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3850, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2976, d2.loss_cls: 0.1090, d2.loss_bbox: 0.2786, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2692, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2579, loss: 2.4742, grad_norm: 24.7235
2025-06-17 21:01:14,852 - mmdet - INFO - Epoch [1][5250/7033]	lr: 2.000e-04, eta: 1 day, 3:23:15, time: 2.359, data_time: 0.050, memory: 17974, loss_cls: 0.0933, loss_bbox: 0.2504, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3787, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2867, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2701, d3.loss_cls: 0.0995, d3.loss_bbox: 0.2620, d4.loss_cls: 0.0945, d4.loss_bbox: 0.2510, loss: 2.4030, grad_norm: 31.2669
2025-06-17 21:03:13,875 - mmdet - INFO - Epoch [1][5300/7033]	lr: 2.000e-04, eta: 1 day, 3:19:21, time: 2.380, data_time: 0.051, memory: 17974, loss_cls: 0.0891, loss_bbox: 0.2441, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3737, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2790, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2638, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2545, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2429, loss: 2.3385, grad_norm: 31.6606
2025-06-17 21:05:12,045 - mmdet - INFO - Epoch [1][5350/7033]	lr: 2.000e-04, eta: 1 day, 3:15:24, time: 2.363, data_time: 0.051, memory: 17974, loss_cls: 0.1076, loss_bbox: 0.2652, d0.loss_cls: 0.2017, d0.loss_bbox: 0.4056, d1.loss_cls: 0.1422, d1.loss_bbox: 0.3129, d2.loss_cls: 0.1219, d2.loss_bbox: 0.2877, d3.loss_cls: 0.1171, d3.loss_bbox: 0.2775, d4.loss_cls: 0.1088, d4.loss_bbox: 0.2665, loss: 2.6149, grad_norm: 27.3918
2025-06-17 21:07:09,568 - mmdet - INFO - Epoch [1][5400/7033]	lr: 2.000e-04, eta: 1 day, 3:11:24, time: 2.350, data_time: 0.051, memory: 17974, loss_cls: 0.1042, loss_bbox: 0.2575, d0.loss_cls: 0.1971, d0.loss_bbox: 0.4044, d1.loss_cls: 0.1393, d1.loss_bbox: 0.3052, d2.loss_cls: 0.1181, d2.loss_bbox: 0.2819, d3.loss_cls: 0.1119, d3.loss_bbox: 0.2701, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2580, loss: 2.5547, grad_norm: 47.8386
2025-06-17 21:09:07,688 - mmdet - INFO - Epoch [1][5450/7033]	lr: 2.000e-04, eta: 1 day, 3:07:31, time: 2.362, data_time: 0.052, memory: 17974, loss_cls: 0.0978, loss_bbox: 0.2555, d0.loss_cls: 0.1933, d0.loss_bbox: 0.4051, d1.loss_cls: 0.1370, d1.loss_bbox: 0.3048, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2790, d3.loss_cls: 0.1055, d3.loss_bbox: 0.2721, d4.loss_cls: 0.1000, d4.loss_bbox: 0.2566, loss: 2.5237, grad_norm: 47.5158
2025-06-17 21:11:05,995 - mmdet - INFO - Epoch [1][5500/7033]	lr: 2.000e-04, eta: 1 day, 3:03:41, time: 2.366, data_time: 0.050, memory: 17974, loss_cls: 0.1009, loss_bbox: 0.2463, d0.loss_cls: 0.1964, d0.loss_bbox: 0.3870, d1.loss_cls: 0.1426, d1.loss_bbox: 0.2904, d2.loss_cls: 0.1153, d2.loss_bbox: 0.2714, d3.loss_cls: 0.1085, d3.loss_bbox: 0.2591, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2452, loss: 2.4676, grad_norm: 85.3590
2025-06-17 21:13:02,685 - mmdet - INFO - Epoch [1][5550/7033]	lr: 2.000e-04, eta: 1 day, 2:59:42, time: 2.334, data_time: 0.050, memory: 17974, loss_cls: 0.0942, loss_bbox: 0.2486, d0.loss_cls: 0.2012, d0.loss_bbox: 0.4029, d1.loss_cls: 0.1348, d1.loss_bbox: 0.2967, d2.loss_cls: 0.1092, d2.loss_bbox: 0.2691, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2574, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2488, loss: 2.4596, grad_norm: 51.4220
2025-06-17 21:15:02,561 - mmdet - INFO - Epoch [1][5600/7033]	lr: 2.000e-04, eta: 1 day, 2:56:06, time: 2.398, data_time: 0.052, memory: 17974, loss_cls: 0.0985, loss_bbox: 0.2569, d0.loss_cls: 0.1926, d0.loss_bbox: 0.4018, d1.loss_cls: 0.1304, d1.loss_bbox: 0.2995, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2768, d3.loss_cls: 0.1060, d3.loss_bbox: 0.2665, d4.loss_cls: 0.0992, d4.loss_bbox: 0.2547, loss: 2.4952, grad_norm: 38.6102
2025-06-17 21:16:59,098 - mmdet - INFO - Epoch [1][5650/7033]	lr: 2.000e-04, eta: 1 day, 2:52:11, time: 2.331, data_time: 0.055, memory: 17974, loss_cls: 0.1007, loss_bbox: 0.2589, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3967, d1.loss_cls: 0.1402, d1.loss_bbox: 0.3020, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2799, d3.loss_cls: 0.1079, d3.loss_bbox: 0.2721, d4.loss_cls: 0.1029, d4.loss_bbox: 0.2586, loss: 2.5337, grad_norm: 26.0392
2025-06-17 21:18:55,692 - mmdet - INFO - Epoch [1][5700/7033]	lr: 2.000e-04, eta: 1 day, 2:48:18, time: 2.332, data_time: 0.058, memory: 17974, loss_cls: 0.0912, loss_bbox: 0.2481, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3783, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2860, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2660, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2571, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2494, loss: 2.3861, grad_norm: 38.7875
2025-06-17 21:20:52,801 - mmdet - INFO - Epoch [1][5750/7033]	lr: 2.000e-04, eta: 1 day, 2:44:30, time: 2.342, data_time: 0.053, memory: 17974, loss_cls: 0.0909, loss_bbox: 0.2409, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3831, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2834, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2625, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2530, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2416, loss: 2.3420, grad_norm: 35.4269
2025-06-17 21:22:51,827 - mmdet - INFO - Epoch [1][5800/7033]	lr: 2.000e-04, eta: 1 day, 2:40:56, time: 2.380, data_time: 0.056, memory: 17974, loss_cls: 0.0876, loss_bbox: 0.2402, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3902, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2906, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2674, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2533, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2411, loss: 2.3644, grad_norm: 215.7596
2025-06-17 21:24:48,391 - mmdet - INFO - Epoch [1][5850/7033]	lr: 2.000e-04, eta: 1 day, 2:37:08, time: 2.331, data_time: 0.055, memory: 17974, loss_cls: 0.0933, loss_bbox: 0.2510, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3878, d1.loss_cls: 0.1260, d1.loss_bbox: 0.2946, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2688, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2617, d4.loss_cls: 0.0966, d4.loss_bbox: 0.2496, loss: 2.4354, grad_norm: 32.5934
2025-06-17 21:26:47,540 - mmdet - INFO - Epoch [1][5900/7033]	lr: 2.000e-04, eta: 1 day, 2:33:39, time: 2.383, data_time: 0.056, memory: 17974, loss_cls: 0.0913, loss_bbox: 0.2482, d0.loss_cls: 0.1932, d0.loss_bbox: 0.3917, d1.loss_cls: 0.1260, d1.loss_bbox: 0.2928, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2704, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2608, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2494, loss: 2.4228, grad_norm: 24.3250
2025-06-17 21:28:43,953 - mmdet - INFO - Epoch [1][5950/7033]	lr: 2.000e-04, eta: 1 day, 2:29:54, time: 2.328, data_time: 0.054, memory: 17974, loss_cls: 0.0982, loss_bbox: 0.2470, d0.loss_cls: 0.1857, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2894, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2715, d3.loss_cls: 0.1024, d3.loss_bbox: 0.2596, d4.loss_cls: 0.0990, d4.loss_bbox: 0.2486, loss: 2.4201, grad_norm: 34.2622
2025-06-17 21:30:41,296 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 21:30:41,296 - mmdet - INFO - Epoch [1][6000/7033]	lr: 2.000e-04, eta: 1 day, 2:26:16, time: 2.347, data_time: 0.056, memory: 18042, loss_cls: 0.0963, loss_bbox: 0.2561, d0.loss_cls: 0.1932, d0.loss_bbox: 0.3922, d1.loss_cls: 0.1369, d1.loss_bbox: 0.2961, d2.loss_cls: 0.1128, d2.loss_bbox: 0.2729, d3.loss_cls: 0.1063, d3.loss_bbox: 0.2721, d4.loss_cls: 0.0987, d4.loss_bbox: 0.2563, loss: 2.4899, grad_norm: 52.4919
2025-06-17 21:32:37,189 - mmdet - INFO - Epoch [1][6050/7033]	lr: 2.000e-04, eta: 1 day, 2:22:32, time: 2.318, data_time: 0.057, memory: 18042, loss_cls: 0.0931, loss_bbox: 0.2402, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3764, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2818, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2589, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2506, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2406, loss: 2.3522, grad_norm: 31.4958
2025-06-17 21:34:34,007 - mmdet - INFO - Epoch [1][6100/7033]	lr: 2.000e-04, eta: 1 day, 2:18:54, time: 2.336, data_time: 0.063, memory: 18042, loss_cls: 0.1096, loss_bbox: 0.2447, d0.loss_cls: 0.1939, d0.loss_bbox: 0.3818, d1.loss_cls: 0.1384, d1.loss_bbox: 0.2899, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2682, d3.loss_cls: 0.1153, d3.loss_bbox: 0.2581, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2451, loss: 2.4757, grad_norm: 70.4652
2025-06-17 21:36:32,069 - mmdet - INFO - Epoch [1][6150/7033]	lr: 2.000e-04, eta: 1 day, 2:15:26, time: 2.361, data_time: 0.056, memory: 18042, loss_cls: 0.0964, loss_bbox: 0.2522, d0.loss_cls: 0.1840, d0.loss_bbox: 0.3830, d1.loss_cls: 0.1333, d1.loss_bbox: 0.2971, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2766, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2676, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2533, loss: 2.4554, grad_norm: 39.5950
2025-06-17 21:38:31,321 - mmdet - INFO - Epoch [1][6200/7033]	lr: 2.000e-04, eta: 1 day, 2:12:06, time: 2.385, data_time: 0.056, memory: 18042, loss_cls: 0.0871, loss_bbox: 0.2386, d0.loss_cls: 0.1859, d0.loss_bbox: 0.3687, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2785, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0941, d3.loss_bbox: 0.2469, d4.loss_cls: 0.0870, d4.loss_bbox: 0.2395, loss: 2.3051, grad_norm: 56.4767
2025-06-17 21:40:27,346 - mmdet - INFO - Epoch [1][6250/7033]	lr: 2.000e-04, eta: 1 day, 2:08:29, time: 2.320, data_time: 0.057, memory: 18042, loss_cls: 0.0942, loss_bbox: 0.2464, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3951, d1.loss_cls: 0.1307, d1.loss_bbox: 0.2921, d2.loss_cls: 0.1101, d2.loss_bbox: 0.2659, d3.loss_cls: 0.1043, d3.loss_bbox: 0.2545, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2453, loss: 2.4227, grad_norm: 45.3279
2025-06-17 21:42:25,554 - mmdet - INFO - Epoch [1][6300/7033]	lr: 2.000e-04, eta: 1 day, 2:05:06, time: 2.364, data_time: 0.096, memory: 18042, loss_cls: 0.0943, loss_bbox: 0.2719, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3978, d1.loss_cls: 0.1373, d1.loss_bbox: 0.3143, d2.loss_cls: 0.1146, d2.loss_bbox: 0.2873, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2791, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2679, loss: 2.5465, grad_norm: 36.9071
2025-06-17 21:44:22,740 - mmdet - INFO - Epoch [1][6350/7033]	lr: 2.000e-04, eta: 1 day, 2:01:38, time: 2.344, data_time: 0.055, memory: 18042, loss_cls: 0.1056, loss_bbox: 0.2590, d0.loss_cls: 0.2080, d0.loss_bbox: 0.3976, d1.loss_cls: 0.1554, d1.loss_bbox: 0.3068, d2.loss_cls: 0.1292, d2.loss_bbox: 0.2771, d3.loss_cls: 0.1183, d3.loss_bbox: 0.2662, d4.loss_cls: 0.1088, d4.loss_bbox: 0.2550, loss: 2.5869, grad_norm: 33.7976
2025-06-17 21:46:18,442 - mmdet - INFO - Epoch [1][6400/7033]	lr: 2.000e-04, eta: 1 day, 1:58:03, time: 2.314, data_time: 0.052, memory: 18042, loss_cls: 0.1066, loss_bbox: 0.2535, d0.loss_cls: 0.2094, d0.loss_bbox: 0.4082, d1.loss_cls: 0.1542, d1.loss_bbox: 0.3112, d2.loss_cls: 0.1245, d2.loss_bbox: 0.2771, d3.loss_cls: 0.1148, d3.loss_bbox: 0.2679, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2539, loss: 2.5899, grad_norm: 38.3644
2025-06-17 21:48:13,886 - mmdet - INFO - Epoch [1][6450/7033]	lr: 2.000e-04, eta: 1 day, 1:54:29, time: 2.309, data_time: 0.052, memory: 18042, loss_cls: 0.0952, loss_bbox: 0.2452, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3907, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2957, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2692, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2586, d4.loss_cls: 0.0950, d4.loss_bbox: 0.2464, loss: 2.4309, grad_norm: 55.2151
2025-06-17 21:50:11,919 - mmdet - INFO - Epoch [1][6500/7033]	lr: 2.000e-04, eta: 1 day, 1:51:10, time: 2.361, data_time: 0.056, memory: 18042, loss_cls: 0.1042, loss_bbox: 0.2597, d0.loss_cls: 0.2063, d0.loss_bbox: 0.4062, d1.loss_cls: 0.1476, d1.loss_bbox: 0.3069, d2.loss_cls: 0.1222, d2.loss_bbox: 0.2830, d3.loss_cls: 0.1100, d3.loss_bbox: 0.2709, d4.loss_cls: 0.1033, d4.loss_bbox: 0.2584, loss: 2.5786, grad_norm: 38.9442
2025-06-17 21:52:06,795 - mmdet - INFO - Epoch [1][6550/7033]	lr: 2.000e-04, eta: 1 day, 1:47:36, time: 2.298, data_time: 0.056, memory: 18042, loss_cls: 0.1016, loss_bbox: 0.2493, d0.loss_cls: 0.1980, d0.loss_bbox: 0.3937, d1.loss_cls: 0.1412, d1.loss_bbox: 0.2975, d2.loss_cls: 0.1182, d2.loss_bbox: 0.2757, d3.loss_cls: 0.1103, d3.loss_bbox: 0.2621, d4.loss_cls: 0.1013, d4.loss_bbox: 0.2533, loss: 2.5019, grad_norm: 32.7415
2025-06-17 21:54:03,349 - mmdet - INFO - Epoch [1][6600/7033]	lr: 2.000e-04, eta: 1 day, 1:44:12, time: 2.331, data_time: 0.055, memory: 18042, loss_cls: 0.0919, loss_bbox: 0.2388, d0.loss_cls: 0.1861, d0.loss_bbox: 0.3825, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2892, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2624, d3.loss_cls: 0.0999, d3.loss_bbox: 0.2513, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2390, loss: 2.3706, grad_norm: 29.2544
2025-06-17 21:55:58,581 - mmdet - INFO - Epoch [1][6650/7033]	lr: 2.000e-04, eta: 1 day, 1:40:42, time: 2.305, data_time: 0.056, memory: 18042, loss_cls: 0.0943, loss_bbox: 0.2472, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3847, d1.loss_cls: 0.1306, d1.loss_bbox: 0.2920, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2689, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2578, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2482, loss: 2.4229, grad_norm: 25.7396
2025-06-17 21:57:54,489 - mmdet - INFO - Epoch [1][6700/7033]	lr: 2.000e-04, eta: 1 day, 1:37:17, time: 2.318, data_time: 0.056, memory: 18042, loss_cls: 0.0954, loss_bbox: 0.2370, d0.loss_cls: 0.1991, d0.loss_bbox: 0.3931, d1.loss_cls: 0.1414, d1.loss_bbox: 0.2863, d2.loss_cls: 0.1122, d2.loss_bbox: 0.2618, d3.loss_cls: 0.1023, d3.loss_bbox: 0.2502, d4.loss_cls: 0.0970, d4.loss_bbox: 0.2372, loss: 2.4132, grad_norm: 35.8533
2025-06-17 21:59:50,334 - mmdet - INFO - Epoch [1][6750/7033]	lr: 2.000e-04, eta: 1 day, 1:33:53, time: 2.317, data_time: 0.052, memory: 18042, loss_cls: 0.0946, loss_bbox: 0.2438, d0.loss_cls: 0.2015, d0.loss_bbox: 0.3955, d1.loss_cls: 0.1333, d1.loss_bbox: 0.2878, d2.loss_cls: 0.1115, d2.loss_bbox: 0.2637, d3.loss_cls: 0.1029, d3.loss_bbox: 0.2538, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2452, loss: 2.4276, grad_norm: 28.3701
2025-06-17 22:01:47,598 - mmdet - INFO - Epoch [1][6800/7033]	lr: 2.000e-04, eta: 1 day, 1:30:38, time: 2.345, data_time: 0.053, memory: 18042, loss_cls: 0.0995, loss_bbox: 0.2462, d0.loss_cls: 0.1977, d0.loss_bbox: 0.3797, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2862, d2.loss_cls: 0.1129, d2.loss_bbox: 0.2630, d3.loss_cls: 0.1058, d3.loss_bbox: 0.2547, d4.loss_cls: 0.1006, d4.loss_bbox: 0.2474, loss: 2.4251, grad_norm: 39.0189
2025-06-17 22:03:42,052 - mmdet - INFO - Epoch [1][6850/7033]	lr: 2.000e-04, eta: 1 day, 1:27:10, time: 2.289, data_time: 0.057, memory: 18042, loss_cls: 0.0866, loss_bbox: 0.2438, d0.loss_cls: 0.1851, d0.loss_bbox: 0.3839, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2863, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2637, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2539, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2444, loss: 2.3558, grad_norm: 30.4953
2025-06-17 22:05:37,893 - mmdet - INFO - Epoch [1][6900/7033]	lr: 2.000e-04, eta: 1 day, 1:23:49, time: 2.316, data_time: 0.051, memory: 18042, loss_cls: 0.0913, loss_bbox: 0.2452, d0.loss_cls: 0.2027, d0.loss_bbox: 0.4046, d1.loss_cls: 0.1487, d1.loss_bbox: 0.3001, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2702, d3.loss_cls: 0.1016, d3.loss_bbox: 0.2575, d4.loss_cls: 0.0942, d4.loss_bbox: 0.2475, loss: 2.4777, grad_norm: 55.8876
2025-06-17 22:07:33,289 - mmdet - INFO - Epoch [1][6950/7033]	lr: 2.000e-04, eta: 1 day, 1:20:28, time: 2.308, data_time: 0.051, memory: 18042, loss_cls: 0.0888, loss_bbox: 0.2348, d0.loss_cls: 0.1947, d0.loss_bbox: 0.4047, d1.loss_cls: 0.1394, d1.loss_bbox: 0.3008, d2.loss_cls: 0.1129, d2.loss_bbox: 0.2655, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2484, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2362, loss: 2.4210, grad_norm: 41.7088
2025-06-17 22:09:30,501 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 22:09:30,501 - mmdet - INFO - Epoch [1][7000/7033]	lr: 2.000e-04, eta: 1 day, 1:17:18, time: 2.344, data_time: 0.053, memory: 18042, loss_cls: 0.0966, loss_bbox: 0.2418, d0.loss_cls: 0.1911, d0.loss_bbox: 0.3959, d1.loss_cls: 0.1334, d1.loss_bbox: 0.2981, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2707, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2543, d4.loss_cls: 0.0989, d4.loss_bbox: 0.2424, loss: 2.4376, grad_norm: 33.2090
2025-06-17 22:11:43,671 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-17 22:58:26,026 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 22:58:26,027 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7827, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8783, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9069, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9206, pts_bbox_NuScenes/car_trans_err: 0.1830, pts_bbox_NuScenes/car_scale_err: 0.1624, pts_bbox_NuScenes/car_orient_err: 0.0607, pts_bbox_NuScenes/car_vel_err: 0.2756, pts_bbox_NuScenes/car_attr_err: 0.1879, pts_bbox_NuScenes/mATE: 0.3021, pts_bbox_NuScenes/mASE: 0.2688, pts_bbox_NuScenes/mAOE: 0.2777, pts_bbox_NuScenes/mAVE: 0.3016, pts_bbox_NuScenes/mAAE: 0.1796, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4058, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.5990, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7005, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7410, pts_bbox_NuScenes/truck_trans_err: 0.3459, pts_bbox_NuScenes/truck_scale_err: 0.2046, pts_bbox_NuScenes/truck_orient_err: 0.0867, pts_bbox_NuScenes/truck_vel_err: 0.2499, pts_bbox_NuScenes/truck_attr_err: 0.2192, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0679, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2133, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4097, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4891, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6378, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4501, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7416, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1100, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2825, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4520, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7140, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8983, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9293, pts_bbox_NuScenes/bus_trans_err: 0.3924, pts_bbox_NuScenes/bus_scale_err: 0.2040, pts_bbox_NuScenes/bus_orient_err: 0.0491, pts_bbox_NuScenes/bus_vel_err: 0.5654, pts_bbox_NuScenes/bus_attr_err: 0.2011, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1629, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4248, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5960, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6807, pts_bbox_NuScenes/trailer_trans_err: 0.4917, pts_bbox_NuScenes/trailer_scale_err: 0.2269, pts_bbox_NuScenes/trailer_orient_err: 0.5880, pts_bbox_NuScenes/trailer_vel_err: 0.2542, pts_bbox_NuScenes/trailer_attr_err: 0.1794, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5490, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6505, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7077, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7256, pts_bbox_NuScenes/barrier_trans_err: 0.2455, pts_bbox_NuScenes/barrier_scale_err: 0.2948, pts_bbox_NuScenes/barrier_orient_err: 0.0746, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6128, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7469, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7773, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7856, pts_bbox_NuScenes/motorcycle_trans_err: 0.2202, pts_bbox_NuScenes/motorcycle_scale_err: 0.2460, pts_bbox_NuScenes/motorcycle_orient_err: 0.2168, pts_bbox_NuScenes/motorcycle_vel_err: 0.5096, pts_bbox_NuScenes/motorcycle_attr_err: 0.2392, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5165, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5705, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5808, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5920, pts_bbox_NuScenes/bicycle_trans_err: 0.1878, pts_bbox_NuScenes/bicycle_scale_err: 0.2666, pts_bbox_NuScenes/bicycle_orient_err: 0.3299, pts_bbox_NuScenes/bicycle_vel_err: 0.2093, pts_bbox_NuScenes/bicycle_attr_err: 0.0066, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7886, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8477, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8761, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8900, pts_bbox_NuScenes/pedestrian_trans_err: 0.1685, pts_bbox_NuScenes/pedestrian_scale_err: 0.2983, pts_bbox_NuScenes/pedestrian_orient_err: 0.3518, pts_bbox_NuScenes/pedestrian_vel_err: 0.2382, pts_bbox_NuScenes/pedestrian_attr_err: 0.1211, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6973, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7383, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7696, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.7971, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1481, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3350, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6944, pts_bbox_NuScenes/mAP: 0.6548
2025-06-17 23:00:26,939 - mmdet - INFO - Epoch [2][50/7033]	lr: 1.866e-04, eta: 1 day, 1:05:39, time: 2.341, data_time: 0.264, memory: 20362, loss_cls: 0.0949, loss_bbox: 0.2417, d0.loss_cls: 0.1942, d0.loss_bbox: 0.3855, d1.loss_cls: 0.1386, d1.loss_bbox: 0.2900, d2.loss_cls: 0.1157, d2.loss_bbox: 0.2677, d3.loss_cls: 0.1057, d3.loss_bbox: 0.2544, d4.loss_cls: 0.0978, d4.loss_bbox: 0.2427, loss: 2.4290, grad_norm: 56.7907
2025-06-17 23:02:14,388 - mmdet - INFO - Epoch [2][100/7033]	lr: 1.866e-04, eta: 1 day, 1:01:46, time: 2.149, data_time: 0.051, memory: 20362, loss_cls: 0.1031, loss_bbox: 0.2510, d0.loss_cls: 0.2053, d0.loss_bbox: 0.3997, d1.loss_cls: 0.1396, d1.loss_bbox: 0.3004, d2.loss_cls: 0.1164, d2.loss_bbox: 0.2748, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2617, d4.loss_cls: 0.1037, d4.loss_bbox: 0.2534, loss: 2.5181, grad_norm: 26.2776
2025-06-17 23:04:05,847 - mmdet - INFO - Epoch [2][150/7033]	lr: 1.866e-04, eta: 1 day, 0:58:14, time: 2.229, data_time: 0.054, memory: 20362, loss_cls: 0.0866, loss_bbox: 0.2360, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3704, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2572, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2363, loss: 2.2939, grad_norm: 29.3805
2025-06-17 23:05:59,214 - mmdet - INFO - Epoch [2][200/7033]	lr: 1.866e-04, eta: 1 day, 0:54:54, time: 2.268, data_time: 0.056, memory: 20362, loss_cls: 0.0944, loss_bbox: 0.2435, d0.loss_cls: 0.1943, d0.loss_bbox: 0.3940, d1.loss_cls: 0.1323, d1.loss_bbox: 0.2878, d2.loss_cls: 0.1186, d2.loss_bbox: 0.2698, d3.loss_cls: 0.1058, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0958, d4.loss_bbox: 0.2457, loss: 2.4384, grad_norm: 31.0247
2025-06-17 23:07:53,089 - mmdet - INFO - Epoch [2][250/7033]	lr: 1.866e-04, eta: 1 day, 0:51:36, time: 2.277, data_time: 0.063, memory: 20362, loss_cls: 0.0922, loss_bbox: 0.2430, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3667, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2725, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2587, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2469, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2358, loss: 2.3267, grad_norm: 28.5878
2025-06-17 23:09:47,219 - mmdet - INFO - Epoch [2][300/7033]	lr: 1.866e-04, eta: 1 day, 0:48:21, time: 2.283, data_time: 0.064, memory: 20362, loss_cls: 0.0876, loss_bbox: 0.2396, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3726, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2796, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2602, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2398, loss: 2.3162, grad_norm: 31.4422
2025-06-17 23:11:44,481 - mmdet - INFO - Epoch [2][350/7033]	lr: 1.866e-04, eta: 1 day, 0:45:22, time: 2.345, data_time: 0.081, memory: 20362, loss_cls: 0.1002, loss_bbox: 0.2402, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3911, d1.loss_cls: 0.1357, d1.loss_bbox: 0.2850, d2.loss_cls: 0.1146, d2.loss_bbox: 0.2610, d3.loss_cls: 0.1100, d3.loss_bbox: 0.2501, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2422, loss: 2.4244, grad_norm: 47.4275
2025-06-17 23:13:40,387 - mmdet - INFO - Epoch [2][400/7033]	lr: 1.866e-04, eta: 1 day, 0:42:18, time: 2.318, data_time: 0.059, memory: 20362, loss_cls: 0.0868, loss_bbox: 0.2289, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3796, d1.loss_cls: 0.1293, d1.loss_bbox: 0.2756, d2.loss_cls: 0.1044, d2.loss_bbox: 0.2524, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2410, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2292, loss: 2.2944, grad_norm: 59.8356
2025-06-17 23:15:38,595 - mmdet - INFO - Epoch [2][450/7033]	lr: 1.866e-04, eta: 1 day, 0:39:25, time: 2.364, data_time: 0.061, memory: 20362, loss_cls: 0.0940, loss_bbox: 0.2465, d0.loss_cls: 0.1953, d0.loss_bbox: 0.3935, d1.loss_cls: 0.1329, d1.loss_bbox: 0.2941, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2676, d3.loss_cls: 0.1041, d3.loss_bbox: 0.2567, d4.loss_cls: 0.0954, d4.loss_bbox: 0.2459, loss: 2.4363, grad_norm: 30.7347
2025-06-17 23:17:36,219 - mmdet - INFO - Epoch [2][500/7033]	lr: 1.866e-04, eta: 1 day, 0:36:30, time: 2.352, data_time: 0.061, memory: 20362, loss_cls: 0.0921, loss_bbox: 0.2397, d0.loss_cls: 0.1929, d0.loss_bbox: 0.3828, d1.loss_cls: 0.1400, d1.loss_bbox: 0.2817, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2540, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2447, d4.loss_cls: 0.0928, d4.loss_bbox: 0.2365, loss: 2.3720, grad_norm: 157.7126
2025-06-17 23:19:32,611 - mmdet - INFO - Epoch [2][550/7033]	lr: 1.866e-04, eta: 1 day, 0:33:30, time: 2.328, data_time: 0.056, memory: 20362, loss_cls: 0.0956, loss_bbox: 0.2478, d0.loss_cls: 0.2072, d0.loss_bbox: 0.4073, d1.loss_cls: 0.1363, d1.loss_bbox: 0.2934, d2.loss_cls: 0.1134, d2.loss_bbox: 0.2669, d3.loss_cls: 0.1059, d3.loss_bbox: 0.2568, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2487, loss: 2.4767, grad_norm: 46.9621
2025-06-17 23:21:30,843 - mmdet - INFO - Epoch [2][600/7033]	lr: 1.866e-04, eta: 1 day, 0:30:40, time: 2.364, data_time: 0.059, memory: 20362, loss_cls: 0.0949, loss_bbox: 0.2489, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3986, d1.loss_cls: 0.1340, d1.loss_bbox: 0.2962, d2.loss_cls: 0.1129, d2.loss_bbox: 0.2707, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2598, d4.loss_cls: 0.0978, d4.loss_bbox: 0.2504, loss: 2.4647, grad_norm: 41.9391
2025-06-17 23:23:29,284 - mmdet - INFO - Epoch [2][650/7033]	lr: 1.866e-04, eta: 1 day, 0:27:51, time: 2.369, data_time: 0.068, memory: 20362, loss_cls: 0.0945, loss_bbox: 0.2468, d0.loss_cls: 0.1938, d0.loss_bbox: 0.3865, d1.loss_cls: 0.1325, d1.loss_bbox: 0.2908, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2693, d3.loss_cls: 0.1019, d3.loss_bbox: 0.2576, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2494, loss: 2.4282, grad_norm: 28.4927
2025-06-17 23:25:27,163 - mmdet - INFO - Epoch [2][700/7033]	lr: 1.866e-04, eta: 1 day, 0:25:00, time: 2.358, data_time: 0.060, memory: 20362, loss_cls: 0.0994, loss_bbox: 0.2525, d0.loss_cls: 0.2034, d0.loss_bbox: 0.4002, d1.loss_cls: 0.1456, d1.loss_bbox: 0.3031, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2759, d3.loss_cls: 0.1083, d3.loss_bbox: 0.2648, d4.loss_cls: 0.1019, d4.loss_bbox: 0.2526, loss: 2.5272, grad_norm: 42.3132
2025-06-17 23:27:26,058 - mmdet - INFO - Epoch [2][750/7033]	lr: 1.866e-04, eta: 1 day, 0:22:14, time: 2.377, data_time: 0.059, memory: 20362, loss_cls: 0.1005, loss_bbox: 0.2396, d0.loss_cls: 0.2051, d0.loss_bbox: 0.4021, d1.loss_cls: 0.1492, d1.loss_bbox: 0.2940, d2.loss_cls: 0.1222, d2.loss_bbox: 0.2646, d3.loss_cls: 0.1099, d3.loss_bbox: 0.2529, d4.loss_cls: 0.1022, d4.loss_bbox: 0.2422, loss: 2.4844, grad_norm: 40.7222
2025-06-17 23:29:24,238 - mmdet - INFO - Epoch [2][800/7033]	lr: 1.866e-04, eta: 1 day, 0:19:26, time: 2.364, data_time: 0.063, memory: 20362, loss_cls: 0.1070, loss_bbox: 0.2399, d0.loss_cls: 0.2041, d0.loss_bbox: 0.3993, d1.loss_cls: 0.1456, d1.loss_bbox: 0.2892, d2.loss_cls: 0.1267, d2.loss_bbox: 0.2613, d3.loss_cls: 0.1150, d3.loss_bbox: 0.2494, d4.loss_cls: 0.1067, d4.loss_bbox: 0.2420, loss: 2.4861, grad_norm: 104.8348
2025-06-17 23:31:22,755 - mmdet - INFO - Epoch [2][850/7033]	lr: 1.866e-04, eta: 1 day, 0:16:40, time: 2.370, data_time: 0.055, memory: 20362, loss_cls: 0.0922, loss_bbox: 0.2374, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3887, d1.loss_cls: 0.1328, d1.loss_bbox: 0.2832, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2583, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2484, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2383, loss: 2.3735, grad_norm: 46.0931
2025-06-17 23:33:22,665 - mmdet - INFO - Epoch [2][900/7033]	lr: 1.866e-04, eta: 1 day, 0:14:00, time: 2.398, data_time: 0.059, memory: 20362, loss_cls: 0.0892, loss_bbox: 0.2380, d0.loss_cls: 0.1951, d0.loss_bbox: 0.3789, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2811, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2587, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2483, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2395, loss: 2.3490, grad_norm: 21.4739
2025-06-17 23:35:24,902 - mmdet - INFO - Epoch [2][950/7033]	lr: 1.866e-04, eta: 1 day, 0:11:31, time: 2.445, data_time: 0.058, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2273, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3637, d1.loss_cls: 0.1220, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2290, loss: 2.2383, grad_norm: 41.3685
2025-06-17 23:37:24,392 - mmdet - INFO - Epoch [2][1000/7033]	lr: 1.866e-04, eta: 1 day, 0:08:51, time: 2.390, data_time: 0.072, memory: 20362, loss_cls: 0.0863, loss_bbox: 0.2355, d0.loss_cls: 0.1859, d0.loss_bbox: 0.3806, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2771, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2533, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2439, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2348, loss: 2.2967, grad_norm: 22.3524
2025-06-17 23:39:23,824 - mmdet - INFO - Epoch [2][1050/7033]	lr: 1.866e-04, eta: 1 day, 0:06:11, time: 2.389, data_time: 0.056, memory: 20362, loss_cls: 0.0863, loss_bbox: 0.2357, d0.loss_cls: 0.1837, d0.loss_bbox: 0.3747, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2828, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2568, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2481, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2356, loss: 2.3053, grad_norm: 25.5054
2025-06-17 23:41:23,417 - mmdet - INFO - Epoch [2][1100/7033]	lr: 1.866e-04, eta: 1 day, 0:03:32, time: 2.392, data_time: 0.056, memory: 20362, loss_cls: 0.0943, loss_bbox: 0.2338, d0.loss_cls: 0.1974, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2751, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0957, d4.loss_bbox: 0.2326, loss: 2.3358, grad_norm: 35.4790
2025-06-17 23:43:22,204 - mmdet - INFO - Epoch [2][1150/7033]	lr: 1.866e-04, eta: 1 day, 0:00:50, time: 2.376, data_time: 0.057, memory: 20362, loss_cls: 0.0847, loss_bbox: 0.2329, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3717, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2730, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2535, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2421, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2328, loss: 2.2734, grad_norm: 28.7828
2025-06-17 23:45:19,987 - mmdet - INFO - Epoch [2][1200/7033]	lr: 1.866e-04, eta: 23:58:05, time: 2.356, data_time: 0.058, memory: 20362, loss_cls: 0.0878, loss_bbox: 0.2294, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3676, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2713, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2401, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2307, loss: 2.2635, grad_norm: 20.9563
2025-06-17 23:47:19,662 - mmdet - INFO - Epoch [2][1250/7033]	lr: 1.866e-04, eta: 23:55:28, time: 2.394, data_time: 0.058, memory: 20362, loss_cls: 0.0815, loss_bbox: 0.2364, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2783, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2475, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2372, loss: 2.2672, grad_norm: 39.3522
2025-06-17 23:49:19,366 - mmdet - INFO - Epoch [2][1300/7033]	lr: 1.866e-04, eta: 23:52:51, time: 2.394, data_time: 0.053, memory: 20362, loss_cls: 0.0869, loss_bbox: 0.2272, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3537, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2292, loss: 2.2363, grad_norm: 53.0360
2025-06-17 23:51:18,838 - mmdet - INFO - Epoch [2][1350/7033]	lr: 1.866e-04, eta: 23:50:14, time: 2.389, data_time: 0.073, memory: 20362, loss_cls: 0.0890, loss_bbox: 0.2267, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3658, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2496, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2404, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2282, loss: 2.2695, grad_norm: 27.1769
2025-06-17 23:53:16,463 - mmdet - INFO - Epoch [2][1400/7033]	lr: 1.866e-04, eta: 23:47:30, time: 2.352, data_time: 0.053, memory: 20362, loss_cls: 0.0953, loss_bbox: 0.2333, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1288, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2535, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2437, d4.loss_cls: 0.0941, d4.loss_bbox: 0.2327, loss: 2.3260, grad_norm: 25.0443
2025-06-17 23:55:16,558 - mmdet - INFO - Epoch [2][1450/7033]	lr: 1.866e-04, eta: 23:44:56, time: 2.402, data_time: 0.053, memory: 20362, loss_cls: 0.0909, loss_bbox: 0.2285, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2527, d3.loss_cls: 0.0969, d3.loss_bbox: 0.2409, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2300, loss: 2.2890, grad_norm: 32.6130
2025-06-17 23:57:16,164 - mmdet - INFO - Epoch [2][1500/7033]	lr: 1.866e-04, eta: 23:42:21, time: 2.392, data_time: 0.052, memory: 20362, loss_cls: 0.0932, loss_bbox: 0.2348, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3734, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1068, d2.loss_bbox: 0.2539, d3.loss_cls: 0.0977, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2375, loss: 2.3301, grad_norm: 35.3199
2025-06-17 23:59:15,257 - mmdet - INFO - Epoch [2][1550/7033]	lr: 1.866e-04, eta: 23:39:44, time: 2.382, data_time: 0.054, memory: 20362, loss_cls: 0.0947, loss_bbox: 0.2292, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3645, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2713, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2507, d3.loss_cls: 0.1028, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0952, d4.loss_bbox: 0.2331, loss: 2.3073, grad_norm: 37.9071
2025-06-18 00:01:14,008 - mmdet - INFO - Epoch [2][1600/7033]	lr: 1.866e-04, eta: 23:37:07, time: 2.375, data_time: 0.056, memory: 20362, loss_cls: 0.0903, loss_bbox: 0.2266, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2665, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2468, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2279, loss: 2.2401, grad_norm: 56.2130
2025-06-18 00:03:14,756 - mmdet - INFO - Epoch [2][1650/7033]	lr: 1.866e-04, eta: 23:34:37, time: 2.415, data_time: 0.054, memory: 20362, loss_cls: 0.0951, loss_bbox: 0.2345, d0.loss_cls: 0.1962, d0.loss_bbox: 0.3726, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2793, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2569, d3.loss_cls: 0.1001, d3.loss_bbox: 0.2449, d4.loss_cls: 0.0957, d4.loss_bbox: 0.2370, loss: 2.3508, grad_norm: 25.9021
2025-06-18 00:05:15,289 - mmdet - INFO - Epoch [2][1700/7033]	lr: 1.866e-04, eta: 23:32:07, time: 2.411, data_time: 0.054, memory: 20362, loss_cls: 0.0841, loss_bbox: 0.2297, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2737, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2498, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2395, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2287, loss: 2.2622, grad_norm: 47.3054
2025-06-18 00:07:15,888 - mmdet - INFO - Epoch [2][1750/7033]	lr: 1.866e-04, eta: 23:29:38, time: 2.412, data_time: 0.055, memory: 20362, loss_cls: 0.0857, loss_bbox: 0.2353, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3712, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2770, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2544, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2462, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2346, loss: 2.2848, grad_norm: 24.5884
2025-06-18 00:09:17,145 - mmdet - INFO - Epoch [2][1800/7033]	lr: 1.866e-04, eta: 23:27:11, time: 2.425, data_time: 0.057, memory: 20362, loss_cls: 0.0876, loss_bbox: 0.2365, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3763, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2772, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2507, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2373, loss: 2.2925, grad_norm: 29.0954
2025-06-18 00:11:17,161 - mmdet - INFO - Epoch [2][1850/7033]	lr: 1.866e-04, eta: 23:24:40, time: 2.400, data_time: 0.056, memory: 20362, loss_cls: 0.0867, loss_bbox: 0.2292, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3629, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2724, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2510, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2403, d4.loss_cls: 0.0873, d4.loss_bbox: 0.2306, loss: 2.2518, grad_norm: 30.4326
2025-06-18 00:13:18,688 - mmdet - INFO - Epoch [2][1900/7033]	lr: 1.866e-04, eta: 23:22:15, time: 2.430, data_time: 0.054, memory: 20362, loss_cls: 0.0861, loss_bbox: 0.2277, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3599, d1.loss_cls: 0.1208, d1.loss_bbox: 0.2691, d2.loss_cls: 0.1044, d2.loss_bbox: 0.2442, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2387, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2293, loss: 2.2392, grad_norm: 22.0433
2025-06-18 00:15:20,066 - mmdet - INFO - Epoch [2][1950/7033]	lr: 1.866e-04, eta: 23:19:50, time: 2.428, data_time: 0.055, memory: 20362, loss_cls: 0.0864, loss_bbox: 0.2265, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3578, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2691, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0888, d4.loss_bbox: 0.2280, loss: 2.2483, grad_norm: 27.8800
2025-06-18 00:17:21,429 - mmdet - INFO - Epoch [2][2000/7033]	lr: 1.866e-04, eta: 23:17:25, time: 2.427, data_time: 0.056, memory: 20362, loss_cls: 0.0800, loss_bbox: 0.2327, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3676, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2706, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2387, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2339, loss: 2.2347, grad_norm: 22.5066
2025-06-18 00:19:22,423 - mmdet - INFO - Epoch [2][2050/7033]	lr: 1.866e-04, eta: 23:14:59, time: 2.420, data_time: 0.059, memory: 20362, loss_cls: 0.0893, loss_bbox: 0.2331, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3762, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2514, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2435, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2367, loss: 2.3056, grad_norm: 99.8151
2025-06-18 00:21:23,422 - mmdet - INFO - Epoch [2][2100/7033]	lr: 1.866e-04, eta: 23:12:33, time: 2.420, data_time: 0.055, memory: 20362, loss_cls: 0.0872, loss_bbox: 0.2260, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3600, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2702, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2455, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2278, loss: 2.2259, grad_norm: 36.5416
2025-06-18 00:23:24,611 - mmdet - INFO - Epoch [2][2150/7033]	lr: 1.866e-04, eta: 23:10:08, time: 2.424, data_time: 0.058, memory: 20362, loss_cls: 0.0951, loss_bbox: 0.2361, d0.loss_cls: 0.1964, d0.loss_bbox: 0.3715, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2883, d2.loss_cls: 0.1210, d2.loss_bbox: 0.2650, d3.loss_cls: 0.1077, d3.loss_bbox: 0.2489, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2370, loss: 2.3995, grad_norm: 28.7812
2025-06-18 00:25:24,528 - mmdet - INFO - Epoch [2][2200/7033]	lr: 1.866e-04, eta: 23:07:39, time: 2.398, data_time: 0.056, memory: 20362, loss_cls: 0.0881, loss_bbox: 0.2336, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3727, d1.loss_cls: 0.1302, d1.loss_bbox: 0.2818, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2657, d3.loss_cls: 0.0969, d3.loss_bbox: 0.2496, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2350, loss: 2.3335, grad_norm: 22.2562
2025-06-18 00:27:28,144 - mmdet - INFO - Epoch [2][2250/7033]	lr: 1.866e-04, eta: 23:05:23, time: 2.472, data_time: 0.057, memory: 20362, loss_cls: 0.0890, loss_bbox: 0.2432, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3854, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2928, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2727, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2596, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2469, loss: 2.3771, grad_norm: 38.6699
2025-06-18 00:29:26,727 - mmdet - INFO - Epoch [2][2300/7033]	lr: 1.866e-04, eta: 23:02:50, time: 2.372, data_time: 0.056, memory: 20362, loss_cls: 0.0986, loss_bbox: 0.2377, d0.loss_cls: 0.1825, d0.loss_bbox: 0.3712, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2830, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2637, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2516, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2392, loss: 2.3611, grad_norm: 30.1861
2025-06-18 00:31:28,390 - mmdet - INFO - Epoch [2][2350/7033]	lr: 1.866e-04, eta: 23:00:28, time: 2.433, data_time: 0.057, memory: 20362, loss_cls: 0.0984, loss_bbox: 0.2259, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2495, d3.loss_cls: 0.1062, d3.loss_bbox: 0.2382, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2274, loss: 2.3029, grad_norm: 29.7104
2025-06-18 00:33:29,671 - mmdet - INFO - Epoch [2][2400/7033]	lr: 1.866e-04, eta: 22:58:04, time: 2.426, data_time: 0.055, memory: 20362, loss_cls: 0.1005, loss_bbox: 0.2382, d0.loss_cls: 0.1986, d0.loss_bbox: 0.3801, d1.loss_cls: 0.1368, d1.loss_bbox: 0.2807, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2610, d3.loss_cls: 0.1093, d3.loss_bbox: 0.2505, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2401, loss: 2.4144, grad_norm: 38.4376
2025-06-18 00:35:29,989 - mmdet - INFO - Epoch [2][2450/7033]	lr: 1.866e-04, eta: 22:55:38, time: 2.406, data_time: 0.055, memory: 20362, loss_cls: 0.0930, loss_bbox: 0.2265, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3709, d1.loss_cls: 0.1274, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2566, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0953, d4.loss_bbox: 0.2292, loss: 2.3075, grad_norm: 28.5183
2025-06-18 00:37:32,171 - mmdet - INFO - Epoch [2][2500/7033]	lr: 1.866e-04, eta: 22:53:18, time: 2.444, data_time: 0.056, memory: 20362, loss_cls: 0.0915, loss_bbox: 0.2304, d0.loss_cls: 0.1790, d0.loss_bbox: 0.3641, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1109, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0994, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2328, loss: 2.3104, grad_norm: 40.8204
2025-06-18 00:39:32,170 - mmdet - INFO - Epoch [2][2550/7033]	lr: 1.866e-04, eta: 22:50:51, time: 2.400, data_time: 0.057, memory: 20362, loss_cls: 0.0940, loss_bbox: 0.2423, d0.loss_cls: 0.1926, d0.loss_bbox: 0.3798, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2876, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2655, d3.loss_cls: 0.1055, d3.loss_bbox: 0.2526, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2435, loss: 2.4074, grad_norm: 30.3953
2025-06-18 00:41:45,760 - mmdet - INFO - Epoch [2][2600/7033]	lr: 1.866e-04, eta: 22:49:11, time: 2.672, data_time: 0.056, memory: 20362, loss_cls: 0.0992, loss_bbox: 0.2535, d0.loss_cls: 0.2037, d0.loss_bbox: 0.3945, d1.loss_cls: 0.1414, d1.loss_bbox: 0.2954, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2729, d3.loss_cls: 0.1120, d3.loss_bbox: 0.2625, d4.loss_cls: 0.1023, d4.loss_bbox: 0.2543, loss: 2.5130, grad_norm: 28.8565
2025-06-18 00:43:47,554 - mmdet - INFO - Epoch [2][2650/7033]	lr: 1.866e-04, eta: 22:46:50, time: 2.436, data_time: 0.057, memory: 20362, loss_cls: 0.0946, loss_bbox: 0.2364, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3711, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2813, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2583, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0953, d4.loss_bbox: 0.2383, loss: 2.3596, grad_norm: 23.1097
2025-06-18 00:45:49,146 - mmdet - INFO - Epoch [2][2700/7033]	lr: 1.866e-04, eta: 22:44:29, time: 2.432, data_time: 0.057, memory: 20362, loss_cls: 0.0966, loss_bbox: 0.2422, d0.loss_cls: 0.1937, d0.loss_bbox: 0.3780, d1.loss_cls: 0.1423, d1.loss_bbox: 0.2826, d2.loss_cls: 0.1254, d2.loss_bbox: 0.2587, d3.loss_cls: 0.1099, d3.loss_bbox: 0.2476, d4.loss_cls: 0.0986, d4.loss_bbox: 0.2408, loss: 2.4163, grad_norm: 38.4483
2025-06-18 00:47:51,804 - mmdet - INFO - Epoch [2][2750/7033]	lr: 1.866e-04, eta: 22:42:11, time: 2.453, data_time: 0.056, memory: 20362, loss_cls: 0.0975, loss_bbox: 0.2361, d0.loss_cls: 0.1906, d0.loss_bbox: 0.3767, d1.loss_cls: 0.1366, d1.loss_bbox: 0.2844, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2586, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2469, d4.loss_cls: 0.0993, d4.loss_bbox: 0.2388, loss: 2.3851, grad_norm: 71.7739
2025-06-18 00:49:52,949 - mmdet - INFO - Epoch [2][2800/7033]	lr: 1.866e-04, eta: 22:39:49, time: 2.423, data_time: 0.055, memory: 20362, loss_cls: 0.0875, loss_bbox: 0.2404, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3699, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2806, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2583, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2470, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2388, loss: 2.3049, grad_norm: 112.6034
2025-06-18 00:51:59,482 - mmdet - INFO - Epoch [2][2850/7033]	lr: 1.866e-04, eta: 22:37:44, time: 2.531, data_time: 0.054, memory: 20362, loss_cls: 0.0917, loss_bbox: 0.2389, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2800, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0980, d3.loss_bbox: 0.2448, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2369, loss: 2.3237, grad_norm: 24.8866
2025-06-18 00:54:01,833 - mmdet - INFO - Epoch [2][2900/7033]	lr: 1.866e-04, eta: 22:35:26, time: 2.447, data_time: 0.055, memory: 20362, loss_cls: 0.0867, loss_bbox: 0.2269, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3673, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2768, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2516, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2395, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2308, loss: 2.2678, grad_norm: 31.1869
2025-06-18 00:56:04,822 - mmdet - INFO - Epoch [2][2950/7033]	lr: 1.866e-04, eta: 22:33:11, time: 2.460, data_time: 0.056, memory: 20362, loss_cls: 0.0866, loss_bbox: 0.2203, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2625, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2246, loss: 2.1902, grad_norm: 45.1487
2025-06-18 00:58:06,689 - mmdet - INFO - Epoch [2][3000/7033]	lr: 1.866e-04, eta: 22:30:51, time: 2.437, data_time: 0.054, memory: 20362, loss_cls: 0.0786, loss_bbox: 0.2234, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2671, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2459, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2269, loss: 2.1782, grad_norm: 23.0671
2025-06-18 01:00:09,890 - mmdet - INFO - Epoch [2][3050/7033]	lr: 1.866e-04, eta: 22:28:36, time: 2.464, data_time: 0.052, memory: 20362, loss_cls: 0.0930, loss_bbox: 0.2296, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2558, d3.loss_cls: 0.1000, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0931, d4.loss_bbox: 0.2332, loss: 2.3049, grad_norm: 31.5500
2025-06-18 01:02:13,033 - mmdet - INFO - Epoch [2][3100/7033]	lr: 1.866e-04, eta: 22:26:21, time: 2.463, data_time: 0.054, memory: 20362, loss_cls: 0.0899, loss_bbox: 0.2443, d0.loss_cls: 0.1929, d0.loss_bbox: 0.3829, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2875, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2633, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2521, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2442, loss: 2.3801, grad_norm: 25.1529
2025-06-18 01:04:15,830 - mmdet - INFO - Epoch [2][3150/7033]	lr: 1.866e-04, eta: 22:24:05, time: 2.456, data_time: 0.051, memory: 20362, loss_cls: 0.0874, loss_bbox: 0.2378, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3763, d1.loss_cls: 0.1243, d1.loss_bbox: 0.2807, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2573, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2443, d4.loss_cls: 0.0894, d4.loss_bbox: 0.2387, loss: 2.3189, grad_norm: 28.3109
2025-06-18 01:06:18,697 - mmdet - INFO - Epoch [2][3200/7033]	lr: 1.866e-04, eta: 22:21:50, time: 2.457, data_time: 0.052, memory: 20362, loss_cls: 0.0959, loss_bbox: 0.2422, d0.loss_cls: 0.1901, d0.loss_bbox: 0.3701, d1.loss_cls: 0.1348, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2602, d3.loss_cls: 0.1031, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0967, d4.loss_bbox: 0.2426, loss: 2.3792, grad_norm: 23.1873
2025-06-18 01:08:22,249 - mmdet - INFO - Epoch [2][3250/7033]	lr: 1.866e-04, eta: 22:19:37, time: 2.471, data_time: 0.054, memory: 20362, loss_cls: 0.0896, loss_bbox: 0.2357, d0.loss_cls: 0.1851, d0.loss_bbox: 0.3765, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2813, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2588, d3.loss_cls: 0.0952, d3.loss_bbox: 0.2462, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2380, loss: 2.3201, grad_norm: 25.1150
2025-06-18 01:10:27,590 - mmdet - INFO - Epoch [2][3300/7033]	lr: 1.866e-04, eta: 22:17:29, time: 2.507, data_time: 0.052, memory: 20362, loss_cls: 0.0841, loss_bbox: 0.2243, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3596, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2686, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2461, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2265, loss: 2.2239, grad_norm: 26.4249
2025-06-18 01:12:31,306 - mmdet - INFO - Epoch [2][3350/7033]	lr: 1.866e-04, eta: 22:15:16, time: 2.474, data_time: 0.056, memory: 20362, loss_cls: 0.0797, loss_bbox: 0.2222, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2633, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2237, loss: 2.1833, grad_norm: 73.3812
2025-06-18 01:14:34,981 - mmdet - INFO - Epoch [2][3400/7033]	lr: 1.866e-04, eta: 22:13:03, time: 2.473, data_time: 0.057, memory: 20362, loss_cls: 0.0902, loss_bbox: 0.2379, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3789, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2823, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2620, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2504, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2396, loss: 2.3577, grad_norm: 20.2954
2025-06-18 01:16:40,367 - mmdet - INFO - Epoch [2][3450/7033]	lr: 1.866e-04, eta: 22:10:56, time: 2.508, data_time: 0.057, memory: 20362, loss_cls: 0.0927, loss_bbox: 0.2244, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3690, d1.loss_cls: 0.1331, d1.loss_bbox: 0.2723, d2.loss_cls: 0.1127, d2.loss_bbox: 0.2506, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2262, loss: 2.3059, grad_norm: 27.6192
2025-06-18 01:18:43,214 - mmdet - INFO - Epoch [2][3500/7033]	lr: 1.866e-04, eta: 22:08:41, time: 2.457, data_time: 0.062, memory: 20362, loss_cls: 0.0905, loss_bbox: 0.2342, d0.loss_cls: 0.1884, d0.loss_bbox: 0.3690, d1.loss_cls: 0.1252, d1.loss_bbox: 0.2780, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2567, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2360, loss: 2.3180, grad_norm: 59.0962
2025-06-18 01:20:47,500 - mmdet - INFO - Epoch [2][3550/7033]	lr: 1.866e-04, eta: 22:06:30, time: 2.486, data_time: 0.054, memory: 20362, loss_cls: 0.0871, loss_bbox: 0.2334, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3589, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2727, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2519, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2329, loss: 2.2578, grad_norm: 30.9089
2025-06-18 01:22:51,462 - mmdet - INFO - Epoch [2][3600/7033]	lr: 1.866e-04, eta: 22:04:19, time: 2.479, data_time: 0.056, memory: 20362, loss_cls: 0.0878, loss_bbox: 0.2357, d0.loss_cls: 0.1824, d0.loss_bbox: 0.3696, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2462, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2367, loss: 2.2995, grad_norm: 38.1508
2025-06-18 01:24:55,860 - mmdet - INFO - Epoch [2][3650/7033]	lr: 1.866e-04, eta: 22:02:09, time: 2.488, data_time: 0.056, memory: 20362, loss_cls: 0.0863, loss_bbox: 0.2337, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3752, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2540, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2416, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2330, loss: 2.2899, grad_norm: 23.1929
2025-06-18 01:26:58,822 - mmdet - INFO - Epoch [2][3700/7033]	lr: 1.866e-04, eta: 21:59:54, time: 2.459, data_time: 0.056, memory: 20362, loss_cls: 0.0972, loss_bbox: 0.2307, d0.loss_cls: 0.1948, d0.loss_bbox: 0.3779, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2785, d2.loss_cls: 0.1120, d2.loss_bbox: 0.2540, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2388, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2311, loss: 2.3506, grad_norm: 33.2835
2025-06-18 01:29:01,133 - mmdet - INFO - Epoch [2][3750/7033]	lr: 1.866e-04, eta: 21:57:38, time: 2.446, data_time: 0.055, memory: 20362, loss_cls: 0.0897, loss_bbox: 0.2257, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3656, d1.loss_cls: 0.1333, d1.loss_bbox: 0.2720, d2.loss_cls: 0.1044, d2.loss_bbox: 0.2463, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2256, loss: 2.2740, grad_norm: 30.7376
2025-06-18 01:31:03,304 - mmdet - INFO - Epoch [2][3800/7033]	lr: 1.866e-04, eta: 21:55:22, time: 2.444, data_time: 0.055, memory: 20362, loss_cls: 0.0870, loss_bbox: 0.2339, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3744, d1.loss_cls: 0.1275, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2579, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2348, loss: 2.3164, grad_norm: 50.1598
2025-06-18 01:33:07,084 - mmdet - INFO - Epoch [2][3850/7033]	lr: 1.866e-04, eta: 21:53:10, time: 2.476, data_time: 0.057, memory: 20362, loss_cls: 0.0909, loss_bbox: 0.2361, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3745, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2553, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2445, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2365, loss: 2.3460, grad_norm: 432.9091
2025-06-18 01:35:10,850 - mmdet - INFO - Epoch [2][3900/7033]	lr: 1.866e-04, eta: 21:50:58, time: 2.475, data_time: 0.055, memory: 20362, loss_cls: 0.0870, loss_bbox: 0.2322, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3708, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2813, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2552, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2421, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2341, loss: 2.3102, grad_norm: 28.2492
2025-06-18 01:37:16,411 - mmdet - INFO - Epoch [2][3950/7033]	lr: 1.866e-04, eta: 21:48:52, time: 2.511, data_time: 0.054, memory: 20362, loss_cls: 0.0853, loss_bbox: 0.2321, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3682, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2780, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2327, loss: 2.2868, grad_norm: 29.1684
2025-06-18 01:39:19,688 - mmdet - INFO - Epoch [2][4000/7033]	lr: 1.866e-04, eta: 21:46:39, time: 2.466, data_time: 0.056, memory: 20362, loss_cls: 0.0990, loss_bbox: 0.2448, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3887, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2978, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2702, d3.loss_cls: 0.1023, d3.loss_bbox: 0.2574, d4.loss_cls: 0.0996, d4.loss_bbox: 0.2475, loss: 2.4484, grad_norm: 45.6751
2025-06-18 01:41:26,805 - mmdet - INFO - Epoch [2][4050/7033]	lr: 1.866e-04, eta: 21:44:37, time: 2.542, data_time: 0.055, memory: 20362, loss_cls: 0.0949, loss_bbox: 0.2516, d0.loss_cls: 0.1988, d0.loss_bbox: 0.3996, d1.loss_cls: 0.1373, d1.loss_bbox: 0.3027, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2767, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2636, d4.loss_cls: 0.0966, d4.loss_bbox: 0.2509, loss: 2.4907, grad_norm: 32.0687
2025-06-18 01:43:30,016 - mmdet - INFO - Epoch [2][4100/7033]	lr: 1.866e-04, eta: 21:42:24, time: 2.464, data_time: 0.049, memory: 20362, loss_cls: 0.0920, loss_bbox: 0.2390, d0.loss_cls: 0.1824, d0.loss_bbox: 0.3844, d1.loss_cls: 0.1228, d1.loss_bbox: 0.2923, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2645, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2544, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2410, loss: 2.3713, grad_norm: 55.0652
2025-06-18 01:45:34,052 - mmdet - INFO - Epoch [2][4150/7033]	lr: 1.866e-04, eta: 21:40:13, time: 2.481, data_time: 0.049, memory: 20362, loss_cls: 0.0888, loss_bbox: 0.2285, d0.loss_cls: 0.1949, d0.loss_bbox: 0.3744, d1.loss_cls: 0.1252, d1.loss_bbox: 0.2790, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2545, d3.loss_cls: 0.0976, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0905, d4.loss_bbox: 0.2302, loss: 2.3086, grad_norm: 41.1356
2025-06-18 01:47:37,056 - mmdet - INFO - Epoch [2][4200/7033]	lr: 1.866e-04, eta: 21:38:00, time: 2.460, data_time: 0.051, memory: 20362, loss_cls: 0.0899, loss_bbox: 0.2352, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3783, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2846, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2619, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2500, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2393, loss: 2.3401, grad_norm: 174.5206
2025-06-18 01:49:40,314 - mmdet - INFO - Epoch [2][4250/7033]	lr: 1.866e-04, eta: 21:35:47, time: 2.465, data_time: 0.052, memory: 20362, loss_cls: 0.0934, loss_bbox: 0.2381, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3690, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2782, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2587, d3.loss_cls: 0.0993, d3.loss_bbox: 0.2489, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2404, loss: 2.3391, grad_norm: 30.4149
2025-06-18 01:51:43,851 - mmdet - INFO - Epoch [2][4300/7033]	lr: 1.866e-04, eta: 21:33:36, time: 2.471, data_time: 0.056, memory: 20362, loss_cls: 0.0902, loss_bbox: 0.2315, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3812, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2800, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2413, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2312, loss: 2.3262, grad_norm: 36.7870
2025-06-18 01:53:48,964 - mmdet - INFO - Epoch [2][4350/7033]	lr: 1.866e-04, eta: 21:31:28, time: 2.502, data_time: 0.057, memory: 20362, loss_cls: 0.0932, loss_bbox: 0.2380, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3791, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2782, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2395, loss: 2.3493, grad_norm: 38.8380
2025-06-18 01:55:52,638 - mmdet - INFO - Epoch [2][4400/7033]	lr: 1.866e-04, eta: 21:29:17, time: 2.474, data_time: 0.058, memory: 20362, loss_cls: 0.0841, loss_bbox: 0.2276, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3601, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2692, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2482, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2379, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2291, loss: 2.2114, grad_norm: 42.2043
2025-06-18 01:57:56,164 - mmdet - INFO - Epoch [2][4450/7033]	lr: 1.866e-04, eta: 21:27:05, time: 2.470, data_time: 0.056, memory: 20362, loss_cls: 0.0905, loss_bbox: 0.2355, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3691, d1.loss_cls: 0.1252, d1.loss_bbox: 0.2714, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2508, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2400, d4.loss_cls: 0.0919, d4.loss_bbox: 0.2332, loss: 2.2951, grad_norm: 24.1547
2025-06-18 01:59:58,776 - mmdet - INFO - Epoch [2][4500/7033]	lr: 1.866e-04, eta: 21:24:51, time: 2.453, data_time: 0.058, memory: 20362, loss_cls: 0.0876, loss_bbox: 0.2297, d0.loss_cls: 0.1942, d0.loss_bbox: 0.3788, d1.loss_cls: 0.1258, d1.loss_bbox: 0.2763, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2541, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2314, loss: 2.3092, grad_norm: 37.0150
2025-06-18 02:02:05,397 - mmdet - INFO - Epoch [2][4550/7033]	lr: 1.866e-04, eta: 21:22:48, time: 2.532, data_time: 0.065, memory: 20362, loss_cls: 0.0880, loss_bbox: 0.2246, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3668, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2494, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2265, loss: 2.2743, grad_norm: 24.9330
2025-06-18 02:04:09,624 - mmdet - INFO - Epoch [2][4600/7033]	lr: 1.866e-04, eta: 21:20:38, time: 2.485, data_time: 0.055, memory: 20362, loss_cls: 0.0935, loss_bbox: 0.2293, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3668, d1.loss_cls: 0.1342, d1.loss_bbox: 0.2746, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2462, d3.loss_cls: 0.1031, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2288, loss: 2.3136, grad_norm: 56.3277
2025-06-18 02:06:15,474 - mmdet - INFO - Epoch [2][4650/7033]	lr: 1.866e-04, eta: 21:18:25, time: 2.457, data_time: 0.058, memory: 20362, loss_cls: 0.0900, loss_bbox: 0.2206, d0.loss_cls: 0.1820, d0.loss_bbox: 0.3641, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2715, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2237, loss: 2.2388, grad_norm: 30.2742
2025-06-18 02:08:17,824 - mmdet - INFO - Epoch [2][4700/7033]	lr: 1.866e-04, eta: 21:16:19, time: 2.507, data_time: 0.115, memory: 20362, loss_cls: 0.0843, loss_bbox: 0.2248, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2679, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2452, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2277, loss: 2.2215, grad_norm: 36.7690
2025-06-18 02:10:20,214 - mmdet - INFO - Epoch [2][4750/7033]	lr: 1.866e-04, eta: 21:14:05, time: 2.448, data_time: 0.059, memory: 20362, loss_cls: 0.0850, loss_bbox: 0.2325, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3700, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2522, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2311, loss: 2.2521, grad_norm: 22.1515
2025-06-18 02:12:21,812 - mmdet - INFO - Epoch [2][4800/7033]	lr: 1.866e-04, eta: 21:11:48, time: 2.432, data_time: 0.056, memory: 20362, loss_cls: 0.0869, loss_bbox: 0.2380, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3617, d1.loss_cls: 0.1243, d1.loss_bbox: 0.2762, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2453, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2385, loss: 2.2984, grad_norm: 22.9667
2025-06-18 02:14:24,226 - mmdet - INFO - Epoch [2][4850/7033]	lr: 1.866e-04, eta: 21:09:35, time: 2.448, data_time: 0.054, memory: 20362, loss_cls: 0.0813, loss_bbox: 0.2251, d0.loss_cls: 0.1804, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2675, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0887, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2252, loss: 2.2031, grad_norm: 24.6318
2025-06-18 02:16:25,543 - mmdet - INFO - Epoch [2][4900/7033]	lr: 1.866e-04, eta: 21:07:18, time: 2.426, data_time: 0.054, memory: 20362, loss_cls: 0.0925, loss_bbox: 0.2336, d0.loss_cls: 0.1905, d0.loss_bbox: 0.3653, d1.loss_cls: 0.1237, d1.loss_bbox: 0.2787, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2543, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2421, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2347, loss: 2.3084, grad_norm: 24.6440
2025-06-18 02:18:27,828 - mmdet - INFO - Epoch [2][4950/7033]	lr: 1.866e-04, eta: 21:05:04, time: 2.446, data_time: 0.056, memory: 20362, loss_cls: 0.0901, loss_bbox: 0.2278, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3632, d1.loss_cls: 0.1228, d1.loss_bbox: 0.2720, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2474, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2287, loss: 2.2725, grad_norm: 19.3557
2025-06-18 02:20:29,334 - mmdet - INFO - Epoch [2][5000/7033]	lr: 1.866e-04, eta: 21:02:48, time: 2.430, data_time: 0.055, memory: 20362, loss_cls: 0.0886, loss_bbox: 0.2240, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3638, d1.loss_cls: 0.1268, d1.loss_bbox: 0.2698, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2375, d4.loss_cls: 0.0907, d4.loss_bbox: 0.2277, loss: 2.2597, grad_norm: 21.4617
2025-06-18 02:22:30,923 - mmdet - INFO - Epoch [2][5050/7033]	lr: 1.866e-04, eta: 21:00:32, time: 2.432, data_time: 0.058, memory: 20362, loss_cls: 0.1005, loss_bbox: 0.2344, d0.loss_cls: 0.2015, d0.loss_bbox: 0.3920, d1.loss_cls: 0.1400, d1.loss_bbox: 0.2889, d2.loss_cls: 0.1211, d2.loss_bbox: 0.2607, d3.loss_cls: 0.1081, d3.loss_bbox: 0.2460, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2366, loss: 2.4321, grad_norm: 30.5356
2025-06-18 02:24:31,213 - mmdet - INFO - Epoch [2][5100/7033]	lr: 1.866e-04, eta: 20:58:14, time: 2.406, data_time: 0.056, memory: 20362, loss_cls: 0.0861, loss_bbox: 0.2316, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3859, d1.loss_cls: 0.1314, d1.loss_bbox: 0.2770, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2530, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2411, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2335, loss: 2.3209, grad_norm: 23.5358
2025-06-18 02:26:33,061 - mmdet - INFO - Epoch [2][5150/7033]	lr: 1.866e-04, eta: 20:55:59, time: 2.436, data_time: 0.055, memory: 20362, loss_cls: 0.0907, loss_bbox: 0.2338, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3789, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2836, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2605, d3.loss_cls: 0.0982, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0928, d4.loss_bbox: 0.2386, loss: 2.3404, grad_norm: 28.5042
2025-06-18 02:28:34,077 - mmdet - INFO - Epoch [2][5200/7033]	lr: 1.866e-04, eta: 20:53:43, time: 2.421, data_time: 0.058, memory: 20362, loss_cls: 0.0959, loss_bbox: 0.2273, d0.loss_cls: 0.1932, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1118, d2.loss_bbox: 0.2509, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2374, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2303, loss: 2.3231, grad_norm: 24.8963
2025-06-18 02:30:35,176 - mmdet - INFO - Epoch [2][5250/7033]	lr: 1.866e-04, eta: 20:51:26, time: 2.422, data_time: 0.060, memory: 20362, loss_cls: 0.0923, loss_bbox: 0.2336, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3765, d1.loss_cls: 0.1317, d1.loss_bbox: 0.2788, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2554, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0942, d4.loss_bbox: 0.2343, loss: 2.3369, grad_norm: 24.0974
2025-06-18 02:32:37,461 - mmdet - INFO - Epoch [2][5300/7033]	lr: 1.866e-04, eta: 20:49:13, time: 2.446, data_time: 0.057, memory: 20362, loss_cls: 0.0924, loss_bbox: 0.2387, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3720, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2580, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2478, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2405, loss: 2.3336, grad_norm: 41.1722
2025-06-18 02:34:36,271 - mmdet - INFO - Epoch [2][5350/7033]	lr: 1.866e-04, eta: 20:46:51, time: 2.376, data_time: 0.056, memory: 20362, loss_cls: 0.0910, loss_bbox: 0.2273, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2514, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2401, d4.loss_cls: 0.0937, d4.loss_bbox: 0.2298, loss: 2.2926, grad_norm: 48.4542
2025-06-18 02:36:37,107 - mmdet - INFO - Epoch [2][5400/7033]	lr: 1.866e-04, eta: 20:44:35, time: 2.417, data_time: 0.056, memory: 20362, loss_cls: 0.0842, loss_bbox: 0.2197, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3675, d1.loss_cls: 0.1264, d1.loss_bbox: 0.2677, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2482, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2223, loss: 2.2294, grad_norm: 25.8737
2025-06-18 02:38:36,766 - mmdet - INFO - Epoch [2][5450/7033]	lr: 1.866e-04, eta: 20:42:16, time: 2.393, data_time: 0.055, memory: 20362, loss_cls: 0.0780, loss_bbox: 0.2246, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3635, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2717, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2271, loss: 2.2010, grad_norm: 26.5487
2025-06-18 02:40:37,080 - mmdet - INFO - Epoch [2][5500/7033]	lr: 1.866e-04, eta: 20:39:58, time: 2.406, data_time: 0.054, memory: 20362, loss_cls: 0.0872, loss_bbox: 0.2318, d0.loss_cls: 0.1939, d0.loss_bbox: 0.3611, d1.loss_cls: 0.1283, d1.loss_bbox: 0.2761, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2563, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2447, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2338, loss: 2.3027, grad_norm: 21.4216
2025-06-18 02:42:34,196 - mmdet - INFO - Epoch [2][5550/7033]	lr: 1.866e-04, eta: 20:37:33, time: 2.342, data_time: 0.054, memory: 20362, loss_cls: 0.0881, loss_bbox: 0.2286, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3692, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2744, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2537, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2409, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2311, loss: 2.2714, grad_norm: 24.2269
2025-06-18 02:44:32,252 - mmdet - INFO - Epoch [2][5600/7033]	lr: 1.866e-04, eta: 20:35:11, time: 2.361, data_time: 0.056, memory: 20362, loss_cls: 0.0883, loss_bbox: 0.2282, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3613, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1058, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2396, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2313, loss: 2.2742, grad_norm: 35.0258
2025-06-18 02:46:31,644 - mmdet - INFO - Epoch [2][5650/7033]	lr: 1.866e-04, eta: 20:32:52, time: 2.388, data_time: 0.055, memory: 20362, loss_cls: 0.0842, loss_bbox: 0.2355, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3740, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2781, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2584, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2463, d4.loss_cls: 0.0864, d4.loss_bbox: 0.2372, loss: 2.2938, grad_norm: 22.6782
2025-06-18 02:48:28,116 - mmdet - INFO - Epoch [2][5700/7033]	lr: 1.866e-04, eta: 20:30:26, time: 2.329, data_time: 0.056, memory: 20362, loss_cls: 0.0779, loss_bbox: 0.2238, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3497, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2669, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2449, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2249, loss: 2.1673, grad_norm: 38.1432
2025-06-18 02:50:28,193 - mmdet - INFO - Epoch [2][5750/7033]	lr: 1.866e-04, eta: 20:28:09, time: 2.402, data_time: 0.056, memory: 20362, loss_cls: 0.0921, loss_bbox: 0.2319, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3606, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2737, d2.loss_cls: 0.1110, d2.loss_bbox: 0.2504, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2315, loss: 2.3005, grad_norm: 41.1105
2025-06-18 02:52:24,270 - mmdet - INFO - Epoch [2][5800/7033]	lr: 1.866e-04, eta: 20:25:42, time: 2.321, data_time: 0.058, memory: 20362, loss_cls: 0.0815, loss_bbox: 0.2241, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3558, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2628, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2424, d3.loss_cls: 0.0910, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2256, loss: 2.2013, grad_norm: 25.1301
2025-06-18 02:54:23,488 - mmdet - INFO - Epoch [2][5850/7033]	lr: 1.866e-04, eta: 20:23:23, time: 2.384, data_time: 0.055, memory: 20362, loss_cls: 0.0891, loss_bbox: 0.2243, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3587, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2363, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2274, loss: 2.2327, grad_norm: 22.6041
2025-06-18 02:56:23,738 - mmdet - INFO - Epoch [2][5900/7033]	lr: 1.866e-04, eta: 20:21:07, time: 2.405, data_time: 0.056, memory: 20362, loss_cls: 0.0853, loss_bbox: 0.2244, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1201, d1.loss_bbox: 0.2740, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2467, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2280, loss: 2.2479, grad_norm: 25.1902
2025-06-18 02:58:21,806 - mmdet - INFO - Epoch [2][5950/7033]	lr: 1.866e-04, eta: 20:18:46, time: 2.361, data_time: 0.057, memory: 20362, loss_cls: 0.0876, loss_bbox: 0.2321, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2543, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2430, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2333, loss: 2.3085, grad_norm: 54.5018
2025-06-18 03:00:19,608 - mmdet - INFO - Epoch [2][6000/7033]	lr: 1.866e-04, eta: 20:16:24, time: 2.356, data_time: 0.056, memory: 20362, loss_cls: 0.0772, loss_bbox: 0.2200, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3557, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2628, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2418, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2217, loss: 2.1500, grad_norm: 57.9640
2025-06-18 03:02:16,747 - mmdet - INFO - Epoch [2][6050/7033]	lr: 1.866e-04, eta: 20:14:01, time: 2.343, data_time: 0.057, memory: 20362, loss_cls: 0.0870, loss_bbox: 0.2338, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3738, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2574, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2363, loss: 2.2975, grad_norm: 26.9162
2025-06-18 03:04:13,543 - mmdet - INFO - Epoch [2][6100/7033]	lr: 1.866e-04, eta: 20:11:38, time: 2.336, data_time: 0.060, memory: 20362, loss_cls: 0.0807, loss_bbox: 0.2236, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2621, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2321, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2273, loss: 2.1899, grad_norm: 32.9131
2025-06-18 03:06:09,178 - mmdet - INFO - Epoch [2][6150/7033]	lr: 1.866e-04, eta: 20:09:12, time: 2.313, data_time: 0.054, memory: 20362, loss_cls: 0.0876, loss_bbox: 0.2292, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3678, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2753, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2491, d3.loss_cls: 0.0944, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2285, loss: 2.2585, grad_norm: 22.9931
2025-06-18 03:08:04,696 - mmdet - INFO - Epoch [2][6200/7033]	lr: 1.866e-04, eta: 20:06:46, time: 2.311, data_time: 0.055, memory: 20362, loss_cls: 0.0830, loss_bbox: 0.2257, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3684, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2746, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2522, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2375, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2305, loss: 2.2455, grad_norm: 266.4322
2025-06-18 03:10:00,850 - mmdet - INFO - Epoch [2][6250/7033]	lr: 1.866e-04, eta: 20:04:22, time: 2.323, data_time: 0.059, memory: 20362, loss_cls: 0.0925, loss_bbox: 0.2239, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3740, d1.loss_cls: 0.1332, d1.loss_bbox: 0.2723, d2.loss_cls: 0.1095, d2.loss_bbox: 0.2498, d3.loss_cls: 0.1016, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2273, loss: 2.3007, grad_norm: 28.4937
2025-06-18 03:11:56,892 - mmdet - INFO - Epoch [2][6300/7033]	lr: 1.866e-04, eta: 20:01:58, time: 2.321, data_time: 0.059, memory: 20362, loss_cls: 0.0853, loss_bbox: 0.2233, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3652, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2702, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2489, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2256, loss: 2.2357, grad_norm: 30.1194
2025-06-18 03:13:52,083 - mmdet - INFO - Epoch [2][6350/7033]	lr: 1.866e-04, eta: 19:59:32, time: 2.304, data_time: 0.053, memory: 20362, loss_cls: 0.0933, loss_bbox: 0.2312, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3745, d1.loss_cls: 0.1338, d1.loss_bbox: 0.2789, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2558, d3.loss_cls: 0.1010, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0961, d4.loss_bbox: 0.2342, loss: 2.3371, grad_norm: 23.5163
2025-06-18 03:15:51,273 - mmdet - INFO - Epoch [2][6400/7033]	lr: 1.866e-04, eta: 19:57:15, time: 2.383, data_time: 0.051, memory: 20362, loss_cls: 0.0953, loss_bbox: 0.2398, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2826, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2628, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2496, d4.loss_cls: 0.0981, d4.loss_bbox: 0.2418, loss: 2.3625, grad_norm: 43.3470
2025-06-18 03:17:47,029 - mmdet - INFO - Epoch [2][6450/7033]	lr: 1.866e-04, eta: 19:54:51, time: 2.316, data_time: 0.053, memory: 20362, loss_cls: 0.0906, loss_bbox: 0.2282, d0.loss_cls: 0.1881, d0.loss_bbox: 0.3782, d1.loss_cls: 0.1327, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1111, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0980, d3.loss_bbox: 0.2410, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2307, loss: 2.3291, grad_norm: 25.1752
2025-06-18 03:19:44,146 - mmdet - INFO - Epoch [2][6500/7033]	lr: 1.866e-04, eta: 19:52:29, time: 2.342, data_time: 0.055, memory: 20362, loss_cls: 0.0861, loss_bbox: 0.2177, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3545, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2649, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2412, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2203, loss: 2.1999, grad_norm: 23.1430
2025-06-18 03:21:40,147 - mmdet - INFO - Epoch [2][6550/7033]	lr: 1.866e-04, eta: 19:50:06, time: 2.320, data_time: 0.057, memory: 20362, loss_cls: 0.0763, loss_bbox: 0.2192, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2463, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2210, loss: 2.1457, grad_norm: 52.0268
2025-06-18 03:23:34,746 - mmdet - INFO - Epoch [2][6600/7033]	lr: 1.866e-04, eta: 19:47:40, time: 2.292, data_time: 0.058, memory: 20362, loss_cls: 0.0848, loss_bbox: 0.2179, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2324, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2207, loss: 2.2299, grad_norm: 53.5845
2025-06-18 03:25:31,239 - mmdet - INFO - Epoch [2][6650/7033]	lr: 1.866e-04, eta: 19:45:18, time: 2.330, data_time: 0.060, memory: 20362, loss_cls: 0.0897, loss_bbox: 0.2235, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3702, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2725, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2483, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2259, loss: 2.2696, grad_norm: 34.4301
2025-06-18 03:27:25,229 - mmdet - INFO - Epoch [2][6700/7033]	lr: 1.866e-04, eta: 19:42:51, time: 2.280, data_time: 0.059, memory: 20362, loss_cls: 0.0804, loss_bbox: 0.2195, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3609, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2718, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2461, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2329, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2237, loss: 2.1958, grad_norm: 35.3841
2025-06-18 03:29:21,197 - mmdet - INFO - Epoch [2][6750/7033]	lr: 1.866e-04, eta: 19:40:29, time: 2.319, data_time: 0.056, memory: 20362, loss_cls: 0.0866, loss_bbox: 0.2289, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3655, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2737, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2532, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2425, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2325, loss: 2.2651, grad_norm: 36.3424
2025-06-18 03:31:15,368 - mmdet - INFO - Epoch [2][6800/7033]	lr: 1.866e-04, eta: 19:38:02, time: 2.283, data_time: 0.058, memory: 20362, loss_cls: 0.0856, loss_bbox: 0.2209, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3755, d1.loss_cls: 0.1251, d1.loss_bbox: 0.2670, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2437, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2233, loss: 2.2434, grad_norm: 53.2861
2025-06-18 03:33:12,069 - mmdet - INFO - Epoch [2][6850/7033]	lr: 1.866e-04, eta: 19:35:42, time: 2.334, data_time: 0.056, memory: 20362, loss_cls: 0.0916, loss_bbox: 0.2287, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3827, d1.loss_cls: 0.1283, d1.loss_bbox: 0.2790, d2.loss_cls: 0.1071, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0987, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2312, loss: 2.3212, grad_norm: 25.2739
2025-06-18 03:35:07,871 - mmdet - INFO - Epoch [2][6900/7033]	lr: 1.866e-04, eta: 19:33:19, time: 2.316, data_time: 0.055, memory: 20362, loss_cls: 0.0934, loss_bbox: 0.2356, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3781, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2831, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2618, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2478, d4.loss_cls: 0.0965, d4.loss_bbox: 0.2374, loss: 2.3706, grad_norm: 30.5243
2025-06-18 03:37:01,876 - mmdet - INFO - Epoch [2][6950/7033]	lr: 1.866e-04, eta: 19:30:54, time: 2.280, data_time: 0.054, memory: 20362, loss_cls: 0.0832, loss_bbox: 0.2215, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2686, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2418, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2211, loss: 2.2361, grad_norm: 27.6716
2025-06-18 03:38:59,816 - mmdet - INFO - Epoch [2][7000/7033]	lr: 1.866e-04, eta: 19:28:36, time: 2.359, data_time: 0.053, memory: 20362, loss_cls: 0.0880, loss_bbox: 0.2308, d0.loss_cls: 0.1922, d0.loss_bbox: 0.3739, d1.loss_cls: 0.1372, d1.loss_bbox: 0.2782, d2.loss_cls: 0.1108, d2.loss_bbox: 0.2530, d3.loss_cls: 0.0981, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2310, loss: 2.3272, grad_norm: 41.9629
2025-06-18 03:40:15,321 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-18 04:24:23,728 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-18 04:24:23,728 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7931, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8827, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9108, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9232, pts_bbox_NuScenes/car_trans_err: 0.1864, pts_bbox_NuScenes/car_scale_err: 0.1525, pts_bbox_NuScenes/car_orient_err: 0.0476, pts_bbox_NuScenes/car_vel_err: 0.3389, pts_bbox_NuScenes/car_attr_err: 0.1708, pts_bbox_NuScenes/mATE: 0.2940, pts_bbox_NuScenes/mASE: 0.2672, pts_bbox_NuScenes/mAOE: 0.2551, pts_bbox_NuScenes/mAVE: 0.2998, pts_bbox_NuScenes/mAAE: 0.1804, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4363, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6268, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7298, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7729, pts_bbox_NuScenes/truck_trans_err: 0.3338, pts_bbox_NuScenes/truck_scale_err: 0.2061, pts_bbox_NuScenes/truck_orient_err: 0.0561, pts_bbox_NuScenes/truck_vel_err: 0.2607, pts_bbox_NuScenes/truck_attr_err: 0.2203, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0567, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2054, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4010, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4708, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6548, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4380, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7945, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1056, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2763, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5207, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7489, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9020, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9256, pts_bbox_NuScenes/bus_trans_err: 0.3315, pts_bbox_NuScenes/bus_scale_err: 0.1884, pts_bbox_NuScenes/bus_orient_err: 0.0592, pts_bbox_NuScenes/bus_vel_err: 0.5460, pts_bbox_NuScenes/bus_attr_err: 0.2298, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1596, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4193, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5865, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6703, pts_bbox_NuScenes/trailer_trans_err: 0.4824, pts_bbox_NuScenes/trailer_scale_err: 0.2394, pts_bbox_NuScenes/trailer_orient_err: 0.4217, pts_bbox_NuScenes/trailer_vel_err: 0.2286, pts_bbox_NuScenes/trailer_attr_err: 0.1913, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5783, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6806, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7365, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7614, pts_bbox_NuScenes/barrier_trans_err: 0.2280, pts_bbox_NuScenes/barrier_scale_err: 0.2903, pts_bbox_NuScenes/barrier_orient_err: 0.0568, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6096, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7526, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7882, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7970, pts_bbox_NuScenes/motorcycle_trans_err: 0.2280, pts_bbox_NuScenes/motorcycle_scale_err: 0.2543, pts_bbox_NuScenes/motorcycle_orient_err: 0.2141, pts_bbox_NuScenes/motorcycle_vel_err: 0.4650, pts_bbox_NuScenes/motorcycle_attr_err: 0.2384, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5153, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5685, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5808, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5913, pts_bbox_NuScenes/bicycle_trans_err: 0.1836, pts_bbox_NuScenes/bicycle_scale_err: 0.2696, pts_bbox_NuScenes/bicycle_orient_err: 0.3316, pts_bbox_NuScenes/bicycle_vel_err: 0.2200, pts_bbox_NuScenes/bicycle_attr_err: 0.0081, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8032, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8520, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8771, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8909, pts_bbox_NuScenes/pedestrian_trans_err: 0.1619, pts_bbox_NuScenes/pedestrian_scale_err: 0.2961, pts_bbox_NuScenes/pedestrian_orient_err: 0.3140, pts_bbox_NuScenes/pedestrian_vel_err: 0.2334, pts_bbox_NuScenes/pedestrian_attr_err: 0.1083, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7091, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7514, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7782, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8060, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1497, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3374, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7025, pts_bbox_NuScenes/mAP: 0.6643
2025-06-18 04:26:11,264 - mmdet - INFO - Epoch [3][50/7033]	lr: 1.501e-04, eta: 19:21:43, time: 2.058, data_time: 0.176, memory: 20362, loss_cls: 0.0843, loss_bbox: 0.2202, d0.loss_cls: 0.1957, d0.loss_bbox: 0.3639, d1.loss_cls: 0.1358, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2436, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2225, loss: 2.2501, grad_norm: 31.8374
2025-06-18 04:27:54,075 - mmdet - INFO - Epoch [3][100/7033]	lr: 1.501e-04, eta: 19:18:57, time: 2.056, data_time: 0.055, memory: 20362, loss_cls: 0.0867, loss_bbox: 0.2226, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3701, d1.loss_cls: 0.1254, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2494, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2254, loss: 2.2662, grad_norm: 24.9378
2025-06-18 04:29:38,506 - mmdet - INFO - Epoch [3][150/7033]	lr: 1.501e-04, eta: 19:16:14, time: 2.088, data_time: 0.056, memory: 20362, loss_cls: 0.0748, loss_bbox: 0.2099, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3558, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2140, loss: 2.1125, grad_norm: 23.0347
2025-06-18 04:31:25,198 - mmdet - INFO - Epoch [3][200/7033]	lr: 1.501e-04, eta: 19:13:36, time: 2.134, data_time: 0.053, memory: 20362, loss_cls: 0.0875, loss_bbox: 0.2278, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3727, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2846, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2535, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2425, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2314, loss: 2.2822, grad_norm: 19.9877
2025-06-18 04:33:13,117 - mmdet - INFO - Epoch [3][250/7033]	lr: 1.501e-04, eta: 19:11:01, time: 2.158, data_time: 0.053, memory: 20362, loss_cls: 0.0790, loss_bbox: 0.2159, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2653, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2401, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2177, loss: 2.1382, grad_norm: 24.6601
2025-06-18 04:35:00,958 - mmdet - INFO - Epoch [3][300/7033]	lr: 1.501e-04, eta: 19:08:26, time: 2.157, data_time: 0.054, memory: 20362, loss_cls: 0.0793, loss_bbox: 0.2144, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3505, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2641, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2272, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2164, loss: 2.1526, grad_norm: 21.6015
2025-06-18 04:36:48,768 - mmdet - INFO - Epoch [3][350/7033]	lr: 1.501e-04, eta: 19:05:52, time: 2.156, data_time: 0.050, memory: 20362, loss_cls: 0.0783, loss_bbox: 0.2234, d0.loss_cls: 0.1772, d0.loss_bbox: 0.3549, d1.loss_cls: 0.1168, d1.loss_bbox: 0.2691, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2245, loss: 2.1836, grad_norm: 27.6773
2025-06-18 04:38:38,014 - mmdet - INFO - Epoch [3][400/7033]	lr: 1.501e-04, eta: 19:03:20, time: 2.185, data_time: 0.049, memory: 20362, loss_cls: 0.0826, loss_bbox: 0.2134, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1225, d1.loss_bbox: 0.2615, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2165, loss: 2.1610, grad_norm: 34.3752
2025-06-18 04:40:24,581 - mmdet - INFO - Epoch [3][450/7033]	lr: 1.501e-04, eta: 19:00:44, time: 2.131, data_time: 0.049, memory: 20362, loss_cls: 0.0894, loss_bbox: 0.2163, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3505, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1249, d2.loss_bbox: 0.2406, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2275, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2188, loss: 2.2446, grad_norm: 263.0483
2025-06-18 04:42:13,502 - mmdet - INFO - Epoch [3][500/7033]	lr: 1.501e-04, eta: 18:58:12, time: 2.178, data_time: 0.054, memory: 20362, loss_cls: 0.0896, loss_bbox: 0.2175, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2679, d2.loss_cls: 0.1094, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0921, d4.loss_bbox: 0.2207, loss: 2.2212, grad_norm: 28.4133
2025-06-18 04:44:01,564 - mmdet - INFO - Epoch [3][550/7033]	lr: 1.501e-04, eta: 18:55:40, time: 2.161, data_time: 0.056, memory: 20362, loss_cls: 0.0752, loss_bbox: 0.2145, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2664, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2275, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2183, loss: 2.1366, grad_norm: 19.0364
2025-06-18 04:45:51,873 - mmdet - INFO - Epoch [3][600/7033]	lr: 1.501e-04, eta: 18:53:11, time: 2.206, data_time: 0.049, memory: 20362, loss_cls: 0.0892, loss_bbox: 0.2273, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2722, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2501, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2301, loss: 2.2592, grad_norm: 29.8200
2025-06-18 04:47:41,718 - mmdet - INFO - Epoch [3][650/7033]	lr: 1.501e-04, eta: 18:50:42, time: 2.197, data_time: 0.050, memory: 20362, loss_cls: 0.0784, loss_bbox: 0.2133, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3442, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2384, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2241, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2158, loss: 2.1166, grad_norm: 50.6192
2025-06-18 04:49:30,137 - mmdet - INFO - Epoch [3][700/7033]	lr: 1.501e-04, eta: 18:48:11, time: 2.168, data_time: 0.051, memory: 20362, loss_cls: 0.0823, loss_bbox: 0.2218, d0.loss_cls: 0.1881, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2785, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2258, loss: 2.2387, grad_norm: 21.2338
2025-06-18 04:51:19,140 - mmdet - INFO - Epoch [3][750/7033]	lr: 1.501e-04, eta: 18:45:41, time: 2.180, data_time: 0.052, memory: 20362, loss_cls: 0.0831, loss_bbox: 0.2223, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3638, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2756, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2487, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2257, loss: 2.2197, grad_norm: 60.5644
2025-06-18 04:53:10,878 - mmdet - INFO - Epoch [3][800/7033]	lr: 1.501e-04, eta: 18:43:16, time: 2.235, data_time: 0.072, memory: 20362, loss_cls: 0.0853, loss_bbox: 0.2194, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2720, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2203, loss: 2.2194, grad_norm: 24.0432
2025-06-18 04:54:59,428 - mmdet - INFO - Epoch [3][850/7033]	lr: 1.501e-04, eta: 18:40:46, time: 2.171, data_time: 0.053, memory: 20362, loss_cls: 0.0762, loss_bbox: 0.2164, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3558, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2689, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2427, d3.loss_cls: 0.0830, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2209, loss: 2.1470, grad_norm: 21.3310
2025-06-18 04:56:50,264 - mmdet - INFO - Epoch [3][900/7033]	lr: 1.501e-04, eta: 18:38:20, time: 2.217, data_time: 0.049, memory: 20362, loss_cls: 0.0827, loss_bbox: 0.2235, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3769, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2756, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2268, loss: 2.2683, grad_norm: 23.9718
2025-06-18 04:58:41,272 - mmdet - INFO - Epoch [3][950/7033]	lr: 1.501e-04, eta: 18:35:55, time: 2.220, data_time: 0.050, memory: 20362, loss_cls: 0.0840, loss_bbox: 0.2177, d0.loss_cls: 0.1953, d0.loss_bbox: 0.3761, d1.loss_cls: 0.1307, d1.loss_bbox: 0.2760, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2205, loss: 2.2541, grad_norm: 33.1452
2025-06-18 05:00:31,667 - mmdet - INFO - Epoch [3][1000/7033]	lr: 1.501e-04, eta: 18:33:29, time: 2.208, data_time: 0.053, memory: 20362, loss_cls: 0.0780, loss_bbox: 0.2168, d0.loss_cls: 0.1866, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1237, d1.loss_bbox: 0.2660, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0850, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2194, loss: 2.1798, grad_norm: 42.7320
2025-06-18 05:02:23,497 - mmdet - INFO - Epoch [3][1050/7033]	lr: 1.501e-04, eta: 18:31:06, time: 2.237, data_time: 0.056, memory: 20362, loss_cls: 0.0699, loss_bbox: 0.2155, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2580, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2185, loss: 2.1229, grad_norm: 23.4999
2025-06-18 05:04:13,105 - mmdet - INFO - Epoch [3][1100/7033]	lr: 1.501e-04, eta: 18:28:38, time: 2.192, data_time: 0.055, memory: 20362, loss_cls: 0.0775, loss_bbox: 0.2164, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3653, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2699, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0890, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2199, loss: 2.1912, grad_norm: 22.4240
2025-06-18 05:06:05,494 - mmdet - INFO - Epoch [3][1150/7033]	lr: 1.501e-04, eta: 18:26:16, time: 2.248, data_time: 0.055, memory: 20362, loss_cls: 0.0855, loss_bbox: 0.2319, d0.loss_cls: 0.1973, d0.loss_bbox: 0.3718, d1.loss_cls: 0.1320, d1.loss_bbox: 0.2818, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2565, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2425, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2331, loss: 2.3207, grad_norm: 84.9724
2025-06-18 05:07:55,265 - mmdet - INFO - Epoch [3][1200/7033]	lr: 1.501e-04, eta: 18:23:50, time: 2.195, data_time: 0.054, memory: 20362, loss_cls: 0.0734, loss_bbox: 0.2168, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3671, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2673, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2208, loss: 2.1653, grad_norm: 24.9973
2025-06-18 05:09:45,540 - mmdet - INFO - Epoch [3][1250/7033]	lr: 1.501e-04, eta: 18:21:25, time: 2.205, data_time: 0.054, memory: 20362, loss_cls: 0.0805, loss_bbox: 0.2237, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3696, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2726, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2323, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2245, loss: 2.2299, grad_norm: 33.4948
2025-06-18 05:11:36,476 - mmdet - INFO - Epoch [3][1300/7033]	lr: 1.501e-04, eta: 18:19:01, time: 2.219, data_time: 0.052, memory: 20362, loss_cls: 0.0806, loss_bbox: 0.2154, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0820, d4.loss_bbox: 0.2172, loss: 2.1560, grad_norm: 26.8746
2025-06-18 05:13:24,777 - mmdet - INFO - Epoch [3][1350/7033]	lr: 1.501e-04, eta: 18:16:33, time: 2.166, data_time: 0.053, memory: 20362, loss_cls: 0.0824, loss_bbox: 0.2173, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2649, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2260, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2186, loss: 2.1763, grad_norm: 29.4906
2025-06-18 05:15:15,926 - mmdet - INFO - Epoch [3][1400/7033]	lr: 1.501e-04, eta: 18:14:10, time: 2.223, data_time: 0.064, memory: 20362, loss_cls: 0.0806, loss_bbox: 0.2158, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3610, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2638, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0828, d4.loss_bbox: 0.2197, loss: 2.1758, grad_norm: 27.9313
2025-06-18 05:17:06,691 - mmdet - INFO - Epoch [3][1450/7033]	lr: 1.501e-04, eta: 18:11:46, time: 2.215, data_time: 0.055, memory: 20362, loss_cls: 0.0879, loss_bbox: 0.2185, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3706, d1.loss_cls: 0.1187, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2435, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2222, loss: 2.2320, grad_norm: 19.7965
2025-06-18 05:18:56,741 - mmdet - INFO - Epoch [3][1500/7033]	lr: 1.501e-04, eta: 18:09:22, time: 2.201, data_time: 0.053, memory: 20362, loss_cls: 0.0814, loss_bbox: 0.2158, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2366, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2175, loss: 2.1559, grad_norm: 26.0534
2025-06-18 05:20:48,716 - mmdet - INFO - Epoch [3][1550/7033]	lr: 1.501e-04, eta: 18:07:01, time: 2.240, data_time: 0.052, memory: 20362, loss_cls: 0.0807, loss_bbox: 0.2132, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2638, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2157, loss: 2.1359, grad_norm: 34.5187
2025-06-18 05:22:37,354 - mmdet - INFO - Epoch [3][1600/7033]	lr: 1.501e-04, eta: 18:04:34, time: 2.173, data_time: 0.054, memory: 20362, loss_cls: 0.0794, loss_bbox: 0.2167, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3540, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2625, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2367, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2192, loss: 2.1387, grad_norm: 19.8912
2025-06-18 05:24:28,927 - mmdet - INFO - Epoch [3][1650/7033]	lr: 1.501e-04, eta: 18:02:13, time: 2.231, data_time: 0.057, memory: 20362, loss_cls: 0.0783, loss_bbox: 0.2133, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3540, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2613, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2166, loss: 2.1337, grad_norm: 20.2574
2025-06-18 05:26:19,844 - mmdet - INFO - Epoch [3][1700/7033]	lr: 1.501e-04, eta: 17:59:51, time: 2.218, data_time: 0.055, memory: 20362, loss_cls: 0.0712, loss_bbox: 0.2026, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2141, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2044, loss: 2.0316, grad_norm: 106.9593
2025-06-18 05:28:09,071 - mmdet - INFO - Epoch [3][1750/7033]	lr: 1.501e-04, eta: 17:57:26, time: 2.185, data_time: 0.054, memory: 20362, loss_cls: 0.0737, loss_bbox: 0.2083, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3474, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2558, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2109, loss: 2.0852, grad_norm: 43.1352
2025-06-18 05:29:59,375 - mmdet - INFO - Epoch [3][1800/7033]	lr: 1.501e-04, eta: 17:55:03, time: 2.206, data_time: 0.052, memory: 20362, loss_cls: 0.0763, loss_bbox: 0.2111, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3420, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2553, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2132, loss: 2.1013, grad_norm: 17.0644
2025-06-18 05:31:50,379 - mmdet - INFO - Epoch [3][1850/7033]	lr: 1.501e-04, eta: 17:52:42, time: 2.220, data_time: 0.054, memory: 20362, loss_cls: 0.0787, loss_bbox: 0.2125, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2576, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2331, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2156, loss: 2.1251, grad_norm: 171.7292
2025-06-18 05:33:40,326 - mmdet - INFO - Epoch [3][1900/7033]	lr: 1.501e-04, eta: 17:50:19, time: 2.199, data_time: 0.056, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2209, d0.loss_cls: 0.1843, d0.loss_bbox: 0.3691, d1.loss_cls: 0.1226, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2227, loss: 2.2264, grad_norm: 23.1818
2025-06-18 05:35:30,154 - mmdet - INFO - Epoch [3][1950/7033]	lr: 1.501e-04, eta: 17:47:56, time: 2.197, data_time: 0.054, memory: 20362, loss_cls: 0.0815, loss_bbox: 0.2214, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3584, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2656, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2233, loss: 2.2027, grad_norm: 34.2541
2025-06-18 05:37:18,816 - mmdet - INFO - Epoch [3][2000/7033]	lr: 1.501e-04, eta: 17:45:31, time: 2.173, data_time: 0.055, memory: 20362, loss_cls: 0.0815, loss_bbox: 0.2124, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2602, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0841, d4.loss_bbox: 0.2156, loss: 2.1465, grad_norm: 61.1143
2025-06-18 05:39:10,849 - mmdet - INFO - Epoch [3][2050/7033]	lr: 1.501e-04, eta: 17:43:12, time: 2.241, data_time: 0.059, memory: 20362, loss_cls: 0.0720, loss_bbox: 0.2110, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2115, loss: 2.0739, grad_norm: 23.0239
2025-06-18 05:40:59,846 - mmdet - INFO - Epoch [3][2100/7033]	lr: 1.501e-04, eta: 17:40:49, time: 2.180, data_time: 0.056, memory: 20362, loss_cls: 0.0835, loss_bbox: 0.2222, d0.loss_cls: 0.1820, d0.loss_bbox: 0.3645, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0846, d4.loss_bbox: 0.2246, loss: 2.2264, grad_norm: 22.4890
2025-06-18 05:42:48,943 - mmdet - INFO - Epoch [3][2150/7033]	lr: 1.501e-04, eta: 17:38:25, time: 2.182, data_time: 0.052, memory: 20362, loss_cls: 0.0761, loss_bbox: 0.2103, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3434, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2107, loss: 2.0803, grad_norm: 16.6077
2025-06-18 05:44:39,396 - mmdet - INFO - Epoch [3][2200/7033]	lr: 1.501e-04, eta: 17:36:04, time: 2.209, data_time: 0.057, memory: 20362, loss_cls: 0.0824, loss_bbox: 0.2102, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3474, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2581, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2133, loss: 2.1258, grad_norm: 26.4192
2025-06-18 05:46:28,899 - mmdet - INFO - Epoch [3][2250/7033]	lr: 1.501e-04, eta: 17:33:42, time: 2.190, data_time: 0.056, memory: 20362, loss_cls: 0.0792, loss_bbox: 0.2139, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3530, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2634, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2166, loss: 2.1416, grad_norm: 20.4509
2025-06-18 05:48:30,166 - mmdet - INFO - Epoch [3][2300/7033]	lr: 1.501e-04, eta: 17:31:39, time: 2.425, data_time: 0.059, memory: 20362, loss_cls: 0.0825, loss_bbox: 0.2190, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3551, d1.loss_cls: 0.1172, d1.loss_bbox: 0.2612, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2289, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2200, loss: 2.1751, grad_norm: 37.3355
2025-06-18 05:50:18,807 - mmdet - INFO - Epoch [3][2350/7033]	lr: 1.501e-04, eta: 17:29:15, time: 2.173, data_time: 0.058, memory: 20362, loss_cls: 0.0808, loss_bbox: 0.2320, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3651, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2731, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2383, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2324, loss: 2.2462, grad_norm: 41.8156
2025-06-18 05:52:10,110 - mmdet - INFO - Epoch [3][2400/7033]	lr: 1.501e-04, eta: 17:26:56, time: 2.226, data_time: 0.057, memory: 20362, loss_cls: 0.0803, loss_bbox: 0.2300, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3648, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2743, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2496, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2374, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2313, loss: 2.2205, grad_norm: 26.6968
2025-06-18 05:53:59,243 - mmdet - INFO - Epoch [3][2450/7033]	lr: 1.501e-04, eta: 17:24:34, time: 2.182, data_time: 0.057, memory: 20362, loss_cls: 0.0752, loss_bbox: 0.2142, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2583, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2234, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2151, loss: 2.0983, grad_norm: 25.8042
2025-06-18 05:55:47,791 - mmdet - INFO - Epoch [3][2500/7033]	lr: 1.501e-04, eta: 17:22:11, time: 2.171, data_time: 0.057, memory: 20362, loss_cls: 0.0767, loss_bbox: 0.2159, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2173, loss: 2.1284, grad_norm: 108.2557
2025-06-18 05:57:37,861 - mmdet - INFO - Epoch [3][2550/7033]	lr: 1.501e-04, eta: 17:19:51, time: 2.201, data_time: 0.057, memory: 20362, loss_cls: 0.0763, loss_bbox: 0.2215, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2650, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2229, loss: 2.1481, grad_norm: 20.5884
2025-06-18 05:59:27,373 - mmdet - INFO - Epoch [3][2600/7033]	lr: 1.501e-04, eta: 17:17:30, time: 2.191, data_time: 0.059, memory: 20362, loss_cls: 0.0800, loss_bbox: 0.2257, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2439, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2265, loss: 2.1647, grad_norm: 23.1036
2025-06-18 06:01:17,267 - mmdet - INFO - Epoch [3][2650/7033]	lr: 1.501e-04, eta: 17:15:10, time: 2.198, data_time: 0.056, memory: 20362, loss_cls: 0.0793, loss_bbox: 0.2176, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0987, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2177, loss: 2.1397, grad_norm: 42.4999
2025-06-18 06:03:06,473 - mmdet - INFO - Epoch [3][2700/7033]	lr: 1.501e-04, eta: 17:12:49, time: 2.184, data_time: 0.058, memory: 20362, loss_cls: 0.0832, loss_bbox: 0.2271, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3554, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2756, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2366, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2267, loss: 2.2186, grad_norm: 30.1397
2025-06-18 06:04:57,896 - mmdet - INFO - Epoch [3][2750/7033]	lr: 1.501e-04, eta: 17:10:31, time: 2.228, data_time: 0.060, memory: 20362, loss_cls: 0.0807, loss_bbox: 0.2127, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3433, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2382, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2159, loss: 2.1271, grad_norm: 33.3959
2025-06-18 06:06:46,868 - mmdet - INFO - Epoch [3][2800/7033]	lr: 1.501e-04, eta: 17:08:10, time: 2.180, data_time: 0.059, memory: 20362, loss_cls: 0.0837, loss_bbox: 0.2207, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2718, d2.loss_cls: 0.0987, d2.loss_bbox: 0.2480, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2244, loss: 2.2129, grad_norm: 24.1188
2025-06-18 06:08:37,148 - mmdet - INFO - Epoch [3][2850/7033]	lr: 1.501e-04, eta: 17:05:51, time: 2.205, data_time: 0.057, memory: 20362, loss_cls: 0.0835, loss_bbox: 0.2202, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2695, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2331, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2255, loss: 2.2103, grad_norm: 30.6888
2025-06-18 06:10:28,386 - mmdet - INFO - Epoch [3][2900/7033]	lr: 1.501e-04, eta: 17:03:34, time: 2.225, data_time: 0.077, memory: 20362, loss_cls: 0.0775, loss_bbox: 0.2144, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2607, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2167, loss: 2.1269, grad_norm: 25.8348
2025-06-18 06:12:19,065 - mmdet - INFO - Epoch [3][2950/7033]	lr: 1.501e-04, eta: 17:01:16, time: 2.214, data_time: 0.060, memory: 20362, loss_cls: 0.0773, loss_bbox: 0.2103, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2599, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2128, loss: 2.1084, grad_norm: 27.6869
2025-06-18 06:14:07,818 - mmdet - INFO - Epoch [3][3000/7033]	lr: 1.501e-04, eta: 16:58:55, time: 2.175, data_time: 0.057, memory: 20362, loss_cls: 0.0776, loss_bbox: 0.2154, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3561, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2678, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2404, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2189, loss: 2.1609, grad_norm: 20.4508
2025-06-18 06:15:59,983 - mmdet - INFO - Epoch [3][3050/7033]	lr: 1.501e-04, eta: 16:56:40, time: 2.243, data_time: 0.052, memory: 20362, loss_cls: 0.0824, loss_bbox: 0.2255, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3632, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2744, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2454, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2266, loss: 2.2206, grad_norm: 39.8117
2025-06-18 06:17:49,209 - mmdet - INFO - Epoch [3][3100/7033]	lr: 1.501e-04, eta: 16:54:20, time: 2.184, data_time: 0.058, memory: 20362, loss_cls: 0.0836, loss_bbox: 0.2160, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3567, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2646, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0924, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2201, loss: 2.1776, grad_norm: 26.8751
2025-06-18 06:19:38,930 - mmdet - INFO - Epoch [3][3150/7033]	lr: 1.501e-04, eta: 16:52:01, time: 2.194, data_time: 0.059, memory: 20362, loss_cls: 0.0794, loss_bbox: 0.2125, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3459, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2583, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2161, loss: 2.1117, grad_norm: 24.8758
2025-06-18 06:21:29,258 - mmdet - INFO - Epoch [3][3200/7033]	lr: 1.501e-04, eta: 16:49:43, time: 2.207, data_time: 0.057, memory: 20362, loss_cls: 0.0837, loss_bbox: 0.2204, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3671, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2762, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2483, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2246, loss: 2.2117, grad_norm: 24.6019
2025-06-18 06:23:20,298 - mmdet - INFO - Epoch [3][3250/7033]	lr: 1.501e-04, eta: 16:47:27, time: 2.221, data_time: 0.055, memory: 20362, loss_cls: 0.0799, loss_bbox: 0.2100, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2134, loss: 2.1126, grad_norm: 127.5338
2025-06-18 06:25:11,265 - mmdet - INFO - Epoch [3][3300/7033]	lr: 1.501e-04, eta: 16:45:10, time: 2.219, data_time: 0.078, memory: 20362, loss_cls: 0.0832, loss_bbox: 0.2186, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3640, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2676, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2320, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2216, loss: 2.1917, grad_norm: 52.1495
2025-06-18 06:27:04,288 - mmdet - INFO - Epoch [3][3350/7033]	lr: 1.501e-04, eta: 16:42:57, time: 2.260, data_time: 0.055, memory: 20362, loss_cls: 0.0788, loss_bbox: 0.2235, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2714, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2261, loss: 2.1852, grad_norm: 24.0450
2025-06-18 06:28:53,570 - mmdet - INFO - Epoch [3][3400/7033]	lr: 1.501e-04, eta: 16:40:38, time: 2.186, data_time: 0.056, memory: 20362, loss_cls: 0.0770, loss_bbox: 0.2118, d0.loss_cls: 0.1707, d0.loss_bbox: 0.3596, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2626, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2397, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2138, loss: 2.1243, grad_norm: 24.8865
2025-06-18 06:30:49,137 - mmdet - INFO - Epoch [3][3450/7033]	lr: 1.501e-04, eta: 16:38:29, time: 2.311, data_time: 0.057, memory: 20362, loss_cls: 0.0837, loss_bbox: 0.2239, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3638, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2687, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2261, loss: 2.2086, grad_norm: 35.3264
2025-06-18 06:32:38,909 - mmdet - INFO - Epoch [3][3500/7033]	lr: 1.501e-04, eta: 16:36:11, time: 2.195, data_time: 0.057, memory: 20362, loss_cls: 0.0787, loss_bbox: 0.2115, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2587, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2136, loss: 2.1058, grad_norm: 46.7595
2025-06-18 06:34:28,085 - mmdet - INFO - Epoch [3][3550/7033]	lr: 1.501e-04, eta: 16:33:53, time: 2.183, data_time: 0.057, memory: 20362, loss_cls: 0.0749, loss_bbox: 0.2119, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3459, d1.loss_cls: 0.1102, d1.loss_bbox: 0.2557, d2.loss_cls: 0.0908, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2139, loss: 2.0874, grad_norm: 28.4786
2025-06-18 06:36:19,756 - mmdet - INFO - Epoch [3][3600/7033]	lr: 1.501e-04, eta: 16:31:38, time: 2.233, data_time: 0.065, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2167, d0.loss_cls: 0.1791, d0.loss_bbox: 0.3519, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2588, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2185, loss: 2.1567, grad_norm: 27.2586
2025-06-18 06:38:10,433 - mmdet - INFO - Epoch [3][3650/7033]	lr: 1.501e-04, eta: 16:29:22, time: 2.213, data_time: 0.060, memory: 20362, loss_cls: 0.0783, loss_bbox: 0.2238, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2673, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2248, loss: 2.1899, grad_norm: 29.4414
2025-06-18 06:40:03,569 - mmdet - INFO - Epoch [3][3700/7033]	lr: 1.501e-04, eta: 16:27:10, time: 2.263, data_time: 0.061, memory: 20362, loss_cls: 0.0954, loss_bbox: 0.2183, d0.loss_cls: 0.1941, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2616, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2412, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2214, loss: 2.2335, grad_norm: 21.2177
2025-06-18 06:41:56,793 - mmdet - INFO - Epoch [3][3750/7033]	lr: 1.501e-04, eta: 16:24:58, time: 2.264, data_time: 0.057, memory: 20362, loss_cls: 0.0852, loss_bbox: 0.2328, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3631, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2711, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2497, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2427, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2390, loss: 2.2592, grad_norm: 46.2163
2025-06-18 06:43:50,285 - mmdet - INFO - Epoch [3][3800/7033]	lr: 1.501e-04, eta: 16:22:46, time: 2.270, data_time: 0.056, memory: 20362, loss_cls: 0.0790, loss_bbox: 0.2290, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2415, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2307, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2259, loss: 2.1646, grad_norm: 26.7764
2025-06-18 06:45:42,136 - mmdet - INFO - Epoch [3][3850/7033]	lr: 1.501e-04, eta: 16:20:32, time: 2.237, data_time: 0.059, memory: 20362, loss_cls: 0.0855, loss_bbox: 0.2253, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2652, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2446, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2264, loss: 2.2212, grad_norm: 31.2407
2025-06-18 06:47:35,457 - mmdet - INFO - Epoch [3][3900/7033]	lr: 1.501e-04, eta: 16:18:21, time: 2.266, data_time: 0.058, memory: 20362, loss_cls: 0.0784, loss_bbox: 0.2163, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2346, d3.loss_cls: 0.0830, d3.loss_bbox: 0.2276, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2173, loss: 2.1401, grad_norm: 28.6715
2025-06-18 06:49:32,336 - mmdet - INFO - Epoch [3][3950/7033]	lr: 1.501e-04, eta: 16:16:14, time: 2.338, data_time: 0.065, memory: 20362, loss_cls: 0.0805, loss_bbox: 0.2211, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2415, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2232, loss: 2.1917, grad_norm: 69.3028
2025-06-18 06:51:25,759 - mmdet - INFO - Epoch [3][4000/7033]	lr: 1.501e-04, eta: 16:14:02, time: 2.268, data_time: 0.056, memory: 20362, loss_cls: 0.0781, loss_bbox: 0.2091, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3497, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2546, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2128, loss: 2.1097, grad_norm: 19.8440
2025-06-18 06:53:19,913 - mmdet - INFO - Epoch [3][4050/7033]	lr: 1.501e-04, eta: 16:11:52, time: 2.283, data_time: 0.056, memory: 20362, loss_cls: 0.0737, loss_bbox: 0.2019, d0.loss_cls: 0.1824, d0.loss_bbox: 0.3362, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2050, loss: 2.0566, grad_norm: 34.7108
2025-06-18 06:55:14,086 - mmdet - INFO - Epoch [3][4100/7033]	lr: 1.501e-04, eta: 16:09:42, time: 2.284, data_time: 0.056, memory: 20362, loss_cls: 0.0807, loss_bbox: 0.2151, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2630, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2176, loss: 2.1688, grad_norm: 35.2227
2025-06-18 06:57:08,288 - mmdet - INFO - Epoch [3][4150/7033]	lr: 1.501e-04, eta: 16:07:32, time: 2.284, data_time: 0.053, memory: 20362, loss_cls: 0.0704, loss_bbox: 0.2144, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2583, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2176, loss: 2.0738, grad_norm: 20.5717
2025-06-18 06:59:03,855 - mmdet - INFO - Epoch [3][4200/7033]	lr: 1.501e-04, eta: 16:05:24, time: 2.311, data_time: 0.089, memory: 20362, loss_cls: 0.0780, loss_bbox: 0.2127, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3483, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2576, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2141, loss: 2.1125, grad_norm: 42.3791
2025-06-18 07:00:58,248 - mmdet - INFO - Epoch [3][4250/7033]	lr: 1.501e-04, eta: 16:03:14, time: 2.288, data_time: 0.054, memory: 20362, loss_cls: 0.0856, loss_bbox: 0.2259, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2716, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2481, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2275, loss: 2.2571, grad_norm: 30.5568
2025-06-18 07:02:53,239 - mmdet - INFO - Epoch [3][4300/7033]	lr: 1.501e-04, eta: 16:01:05, time: 2.300, data_time: 0.058, memory: 20362, loss_cls: 0.0861, loss_bbox: 0.2278, d0.loss_cls: 0.1804, d0.loss_bbox: 0.3749, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2796, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2517, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2387, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2306, loss: 2.2696, grad_norm: 28.7221
2025-06-18 07:04:50,421 - mmdet - INFO - Epoch [3][4350/7033]	lr: 1.501e-04, eta: 15:58:59, time: 2.344, data_time: 0.059, memory: 20362, loss_cls: 0.0824, loss_bbox: 0.2181, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2220, loss: 2.1942, grad_norm: 26.0990
2025-06-18 07:06:46,718 - mmdet - INFO - Epoch [3][4400/7033]	lr: 1.501e-04, eta: 15:56:52, time: 2.326, data_time: 0.075, memory: 20362, loss_cls: 0.0755, loss_bbox: 0.2171, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3480, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2184, loss: 2.1117, grad_norm: 41.2536
2025-06-18 07:08:42,532 - mmdet - INFO - Epoch [3][4450/7033]	lr: 1.501e-04, eta: 15:54:45, time: 2.316, data_time: 0.054, memory: 20362, loss_cls: 0.0712, loss_bbox: 0.2110, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3459, d1.loss_cls: 0.1063, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2135, loss: 2.0519, grad_norm: 26.1363
2025-06-18 07:10:38,048 - mmdet - INFO - Epoch [3][4500/7033]	lr: 1.501e-04, eta: 15:52:37, time: 2.310, data_time: 0.054, memory: 20362, loss_cls: 0.0789, loss_bbox: 0.2140, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2608, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2185, loss: 2.1472, grad_norm: 24.2871
2025-06-18 07:12:35,237 - mmdet - INFO - Epoch [3][4550/7033]	lr: 1.501e-04, eta: 15:50:31, time: 2.344, data_time: 0.055, memory: 20362, loss_cls: 0.0787, loss_bbox: 0.2211, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3519, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2652, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2231, loss: 2.1495, grad_norm: 22.6079
2025-06-18 07:14:34,882 - mmdet - INFO - Epoch [3][4600/7033]	lr: 1.501e-04, eta: 15:48:29, time: 2.393, data_time: 0.058, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.2160, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2579, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2341, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2169, loss: 2.0738, grad_norm: 31.5388
2025-06-18 07:16:32,120 - mmdet - INFO - Epoch [3][4650/7033]	lr: 1.501e-04, eta: 15:46:23, time: 2.345, data_time: 0.055, memory: 20362, loss_cls: 0.0847, loss_bbox: 0.2162, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3510, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2411, d3.loss_cls: 0.0916, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2190, loss: 2.1824, grad_norm: 32.1387
2025-06-18 07:18:30,327 - mmdet - INFO - Epoch [3][4700/7033]	lr: 1.501e-04, eta: 15:44:19, time: 2.364, data_time: 0.053, memory: 20362, loss_cls: 0.0806, loss_bbox: 0.2201, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3503, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2657, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2418, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2283, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2224, loss: 2.1638, grad_norm: 31.7050
2025-06-18 07:20:27,409 - mmdet - INFO - Epoch [3][4750/7033]	lr: 1.501e-04, eta: 15:42:13, time: 2.342, data_time: 0.049, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.2118, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2544, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2136, loss: 2.0845, grad_norm: 20.7285
2025-06-18 07:22:26,090 - mmdet - INFO - Epoch [3][4800/7033]	lr: 1.501e-04, eta: 15:40:10, time: 2.374, data_time: 0.052, memory: 20362, loss_cls: 0.0832, loss_bbox: 0.2164, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2186, loss: 2.1503, grad_norm: 23.7299
2025-06-18 07:24:24,256 - mmdet - INFO - Epoch [3][4850/7033]	lr: 1.501e-04, eta: 15:38:05, time: 2.363, data_time: 0.051, memory: 20362, loss_cls: 0.0734, loss_bbox: 0.2029, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3277, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2060, loss: 2.0220, grad_norm: 24.7886
2025-06-18 07:26:23,512 - mmdet - INFO - Epoch [3][4900/7033]	lr: 1.501e-04, eta: 15:36:03, time: 2.385, data_time: 0.059, memory: 20362, loss_cls: 0.0783, loss_bbox: 0.2165, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2603, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2369, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2260, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2180, loss: 2.1242, grad_norm: 23.8443
2025-06-18 07:28:23,065 - mmdet - INFO - Epoch [3][4950/7033]	lr: 1.501e-04, eta: 15:34:00, time: 2.391, data_time: 0.053, memory: 20362, loss_cls: 0.0733, loss_bbox: 0.2047, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3350, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2068, loss: 2.0172, grad_norm: 110.6809
2025-06-18 07:30:22,705 - mmdet - INFO - Epoch [3][5000/7033]	lr: 1.501e-04, eta: 15:31:58, time: 2.393, data_time: 0.052, memory: 20362, loss_cls: 0.0771, loss_bbox: 0.2188, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3376, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2211, loss: 2.1175, grad_norm: 22.3169
2025-06-18 07:32:20,747 - mmdet - INFO - Epoch [3][5050/7033]	lr: 1.501e-04, eta: 15:29:53, time: 2.361, data_time: 0.057, memory: 20362, loss_cls: 0.0869, loss_bbox: 0.2193, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3474, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2607, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2207, loss: 2.1876, grad_norm: 25.2175
2025-06-18 07:34:24,607 - mmdet - INFO - Epoch [3][5100/7033]	lr: 1.501e-04, eta: 15:27:56, time: 2.477, data_time: 0.051, memory: 20362, loss_cls: 0.0712, loss_bbox: 0.2051, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2062, loss: 2.0213, grad_norm: 48.6554
2025-06-18 07:36:24,940 - mmdet - INFO - Epoch [3][5150/7033]	lr: 1.501e-04, eta: 15:25:55, time: 2.407, data_time: 0.051, memory: 20362, loss_cls: 0.0738, loss_bbox: 0.2170, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2189, loss: 2.0940, grad_norm: 21.1473
2025-06-18 07:38:27,126 - mmdet - INFO - Epoch [3][5200/7033]	lr: 1.501e-04, eta: 15:23:55, time: 2.444, data_time: 0.075, memory: 20362, loss_cls: 0.0752, loss_bbox: 0.2114, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2122, loss: 2.0847, grad_norm: 22.5708
2025-06-18 07:40:27,899 - mmdet - INFO - Epoch [3][5250/7033]	lr: 1.501e-04, eta: 15:21:54, time: 2.415, data_time: 0.054, memory: 20362, loss_cls: 0.0772, loss_bbox: 0.2152, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3428, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2553, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2345, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2169, loss: 2.1058, grad_norm: 18.9710
2025-06-18 07:42:30,329 - mmdet - INFO - Epoch [3][5300/7033]	lr: 1.501e-04, eta: 15:19:55, time: 2.449, data_time: 0.056, memory: 20362, loss_cls: 0.0742, loss_bbox: 0.2231, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2201, loss: 2.1182, grad_norm: 19.5608
2025-06-18 07:44:34,810 - mmdet - INFO - Epoch [3][5350/7033]	lr: 1.501e-04, eta: 15:17:59, time: 2.490, data_time: 0.058, memory: 20362, loss_cls: 0.0743, loss_bbox: 0.2092, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3378, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2084, loss: 2.0348, grad_norm: 514.6188
2025-06-18 07:46:42,771 - mmdet - INFO - Epoch [3][5400/7033]	lr: 1.501e-04, eta: 15:16:06, time: 2.559, data_time: 0.050, memory: 20362, loss_cls: 0.0816, loss_bbox: 0.2171, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2579, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2194, loss: 2.1402, grad_norm: 22.3745
2025-06-18 07:48:46,324 - mmdet - INFO - Epoch [3][5450/7033]	lr: 1.501e-04, eta: 15:14:08, time: 2.471, data_time: 0.061, memory: 20362, loss_cls: 0.0869, loss_bbox: 0.2146, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2595, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0939, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2181, loss: 2.1654, grad_norm: 27.4171
2025-06-18 07:50:50,382 - mmdet - INFO - Epoch [3][5500/7033]	lr: 1.501e-04, eta: 15:12:11, time: 2.482, data_time: 0.061, memory: 20362, loss_cls: 0.0749, loss_bbox: 0.2110, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2558, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2132, loss: 2.0787, grad_norm: 20.9634
2025-06-18 07:52:54,040 - mmdet - INFO - Epoch [3][5550/7033]	lr: 1.501e-04, eta: 15:10:13, time: 2.473, data_time: 0.058, memory: 20362, loss_cls: 0.0702, loss_bbox: 0.2071, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3302, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2481, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2094, loss: 2.0127, grad_norm: 27.9167
2025-06-18 07:54:57,577 - mmdet - INFO - Epoch [3][5600/7033]	lr: 1.501e-04, eta: 15:08:15, time: 2.471, data_time: 0.057, memory: 20362, loss_cls: 0.0707, loss_bbox: 0.2061, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2550, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2079, loss: 2.0476, grad_norm: 20.4176
2025-06-18 07:57:01,966 - mmdet - INFO - Epoch [3][5650/7033]	lr: 1.501e-04, eta: 15:06:18, time: 2.488, data_time: 0.059, memory: 20362, loss_cls: 0.0721, loss_bbox: 0.2087, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3425, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2114, loss: 2.0530, grad_norm: 26.5777
2025-06-18 07:59:10,694 - mmdet - INFO - Epoch [3][5700/7033]	lr: 1.501e-04, eta: 15:04:26, time: 2.575, data_time: 0.059, memory: 20362, loss_cls: 0.0769, loss_bbox: 0.2122, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3421, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2571, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2312, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2144, loss: 2.0807, grad_norm: 22.4466
2025-06-18 08:01:17,943 - mmdet - INFO - Epoch [3][5750/7033]	lr: 1.501e-04, eta: 15:02:32, time: 2.545, data_time: 0.059, memory: 20362, loss_cls: 0.0812, loss_bbox: 0.2184, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2344, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2272, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2196, loss: 2.1451, grad_norm: 78.2111
2025-06-18 08:03:28,763 - mmdet - INFO - Epoch [3][5800/7033]	lr: 1.501e-04, eta: 15:00:42, time: 2.616, data_time: 0.058, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2136, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3620, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2179, loss: 2.1713, grad_norm: 32.7885
2025-06-18 08:05:36,900 - mmdet - INFO - Epoch [3][5850/7033]	lr: 1.501e-04, eta: 14:58:49, time: 2.563, data_time: 0.056, memory: 20362, loss_cls: 0.0814, loss_bbox: 0.2162, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3701, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2732, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2198, loss: 2.1974, grad_norm: 32.9263
2025-06-18 08:07:45,544 - mmdet - INFO - Epoch [3][5900/7033]	lr: 1.501e-04, eta: 14:56:57, time: 2.573, data_time: 0.057, memory: 20362, loss_cls: 0.0823, loss_bbox: 0.2271, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2774, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2388, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2302, loss: 2.2525, grad_norm: 20.7995
2025-06-18 08:09:56,690 - mmdet - INFO - Epoch [3][5950/7033]	lr: 1.501e-04, eta: 14:55:07, time: 2.623, data_time: 0.057, memory: 20362, loss_cls: 0.0810, loss_bbox: 0.2169, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3627, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2644, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2367, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2216, loss: 2.1828, grad_norm: 35.8684
2025-06-18 08:12:04,900 - mmdet - INFO - Epoch [3][6000/7033]	lr: 1.501e-04, eta: 14:53:14, time: 2.564, data_time: 0.053, memory: 20362, loss_cls: 0.0861, loss_bbox: 0.2243, d0.loss_cls: 0.1854, d0.loss_bbox: 0.3581, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2673, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2267, loss: 2.2301, grad_norm: 27.0754
2025-06-18 08:14:09,280 - mmdet - INFO - Epoch [3][6050/7033]	lr: 1.501e-04, eta: 14:51:16, time: 2.488, data_time: 0.063, memory: 20362, loss_cls: 0.0879, loss_bbox: 0.2260, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2602, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2382, d3.loss_cls: 0.0951, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2259, loss: 2.1996, grad_norm: 53.5152
2025-06-18 08:16:12,971 - mmdet - INFO - Epoch [3][6100/7033]	lr: 1.501e-04, eta: 14:49:18, time: 2.474, data_time: 0.057, memory: 20362, loss_cls: 0.0799, loss_bbox: 0.2131, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2140, loss: 2.1123, grad_norm: 21.2798
2025-06-18 08:18:13,182 - mmdet - INFO - Epoch [3][6150/7033]	lr: 1.501e-04, eta: 14:47:16, time: 2.404, data_time: 0.062, memory: 20362, loss_cls: 0.0854, loss_bbox: 0.2238, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3581, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2676, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2342, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2267, loss: 2.2004, grad_norm: 23.9203
2025-06-18 08:20:16,340 - mmdet - INFO - Epoch [3][6200/7033]	lr: 1.501e-04, eta: 14:45:17, time: 2.463, data_time: 0.057, memory: 20362, loss_cls: 0.0835, loss_bbox: 0.2168, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2598, d2.loss_cls: 0.1008, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2275, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2196, loss: 2.1767, grad_norm: 37.0551
2025-06-18 08:22:22,046 - mmdet - INFO - Epoch [3][6250/7033]	lr: 1.501e-04, eta: 14:43:21, time: 2.515, data_time: 0.053, memory: 20362, loss_cls: 0.0790, loss_bbox: 0.2204, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3542, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2669, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2419, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2250, loss: 2.1694, grad_norm: 29.4786
2025-06-18 08:24:25,678 - mmdet - INFO - Epoch [3][6300/7033]	lr: 1.501e-04, eta: 14:41:23, time: 2.473, data_time: 0.060, memory: 20362, loss_cls: 0.0728, loss_bbox: 0.2161, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3394, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2269, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2182, loss: 2.0897, grad_norm: 80.2015
2025-06-18 08:26:29,735 - mmdet - INFO - Epoch [3][6350/7033]	lr: 1.501e-04, eta: 14:39:25, time: 2.481, data_time: 0.060, memory: 20362, loss_cls: 0.0811, loss_bbox: 0.2293, d0.loss_cls: 0.1831, d0.loss_bbox: 0.3560, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2663, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2305, loss: 2.1994, grad_norm: 35.8242
2025-06-18 08:28:38,495 - mmdet - INFO - Epoch [3][6400/7033]	lr: 1.501e-04, eta: 14:37:32, time: 2.576, data_time: 0.059, memory: 20362, loss_cls: 0.0759, loss_bbox: 0.2125, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2576, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2317, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2171, loss: 2.1030, grad_norm: 28.0036
2025-06-18 08:30:42,087 - mmdet - INFO - Epoch [3][6450/7033]	lr: 1.501e-04, eta: 14:35:33, time: 2.471, data_time: 0.060, memory: 20362, loss_cls: 0.0804, loss_bbox: 0.2187, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0957, d2.loss_bbox: 0.2384, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2221, loss: 2.1667, grad_norm: 18.6067
2025-06-18 08:32:49,159 - mmdet - INFO - Epoch [3][6500/7033]	lr: 1.501e-04, eta: 14:33:38, time: 2.542, data_time: 0.062, memory: 20362, loss_cls: 0.0779, loss_bbox: 0.2185, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1185, d1.loss_bbox: 0.2642, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2383, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2210, loss: 2.1572, grad_norm: 63.1195
2025-06-18 08:34:56,026 - mmdet - INFO - Epoch [3][6550/7033]	lr: 1.501e-04, eta: 14:31:43, time: 2.537, data_time: 0.056, memory: 20362, loss_cls: 0.0803, loss_bbox: 0.2258, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3631, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2709, d2.loss_cls: 0.0987, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2279, loss: 2.2154, grad_norm: 65.0597
2025-06-18 08:36:59,918 - mmdet - INFO - Epoch [3][6600/7033]	lr: 1.501e-04, eta: 14:29:45, time: 2.477, data_time: 0.060, memory: 20362, loss_cls: 0.0735, loss_bbox: 0.2217, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2661, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2219, loss: 2.1410, grad_norm: 20.1160
2025-06-18 08:39:08,462 - mmdet - INFO - Epoch [3][6650/7033]	lr: 1.501e-04, eta: 14:27:51, time: 2.572, data_time: 0.057, memory: 20362, loss_cls: 0.0741, loss_bbox: 0.2119, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3538, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2550, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2139, loss: 2.1052, grad_norm: 29.3369
2025-06-18 08:41:14,132 - mmdet - INFO - Epoch [3][6700/7033]	lr: 1.501e-04, eta: 14:25:55, time: 2.513, data_time: 0.061, memory: 20362, loss_cls: 0.0765, loss_bbox: 0.2133, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3456, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2577, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2162, loss: 2.1148, grad_norm: 22.8117
2025-06-18 08:43:21,139 - mmdet - INFO - Epoch [3][6750/7033]	lr: 1.501e-04, eta: 14:23:59, time: 2.540, data_time: 0.060, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2215, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2669, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2232, loss: 2.1924, grad_norm: 32.7155
2025-06-18 08:45:29,755 - mmdet - INFO - Epoch [3][6800/7033]	lr: 1.501e-04, eta: 14:22:06, time: 2.573, data_time: 0.061, memory: 20362, loss_cls: 0.0782, loss_bbox: 0.2122, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3445, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2156, loss: 2.1139, grad_norm: 44.9801
2025-06-18 08:47:35,759 - mmdet - INFO - Epoch [3][6850/7033]	lr: 1.501e-04, eta: 14:20:09, time: 2.520, data_time: 0.059, memory: 20362, loss_cls: 0.0843, loss_bbox: 0.2307, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1300, d1.loss_bbox: 0.2756, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0944, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2331, loss: 2.2842, grad_norm: 29.5988
2025-06-18 08:49:43,617 - mmdet - INFO - Epoch [3][6900/7033]	lr: 1.501e-04, eta: 14:18:15, time: 2.557, data_time: 0.058, memory: 20362, loss_cls: 0.0835, loss_bbox: 0.2194, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3516, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2661, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2407, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2290, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2226, loss: 2.1869, grad_norm: 23.1264
2025-06-18 08:51:51,930 - mmdet - INFO - Epoch [3][6950/7033]	lr: 1.501e-04, eta: 14:16:21, time: 2.566, data_time: 0.048, memory: 20362, loss_cls: 0.0788, loss_bbox: 0.2260, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3535, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2734, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2458, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2299, loss: 2.1994, grad_norm: 21.9258
2025-06-18 08:54:00,170 - mmdet - INFO - Epoch [3][7000/7033]	lr: 1.501e-04, eta: 14:14:26, time: 2.565, data_time: 0.059, memory: 20362, loss_cls: 0.0777, loss_bbox: 0.2212, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2661, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2424, d3.loss_cls: 0.0857, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2242, loss: 2.1618, grad_norm: 39.5312
2025-06-18 08:55:25,981 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-18 09:45:18,969 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-18 09:45:18,969 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7938, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8798, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9040, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9174, pts_bbox_NuScenes/car_trans_err: 0.1807, pts_bbox_NuScenes/car_scale_err: 0.1554, pts_bbox_NuScenes/car_orient_err: 0.0498, pts_bbox_NuScenes/car_vel_err: 0.3035, pts_bbox_NuScenes/car_attr_err: 0.1653, pts_bbox_NuScenes/mATE: 0.2919, pts_bbox_NuScenes/mASE: 0.2717, pts_bbox_NuScenes/mAOE: 0.2683, pts_bbox_NuScenes/mAVE: 0.2827, pts_bbox_NuScenes/mAAE: 0.1749, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4327, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6199, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7120, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7448, pts_bbox_NuScenes/truck_trans_err: 0.3238, pts_bbox_NuScenes/truck_scale_err: 0.2025, pts_bbox_NuScenes/truck_orient_err: 0.0518, pts_bbox_NuScenes/truck_vel_err: 0.2559, pts_bbox_NuScenes/truck_attr_err: 0.2067, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0571, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2054, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4108, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4797, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6655, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4305, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7999, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1166, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2919, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5301, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7662, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8972, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9208, pts_bbox_NuScenes/bus_trans_err: 0.3297, pts_bbox_NuScenes/bus_scale_err: 0.2347, pts_bbox_NuScenes/bus_orient_err: 0.0478, pts_bbox_NuScenes/bus_vel_err: 0.4984, pts_bbox_NuScenes/bus_attr_err: 0.2845, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1632, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4082, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5668, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6477, pts_bbox_NuScenes/trailer_trans_err: 0.4929, pts_bbox_NuScenes/trailer_scale_err: 0.2530, pts_bbox_NuScenes/trailer_orient_err: 0.4773, pts_bbox_NuScenes/trailer_vel_err: 0.2381, pts_bbox_NuScenes/trailer_attr_err: 0.1637, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5663, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6592, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7122, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7322, pts_bbox_NuScenes/barrier_trans_err: 0.2243, pts_bbox_NuScenes/barrier_scale_err: 0.2902, pts_bbox_NuScenes/barrier_orient_err: 0.0545, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6231, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7470, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7731, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7807, pts_bbox_NuScenes/motorcycle_trans_err: 0.2255, pts_bbox_NuScenes/motorcycle_scale_err: 0.2577, pts_bbox_NuScenes/motorcycle_orient_err: 0.2316, pts_bbox_NuScenes/motorcycle_vel_err: 0.4328, pts_bbox_NuScenes/motorcycle_attr_err: 0.1734, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5090, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5625, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5690, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5793, pts_bbox_NuScenes/bicycle_trans_err: 0.1801, pts_bbox_NuScenes/bicycle_scale_err: 0.2684, pts_bbox_NuScenes/bicycle_orient_err: 0.3788, pts_bbox_NuScenes/bicycle_vel_err: 0.1889, pts_bbox_NuScenes/bicycle_attr_err: 0.0045, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7997, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8455, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8697, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8821, pts_bbox_NuScenes/pedestrian_trans_err: 0.1524, pts_bbox_NuScenes/pedestrian_scale_err: 0.2958, pts_bbox_NuScenes/pedestrian_orient_err: 0.3234, pts_bbox_NuScenes/pedestrian_vel_err: 0.2276, pts_bbox_NuScenes/pedestrian_attr_err: 0.1092, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7105, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7546, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7789, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8017, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1440, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3291, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7000, pts_bbox_NuScenes/mAP: 0.6579
2025-06-18 09:47:29,937 - mmdet - INFO - Epoch [4][50/7033]	lr: 1.001e-04, eta: 14:09:51, time: 2.532, data_time: 0.249, memory: 20362, loss_cls: 0.0766, loss_bbox: 0.2072, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3520, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2549, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2178, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2099, loss: 2.1232, grad_norm: 43.4745
2025-06-18 09:49:27,606 - mmdet - INFO - Epoch [4][100/7033]	lr: 1.001e-04, eta: 14:07:46, time: 2.353, data_time: 0.056, memory: 20362, loss_cls: 0.0757, loss_bbox: 0.2013, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3303, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2044, loss: 2.0397, grad_norm: 22.8484
2025-06-18 09:51:29,001 - mmdet - INFO - Epoch [4][150/7033]	lr: 1.001e-04, eta: 14:05:45, time: 2.428, data_time: 0.049, memory: 20362, loss_cls: 0.0711, loss_bbox: 0.2019, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2058, loss: 2.0336, grad_norm: 27.3813
2025-06-18 09:53:30,594 - mmdet - INFO - Epoch [4][200/7033]	lr: 1.001e-04, eta: 14:03:44, time: 2.432, data_time: 0.054, memory: 20362, loss_cls: 0.0728, loss_bbox: 0.2105, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3382, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2133, loss: 2.0670, grad_norm: 28.9443
2025-06-18 09:55:33,053 - mmdet - INFO - Epoch [4][250/7033]	lr: 1.001e-04, eta: 14:01:45, time: 2.449, data_time: 0.054, memory: 20362, loss_cls: 0.0743, loss_bbox: 0.1977, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2006, loss: 2.0247, grad_norm: 28.2137
2025-06-18 09:57:36,581 - mmdet - INFO - Epoch [4][300/7033]	lr: 1.001e-04, eta: 13:59:46, time: 2.471, data_time: 0.056, memory: 20362, loss_cls: 0.0705, loss_bbox: 0.2001, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2025, loss: 2.0281, grad_norm: 24.8651
2025-06-18 09:59:43,741 - mmdet - INFO - Epoch [4][350/7033]	lr: 1.001e-04, eta: 13:57:51, time: 2.543, data_time: 0.053, memory: 20362, loss_cls: 0.0732, loss_bbox: 0.2076, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3404, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2534, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2283, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2093, loss: 2.0621, grad_norm: 24.6353
2025-06-18 10:01:49,184 - mmdet - INFO - Epoch [4][400/7033]	lr: 1.001e-04, eta: 13:55:54, time: 2.509, data_time: 0.048, memory: 20362, loss_cls: 0.0792, loss_bbox: 0.2038, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3431, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2546, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2055, loss: 2.0823, grad_norm: 26.5379
2025-06-18 10:03:56,365 - mmdet - INFO - Epoch [4][450/7033]	lr: 1.001e-04, eta: 13:53:58, time: 2.544, data_time: 0.058, memory: 20362, loss_cls: 0.0674, loss_bbox: 0.1990, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3283, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2013, loss: 1.9639, grad_norm: 22.6873
2025-06-18 10:06:02,354 - mmdet - INFO - Epoch [4][500/7033]	lr: 1.001e-04, eta: 13:52:02, time: 2.520, data_time: 0.054, memory: 20362, loss_cls: 0.0757, loss_bbox: 0.2027, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3354, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0957, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0850, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2056, loss: 2.0577, grad_norm: 33.6731
2025-06-18 10:08:08,496 - mmdet - INFO - Epoch [4][550/7033]	lr: 1.001e-04, eta: 13:50:05, time: 2.523, data_time: 0.063, memory: 20362, loss_cls: 0.0725, loss_bbox: 0.2082, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2271, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2109, loss: 2.0706, grad_norm: 30.5617
2025-06-18 10:10:19,197 - mmdet - INFO - Epoch [4][600/7033]	lr: 1.001e-04, eta: 13:48:13, time: 2.614, data_time: 0.060, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.1992, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3234, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2022, loss: 2.0023, grad_norm: 23.8232
2025-06-18 10:12:25,775 - mmdet - INFO - Epoch [4][650/7033]	lr: 1.001e-04, eta: 13:46:17, time: 2.532, data_time: 0.050, memory: 20362, loss_cls: 0.0727, loss_bbox: 0.2104, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2133, loss: 2.0632, grad_norm: 21.9500
2025-06-18 10:14:33,839 - mmdet - INFO - Epoch [4][700/7033]	lr: 1.001e-04, eta: 13:44:22, time: 2.561, data_time: 0.051, memory: 20362, loss_cls: 0.0782, loss_bbox: 0.2065, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3412, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2547, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2093, loss: 2.0841, grad_norm: 68.6092
2025-06-18 10:16:41,918 - mmdet - INFO - Epoch [4][750/7033]	lr: 1.001e-04, eta: 13:42:27, time: 2.561, data_time: 0.053, memory: 20362, loss_cls: 0.0763, loss_bbox: 0.2041, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3365, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2505, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2074, loss: 2.0465, grad_norm: 44.3039
2025-06-18 10:18:50,680 - mmdet - INFO - Epoch [4][800/7033]	lr: 1.001e-04, eta: 13:40:33, time: 2.576, data_time: 0.056, memory: 20362, loss_cls: 0.0713, loss_bbox: 0.2070, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2111, loss: 2.0275, grad_norm: 65.7455
2025-06-18 10:21:01,605 - mmdet - INFO - Epoch [4][850/7033]	lr: 1.001e-04, eta: 13:38:40, time: 2.618, data_time: 0.054, memory: 20362, loss_cls: 0.0715, loss_bbox: 0.2105, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2147, loss: 2.0644, grad_norm: 48.3425
2025-06-18 10:23:15,506 - mmdet - INFO - Epoch [4][900/7033]	lr: 1.001e-04, eta: 13:36:51, time: 2.678, data_time: 0.057, memory: 20362, loss_cls: 0.0681, loss_bbox: 0.2049, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2069, loss: 2.0026, grad_norm: 32.3424
2025-06-18 10:25:26,442 - mmdet - INFO - Epoch [4][950/7033]	lr: 1.001e-04, eta: 13:34:58, time: 2.619, data_time: 0.057, memory: 20362, loss_cls: 0.0761, loss_bbox: 0.2125, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2145, loss: 2.1047, grad_norm: 16.6994
2025-06-18 10:27:41,334 - mmdet - INFO - Epoch [4][1000/7033]	lr: 1.001e-04, eta: 13:33:09, time: 2.698, data_time: 0.074, memory: 20362, loss_cls: 0.0760, loss_bbox: 0.2190, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3415, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2617, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2208, loss: 2.1147, grad_norm: 30.5358
2025-06-18 10:29:53,056 - mmdet - INFO - Epoch [4][1050/7033]	lr: 1.001e-04, eta: 13:31:17, time: 2.634, data_time: 0.060, memory: 20362, loss_cls: 0.0654, loss_bbox: 0.1998, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3270, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2024, loss: 1.9419, grad_norm: 19.8939
2025-06-18 10:32:06,339 - mmdet - INFO - Epoch [4][1100/7033]	lr: 1.001e-04, eta: 13:29:27, time: 2.666, data_time: 0.045, memory: 20362, loss_cls: 0.0730, loss_bbox: 0.2103, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3430, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2299, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2138, loss: 2.0717, grad_norm: 17.4965
2025-06-18 10:34:19,097 - mmdet - INFO - Epoch [4][1150/7033]	lr: 1.001e-04, eta: 13:27:35, time: 2.655, data_time: 0.052, memory: 20362, loss_cls: 0.0864, loss_bbox: 0.2130, d0.loss_cls: 0.1644, d0.loss_bbox: 0.3436, d1.loss_cls: 0.2477, d1.loss_bbox: 0.4408, d2.loss_cls: 0.1202, d2.loss_bbox: 0.2599, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2328, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2174, loss: 2.5181, grad_norm: 23.8672
2025-06-18 10:36:30,798 - mmdet - INFO - Epoch [4][1200/7033]	lr: 1.001e-04, eta: 13:25:43, time: 2.634, data_time: 0.057, memory: 20362, loss_cls: 0.0729, loss_bbox: 0.2050, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3380, d1.loss_cls: 0.1326, d1.loss_bbox: 0.2867, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2178, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2088, loss: 2.1147, grad_norm: 27.5615
2025-06-18 10:38:46,749 - mmdet - INFO - Epoch [4][1250/7033]	lr: 1.001e-04, eta: 13:23:54, time: 2.719, data_time: 0.057, memory: 20362, loss_cls: 0.0798, loss_bbox: 0.2139, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1452, d1.loss_bbox: 0.3202, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2437, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2175, loss: 2.2500, grad_norm: 31.3702
2025-06-18 10:40:57,684 - mmdet - INFO - Epoch [4][1300/7033]	lr: 1.001e-04, eta: 13:22:01, time: 2.619, data_time: 0.056, memory: 20362, loss_cls: 0.0783, loss_bbox: 0.2104, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3445, d1.loss_cls: 0.1591, d1.loss_bbox: 0.3436, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2416, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2137, loss: 2.2635, grad_norm: 29.8549
2025-06-18 10:43:11,196 - mmdet - INFO - Epoch [4][1350/7033]	lr: 1.001e-04, eta: 13:20:10, time: 2.670, data_time: 0.055, memory: 20362, loss_cls: 0.0794, loss_bbox: 0.2083, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1972, d1.loss_bbox: 0.5257, d2.loss_cls: 0.1139, d2.loss_bbox: 0.2504, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2124, loss: 2.5046, grad_norm: 22.5337
2025-06-18 10:45:31,897 - mmdet - INFO - Epoch [4][1400/7033]	lr: 1.001e-04, eta: 13:18:25, time: 2.815, data_time: 0.283, memory: 20362, loss_cls: 0.0766, loss_bbox: 0.2145, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3498, d1.loss_cls: 0.1733, d1.loss_bbox: 0.5019, d2.loss_cls: 0.1127, d2.loss_bbox: 0.2537, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2180, loss: 2.4730, grad_norm: 23.1126
2025-06-18 10:47:43,497 - mmdet - INFO - Epoch [4][1450/7033]	lr: 1.001e-04, eta: 13:16:33, time: 2.632, data_time: 0.051, memory: 20362, loss_cls: 0.0752, loss_bbox: 0.2034, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1631, d1.loss_bbox: 0.5075, d2.loss_cls: 0.1053, d2.loss_bbox: 0.2428, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2078, loss: 2.4056, grad_norm: 21.8999
2025-06-18 10:49:57,510 - mmdet - INFO - Epoch [4][1500/7033]	lr: 1.001e-04, eta: 13:14:42, time: 2.681, data_time: 0.059, memory: 20362, loss_cls: 0.0776, loss_bbox: 0.2114, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1533, d1.loss_bbox: 0.5443, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2535, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2152, loss: 2.4744, grad_norm: 41.9925
2025-06-18 10:52:07,333 - mmdet - INFO - Epoch [4][1550/7033]	lr: 1.001e-04, eta: 13:12:47, time: 2.596, data_time: 0.050, memory: 20362, loss_cls: 0.0726, loss_bbox: 0.2094, d0.loss_cls: 0.1630, d0.loss_bbox: 0.3371, d1.loss_cls: 0.1443, d1.loss_bbox: 0.5023, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2450, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2138, loss: 2.3668, grad_norm: 31.9909
2025-06-18 10:54:19,857 - mmdet - INFO - Epoch [4][1600/7033]	lr: 1.001e-04, eta: 13:10:55, time: 2.650, data_time: 0.047, memory: 20362, loss_cls: 0.0735, loss_bbox: 0.2078, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3411, d1.loss_cls: 0.1219, d1.loss_bbox: 0.3377, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2106, loss: 2.1695, grad_norm: 31.4971
2025-06-18 10:56:31,341 - mmdet - INFO - Epoch [4][1650/7033]	lr: 1.001e-04, eta: 13:09:01, time: 2.630, data_time: 0.054, memory: 20362, loss_cls: 0.0704, loss_bbox: 0.2050, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1149, d1.loss_bbox: 0.3153, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2074, loss: 2.1035, grad_norm: 22.2335
2025-06-18 10:58:40,808 - mmdet - INFO - Epoch [4][1700/7033]	lr: 1.001e-04, eta: 13:07:06, time: 2.589, data_time: 0.056, memory: 20362, loss_cls: 0.0756, loss_bbox: 0.2164, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1218, d1.loss_bbox: 0.3511, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2293, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2202, loss: 2.2289, grad_norm: 24.4658
2025-06-18 11:00:56,143 - mmdet - INFO - Epoch [4][1750/7033]	lr: 1.001e-04, eta: 13:05:16, time: 2.706, data_time: 0.063, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.2057, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3407, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2910, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2312, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2083, loss: 2.1067, grad_norm: 54.3403
2025-06-18 11:03:05,195 - mmdet - INFO - Epoch [4][1800/7033]	lr: 1.001e-04, eta: 13:03:21, time: 2.581, data_time: 0.056, memory: 20362, loss_cls: 0.0684, loss_bbox: 0.1990, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3275, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2693, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2006, loss: 1.9985, grad_norm: 19.0174
2025-06-18 11:05:20,394 - mmdet - INFO - Epoch [4][1850/7033]	lr: 1.001e-04, eta: 13:01:30, time: 2.704, data_time: 0.048, memory: 20362, loss_cls: 0.0737, loss_bbox: 0.2059, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3298, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2644, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2093, loss: 2.0458, grad_norm: 21.2372
2025-06-18 11:07:33,164 - mmdet - INFO - Epoch [4][1900/7033]	lr: 1.001e-04, eta: 12:59:37, time: 2.655, data_time: 0.058, memory: 20362, loss_cls: 0.0757, loss_bbox: 0.2081, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2766, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2111, loss: 2.0896, grad_norm: 28.3214
2025-06-18 11:09:44,132 - mmdet - INFO - Epoch [4][1950/7033]	lr: 1.001e-04, eta: 12:57:43, time: 2.619, data_time: 0.060, memory: 20362, loss_cls: 0.0787, loss_bbox: 0.2136, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2959, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2411, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2180, loss: 2.1804, grad_norm: 63.0878
2025-06-18 11:11:58,743 - mmdet - INFO - Epoch [4][2000/7033]	lr: 1.001e-04, eta: 12:55:52, time: 2.692, data_time: 0.051, memory: 20362, loss_cls: 0.0717, loss_bbox: 0.2131, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3459, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2825, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2228, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2150, loss: 2.1086, grad_norm: 69.2383
2025-06-18 11:14:08,580 - mmdet - INFO - Epoch [4][2050/7033]	lr: 1.001e-04, eta: 12:53:56, time: 2.596, data_time: 0.046, memory: 20362, loss_cls: 0.0704, loss_bbox: 0.2070, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3364, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2710, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2096, loss: 2.0514, grad_norm: 20.8260
2025-06-18 11:16:23,215 - mmdet - INFO - Epoch [4][2100/7033]	lr: 1.001e-04, eta: 12:52:05, time: 2.694, data_time: 0.049, memory: 20362, loss_cls: 0.0692, loss_bbox: 0.1984, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2535, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2009, loss: 1.9859, grad_norm: 53.6737
2025-06-18 11:18:34,289 - mmdet - INFO - Epoch [4][2150/7033]	lr: 1.001e-04, eta: 12:50:11, time: 2.621, data_time: 0.052, memory: 20362, loss_cls: 0.0825, loss_bbox: 0.2079, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3331, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2616, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2103, loss: 2.1105, grad_norm: 34.8434
2025-06-18 11:20:44,762 - mmdet - INFO - Epoch [4][2200/7033]	lr: 1.001e-04, eta: 12:48:16, time: 2.610, data_time: 0.055, memory: 20362, loss_cls: 0.0709, loss_bbox: 0.2045, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3402, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2597, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2080, loss: 2.0469, grad_norm: 27.7116
2025-06-18 11:22:58,273 - mmdet - INFO - Epoch [4][2250/7033]	lr: 1.001e-04, eta: 12:46:23, time: 2.670, data_time: 0.055, memory: 20362, loss_cls: 0.0651, loss_bbox: 0.1995, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3209, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2471, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0657, d4.loss_bbox: 0.2022, loss: 1.9447, grad_norm: 18.1304
2025-06-18 11:25:09,082 - mmdet - INFO - Epoch [4][2300/7033]	lr: 1.001e-04, eta: 12:44:28, time: 2.616, data_time: 0.057, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.2144, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3351, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2625, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2345, d3.loss_cls: 0.0826, d3.loss_bbox: 0.2221, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2180, loss: 2.0856, grad_norm: 36.4570
2025-06-18 11:27:20,397 - mmdet - INFO - Epoch [4][2350/7033]	lr: 1.001e-04, eta: 12:42:34, time: 2.626, data_time: 0.052, memory: 20362, loss_cls: 0.0673, loss_bbox: 0.1980, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3304, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2459, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1996, loss: 1.9578, grad_norm: 43.6232
2025-06-18 11:29:32,814 - mmdet - INFO - Epoch [4][2400/7033]	lr: 1.001e-04, eta: 12:40:40, time: 2.649, data_time: 0.063, memory: 20362, loss_cls: 0.0733, loss_bbox: 0.2049, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3355, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2061, loss: 2.0484, grad_norm: 26.8689
2025-06-18 11:31:45,101 - mmdet - INFO - Epoch [4][2450/7033]	lr: 1.001e-04, eta: 12:38:46, time: 2.646, data_time: 0.074, memory: 20362, loss_cls: 0.0758, loss_bbox: 0.2127, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3387, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2151, loss: 2.0870, grad_norm: 17.7250
2025-06-18 11:34:01,236 - mmdet - INFO - Epoch [4][2500/7033]	lr: 1.001e-04, eta: 12:36:55, time: 2.723, data_time: 0.055, memory: 20362, loss_cls: 0.0730, loss_bbox: 0.2129, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2597, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2157, loss: 2.0819, grad_norm: 27.7965
2025-06-18 11:36:14,171 - mmdet - INFO - Epoch [4][2550/7033]	lr: 1.001e-04, eta: 12:35:01, time: 2.659, data_time: 0.054, memory: 20362, loss_cls: 0.0780, loss_bbox: 0.2142, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3412, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2621, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2352, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2160, loss: 2.1212, grad_norm: 32.4987
2025-06-18 11:38:27,397 - mmdet - INFO - Epoch [4][2600/7033]	lr: 1.001e-04, eta: 12:33:08, time: 2.665, data_time: 0.051, memory: 20362, loss_cls: 0.0732, loss_bbox: 0.2185, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2649, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0847, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2202, loss: 2.1222, grad_norm: 41.1882
2025-06-18 11:40:43,568 - mmdet - INFO - Epoch [4][2650/7033]	lr: 1.001e-04, eta: 12:31:17, time: 2.723, data_time: 0.047, memory: 20362, loss_cls: 0.0689, loss_bbox: 0.2123, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2299, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2153, loss: 2.0419, grad_norm: 39.8964
2025-06-18 11:42:59,074 - mmdet - INFO - Epoch [4][2700/7033]	lr: 1.001e-04, eta: 12:29:25, time: 2.710, data_time: 0.052, memory: 20362, loss_cls: 0.0713, loss_bbox: 0.2077, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2271, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2112, loss: 2.0142, grad_norm: 26.0189
2025-06-18 11:45:14,193 - mmdet - INFO - Epoch [4][2750/7033]	lr: 1.001e-04, eta: 12:27:33, time: 2.702, data_time: 0.052, memory: 20362, loss_cls: 0.0707, loss_bbox: 0.2039, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2058, loss: 2.0109, grad_norm: 26.0836
2025-06-18 11:47:41,147 - mmdet - INFO - Epoch [4][2800/7033]	lr: 1.001e-04, eta: 12:25:49, time: 2.940, data_time: 0.056, memory: 20362, loss_cls: 0.0711, loss_bbox: 0.2062, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2580, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2077, loss: 2.0273, grad_norm: 139.8786
2025-06-18 11:49:58,245 - mmdet - INFO - Epoch [4][2850/7033]	lr: 1.001e-04, eta: 12:23:58, time: 2.742, data_time: 0.055, memory: 20362, loss_cls: 0.0764, loss_bbox: 0.2208, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2906, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2453, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2235, loss: 2.2073, grad_norm: 35.2957
2025-06-18 11:52:16,758 - mmdet - INFO - Epoch [4][2900/7033]	lr: 1.001e-04, eta: 12:22:08, time: 2.770, data_time: 0.051, memory: 20362, loss_cls: 0.0731, loss_bbox: 0.2151, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2729, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2380, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2183, loss: 2.1303, grad_norm: 25.5779
2025-06-18 11:54:33,488 - mmdet - INFO - Epoch [4][2950/7033]	lr: 1.001e-04, eta: 12:20:17, time: 2.735, data_time: 0.048, memory: 20362, loss_cls: 0.0674, loss_bbox: 0.2085, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2634, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2271, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2092, loss: 2.0274, grad_norm: 21.7800
2025-06-18 11:56:53,710 - mmdet - INFO - Epoch [4][3000/7033]	lr: 1.001e-04, eta: 12:18:28, time: 2.804, data_time: 0.051, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.2136, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2672, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2165, loss: 2.1062, grad_norm: 21.2830
2025-06-18 11:59:13,102 - mmdet - INFO - Epoch [4][3050/7033]	lr: 1.001e-04, eta: 12:16:38, time: 2.788, data_time: 0.052, memory: 20362, loss_cls: 0.0744, loss_bbox: 0.2120, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2598, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2145, loss: 2.0779, grad_norm: 24.9967
2025-06-18 12:01:33,991 - mmdet - INFO - Epoch [4][3100/7033]	lr: 1.001e-04, eta: 12:14:49, time: 2.818, data_time: 0.049, memory: 20362, loss_cls: 0.0713, loss_bbox: 0.2101, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3380, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2538, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2130, loss: 2.0448, grad_norm: 27.0959
2025-06-18 12:03:53,467 - mmdet - INFO - Epoch [4][3150/7033]	lr: 1.001e-04, eta: 12:13:00, time: 2.789, data_time: 0.052, memory: 20362, loss_cls: 0.0711, loss_bbox: 0.2109, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2540, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2133, loss: 2.0341, grad_norm: 22.0963
2025-06-18 12:06:13,942 - mmdet - INFO - Epoch [4][3200/7033]	lr: 1.001e-04, eta: 12:11:10, time: 2.810, data_time: 0.049, memory: 20362, loss_cls: 0.0732, loss_bbox: 0.2057, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3333, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2090, loss: 2.0333, grad_norm: 33.1111
2025-06-18 12:08:36,655 - mmdet - INFO - Epoch [4][3250/7033]	lr: 1.001e-04, eta: 12:09:22, time: 2.854, data_time: 0.047, memory: 20362, loss_cls: 0.0687, loss_bbox: 0.2146, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2158, loss: 2.0727, grad_norm: 23.7873
2025-06-18 12:10:56,251 - mmdet - INFO - Epoch [4][3300/7033]	lr: 1.001e-04, eta: 12:07:32, time: 2.791, data_time: 0.047, memory: 20362, loss_cls: 0.0676, loss_bbox: 0.1957, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3197, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2021, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1974, loss: 1.9320, grad_norm: 20.5374
2025-06-18 12:13:19,026 - mmdet - INFO - Epoch [4][3350/7033]	lr: 1.001e-04, eta: 12:05:44, time: 2.856, data_time: 0.051, memory: 20362, loss_cls: 0.0745, loss_bbox: 0.2091, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2108, loss: 2.0579, grad_norm: 22.6310
2025-06-18 12:15:40,394 - mmdet - INFO - Epoch [4][3400/7033]	lr: 1.001e-04, eta: 12:03:55, time: 2.827, data_time: 0.050, memory: 20362, loss_cls: 0.0701, loss_bbox: 0.2121, d0.loss_cls: 0.1606, d0.loss_bbox: 0.3430, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2155, loss: 2.0494, grad_norm: 27.8604
2025-06-18 12:18:03,413 - mmdet - INFO - Epoch [4][3450/7033]	lr: 1.001e-04, eta: 12:02:07, time: 2.860, data_time: 0.050, memory: 20362, loss_cls: 0.0753, loss_bbox: 0.2104, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3378, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2575, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2125, loss: 2.0752, grad_norm: 31.4322
2025-06-18 12:20:20,902 - mmdet - INFO - Epoch [4][3500/7033]	lr: 1.001e-04, eta: 12:00:15, time: 2.750, data_time: 0.049, memory: 20362, loss_cls: 0.0749, loss_bbox: 0.2078, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3371, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2536, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2102, loss: 2.0579, grad_norm: 27.1683
2025-06-18 12:22:41,876 - mmdet - INFO - Epoch [4][3550/7033]	lr: 1.001e-04, eta: 11:58:25, time: 2.819, data_time: 0.049, memory: 20362, loss_cls: 0.0772, loss_bbox: 0.2123, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3457, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2133, loss: 2.0961, grad_norm: 21.1195
2025-06-18 12:25:00,725 - mmdet - INFO - Epoch [4][3600/7033]	lr: 1.001e-04, eta: 11:56:34, time: 2.777, data_time: 0.048, memory: 20362, loss_cls: 0.0817, loss_bbox: 0.2193, d0.loss_cls: 0.1707, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2650, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2211, loss: 2.1482, grad_norm: 19.3606
2025-06-18 12:27:23,592 - mmdet - INFO - Epoch [4][3650/7033]	lr: 1.001e-04, eta: 11:54:45, time: 2.857, data_time: 0.050, memory: 20362, loss_cls: 0.0701, loss_bbox: 0.2055, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2082, loss: 2.0128, grad_norm: 31.2552
2025-06-18 12:29:47,134 - mmdet - INFO - Epoch [4][3700/7033]	lr: 1.001e-04, eta: 11:52:56, time: 2.871, data_time: 0.070, memory: 20362, loss_cls: 0.0721, loss_bbox: 0.2100, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3320, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2124, loss: 2.0334, grad_norm: 23.5916
2025-06-18 12:32:06,235 - mmdet - INFO - Epoch [4][3750/7033]	lr: 1.001e-04, eta: 11:51:05, time: 2.782, data_time: 0.051, memory: 20362, loss_cls: 0.0684, loss_bbox: 0.2021, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2034, loss: 1.9776, grad_norm: 26.0776
2025-06-18 12:34:30,486 - mmdet - INFO - Epoch [4][3800/7033]	lr: 1.001e-04, eta: 11:49:17, time: 2.885, data_time: 0.050, memory: 20362, loss_cls: 0.0745, loss_bbox: 0.2032, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2139, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2062, loss: 2.0143, grad_norm: 23.5732
2025-06-18 12:36:51,951 - mmdet - INFO - Epoch [4][3850/7033]	lr: 1.001e-04, eta: 11:47:26, time: 2.829, data_time: 0.053, memory: 20362, loss_cls: 0.0657, loss_bbox: 0.1937, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3160, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1956, loss: 1.9106, grad_norm: 21.7296
2025-06-18 12:39:14,519 - mmdet - INFO - Epoch [4][3900/7033]	lr: 1.001e-04, eta: 11:45:37, time: 2.851, data_time: 0.047, memory: 20362, loss_cls: 0.0662, loss_bbox: 0.1938, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3234, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1979, loss: 1.9278, grad_norm: 17.5126
2025-06-18 12:41:38,841 - mmdet - INFO - Epoch [4][3950/7033]	lr: 1.001e-04, eta: 11:43:48, time: 2.886, data_time: 0.051, memory: 20362, loss_cls: 0.0718, loss_bbox: 0.2067, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3287, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2077, loss: 2.0191, grad_norm: 15.9894
2025-06-18 12:43:57,836 - mmdet - INFO - Epoch [4][4000/7033]	lr: 1.001e-04, eta: 11:41:56, time: 2.780, data_time: 0.054, memory: 20362, loss_cls: 0.0773, loss_bbox: 0.2080, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3436, d1.loss_cls: 0.1116, d1.loss_bbox: 0.2547, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2110, loss: 2.0798, grad_norm: 20.9750
2025-06-18 12:46:25,016 - mmdet - INFO - Epoch [4][4050/7033]	lr: 1.001e-04, eta: 11:40:09, time: 2.944, data_time: 0.049, memory: 20362, loss_cls: 0.0687, loss_bbox: 0.2021, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2469, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2100, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2036, loss: 1.9869, grad_norm: 76.4646
2025-06-18 12:48:46,239 - mmdet - INFO - Epoch [4][4100/7033]	lr: 1.001e-04, eta: 11:38:18, time: 2.824, data_time: 0.052, memory: 20362, loss_cls: 0.0712, loss_bbox: 0.2099, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3378, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2545, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2138, loss: 2.0562, grad_norm: 147.1641
2025-06-18 12:51:12,272 - mmdet - INFO - Epoch [4][4150/7033]	lr: 1.001e-04, eta: 11:36:30, time: 2.921, data_time: 0.051, memory: 20362, loss_cls: 0.0720, loss_bbox: 0.2072, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2081, loss: 2.0197, grad_norm: 25.9209
2025-06-18 12:53:33,693 - mmdet - INFO - Epoch [4][4200/7033]	lr: 1.001e-04, eta: 11:34:39, time: 2.828, data_time: 0.053, memory: 20362, loss_cls: 0.0679, loss_bbox: 0.2113, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3394, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2557, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2138, loss: 2.0315, grad_norm: 22.8594
2025-06-18 12:56:01,734 - mmdet - INFO - Epoch [4][4250/7033]	lr: 1.001e-04, eta: 11:32:52, time: 2.961, data_time: 0.048, memory: 20362, loss_cls: 0.0740, loss_bbox: 0.2088, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3304, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2098, loss: 2.0272, grad_norm: 24.3932
2025-06-18 12:58:25,471 - mmdet - INFO - Epoch [4][4300/7033]	lr: 1.001e-04, eta: 11:31:02, time: 2.874, data_time: 0.048, memory: 20362, loss_cls: 0.0793, loss_bbox: 0.2114, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3370, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2197, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2137, loss: 2.1055, grad_norm: 33.1336
2025-06-18 13:00:50,268 - mmdet - INFO - Epoch [4][4350/7033]	lr: 1.001e-04, eta: 11:29:13, time: 2.896, data_time: 0.051, memory: 20362, loss_cls: 0.0727, loss_bbox: 0.2066, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3350, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2079, loss: 2.0375, grad_norm: 33.2286
2025-06-18 13:03:14,785 - mmdet - INFO - Epoch [4][4400/7033]	lr: 1.001e-04, eta: 11:27:23, time: 2.891, data_time: 0.052, memory: 20362, loss_cls: 0.0771, loss_bbox: 0.2140, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3439, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2626, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2179, loss: 2.1032, grad_norm: 15.6398
2025-06-18 13:05:36,675 - mmdet - INFO - Epoch [4][4450/7033]	lr: 1.001e-04, eta: 11:25:32, time: 2.838, data_time: 0.072, memory: 20362, loss_cls: 0.0672, loss_bbox: 0.2001, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2017, loss: 1.9434, grad_norm: 36.1085
2025-06-18 13:08:02,504 - mmdet - INFO - Epoch [4][4500/7033]	lr: 1.001e-04, eta: 11:23:43, time: 2.917, data_time: 0.046, memory: 20362, loss_cls: 0.0696, loss_bbox: 0.2012, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3229, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2038, loss: 1.9548, grad_norm: 35.7270
2025-06-18 13:10:24,643 - mmdet - INFO - Epoch [4][4550/7033]	lr: 1.001e-04, eta: 11:21:51, time: 2.843, data_time: 0.051, memory: 20362, loss_cls: 0.0687, loss_bbox: 0.2019, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2045, loss: 1.9940, grad_norm: 21.4907
2025-06-18 13:12:51,525 - mmdet - INFO - Epoch [4][4600/7033]	lr: 1.001e-04, eta: 11:20:02, time: 2.938, data_time: 0.050, memory: 20362, loss_cls: 0.0746, loss_bbox: 0.2103, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3498, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2146, loss: 2.0920, grad_norm: 62.6073
2025-06-18 13:15:16,091 - mmdet - INFO - Epoch [4][4650/7033]	lr: 1.001e-04, eta: 11:18:12, time: 2.891, data_time: 0.050, memory: 20362, loss_cls: 0.0714, loss_bbox: 0.2022, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2048, loss: 2.0257, grad_norm: 47.7434
2025-06-18 13:17:41,049 - mmdet - INFO - Epoch [4][4700/7033]	lr: 1.001e-04, eta: 11:16:22, time: 2.899, data_time: 0.052, memory: 20362, loss_cls: 0.0738, loss_bbox: 0.2058, d0.loss_cls: 0.1881, d0.loss_bbox: 0.3425, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2172, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2077, loss: 2.0979, grad_norm: 51.5800
2025-06-18 13:20:03,145 - mmdet - INFO - Epoch [4][4750/7033]	lr: 1.001e-04, eta: 11:14:30, time: 2.841, data_time: 0.047, memory: 20362, loss_cls: 0.0749, loss_bbox: 0.2104, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3421, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2540, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2138, loss: 2.0941, grad_norm: 17.4960
2025-06-18 13:22:25,981 - mmdet - INFO - Epoch [4][4800/7033]	lr: 1.001e-04, eta: 11:12:38, time: 2.857, data_time: 0.052, memory: 20362, loss_cls: 0.0761, loss_bbox: 0.2069, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3296, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2090, loss: 2.0533, grad_norm: 36.2910
2025-06-18 13:24:55,678 - mmdet - INFO - Epoch [4][4850/7033]	lr: 1.001e-04, eta: 11:10:50, time: 2.994, data_time: 0.050, memory: 20362, loss_cls: 0.0665, loss_bbox: 0.1966, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0676, d4.loss_bbox: 0.2003, loss: 1.9423, grad_norm: 21.0317
2025-06-18 13:27:17,603 - mmdet - INFO - Epoch [4][4900/7033]	lr: 1.001e-04, eta: 11:08:58, time: 2.838, data_time: 0.049, memory: 20362, loss_cls: 0.0787, loss_bbox: 0.2068, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2535, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0857, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2098, loss: 2.0768, grad_norm: 21.2255
2025-06-18 13:29:42,043 - mmdet - INFO - Epoch [4][4950/7033]	lr: 1.001e-04, eta: 11:07:07, time: 2.889, data_time: 0.049, memory: 20362, loss_cls: 0.0640, loss_bbox: 0.1987, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2462, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0657, d4.loss_bbox: 0.2006, loss: 1.9650, grad_norm: 18.4490
2025-06-18 13:32:03,308 - mmdet - INFO - Epoch [4][5000/7033]	lr: 1.001e-04, eta: 11:05:13, time: 2.825, data_time: 0.055, memory: 20362, loss_cls: 0.0786, loss_bbox: 0.2057, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3362, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2086, loss: 2.0582, grad_norm: 16.3366
2025-06-18 13:34:26,296 - mmdet - INFO - Epoch [4][5050/7033]	lr: 1.001e-04, eta: 11:03:21, time: 2.860, data_time: 0.050, memory: 20362, loss_cls: 0.0733, loss_bbox: 0.2030, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3257, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2044, loss: 1.9980, grad_norm: 29.3559
2025-06-18 13:36:50,488 - mmdet - INFO - Epoch [4][5100/7033]	lr: 1.001e-04, eta: 11:01:29, time: 2.884, data_time: 0.050, memory: 20362, loss_cls: 0.0818, loss_bbox: 0.2216, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2666, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2390, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2234, loss: 2.1915, grad_norm: 18.9806
2025-06-18 13:39:12,676 - mmdet - INFO - Epoch [4][5150/7033]	lr: 1.001e-04, eta: 10:59:36, time: 2.843, data_time: 0.048, memory: 20362, loss_cls: 0.0732, loss_bbox: 0.1992, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3329, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2017, loss: 1.9959, grad_norm: 48.8511
2025-06-18 13:41:34,814 - mmdet - INFO - Epoch [4][5200/7033]	lr: 1.001e-04, eta: 10:57:43, time: 2.843, data_time: 0.052, memory: 20362, loss_cls: 0.0664, loss_bbox: 0.1991, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3231, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2001, loss: 1.9434, grad_norm: 33.3211
2025-06-18 13:43:58,362 - mmdet - INFO - Epoch [4][5250/7033]	lr: 1.001e-04, eta: 10:55:51, time: 2.871, data_time: 0.055, memory: 20362, loss_cls: 0.0689, loss_bbox: 0.1985, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3122, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2001, loss: 1.9381, grad_norm: 52.2331
2025-06-18 13:46:21,819 - mmdet - INFO - Epoch [4][5300/7033]	lr: 1.001e-04, eta: 10:53:58, time: 2.869, data_time: 0.050, memory: 20362, loss_cls: 0.0713, loss_bbox: 0.2008, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3219, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2402, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2022, loss: 1.9646, grad_norm: 33.9703
2025-06-18 13:48:41,676 - mmdet - INFO - Epoch [4][5350/7033]	lr: 1.001e-04, eta: 10:52:03, time: 2.797, data_time: 0.051, memory: 20362, loss_cls: 0.0674, loss_bbox: 0.1989, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3291, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0684, d4.loss_bbox: 0.2019, loss: 1.9524, grad_norm: 22.1415
2025-06-18 13:51:03,553 - mmdet - INFO - Epoch [4][5400/7033]	lr: 1.001e-04, eta: 10:50:10, time: 2.838, data_time: 0.049, memory: 20362, loss_cls: 0.0708, loss_bbox: 0.2020, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3244, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2045, loss: 1.9837, grad_norm: 35.3685
2025-06-18 13:53:27,190 - mmdet - INFO - Epoch [4][5450/7033]	lr: 1.001e-04, eta: 10:48:17, time: 2.873, data_time: 0.051, memory: 20362, loss_cls: 0.0673, loss_bbox: 0.1888, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3170, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0731, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1904, loss: 1.8855, grad_norm: 32.0807
2025-06-18 13:55:48,726 - mmdet - INFO - Epoch [4][5500/7033]	lr: 1.001e-04, eta: 10:46:23, time: 2.831, data_time: 0.046, memory: 20362, loss_cls: 0.0680, loss_bbox: 0.2003, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3273, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2035, loss: 1.9608, grad_norm: 26.2257
2025-06-18 13:58:11,020 - mmdet - INFO - Epoch [4][5550/7033]	lr: 1.001e-04, eta: 10:44:29, time: 2.846, data_time: 0.050, memory: 20362, loss_cls: 0.0705, loss_bbox: 0.1938, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0708, d4.loss_bbox: 0.1964, loss: 1.9375, grad_norm: 79.0523
2025-06-18 14:00:32,170 - mmdet - INFO - Epoch [4][5600/7033]	lr: 1.001e-04, eta: 10:42:34, time: 2.823, data_time: 0.048, memory: 20362, loss_cls: 0.0658, loss_bbox: 0.1898, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3186, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0680, d4.loss_bbox: 0.1917, loss: 1.8943, grad_norm: 49.5580
2025-06-18 14:02:56,955 - mmdet - INFO - Epoch [4][5650/7033]	lr: 1.001e-04, eta: 10:40:42, time: 2.896, data_time: 0.050, memory: 20362, loss_cls: 0.0772, loss_bbox: 0.2045, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0850, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2065, loss: 2.0659, grad_norm: 25.1701
2025-06-18 14:05:17,357 - mmdet - INFO - Epoch [4][5700/7033]	lr: 1.001e-04, eta: 10:38:46, time: 2.808, data_time: 0.050, memory: 20362, loss_cls: 0.0763, loss_bbox: 0.2049, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3421, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2535, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2082, loss: 2.0583, grad_norm: 53.6901
2025-06-18 14:07:39,835 - mmdet - INFO - Epoch [4][5750/7033]	lr: 1.001e-04, eta: 10:36:52, time: 2.850, data_time: 0.054, memory: 20362, loss_cls: 0.0718, loss_bbox: 0.2110, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2128, loss: 2.0588, grad_norm: 20.8945
2025-06-18 14:10:00,437 - mmdet - INFO - Epoch [4][5800/7033]	lr: 1.001e-04, eta: 10:34:57, time: 2.812, data_time: 0.052, memory: 20362, loss_cls: 0.0751, loss_bbox: 0.2007, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3345, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2036, loss: 2.0215, grad_norm: 39.5046
2025-06-18 14:12:24,490 - mmdet - INFO - Epoch [4][5850/7033]	lr: 1.001e-04, eta: 10:33:04, time: 2.881, data_time: 0.048, memory: 20362, loss_cls: 0.0759, loss_bbox: 0.2147, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3463, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2613, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2175, loss: 2.1050, grad_norm: 30.0463
2025-06-18 14:14:46,114 - mmdet - INFO - Epoch [4][5900/7033]	lr: 1.001e-04, eta: 10:31:09, time: 2.832, data_time: 0.053, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.2034, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3291, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2058, loss: 2.0131, grad_norm: 3577.2435
2025-06-18 14:17:05,175 - mmdet - INFO - Epoch [4][5950/7033]	lr: 1.001e-04, eta: 10:29:12, time: 2.781, data_time: 0.053, memory: 20362, loss_cls: 0.0697, loss_bbox: 0.1955, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3361, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0726, d4.loss_bbox: 0.1979, loss: 1.9809, grad_norm: 23.7744
2025-06-18 14:19:26,737 - mmdet - INFO - Epoch [4][6000/7033]	lr: 1.001e-04, eta: 10:27:17, time: 2.831, data_time: 0.053, memory: 20362, loss_cls: 0.0709, loss_bbox: 0.2007, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3347, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2036, loss: 2.0190, grad_norm: 23.9664
2025-06-18 14:21:44,378 - mmdet - INFO - Epoch [4][6050/7033]	lr: 1.001e-04, eta: 10:25:20, time: 2.753, data_time: 0.059, memory: 20362, loss_cls: 0.0729, loss_bbox: 0.1977, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3380, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0743, d4.loss_bbox: 0.1998, loss: 2.0040, grad_norm: 247.7033
2025-06-18 14:24:06,423 - mmdet - INFO - Epoch [4][6100/7033]	lr: 1.001e-04, eta: 10:23:24, time: 2.841, data_time: 0.061, memory: 20362, loss_cls: 0.0690, loss_bbox: 0.1950, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0713, d4.loss_bbox: 0.1986, loss: 1.9644, grad_norm: 43.8731
2025-06-18 14:26:23,093 - mmdet - INFO - Epoch [4][6150/7033]	lr: 1.001e-04, eta: 10:21:26, time: 2.733, data_time: 0.059, memory: 20362, loss_cls: 0.0780, loss_bbox: 0.2068, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3425, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2542, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2101, loss: 2.0849, grad_norm: 39.6478
2025-06-18 14:28:43,130 - mmdet - INFO - Epoch [4][6200/7033]	lr: 1.001e-04, eta: 10:19:30, time: 2.801, data_time: 0.058, memory: 20362, loss_cls: 0.0750, loss_bbox: 0.1990, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3318, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2019, loss: 2.0136, grad_norm: 54.0039
2025-06-18 14:31:02,606 - mmdet - INFO - Epoch [4][6250/7033]	lr: 1.001e-04, eta: 10:17:33, time: 2.790, data_time: 0.059, memory: 20362, loss_cls: 0.0740, loss_bbox: 0.1988, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3370, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2196, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2101, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2028, loss: 2.0123, grad_norm: 69.3668
2025-06-18 14:33:18,398 - mmdet - INFO - Epoch [4][6300/7033]	lr: 1.001e-04, eta: 10:15:34, time: 2.716, data_time: 0.060, memory: 20362, loss_cls: 0.0754, loss_bbox: 0.2077, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3474, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2108, loss: 2.0823, grad_norm: 50.8562
2025-06-18 14:35:38,990 - mmdet - INFO - Epoch [4][6350/7033]	lr: 1.001e-04, eta: 10:13:38, time: 2.811, data_time: 0.053, memory: 20362, loss_cls: 0.0728, loss_bbox: 0.2030, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3423, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2518, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2062, loss: 2.0385, grad_norm: 31.0355
2025-06-18 14:37:54,901 - mmdet - INFO - Epoch [4][6400/7033]	lr: 1.001e-04, eta: 10:11:39, time: 2.719, data_time: 0.059, memory: 20362, loss_cls: 0.0754, loss_bbox: 0.1980, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2005, loss: 1.9828, grad_norm: 20.7665
2025-06-18 14:40:14,132 - mmdet - INFO - Epoch [4][6450/7033]	lr: 1.001e-04, eta: 10:09:42, time: 2.784, data_time: 0.084, memory: 20362, loss_cls: 0.0719, loss_bbox: 0.1982, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3283, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2015, loss: 1.9962, grad_norm: 19.9299
2025-06-18 14:42:31,986 - mmdet - INFO - Epoch [4][6500/7033]	lr: 1.001e-04, eta: 10:07:44, time: 2.757, data_time: 0.053, memory: 20362, loss_cls: 0.0724, loss_bbox: 0.2022, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3352, d1.loss_cls: 0.1083, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2049, loss: 2.0270, grad_norm: 80.8086
2025-06-18 14:44:47,999 - mmdet - INFO - Epoch [4][6550/7033]	lr: 1.001e-04, eta: 10:05:45, time: 2.721, data_time: 0.053, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.1988, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2066, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2005, loss: 1.9794, grad_norm: 25.8901
2025-06-18 14:47:06,058 - mmdet - INFO - Epoch [4][6600/7033]	lr: 1.001e-04, eta: 10:03:47, time: 2.761, data_time: 0.050, memory: 20362, loss_cls: 0.0769, loss_bbox: 0.2053, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3221, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2141, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2084, loss: 2.0492, grad_norm: 33.4973
2025-06-18 14:49:20,224 - mmdet - INFO - Epoch [4][6650/7033]	lr: 1.001e-04, eta: 10:01:47, time: 2.683, data_time: 0.051, memory: 20362, loss_cls: 0.0695, loss_bbox: 0.1984, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3230, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2010, loss: 1.9753, grad_norm: 52.3570
2025-06-18 14:51:37,491 - mmdet - INFO - Epoch [4][6700/7033]	lr: 1.001e-04, eta: 9:59:48, time: 2.745, data_time: 0.051, memory: 20362, loss_cls: 0.0805, loss_bbox: 0.2030, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3316, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2469, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2202, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2068, loss: 2.0573, grad_norm: 24.9135
2025-06-18 14:53:53,631 - mmdet - INFO - Epoch [4][6750/7033]	lr: 1.001e-04, eta: 9:57:49, time: 2.723, data_time: 0.054, memory: 20362, loss_cls: 0.0766, loss_bbox: 0.2068, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1116, d1.loss_bbox: 0.2542, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2094, loss: 2.0791, grad_norm: 25.7610
2025-06-18 14:56:08,386 - mmdet - INFO - Epoch [4][6800/7033]	lr: 1.001e-04, eta: 9:55:49, time: 2.695, data_time: 0.049, memory: 20362, loss_cls: 0.0671, loss_bbox: 0.1969, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2003, loss: 1.9502, grad_norm: 20.7745
2025-06-18 14:58:23,990 - mmdet - INFO - Epoch [4][6850/7033]	lr: 1.001e-04, eta: 9:53:50, time: 2.712, data_time: 0.050, memory: 20362, loss_cls: 0.0631, loss_bbox: 0.1947, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3218, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1974, loss: 1.9082, grad_norm: 18.7803
2025-06-18 15:00:38,663 - mmdet - INFO - Epoch [4][6900/7033]	lr: 1.001e-04, eta: 9:51:49, time: 2.693, data_time: 0.054, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.2060, d0.loss_cls: 0.1619, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2164, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2096, loss: 2.0327, grad_norm: 73.2545
2025-06-18 15:02:53,626 - mmdet - INFO - Epoch [4][6950/7033]	lr: 1.001e-04, eta: 9:49:49, time: 2.699, data_time: 0.056, memory: 20362, loss_cls: 0.0699, loss_bbox: 0.2070, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3349, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2106, loss: 2.0311, grad_norm: 26.8095
2025-06-18 15:05:10,429 - mmdet - INFO - Epoch [4][7000/7033]	lr: 1.001e-04, eta: 9:47:50, time: 2.736, data_time: 0.054, memory: 20362, loss_cls: 0.0720, loss_bbox: 0.2076, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2112, loss: 2.0367, grad_norm: 26.3798
2025-06-18 15:06:39,503 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-06-18 15:57:47,282 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-18 15:57:47,282 - mmdet - INFO - Epoch(val) [4][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7979, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8855, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9112, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9230, pts_bbox_NuScenes/car_trans_err: 0.1796, pts_bbox_NuScenes/car_scale_err: 0.1540, pts_bbox_NuScenes/car_orient_err: 0.0440, pts_bbox_NuScenes/car_vel_err: 0.2734, pts_bbox_NuScenes/car_attr_err: 0.1875, pts_bbox_NuScenes/mATE: 0.2949, pts_bbox_NuScenes/mASE: 0.2652, pts_bbox_NuScenes/mAOE: 0.2419, pts_bbox_NuScenes/mAVE: 0.2749, pts_bbox_NuScenes/mAAE: 0.1794, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4244, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6208, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7210, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7578, pts_bbox_NuScenes/truck_trans_err: 0.3406, pts_bbox_NuScenes/truck_scale_err: 0.1938, pts_bbox_NuScenes/truck_orient_err: 0.0449, pts_bbox_NuScenes/truck_vel_err: 0.2805, pts_bbox_NuScenes/truck_attr_err: 0.2018, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0596, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2063, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4067, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4663, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6487, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4444, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7947, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1066, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2955, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4944, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7283, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9036, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9278, pts_bbox_NuScenes/bus_trans_err: 0.3672, pts_bbox_NuScenes/bus_scale_err: 0.1836, pts_bbox_NuScenes/bus_orient_err: 0.0383, pts_bbox_NuScenes/bus_vel_err: 0.4649, pts_bbox_NuScenes/bus_attr_err: 0.2504, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1652, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4116, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5822, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6654, pts_bbox_NuScenes/trailer_trans_err: 0.4928, pts_bbox_NuScenes/trailer_scale_err: 0.2196, pts_bbox_NuScenes/trailer_orient_err: 0.3721, pts_bbox_NuScenes/trailer_vel_err: 0.2580, pts_bbox_NuScenes/trailer_attr_err: 0.1510, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5865, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6824, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7290, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7459, pts_bbox_NuScenes/barrier_trans_err: 0.2166, pts_bbox_NuScenes/barrier_scale_err: 0.2929, pts_bbox_NuScenes/barrier_orient_err: 0.0495, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6127, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7411, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7741, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7828, pts_bbox_NuScenes/motorcycle_trans_err: 0.2246, pts_bbox_NuScenes/motorcycle_scale_err: 0.2578, pts_bbox_NuScenes/motorcycle_orient_err: 0.2121, pts_bbox_NuScenes/motorcycle_vel_err: 0.3826, pts_bbox_NuScenes/motorcycle_attr_err: 0.2334, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5461, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5956, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6086, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6160, pts_bbox_NuScenes/bicycle_trans_err: 0.1845, pts_bbox_NuScenes/bicycle_scale_err: 0.2758, pts_bbox_NuScenes/bicycle_orient_err: 0.3007, pts_bbox_NuScenes/bicycle_vel_err: 0.2114, pts_bbox_NuScenes/bicycle_attr_err: 0.0078, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8106, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8524, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8735, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8853, pts_bbox_NuScenes/pedestrian_trans_err: 0.1542, pts_bbox_NuScenes/pedestrian_scale_err: 0.2958, pts_bbox_NuScenes/pedestrian_orient_err: 0.3205, pts_bbox_NuScenes/pedestrian_vel_err: 0.2221, pts_bbox_NuScenes/pedestrian_attr_err: 0.1080, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7272, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7617, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7869, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8104, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1398, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3338, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7067, pts_bbox_NuScenes/mAP: 0.6647
2025-06-18 16:00:00,619 - mmdet - INFO - Epoch [5][50/7033]	lr: 5.015e-05, eta: 9:43:44, time: 2.592, data_time: 0.247, memory: 20362, loss_cls: 0.0733, loss_bbox: 0.2023, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1107, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2054, loss: 2.0349, grad_norm: 17.2690
2025-06-18 16:02:04,122 - mmdet - INFO - Epoch [5][100/7033]	lr: 5.015e-05, eta: 9:41:38, time: 2.470, data_time: 0.076, memory: 20362, loss_cls: 0.0683, loss_bbox: 0.1980, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3212, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2067, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2008, loss: 1.9464, grad_norm: 35.6551
2025-06-18 16:04:09,854 - mmdet - INFO - Epoch [5][150/7033]	lr: 5.015e-05, eta: 9:39:33, time: 2.514, data_time: 0.052, memory: 20362, loss_cls: 0.0766, loss_bbox: 0.2037, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2534, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2069, loss: 2.0559, grad_norm: 24.6561
2025-06-18 16:06:14,623 - mmdet - INFO - Epoch [5][200/7033]	lr: 5.015e-05, eta: 9:37:28, time: 2.495, data_time: 0.055, memory: 20362, loss_cls: 0.0685, loss_bbox: 0.1992, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3227, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2018, loss: 1.9634, grad_norm: 19.9651
2025-06-18 16:08:20,820 - mmdet - INFO - Epoch [5][250/7033]	lr: 5.015e-05, eta: 9:35:24, time: 2.524, data_time: 0.054, memory: 20362, loss_cls: 0.0691, loss_bbox: 0.2064, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3213, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2099, loss: 1.9892, grad_norm: 21.6232
2025-06-18 16:10:30,250 - mmdet - INFO - Epoch [5][300/7033]	lr: 5.015e-05, eta: 9:33:21, time: 2.588, data_time: 0.054, memory: 20362, loss_cls: 0.0710, loss_bbox: 0.2039, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3245, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2080, loss: 2.0057, grad_norm: 18.3928
2025-06-18 16:12:37,862 - mmdet - INFO - Epoch [5][350/7033]	lr: 5.015e-05, eta: 9:31:18, time: 2.553, data_time: 0.049, memory: 20362, loss_cls: 0.0658, loss_bbox: 0.1996, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3260, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2012, loss: 1.9623, grad_norm: 48.3599
2025-06-18 16:14:50,081 - mmdet - INFO - Epoch [5][400/7033]	lr: 5.015e-05, eta: 9:29:16, time: 2.644, data_time: 0.053, memory: 20362, loss_cls: 0.0607, loss_bbox: 0.1890, d0.loss_cls: 0.1515, d0.loss_bbox: 0.3165, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0678, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0616, d4.loss_bbox: 0.1929, loss: 1.8539, grad_norm: 26.0265
2025-06-18 16:17:01,052 - mmdet - INFO - Epoch [5][450/7033]	lr: 5.015e-05, eta: 9:27:14, time: 2.619, data_time: 0.051, memory: 20362, loss_cls: 0.0714, loss_bbox: 0.1995, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2024, loss: 1.9802, grad_norm: 19.4865
2025-06-18 16:19:10,940 - mmdet - INFO - Epoch [5][500/7033]	lr: 5.015e-05, eta: 9:25:11, time: 2.598, data_time: 0.055, memory: 20362, loss_cls: 0.0774, loss_bbox: 0.2056, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3335, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2246, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2152, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2079, loss: 2.0599, grad_norm: 31.7382
2025-06-18 16:21:22,828 - mmdet - INFO - Epoch [5][550/7033]	lr: 5.015e-05, eta: 9:23:10, time: 2.638, data_time: 0.053, memory: 20362, loss_cls: 0.0663, loss_bbox: 0.1944, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3152, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0675, d4.loss_bbox: 0.1971, loss: 1.9250, grad_norm: 24.7162
2025-06-18 16:23:30,367 - mmdet - INFO - Epoch [5][600/7033]	lr: 5.015e-05, eta: 9:21:06, time: 2.551, data_time: 0.051, memory: 20362, loss_cls: 0.0704, loss_bbox: 0.1964, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3277, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2000, loss: 1.9740, grad_norm: 24.1275
2025-06-18 16:25:41,334 - mmdet - INFO - Epoch [5][650/7033]	lr: 5.015e-05, eta: 9:19:04, time: 2.619, data_time: 0.049, memory: 20362, loss_cls: 0.0677, loss_bbox: 0.2033, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3235, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2509, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2064, loss: 1.9806, grad_norm: 45.6204
2025-06-18 16:27:51,970 - mmdet - INFO - Epoch [5][700/7033]	lr: 5.015e-05, eta: 9:17:01, time: 2.613, data_time: 0.051, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1938, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3194, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1960, loss: 1.9092, grad_norm: 16.9087
2025-06-18 16:30:01,826 - mmdet - INFO - Epoch [5][750/7033]	lr: 5.015e-05, eta: 9:14:58, time: 2.597, data_time: 0.051, memory: 20362, loss_cls: 0.0669, loss_bbox: 0.1895, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3183, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0686, d4.loss_bbox: 0.1918, loss: 1.9171, grad_norm: 27.9144
2025-06-18 16:32:16,283 - mmdet - INFO - Epoch [5][800/7033]	lr: 5.015e-05, eta: 9:12:58, time: 2.689, data_time: 0.071, memory: 20362, loss_cls: 0.0639, loss_bbox: 0.2020, d0.loss_cls: 0.1535, d0.loss_bbox: 0.3281, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2046, loss: 1.9549, grad_norm: 21.6518
2025-06-18 16:34:25,275 - mmdet - INFO - Epoch [5][850/7033]	lr: 5.015e-05, eta: 9:10:54, time: 2.580, data_time: 0.054, memory: 20362, loss_cls: 0.0647, loss_bbox: 0.2016, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3318, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2038, loss: 1.9779, grad_norm: 26.4511
2025-06-18 16:36:35,831 - mmdet - INFO - Epoch [5][900/7033]	lr: 5.015e-05, eta: 9:08:52, time: 2.611, data_time: 0.050, memory: 20362, loss_cls: 0.0643, loss_bbox: 0.1974, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3197, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0666, d4.loss_bbox: 0.2001, loss: 1.9391, grad_norm: 48.6672
2025-06-18 16:38:47,798 - mmdet - INFO - Epoch [5][950/7033]	lr: 5.015e-05, eta: 9:06:50, time: 2.639, data_time: 0.050, memory: 20362, loss_cls: 0.0562, loss_bbox: 0.1850, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3148, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0724, d2.loss_bbox: 0.2064, d3.loss_cls: 0.0622, d3.loss_bbox: 0.1956, d4.loss_cls: 0.0581, d4.loss_bbox: 0.1879, loss: 1.8182, grad_norm: 40.5880
2025-06-18 16:40:58,300 - mmdet - INFO - Epoch [5][1000/7033]	lr: 5.015e-05, eta: 9:04:47, time: 2.610, data_time: 0.052, memory: 20362, loss_cls: 0.0693, loss_bbox: 0.2016, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3258, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2456, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2056, loss: 1.9749, grad_norm: 18.8767
2025-06-18 16:43:13,313 - mmdet - INFO - Epoch [5][1050/7033]	lr: 5.015e-05, eta: 9:02:47, time: 2.700, data_time: 0.072, memory: 20362, loss_cls: 0.0626, loss_bbox: 0.1943, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3219, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1982, loss: 1.9129, grad_norm: 20.1681
2025-06-18 16:45:26,602 - mmdet - INFO - Epoch [5][1100/7033]	lr: 5.015e-05, eta: 9:00:45, time: 2.666, data_time: 0.047, memory: 20362, loss_cls: 0.0614, loss_bbox: 0.1933, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3247, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2128, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2036, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1953, loss: 1.9085, grad_norm: 17.6754
2025-06-18 16:47:52,199 - mmdet - INFO - Epoch [5][1150/7033]	lr: 5.015e-05, eta: 8:58:49, time: 2.911, data_time: 0.053, memory: 20362, loss_cls: 0.0606, loss_bbox: 0.1898, d0.loss_cls: 0.1467, d0.loss_bbox: 0.3075, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0699, d3.loss_bbox: 0.1988, d4.loss_cls: 0.0616, d4.loss_bbox: 0.1929, loss: 1.8497, grad_norm: 19.7591
2025-06-18 16:50:05,615 - mmdet - INFO - Epoch [5][1200/7033]	lr: 5.015e-05, eta: 8:56:48, time: 2.669, data_time: 0.054, memory: 20362, loss_cls: 0.0631, loss_bbox: 0.1875, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3141, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2091, d3.loss_cls: 0.0705, d3.loss_bbox: 0.1988, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1898, loss: 1.8762, grad_norm: 16.1668
2025-06-18 16:52:20,126 - mmdet - INFO - Epoch [5][1250/7033]	lr: 5.015e-05, eta: 8:54:46, time: 2.690, data_time: 0.053, memory: 20362, loss_cls: 0.0633, loss_bbox: 0.1928, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1957, loss: 1.9089, grad_norm: 20.0104
2025-06-18 16:54:32,917 - mmdet - INFO - Epoch [5][1300/7033]	lr: 5.015e-05, eta: 8:52:45, time: 2.656, data_time: 0.048, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1914, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2002, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1944, loss: 1.8685, grad_norm: 25.5804
2025-06-18 16:56:44,672 - mmdet - INFO - Epoch [5][1350/7033]	lr: 5.015e-05, eta: 8:50:42, time: 2.635, data_time: 0.051, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.1965, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2070, d4.loss_cls: 0.0721, d4.loss_bbox: 0.1995, loss: 1.9791, grad_norm: 16.7702
2025-06-18 16:58:58,966 - mmdet - INFO - Epoch [5][1400/7033]	lr: 5.015e-05, eta: 8:48:41, time: 2.686, data_time: 0.050, memory: 20362, loss_cls: 0.0634, loss_bbox: 0.1862, d0.loss_cls: 0.1549, d0.loss_bbox: 0.3145, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2061, d3.loss_cls: 0.0710, d3.loss_bbox: 0.1958, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1890, loss: 1.8596, grad_norm: 21.1256
2025-06-18 17:01:14,458 - mmdet - INFO - Epoch [5][1450/7033]	lr: 5.015e-05, eta: 8:46:40, time: 2.710, data_time: 0.049, memory: 20362, loss_cls: 0.0685, loss_bbox: 0.2003, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3288, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2463, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2040, loss: 1.9874, grad_norm: 38.6424
2025-06-18 17:03:27,213 - mmdet - INFO - Epoch [5][1500/7033]	lr: 5.015e-05, eta: 8:44:38, time: 2.655, data_time: 0.056, memory: 20362, loss_cls: 0.0675, loss_bbox: 0.1951, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1987, loss: 1.9171, grad_norm: 14.8298
2025-06-18 17:05:43,025 - mmdet - INFO - Epoch [5][1550/7033]	lr: 5.015e-05, eta: 8:42:37, time: 2.716, data_time: 0.052, memory: 20362, loss_cls: 0.0658, loss_bbox: 0.2021, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0685, d4.loss_bbox: 0.2050, loss: 1.9774, grad_norm: 17.8685
2025-06-18 17:07:58,483 - mmdet - INFO - Epoch [5][1600/7033]	lr: 5.015e-05, eta: 8:40:36, time: 2.709, data_time: 0.051, memory: 20362, loss_cls: 0.0653, loss_bbox: 0.1939, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3164, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1966, loss: 1.9085, grad_norm: 24.8525
2025-06-18 17:10:12,621 - mmdet - INFO - Epoch [5][1650/7033]	lr: 5.015e-05, eta: 8:38:35, time: 2.683, data_time: 0.052, memory: 20362, loss_cls: 0.0650, loss_bbox: 0.1980, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3235, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0661, d4.loss_bbox: 0.2020, loss: 1.9434, grad_norm: 118.8581
2025-06-18 17:12:26,256 - mmdet - INFO - Epoch [5][1700/7033]	lr: 5.015e-05, eta: 8:36:33, time: 2.673, data_time: 0.050, memory: 20362, loss_cls: 0.0661, loss_bbox: 0.1929, d0.loss_cls: 0.1587, d0.loss_bbox: 0.3190, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2034, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1960, loss: 1.9130, grad_norm: 29.5794
2025-06-18 17:14:40,985 - mmdet - INFO - Epoch [5][1750/7033]	lr: 5.015e-05, eta: 8:34:31, time: 2.695, data_time: 0.048, memory: 20362, loss_cls: 0.0696, loss_bbox: 0.1914, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3101, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2335, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0758, d3.loss_bbox: 0.1999, d4.loss_cls: 0.0707, d4.loss_bbox: 0.1941, loss: 1.9101, grad_norm: 23.4130
2025-06-18 17:16:56,144 - mmdet - INFO - Epoch [5][1800/7033]	lr: 5.015e-05, eta: 8:32:30, time: 2.703, data_time: 0.052, memory: 20362, loss_cls: 0.0694, loss_bbox: 0.2012, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3284, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2038, loss: 1.9783, grad_norm: 32.0200
2025-06-18 17:19:08,064 - mmdet - INFO - Epoch [5][1850/7033]	lr: 5.015e-05, eta: 8:30:27, time: 2.638, data_time: 0.048, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1959, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3138, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1989, loss: 1.8893, grad_norm: 35.0930
2025-06-18 17:21:22,115 - mmdet - INFO - Epoch [5][1900/7033]	lr: 5.015e-05, eta: 8:28:26, time: 2.681, data_time: 0.049, memory: 20362, loss_cls: 0.0673, loss_bbox: 0.2047, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0691, d4.loss_bbox: 0.2078, loss: 1.9886, grad_norm: 23.2431
2025-06-18 17:23:39,794 - mmdet - INFO - Epoch [5][1950/7033]	lr: 5.015e-05, eta: 8:26:25, time: 2.754, data_time: 0.052, memory: 20362, loss_cls: 0.0678, loss_bbox: 0.1969, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2446, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2182, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2006, loss: 1.9627, grad_norm: 37.9355
2025-06-18 17:25:53,175 - mmdet - INFO - Epoch [5][2000/7033]	lr: 5.015e-05, eta: 8:24:23, time: 2.668, data_time: 0.048, memory: 20362, loss_cls: 0.0636, loss_bbox: 0.1975, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2408, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1992, loss: 1.9257, grad_norm: 20.5680
2025-06-18 17:28:11,784 - mmdet - INFO - Epoch [5][2050/7033]	lr: 5.015e-05, eta: 8:22:23, time: 2.772, data_time: 0.051, memory: 20362, loss_cls: 0.0672, loss_bbox: 0.1962, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3195, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2047, d4.loss_cls: 0.0691, d4.loss_bbox: 0.1991, loss: 1.9240, grad_norm: 41.8217
2025-06-18 17:30:25,259 - mmdet - INFO - Epoch [5][2100/7033]	lr: 5.015e-05, eta: 8:20:21, time: 2.669, data_time: 0.057, memory: 20362, loss_cls: 0.0676, loss_bbox: 0.1916, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1954, loss: 1.9388, grad_norm: 18.7435
2025-06-18 17:32:40,353 - mmdet - INFO - Epoch [5][2150/7033]	lr: 5.015e-05, eta: 8:18:19, time: 2.702, data_time: 0.054, memory: 20362, loss_cls: 0.0642, loss_bbox: 0.1955, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3264, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1987, loss: 1.9226, grad_norm: 39.5493
2025-06-18 17:34:58,211 - mmdet - INFO - Epoch [5][2200/7033]	lr: 5.015e-05, eta: 8:16:18, time: 2.757, data_time: 0.054, memory: 20362, loss_cls: 0.0633, loss_bbox: 0.1919, d0.loss_cls: 0.1606, d0.loss_bbox: 0.3106, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2114, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1950, loss: 1.8947, grad_norm: 57.8223
2025-06-18 17:37:13,265 - mmdet - INFO - Epoch [5][2250/7033]	lr: 5.015e-05, eta: 8:14:16, time: 2.701, data_time: 0.060, memory: 20362, loss_cls: 0.0617, loss_bbox: 0.1993, d0.loss_cls: 0.1531, d0.loss_bbox: 0.3152, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0642, d4.loss_bbox: 0.2014, loss: 1.9059, grad_norm: 23.2134
2025-06-18 17:39:32,742 - mmdet - INFO - Epoch [5][2300/7033]	lr: 5.015e-05, eta: 8:12:16, time: 2.789, data_time: 0.057, memory: 20362, loss_cls: 0.0670, loss_bbox: 0.1958, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3308, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1992, loss: 1.9529, grad_norm: 49.4012
2025-06-18 17:41:47,498 - mmdet - INFO - Epoch [5][2350/7033]	lr: 5.015e-05, eta: 8:10:14, time: 2.696, data_time: 0.055, memory: 20362, loss_cls: 0.0640, loss_bbox: 0.1971, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3202, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2001, loss: 1.9204, grad_norm: 41.6399
2025-06-18 17:44:07,808 - mmdet - INFO - Epoch [5][2400/7033]	lr: 5.015e-05, eta: 8:08:15, time: 2.806, data_time: 0.063, memory: 20362, loss_cls: 0.0652, loss_bbox: 0.1902, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3247, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2112, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0672, d4.loss_bbox: 0.1926, loss: 1.8953, grad_norm: 17.1348
2025-06-18 17:46:22,720 - mmdet - INFO - Epoch [5][2450/7033]	lr: 5.015e-05, eta: 8:06:13, time: 2.698, data_time: 0.061, memory: 20362, loss_cls: 0.0690, loss_bbox: 0.1985, d0.loss_cls: 0.1619, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2019, loss: 1.9670, grad_norm: 19.7903
2025-06-18 17:48:39,972 - mmdet - INFO - Epoch [5][2500/7033]	lr: 5.015e-05, eta: 8:04:11, time: 2.746, data_time: 0.056, memory: 20362, loss_cls: 0.0608, loss_bbox: 0.1862, d0.loss_cls: 0.1457, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0646, d3.loss_bbox: 0.1973, d4.loss_cls: 0.0614, d4.loss_bbox: 0.1891, loss: 1.8217, grad_norm: 20.6798
2025-06-18 17:50:55,976 - mmdet - INFO - Epoch [5][2550/7033]	lr: 5.015e-05, eta: 8:02:10, time: 2.720, data_time: 0.053, memory: 20362, loss_cls: 0.0658, loss_bbox: 0.1891, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3154, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2066, d3.loss_cls: 0.0737, d3.loss_bbox: 0.1987, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1917, loss: 1.8799, grad_norm: 32.4420
2025-06-18 17:53:15,177 - mmdet - INFO - Epoch [5][2600/7033]	lr: 5.015e-05, eta: 8:00:09, time: 2.784, data_time: 0.054, memory: 20362, loss_cls: 0.0730, loss_bbox: 0.1999, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3249, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2016, loss: 1.9909, grad_norm: 41.9699
2025-06-18 17:55:32,294 - mmdet - INFO - Epoch [5][2650/7033]	lr: 5.015e-05, eta: 7:58:08, time: 2.742, data_time: 0.058, memory: 20362, loss_cls: 0.0670, loss_bbox: 0.1943, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3222, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0680, d4.loss_bbox: 0.1978, loss: 1.9172, grad_norm: 18.3412
2025-06-18 17:57:48,841 - mmdet - INFO - Epoch [5][2700/7033]	lr: 5.015e-05, eta: 7:56:06, time: 2.731, data_time: 0.051, memory: 20362, loss_cls: 0.0649, loss_bbox: 0.1944, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3184, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0668, d4.loss_bbox: 0.1973, loss: 1.9077, grad_norm: 30.4836
2025-06-18 18:00:05,942 - mmdet - INFO - Epoch [5][2750/7033]	lr: 5.015e-05, eta: 7:54:05, time: 2.742, data_time: 0.073, memory: 20362, loss_cls: 0.0710, loss_bbox: 0.1996, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3218, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2021, loss: 1.9643, grad_norm: 22.5590
2025-06-18 18:02:24,183 - mmdet - INFO - Epoch [5][2800/7033]	lr: 5.015e-05, eta: 7:52:04, time: 2.765, data_time: 0.052, memory: 20362, loss_cls: 0.0689, loss_bbox: 0.1970, d0.loss_cls: 0.1569, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2005, loss: 1.9424, grad_norm: 72.2515
2025-06-18 18:04:40,882 - mmdet - INFO - Epoch [5][2850/7033]	lr: 5.015e-05, eta: 7:50:02, time: 2.734, data_time: 0.055, memory: 20362, loss_cls: 0.0729, loss_bbox: 0.2030, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2472, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2070, loss: 2.0124, grad_norm: 40.9697
2025-06-18 18:06:59,981 - mmdet - INFO - Epoch [5][2900/7033]	lr: 5.015e-05, eta: 7:48:01, time: 2.782, data_time: 0.053, memory: 20362, loss_cls: 0.0631, loss_bbox: 0.1874, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0971, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2067, d3.loss_cls: 0.0691, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1904, loss: 1.8561, grad_norm: 36.7277
2025-06-18 18:09:17,454 - mmdet - INFO - Epoch [5][2950/7033]	lr: 5.015e-05, eta: 7:45:59, time: 2.750, data_time: 0.048, memory: 20362, loss_cls: 0.0709, loss_bbox: 0.2049, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2092, loss: 2.0085, grad_norm: 24.8004
2025-06-18 18:11:35,357 - mmdet - INFO - Epoch [5][3000/7033]	lr: 5.015e-05, eta: 7:43:58, time: 2.758, data_time: 0.051, memory: 20362, loss_cls: 0.0647, loss_bbox: 0.1901, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3142, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0657, d4.loss_bbox: 0.1944, loss: 1.8837, grad_norm: 28.5065
2025-06-18 18:13:52,459 - mmdet - INFO - Epoch [5][3050/7033]	lr: 5.015e-05, eta: 7:41:56, time: 2.742, data_time: 0.052, memory: 20362, loss_cls: 0.0596, loss_bbox: 0.1973, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3241, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1988, loss: 1.9011, grad_norm: 25.0824
2025-06-18 18:16:08,458 - mmdet - INFO - Epoch [5][3100/7033]	lr: 5.015e-05, eta: 7:39:54, time: 2.720, data_time: 0.048, memory: 20362, loss_cls: 0.0708, loss_bbox: 0.2014, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3302, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2046, loss: 1.9870, grad_norm: 22.1086
2025-06-18 18:18:26,836 - mmdet - INFO - Epoch [5][3150/7033]	lr: 5.015e-05, eta: 7:37:52, time: 2.768, data_time: 0.052, memory: 20362, loss_cls: 0.0625, loss_bbox: 0.1987, d0.loss_cls: 0.1499, d0.loss_bbox: 0.3180, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2100, d4.loss_cls: 0.0631, d4.loss_bbox: 0.2028, loss: 1.9073, grad_norm: 19.5007
2025-06-18 18:20:47,224 - mmdet - INFO - Epoch [5][3200/7033]	lr: 5.015e-05, eta: 7:35:52, time: 2.807, data_time: 0.052, memory: 20362, loss_cls: 0.0614, loss_bbox: 0.1896, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2085, d3.loss_cls: 0.0702, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1912, loss: 1.8726, grad_norm: 104.8803
2025-06-18 18:23:01,756 - mmdet - INFO - Epoch [5][3250/7033]	lr: 5.015e-05, eta: 7:33:49, time: 2.691, data_time: 0.051, memory: 20362, loss_cls: 0.0625, loss_bbox: 0.1942, d0.loss_cls: 0.1584, d0.loss_bbox: 0.3256, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2026, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1969, loss: 1.9066, grad_norm: 15.9287
2025-06-18 18:25:21,033 - mmdet - INFO - Epoch [5][3300/7033]	lr: 5.015e-05, eta: 7:31:48, time: 2.785, data_time: 0.049, memory: 20362, loss_cls: 0.0710, loss_bbox: 0.2027, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3287, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2052, loss: 1.9853, grad_norm: 42.3090
2025-06-18 18:27:36,851 - mmdet - INFO - Epoch [5][3350/7033]	lr: 5.015e-05, eta: 7:29:45, time: 2.716, data_time: 0.052, memory: 20362, loss_cls: 0.0641, loss_bbox: 0.2001, d0.loss_cls: 0.1581, d0.loss_bbox: 0.3238, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2460, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0703, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0656, d4.loss_bbox: 0.2038, loss: 1.9452, grad_norm: 35.0669
2025-06-18 18:29:54,206 - mmdet - INFO - Epoch [5][3400/7033]	lr: 5.015e-05, eta: 7:27:43, time: 2.747, data_time: 0.054, memory: 20362, loss_cls: 0.0644, loss_bbox: 0.1920, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2356, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2009, d4.loss_cls: 0.0665, d4.loss_bbox: 0.1943, loss: 1.9004, grad_norm: 86.9219
2025-06-18 18:32:12,689 - mmdet - INFO - Epoch [5][3450/7033]	lr: 5.015e-05, eta: 7:25:41, time: 2.770, data_time: 0.046, memory: 20362, loss_cls: 0.0628, loss_bbox: 0.1897, d0.loss_cls: 0.1456, d0.loss_bbox: 0.3187, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1930, loss: 1.8587, grad_norm: 19.1066
2025-06-18 18:34:28,857 - mmdet - INFO - Epoch [5][3500/7033]	lr: 5.015e-05, eta: 7:23:39, time: 2.723, data_time: 0.052, memory: 20362, loss_cls: 0.0638, loss_bbox: 0.1911, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3132, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2100, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1932, loss: 1.8801, grad_norm: 18.5230
2025-06-18 18:36:46,011 - mmdet - INFO - Epoch [5][3550/7033]	lr: 5.015e-05, eta: 7:21:36, time: 2.743, data_time: 0.051, memory: 20362, loss_cls: 0.0642, loss_bbox: 0.1919, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3168, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1944, loss: 1.8871, grad_norm: 27.1208
2025-06-18 18:39:06,650 - mmdet - INFO - Epoch [5][3600/7033]	lr: 5.015e-05, eta: 7:19:35, time: 2.813, data_time: 0.071, memory: 20362, loss_cls: 0.0672, loss_bbox: 0.1925, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3134, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1951, loss: 1.9050, grad_norm: 36.4215
2025-06-18 18:41:22,290 - mmdet - INFO - Epoch [5][3650/7033]	lr: 5.015e-05, eta: 7:17:32, time: 2.713, data_time: 0.049, memory: 20362, loss_cls: 0.0686, loss_bbox: 0.1911, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3181, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0706, d4.loss_bbox: 0.1942, loss: 1.9149, grad_norm: 18.2421
2025-06-18 18:43:38,393 - mmdet - INFO - Epoch [5][3700/7033]	lr: 5.015e-05, eta: 7:15:30, time: 2.722, data_time: 0.054, memory: 20362, loss_cls: 0.0621, loss_bbox: 0.1899, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2009, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1936, loss: 1.8903, grad_norm: 35.1918
2025-06-18 18:45:53,318 - mmdet - INFO - Epoch [5][3750/7033]	lr: 5.015e-05, eta: 7:13:27, time: 2.698, data_time: 0.054, memory: 20362, loss_cls: 0.0660, loss_bbox: 0.2006, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2429, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2037, loss: 1.9563, grad_norm: 23.1283
2025-06-18 18:48:08,944 - mmdet - INFO - Epoch [5][3800/7033]	lr: 5.015e-05, eta: 7:11:24, time: 2.713, data_time: 0.047, memory: 20362, loss_cls: 0.0674, loss_bbox: 0.1904, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3116, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2312, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2002, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1937, loss: 1.8956, grad_norm: 16.2008
2025-06-18 18:50:22,144 - mmdet - INFO - Epoch [5][3850/7033]	lr: 5.015e-05, eta: 7:09:20, time: 2.663, data_time: 0.049, memory: 20362, loss_cls: 0.0601, loss_bbox: 0.1876, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3047, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2067, d3.loss_cls: 0.0660, d3.loss_bbox: 0.1973, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1899, loss: 1.8187, grad_norm: 36.2784
2025-06-18 18:52:36,794 - mmdet - INFO - Epoch [5][3900/7033]	lr: 5.015e-05, eta: 7:07:16, time: 2.693, data_time: 0.053, memory: 20362, loss_cls: 0.0676, loss_bbox: 0.1943, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3264, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1976, loss: 1.9387, grad_norm: 29.8692
2025-06-18 18:54:48,965 - mmdet - INFO - Epoch [5][3950/7033]	lr: 5.015e-05, eta: 7:05:12, time: 2.644, data_time: 0.052, memory: 20362, loss_cls: 0.0639, loss_bbox: 0.1905, d0.loss_cls: 0.1480, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2094, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1922, loss: 1.8537, grad_norm: 44.4373
2025-06-18 18:57:04,827 - mmdet - INFO - Epoch [5][4000/7033]	lr: 5.015e-05, eta: 7:03:09, time: 2.717, data_time: 0.051, memory: 20362, loss_cls: 0.0661, loss_bbox: 0.1923, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3137, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2308, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2021, d4.loss_cls: 0.0688, d4.loss_bbox: 0.1947, loss: 1.9048, grad_norm: 26.9689
2025-06-18 18:59:18,679 - mmdet - INFO - Epoch [5][4050/7033]	lr: 5.015e-05, eta: 7:01:06, time: 2.677, data_time: 0.048, memory: 20362, loss_cls: 0.0671, loss_bbox: 0.1930, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3160, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0703, d4.loss_bbox: 0.1945, loss: 1.9070, grad_norm: 58.4562
2025-06-18 19:01:29,264 - mmdet - INFO - Epoch [5][4100/7033]	lr: 5.015e-05, eta: 6:59:01, time: 2.612, data_time: 0.048, memory: 20362, loss_cls: 0.0646, loss_bbox: 0.1944, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3236, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0665, d4.loss_bbox: 0.1973, loss: 1.9125, grad_norm: 23.3959
2025-06-18 19:03:42,706 - mmdet - INFO - Epoch [5][4150/7033]	lr: 5.015e-05, eta: 6:56:57, time: 2.669, data_time: 0.052, memory: 20362, loss_cls: 0.0652, loss_bbox: 0.1941, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3241, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2042, d4.loss_cls: 0.0668, d4.loss_bbox: 0.1966, loss: 1.9230, grad_norm: 23.3737
2025-06-18 19:05:57,574 - mmdet - INFO - Epoch [5][4200/7033]	lr: 5.015e-05, eta: 6:54:53, time: 2.697, data_time: 0.057, memory: 20362, loss_cls: 0.0716, loss_bbox: 0.1958, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2055, d4.loss_cls: 0.0727, d4.loss_bbox: 0.1990, loss: 1.9554, grad_norm: 27.7666
2025-06-18 19:08:12,165 - mmdet - INFO - Epoch [5][4250/7033]	lr: 5.015e-05, eta: 6:52:50, time: 2.692, data_time: 0.055, memory: 20362, loss_cls: 0.0691, loss_bbox: 0.1968, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3160, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2016, loss: 1.9377, grad_norm: 19.0053
2025-06-18 19:10:25,951 - mmdet - INFO - Epoch [5][4300/7033]	lr: 5.015e-05, eta: 6:50:46, time: 2.676, data_time: 0.047, memory: 20362, loss_cls: 0.0642, loss_bbox: 0.1875, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3154, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2053, d3.loss_cls: 0.0731, d3.loss_bbox: 0.1944, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1886, loss: 1.8661, grad_norm: 22.9097
2025-06-18 19:12:39,834 - mmdet - INFO - Epoch [5][4350/7033]	lr: 5.015e-05, eta: 6:48:42, time: 2.678, data_time: 0.051, memory: 20362, loss_cls: 0.0601, loss_bbox: 0.1840, d0.loss_cls: 0.1523, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2269, d2.loss_cls: 0.0754, d2.loss_bbox: 0.2048, d3.loss_cls: 0.0663, d3.loss_bbox: 0.1952, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1873, loss: 1.8147, grad_norm: 107.3795
2025-06-18 19:14:51,591 - mmdet - INFO - Epoch [5][4400/7033]	lr: 5.015e-05, eta: 6:46:37, time: 2.635, data_time: 0.049, memory: 20362, loss_cls: 0.0701, loss_bbox: 0.1927, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3114, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2048, d4.loss_cls: 0.0710, d4.loss_bbox: 0.1967, loss: 1.9097, grad_norm: 16.7369
2025-06-18 19:17:00,732 - mmdet - INFO - Epoch [5][4450/7033]	lr: 5.015e-05, eta: 6:44:32, time: 2.583, data_time: 0.047, memory: 20362, loss_cls: 0.0683, loss_bbox: 0.1989, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3181, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0683, d4.loss_bbox: 0.2041, loss: 1.9394, grad_norm: 18.7926
2025-06-18 19:19:11,696 - mmdet - INFO - Epoch [5][4500/7033]	lr: 5.015e-05, eta: 6:42:27, time: 2.619, data_time: 0.051, memory: 20362, loss_cls: 0.0604, loss_bbox: 0.1889, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3146, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1997, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1918, loss: 1.8412, grad_norm: 37.4010
2025-06-18 19:21:23,850 - mmdet - INFO - Epoch [5][4550/7033]	lr: 5.015e-05, eta: 6:40:23, time: 2.643, data_time: 0.052, memory: 20362, loss_cls: 0.0645, loss_bbox: 0.1954, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3138, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2054, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1980, loss: 1.8909, grad_norm: 38.3455
2025-06-18 19:23:35,136 - mmdet - INFO - Epoch [5][4600/7033]	lr: 5.015e-05, eta: 6:38:18, time: 2.626, data_time: 0.056, memory: 20362, loss_cls: 0.0651, loss_bbox: 0.1944, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3185, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1975, loss: 1.8953, grad_norm: 35.0023
2025-06-18 19:25:45,781 - mmdet - INFO - Epoch [5][4650/7033]	lr: 5.015e-05, eta: 6:36:13, time: 2.613, data_time: 0.053, memory: 20362, loss_cls: 0.0627, loss_bbox: 0.1897, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3199, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0763, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1937, loss: 1.8739, grad_norm: 23.9632
2025-06-18 19:27:58,512 - mmdet - INFO - Epoch [5][4700/7033]	lr: 5.015e-05, eta: 6:34:09, time: 2.655, data_time: 0.054, memory: 20362, loss_cls: 0.0662, loss_bbox: 0.1932, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2383, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1964, loss: 1.9114, grad_norm: 29.5712
2025-06-18 19:30:09,023 - mmdet - INFO - Epoch [5][4750/7033]	lr: 5.015e-05, eta: 6:32:04, time: 2.610, data_time: 0.052, memory: 20362, loss_cls: 0.0631, loss_bbox: 0.1942, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0773, d2.loss_bbox: 0.2098, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1962, loss: 1.8642, grad_norm: 21.0288
2025-06-18 19:32:15,335 - mmdet - INFO - Epoch [5][4800/7033]	lr: 5.015e-05, eta: 6:29:57, time: 2.526, data_time: 0.049, memory: 20362, loss_cls: 0.0656, loss_bbox: 0.1910, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1934, loss: 1.8847, grad_norm: 21.1408
2025-06-18 19:34:24,220 - mmdet - INFO - Epoch [5][4850/7033]	lr: 5.015e-05, eta: 6:27:52, time: 2.578, data_time: 0.052, memory: 20362, loss_cls: 0.0640, loss_bbox: 0.1965, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3217, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0652, d4.loss_bbox: 0.2005, loss: 1.9220, grad_norm: 35.1298
2025-06-18 19:36:31,547 - mmdet - INFO - Epoch [5][4900/7033]	lr: 5.015e-05, eta: 6:25:46, time: 2.547, data_time: 0.051, memory: 20362, loss_cls: 0.0644, loss_bbox: 0.1897, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3199, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0660, d4.loss_bbox: 0.1927, loss: 1.8968, grad_norm: 48.8002
2025-06-18 19:38:38,516 - mmdet - INFO - Epoch [5][4950/7033]	lr: 5.015e-05, eta: 6:23:40, time: 2.539, data_time: 0.049, memory: 20362, loss_cls: 0.0617, loss_bbox: 0.1868, d0.loss_cls: 0.1506, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0703, d3.loss_bbox: 0.1967, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1911, loss: 1.8439, grad_norm: 52.1518
2025-06-18 19:40:46,906 - mmdet - INFO - Epoch [5][5000/7033]	lr: 5.015e-05, eta: 6:21:34, time: 2.568, data_time: 0.051, memory: 20362, loss_cls: 0.0545, loss_bbox: 0.1839, d0.loss_cls: 0.1457, d0.loss_bbox: 0.3069, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0710, d2.loss_bbox: 0.2050, d3.loss_cls: 0.0611, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0555, d4.loss_bbox: 0.1871, loss: 1.7819, grad_norm: 20.9044
2025-06-18 19:42:51,537 - mmdet - INFO - Epoch [5][5050/7033]	lr: 5.015e-05, eta: 6:19:27, time: 2.493, data_time: 0.052, memory: 20362, loss_cls: 0.0578, loss_bbox: 0.1922, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0658, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1950, loss: 1.8375, grad_norm: 23.7169
2025-06-18 19:44:58,600 - mmdet - INFO - Epoch [5][5100/7033]	lr: 5.015e-05, eta: 6:17:21, time: 2.541, data_time: 0.051, memory: 20362, loss_cls: 0.0681, loss_bbox: 0.1983, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3275, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2016, loss: 1.9592, grad_norm: 37.6355
2025-06-18 19:47:06,630 - mmdet - INFO - Epoch [5][5150/7033]	lr: 5.015e-05, eta: 6:15:16, time: 2.561, data_time: 0.050, memory: 20362, loss_cls: 0.0651, loss_bbox: 0.1955, d0.loss_cls: 0.1533, d0.loss_bbox: 0.3142, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1982, loss: 1.9032, grad_norm: 60.9751
2025-06-18 19:49:13,659 - mmdet - INFO - Epoch [5][5200/7033]	lr: 5.015e-05, eta: 6:13:10, time: 2.541, data_time: 0.049, memory: 20362, loss_cls: 0.0655, loss_bbox: 0.1916, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1952, loss: 1.8798, grad_norm: 30.4616
2025-06-18 19:51:21,841 - mmdet - INFO - Epoch [5][5250/7033]	lr: 5.015e-05, eta: 6:11:04, time: 2.564, data_time: 0.047, memory: 20362, loss_cls: 0.0601, loss_bbox: 0.1951, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3241, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2055, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1995, loss: 1.8992, grad_norm: 18.4649
2025-06-18 19:53:28,955 - mmdet - INFO - Epoch [5][5300/7033]	lr: 5.015e-05, eta: 6:08:58, time: 2.542, data_time: 0.054, memory: 20362, loss_cls: 0.0624, loss_bbox: 0.1868, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3116, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1984, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1902, loss: 1.8449, grad_norm: 67.2332
2025-06-18 19:55:34,391 - mmdet - INFO - Epoch [5][5350/7033]	lr: 5.015e-05, eta: 6:06:51, time: 2.509, data_time: 0.051, memory: 20362, loss_cls: 0.0699, loss_bbox: 0.2049, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2495, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2093, loss: 2.0166, grad_norm: 17.5443
2025-06-18 19:57:44,143 - mmdet - INFO - Epoch [5][5400/7033]	lr: 5.015e-05, eta: 6:04:46, time: 2.595, data_time: 0.050, memory: 20362, loss_cls: 0.0615, loss_bbox: 0.1904, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1928, loss: 1.8648, grad_norm: 19.2477
2025-06-18 19:59:52,722 - mmdet - INFO - Epoch [5][5450/7033]	lr: 5.015e-05, eta: 6:02:40, time: 2.572, data_time: 0.050, memory: 20362, loss_cls: 0.0684, loss_bbox: 0.1955, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3183, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0694, d4.loss_bbox: 0.1985, loss: 1.9343, grad_norm: 84.7429
2025-06-18 20:02:01,612 - mmdet - INFO - Epoch [5][5500/7033]	lr: 5.015e-05, eta: 6:00:35, time: 2.578, data_time: 0.049, memory: 20362, loss_cls: 0.0677, loss_bbox: 0.1973, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2015, loss: 1.9485, grad_norm: 84.5587
2025-06-18 20:04:08,944 - mmdet - INFO - Epoch [5][5550/7033]	lr: 5.015e-05, eta: 5:58:29, time: 2.547, data_time: 0.051, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1889, d0.loss_cls: 0.1546, d0.loss_bbox: 0.3161, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0679, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1933, loss: 1.8600, grad_norm: 15.5514
2025-06-18 20:06:16,363 - mmdet - INFO - Epoch [5][5600/7033]	lr: 5.015e-05, eta: 5:56:23, time: 2.548, data_time: 0.052, memory: 20362, loss_cls: 0.0647, loss_bbox: 0.1909, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3165, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2015, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1950, loss: 1.8931, grad_norm: 17.3085
2025-06-18 20:08:22,295 - mmdet - INFO - Epoch [5][5650/7033]	lr: 5.015e-05, eta: 5:54:16, time: 2.519, data_time: 0.050, memory: 20362, loss_cls: 0.0623, loss_bbox: 0.1904, d0.loss_cls: 0.1587, d0.loss_bbox: 0.3186, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2000, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1929, loss: 1.8779, grad_norm: 20.5190
2025-06-18 20:10:27,058 - mmdet - INFO - Epoch [5][5700/7033]	lr: 5.015e-05, eta: 5:52:10, time: 2.495, data_time: 0.047, memory: 20362, loss_cls: 0.0659, loss_bbox: 0.2027, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3160, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2098, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2053, loss: 1.9562, grad_norm: 21.6396
2025-06-18 20:12:30,972 - mmdet - INFO - Epoch [5][5750/7033]	lr: 5.015e-05, eta: 5:50:03, time: 2.478, data_time: 0.051, memory: 20362, loss_cls: 0.0681, loss_bbox: 0.1925, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0694, d4.loss_bbox: 0.1947, loss: 1.9288, grad_norm: 18.3656
2025-06-18 20:14:34,667 - mmdet - INFO - Epoch [5][5800/7033]	lr: 5.015e-05, eta: 5:47:56, time: 2.474, data_time: 0.055, memory: 20362, loss_cls: 0.0598, loss_bbox: 0.1837, d0.loss_cls: 0.1430, d0.loss_bbox: 0.3056, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2254, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2007, d3.loss_cls: 0.0655, d3.loss_bbox: 0.1929, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1864, loss: 1.7927, grad_norm: 26.5940
2025-06-18 20:16:37,807 - mmdet - INFO - Epoch [5][5850/7033]	lr: 5.015e-05, eta: 5:45:49, time: 2.463, data_time: 0.054, memory: 20362, loss_cls: 0.0668, loss_bbox: 0.1959, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1999, loss: 1.9522, grad_norm: 29.7116
2025-06-18 20:18:42,339 - mmdet - INFO - Epoch [5][5900/7033]	lr: 5.015e-05, eta: 5:43:42, time: 2.491, data_time: 0.053, memory: 20362, loss_cls: 0.0676, loss_bbox: 0.1935, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1969, loss: 1.9143, grad_norm: 40.2224
2025-06-18 20:20:44,895 - mmdet - INFO - Epoch [5][5950/7033]	lr: 5.015e-05, eta: 5:41:35, time: 2.451, data_time: 0.051, memory: 20362, loss_cls: 0.0652, loss_bbox: 0.1916, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3159, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2033, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1948, loss: 1.8847, grad_norm: 16.7819
2025-06-18 20:22:46,082 - mmdet - INFO - Epoch [5][6000/7033]	lr: 5.015e-05, eta: 5:39:27, time: 2.424, data_time: 0.052, memory: 20362, loss_cls: 0.0657, loss_bbox: 0.1907, d0.loss_cls: 0.1540, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2117, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1937, loss: 1.8828, grad_norm: 21.3618
2025-06-18 20:24:52,769 - mmdet - INFO - Epoch [5][6050/7033]	lr: 5.015e-05, eta: 5:37:21, time: 2.534, data_time: 0.053, memory: 20362, loss_cls: 0.0623, loss_bbox: 0.1912, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3199, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2008, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1937, loss: 1.8759, grad_norm: 28.4323
2025-06-18 20:26:55,426 - mmdet - INFO - Epoch [5][6100/7033]	lr: 5.015e-05, eta: 5:35:14, time: 2.453, data_time: 0.050, memory: 20362, loss_cls: 0.0677, loss_bbox: 0.1923, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3143, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0684, d4.loss_bbox: 0.1962, loss: 1.8948, grad_norm: 38.5918
2025-06-18 20:28:58,026 - mmdet - INFO - Epoch [5][6150/7033]	lr: 5.015e-05, eta: 5:33:07, time: 2.452, data_time: 0.050, memory: 20362, loss_cls: 0.0691, loss_bbox: 0.1987, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2021, loss: 1.9587, grad_norm: 54.9279
2025-06-18 20:31:02,273 - mmdet - INFO - Epoch [5][6200/7033]	lr: 5.015e-05, eta: 5:31:00, time: 2.485, data_time: 0.053, memory: 20362, loss_cls: 0.0649, loss_bbox: 0.1898, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3220, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1931, loss: 1.8948, grad_norm: 26.4999
2025-06-18 20:33:04,820 - mmdet - INFO - Epoch [5][6250/7033]	lr: 5.015e-05, eta: 5:28:53, time: 2.451, data_time: 0.050, memory: 20362, loss_cls: 0.0656, loss_bbox: 0.1973, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3167, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2385, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2170, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1999, loss: 1.9213, grad_norm: 21.9225
2025-06-18 20:35:05,833 - mmdet - INFO - Epoch [5][6300/7033]	lr: 5.015e-05, eta: 5:26:46, time: 2.420, data_time: 0.053, memory: 20362, loss_cls: 0.0660, loss_bbox: 0.1908, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3032, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2100, d3.loss_cls: 0.0734, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0670, d4.loss_bbox: 0.1937, loss: 1.8670, grad_norm: 28.4703
2025-06-18 20:37:07,766 - mmdet - INFO - Epoch [5][6350/7033]	lr: 5.015e-05, eta: 5:24:39, time: 2.439, data_time: 0.051, memory: 20362, loss_cls: 0.0618, loss_bbox: 0.1911, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3187, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0702, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1935, loss: 1.8717, grad_norm: 17.9310
2025-06-18 20:39:08,627 - mmdet - INFO - Epoch [5][6400/7033]	lr: 5.015e-05, eta: 5:22:31, time: 2.417, data_time: 0.052, memory: 20362, loss_cls: 0.0665, loss_bbox: 0.1915, d0.loss_cls: 0.1541, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2100, d3.loss_cls: 0.0733, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1939, loss: 1.8843, grad_norm: 20.6700
2025-06-18 20:41:11,409 - mmdet - INFO - Epoch [5][6450/7033]	lr: 5.015e-05, eta: 5:20:24, time: 2.456, data_time: 0.052, memory: 20362, loss_cls: 0.0644, loss_bbox: 0.1876, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2346, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0706, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1905, loss: 1.8642, grad_norm: 24.5236
2025-06-18 20:43:14,452 - mmdet - INFO - Epoch [5][6500/7033]	lr: 5.015e-05, eta: 5:18:17, time: 2.460, data_time: 0.052, memory: 20362, loss_cls: 0.0657, loss_bbox: 0.1932, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3245, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0664, d4.loss_bbox: 0.1972, loss: 1.9094, grad_norm: 17.2857
2025-06-18 20:45:15,404 - mmdet - INFO - Epoch [5][6550/7033]	lr: 5.015e-05, eta: 5:16:10, time: 2.420, data_time: 0.053, memory: 20362, loss_cls: 0.0612, loss_bbox: 0.1882, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2000, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1917, loss: 1.8717, grad_norm: 22.9016
2025-06-18 20:47:15,666 - mmdet - INFO - Epoch [5][6600/7033]	lr: 5.015e-05, eta: 5:14:03, time: 2.405, data_time: 0.053, memory: 20362, loss_cls: 0.0627, loss_bbox: 0.2011, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3290, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2202, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0653, d4.loss_bbox: 0.2032, loss: 1.9457, grad_norm: 20.1638
2025-06-18 20:49:17,456 - mmdet - INFO - Epoch [5][6650/7033]	lr: 5.015e-05, eta: 5:11:55, time: 2.436, data_time: 0.058, memory: 20362, loss_cls: 0.0646, loss_bbox: 0.1879, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3196, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2101, d3.loss_cls: 0.0736, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1914, loss: 1.8838, grad_norm: 34.2511
2025-06-18 20:51:22,173 - mmdet - INFO - Epoch [5][6700/7033]	lr: 5.015e-05, eta: 5:09:49, time: 2.494, data_time: 0.061, memory: 20362, loss_cls: 0.0635, loss_bbox: 0.1965, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3220, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2082, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1998, loss: 1.9146, grad_norm: 19.5774
2025-06-18 20:53:22,359 - mmdet - INFO - Epoch [5][6750/7033]	lr: 5.015e-05, eta: 5:07:41, time: 2.404, data_time: 0.061, memory: 20362, loss_cls: 0.0524, loss_bbox: 0.1851, d0.loss_cls: 0.1483, d0.loss_bbox: 0.3020, d1.loss_cls: 0.0875, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0703, d2.loss_bbox: 0.2036, d3.loss_cls: 0.0572, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0527, d4.loss_bbox: 0.1885, loss: 1.7697, grad_norm: 90.1355
2025-06-18 20:55:27,151 - mmdet - INFO - Epoch [5][6800/7033]	lr: 5.015e-05, eta: 5:05:35, time: 2.496, data_time: 0.060, memory: 20362, loss_cls: 0.0635, loss_bbox: 0.1928, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3191, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1959, loss: 1.8926, grad_norm: 38.2153
2025-06-18 20:57:27,073 - mmdet - INFO - Epoch [5][6850/7033]	lr: 5.015e-05, eta: 5:03:28, time: 2.398, data_time: 0.058, memory: 20362, loss_cls: 0.0657, loss_bbox: 0.2062, d0.loss_cls: 0.1541, d0.loss_bbox: 0.3276, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2100, loss: 1.9779, grad_norm: 22.9737
2025-06-18 20:59:26,635 - mmdet - INFO - Epoch [5][6900/7033]	lr: 5.015e-05, eta: 5:01:20, time: 2.391, data_time: 0.058, memory: 20362, loss_cls: 0.0649, loss_bbox: 0.1992, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3210, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2010, loss: 1.9444, grad_norm: 29.1763
2025-06-18 21:01:26,914 - mmdet - INFO - Epoch [5][6950/7033]	lr: 5.015e-05, eta: 4:59:13, time: 2.406, data_time: 0.050, memory: 20362, loss_cls: 0.0670, loss_bbox: 0.2009, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3176, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2032, loss: 1.9307, grad_norm: 25.0464
2025-06-18 21:03:27,372 - mmdet - INFO - Epoch [5][7000/7033]	lr: 5.015e-05, eta: 4:57:05, time: 2.409, data_time: 0.051, memory: 20362, loss_cls: 0.0688, loss_bbox: 0.1963, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3168, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0696, d4.loss_bbox: 0.1997, loss: 1.9244, grad_norm: 34.4635
2025-06-18 21:04:46,876 - mmdet - INFO - Saving checkpoint at 5 epochs
2025-06-18 21:49:11,778 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-18 21:49:11,778 - mmdet - INFO - Epoch(val) [5][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7987, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8844, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9093, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9212, pts_bbox_NuScenes/car_trans_err: 0.1735, pts_bbox_NuScenes/car_scale_err: 0.1509, pts_bbox_NuScenes/car_orient_err: 0.0408, pts_bbox_NuScenes/car_vel_err: 0.3077, pts_bbox_NuScenes/car_attr_err: 0.1813, pts_bbox_NuScenes/mATE: 0.2872, pts_bbox_NuScenes/mASE: 0.2628, pts_bbox_NuScenes/mAOE: 0.2438, pts_bbox_NuScenes/mAVE: 0.2786, pts_bbox_NuScenes/mAAE: 0.1798, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4358, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6202, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7226, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7580, pts_bbox_NuScenes/truck_trans_err: 0.3348, pts_bbox_NuScenes/truck_scale_err: 0.1887, pts_bbox_NuScenes/truck_orient_err: 0.0409, pts_bbox_NuScenes/truck_vel_err: 0.2605, pts_bbox_NuScenes/truck_attr_err: 0.2167, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0535, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2005, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4038, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4778, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6744, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4407, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7871, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1195, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3001, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5261, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7526, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9026, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9266, pts_bbox_NuScenes/bus_trans_err: 0.3332, pts_bbox_NuScenes/bus_scale_err: 0.1866, pts_bbox_NuScenes/bus_orient_err: 0.0410, pts_bbox_NuScenes/bus_vel_err: 0.4969, pts_bbox_NuScenes/bus_attr_err: 0.2673, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1738, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4280, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5964, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6796, pts_bbox_NuScenes/trailer_trans_err: 0.4854, pts_bbox_NuScenes/trailer_scale_err: 0.2185, pts_bbox_NuScenes/trailer_orient_err: 0.4020, pts_bbox_NuScenes/trailer_vel_err: 0.2365, pts_bbox_NuScenes/trailer_attr_err: 0.1502, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6029, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6983, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7486, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7650, pts_bbox_NuScenes/barrier_trans_err: 0.2108, pts_bbox_NuScenes/barrier_scale_err: 0.2902, pts_bbox_NuScenes/barrier_orient_err: 0.0452, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6487, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7653, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7938, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8010, pts_bbox_NuScenes/motorcycle_trans_err: 0.2106, pts_bbox_NuScenes/motorcycle_scale_err: 0.2576, pts_bbox_NuScenes/motorcycle_orient_err: 0.2192, pts_bbox_NuScenes/motorcycle_vel_err: 0.3851, pts_bbox_NuScenes/motorcycle_attr_err: 0.2140, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5554, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6082, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6192, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6277, pts_bbox_NuScenes/bicycle_trans_err: 0.1701, pts_bbox_NuScenes/bicycle_scale_err: 0.2732, pts_bbox_NuScenes/bicycle_orient_err: 0.3055, pts_bbox_NuScenes/bicycle_vel_err: 0.2028, pts_bbox_NuScenes/bicycle_attr_err: 0.0052, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8153, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8554, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8781, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8901, pts_bbox_NuScenes/pedestrian_trans_err: 0.1437, pts_bbox_NuScenes/pedestrian_scale_err: 0.2945, pts_bbox_NuScenes/pedestrian_orient_err: 0.3129, pts_bbox_NuScenes/pedestrian_vel_err: 0.2199, pts_bbox_NuScenes/pedestrian_attr_err: 0.1037, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7338, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7681, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7960, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8169, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1358, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3271, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7118, pts_bbox_NuScenes/mAP: 0.6740
2025-06-18 21:51:03,571 - mmdet - INFO - Epoch [6][50/7033]	lr: 1.358e-05, eta: 4:53:16, time: 2.159, data_time: 0.242, memory: 20362, loss_cls: 0.0653, loss_bbox: 0.1968, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3224, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0662, d4.loss_bbox: 0.2007, loss: 1.9363, grad_norm: 23.1298
2025-06-18 21:52:48,802 - mmdet - INFO - Epoch [6][100/7033]	lr: 1.358e-05, eta: 4:51:06, time: 2.105, data_time: 0.058, memory: 20362, loss_cls: 0.0661, loss_bbox: 0.1957, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0669, d4.loss_bbox: 0.1991, loss: 1.9074, grad_norm: 15.6248
2025-06-18 21:54:36,054 - mmdet - INFO - Epoch [6][150/7033]	lr: 1.358e-05, eta: 4:48:56, time: 2.144, data_time: 0.054, memory: 20362, loss_cls: 0.0633, loss_bbox: 0.1948, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3141, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1984, loss: 1.8887, grad_norm: 17.2734
2025-06-18 21:56:24,935 - mmdet - INFO - Epoch [6][200/7033]	lr: 1.358e-05, eta: 4:46:47, time: 2.178, data_time: 0.053, memory: 20362, loss_cls: 0.0694, loss_bbox: 0.1962, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3187, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0872, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0714, d4.loss_bbox: 0.1989, loss: 1.9458, grad_norm: 23.4657
2025-06-18 21:58:17,535 - mmdet - INFO - Epoch [6][250/7033]	lr: 1.358e-05, eta: 4:44:38, time: 2.252, data_time: 0.053, memory: 20362, loss_cls: 0.0565, loss_bbox: 0.1822, d0.loss_cls: 0.1457, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2246, d2.loss_cls: 0.0743, d2.loss_bbox: 0.2021, d3.loss_cls: 0.0653, d3.loss_bbox: 0.1926, d4.loss_cls: 0.0583, d4.loss_bbox: 0.1858, loss: 1.7809, grad_norm: 21.5113
2025-06-18 22:00:09,047 - mmdet - INFO - Epoch [6][300/7033]	lr: 1.358e-05, eta: 4:42:30, time: 2.230, data_time: 0.052, memory: 20362, loss_cls: 0.0605, loss_bbox: 0.1858, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3035, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2051, d3.loss_cls: 0.0639, d3.loss_bbox: 0.1970, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1889, loss: 1.8216, grad_norm: 101.1767
2025-06-18 22:02:02,242 - mmdet - INFO - Epoch [6][350/7033]	lr: 1.358e-05, eta: 4:40:22, time: 2.264, data_time: 0.052, memory: 20362, loss_cls: 0.0562, loss_bbox: 0.1878, d0.loss_cls: 0.1503, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0651, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0579, d4.loss_bbox: 0.1913, loss: 1.8334, grad_norm: 17.7497
2025-06-18 22:03:54,876 - mmdet - INFO - Epoch [6][400/7033]	lr: 1.358e-05, eta: 4:38:13, time: 2.253, data_time: 0.052, memory: 20362, loss_cls: 0.0669, loss_bbox: 0.1974, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3222, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2016, loss: 1.9462, grad_norm: 28.4808
2025-06-18 22:05:48,597 - mmdet - INFO - Epoch [6][450/7033]	lr: 1.358e-05, eta: 4:36:05, time: 2.274, data_time: 0.050, memory: 20362, loss_cls: 0.0720, loss_bbox: 0.1932, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3203, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0736, d4.loss_bbox: 0.1957, loss: 1.9467, grad_norm: 119.0012
2025-06-18 22:07:45,829 - mmdet - INFO - Epoch [6][500/7033]	lr: 1.358e-05, eta: 4:33:58, time: 2.345, data_time: 0.051, memory: 20362, loss_cls: 0.0596, loss_bbox: 0.1932, d0.loss_cls: 0.1515, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0655, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0597, d4.loss_bbox: 0.1956, loss: 1.8579, grad_norm: 27.6306
2025-06-18 22:09:50,845 - mmdet - INFO - Epoch [6][550/7033]	lr: 1.358e-05, eta: 4:31:52, time: 2.500, data_time: 0.054, memory: 20362, loss_cls: 0.0709, loss_bbox: 0.1943, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3257, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0737, d4.loss_bbox: 0.1968, loss: 1.9556, grad_norm: 17.4085
2025-06-18 22:11:46,690 - mmdet - INFO - Epoch [6][600/7033]	lr: 1.358e-05, eta: 4:29:44, time: 2.317, data_time: 0.056, memory: 20362, loss_cls: 0.0645, loss_bbox: 0.1922, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3039, d1.loss_cls: 0.1000, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1957, loss: 1.8760, grad_norm: 31.8663
2025-06-18 22:13:41,452 - mmdet - INFO - Epoch [6][650/7033]	lr: 1.358e-05, eta: 4:27:36, time: 2.295, data_time: 0.050, memory: 20362, loss_cls: 0.0598, loss_bbox: 0.1882, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3079, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2079, d3.loss_cls: 0.0674, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1926, loss: 1.8430, grad_norm: 39.1928
2025-06-18 22:15:36,763 - mmdet - INFO - Epoch [6][700/7033]	lr: 1.358e-05, eta: 4:25:29, time: 2.306, data_time: 0.054, memory: 20362, loss_cls: 0.0599, loss_bbox: 0.1831, d0.loss_cls: 0.1473, d0.loss_bbox: 0.3030, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2245, d2.loss_cls: 0.0743, d2.loss_bbox: 0.2040, d3.loss_cls: 0.0654, d3.loss_bbox: 0.1943, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1867, loss: 1.7966, grad_norm: 30.1889
2025-06-18 22:17:31,952 - mmdet - INFO - Epoch [6][750/7033]	lr: 1.358e-05, eta: 4:23:21, time: 2.304, data_time: 0.053, memory: 20362, loss_cls: 0.0597, loss_bbox: 0.1858, d0.loss_cls: 0.1481, d0.loss_bbox: 0.3058, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2051, d3.loss_cls: 0.0678, d3.loss_bbox: 0.1962, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1894, loss: 1.8205, grad_norm: 58.3175
2025-06-18 22:19:28,415 - mmdet - INFO - Epoch [6][800/7033]	lr: 1.358e-05, eta: 4:21:14, time: 2.329, data_time: 0.051, memory: 20362, loss_cls: 0.0601, loss_bbox: 0.1920, d0.loss_cls: 0.1501, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0665, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1950, loss: 1.8624, grad_norm: 19.0672
2025-06-18 22:21:23,450 - mmdet - INFO - Epoch [6][850/7033]	lr: 1.358e-05, eta: 4:19:06, time: 2.300, data_time: 0.052, memory: 20362, loss_cls: 0.0632, loss_bbox: 0.1893, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3029, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2002, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1929, loss: 1.8490, grad_norm: 45.5332
2025-06-18 22:23:21,496 - mmdet - INFO - Epoch [6][900/7033]	lr: 1.358e-05, eta: 4:16:59, time: 2.361, data_time: 0.052, memory: 20362, loss_cls: 0.0582, loss_bbox: 0.1821, d0.loss_cls: 0.1507, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2250, d2.loss_cls: 0.0764, d2.loss_bbox: 0.2007, d3.loss_cls: 0.0662, d3.loss_bbox: 0.1917, d4.loss_cls: 0.0592, d4.loss_bbox: 0.1853, loss: 1.7944, grad_norm: 18.8874
2025-06-18 22:25:17,299 - mmdet - INFO - Epoch [6][950/7033]	lr: 1.358e-05, eta: 4:14:52, time: 2.316, data_time: 0.051, memory: 20362, loss_cls: 0.0616, loss_bbox: 0.1845, d0.loss_cls: 0.1520, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2041, d3.loss_cls: 0.0684, d3.loss_bbox: 0.1949, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1872, loss: 1.8277, grad_norm: 18.2898
2025-06-18 22:27:13,916 - mmdet - INFO - Epoch [6][1000/7033]	lr: 1.358e-05, eta: 4:12:45, time: 2.332, data_time: 0.051, memory: 20362, loss_cls: 0.0554, loss_bbox: 0.1777, d0.loss_cls: 0.1509, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2229, d2.loss_cls: 0.0712, d2.loss_bbox: 0.1978, d3.loss_cls: 0.0610, d3.loss_bbox: 0.1889, d4.loss_cls: 0.0570, d4.loss_bbox: 0.1814, loss: 1.7531, grad_norm: 19.2446
2025-06-18 22:29:11,624 - mmdet - INFO - Epoch [6][1050/7033]	lr: 1.358e-05, eta: 4:10:38, time: 2.354, data_time: 0.058, memory: 20362, loss_cls: 0.0662, loss_bbox: 0.1936, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3159, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1965, loss: 1.9023, grad_norm: 21.2652
2025-06-18 22:31:10,479 - mmdet - INFO - Epoch [6][1100/7033]	lr: 1.358e-05, eta: 4:08:31, time: 2.377, data_time: 0.076, memory: 20362, loss_cls: 0.0582, loss_bbox: 0.1912, d0.loss_cls: 0.1500, d0.loss_bbox: 0.3117, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0746, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0658, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1938, loss: 1.8482, grad_norm: 19.8879
2025-06-18 22:33:07,429 - mmdet - INFO - Epoch [6][1150/7033]	lr: 1.358e-05, eta: 4:06:24, time: 2.339, data_time: 0.054, memory: 20362, loss_cls: 0.0636, loss_bbox: 0.1952, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3047, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2040, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1977, loss: 1.8760, grad_norm: 21.3109
2025-06-18 22:35:08,543 - mmdet - INFO - Epoch [6][1200/7033]	lr: 1.358e-05, eta: 4:04:17, time: 2.422, data_time: 0.054, memory: 20362, loss_cls: 0.0662, loss_bbox: 0.1905, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3116, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0678, d4.loss_bbox: 0.1940, loss: 1.8826, grad_norm: 15.9249
2025-06-18 22:37:06,674 - mmdet - INFO - Epoch [6][1250/7033]	lr: 1.358e-05, eta: 4:02:11, time: 2.363, data_time: 0.056, memory: 20362, loss_cls: 0.0626, loss_bbox: 0.1868, d0.loss_cls: 0.1524, d0.loss_bbox: 0.3145, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0677, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1905, loss: 1.8515, grad_norm: 25.0160
2025-06-18 22:39:05,340 - mmdet - INFO - Epoch [6][1300/7033]	lr: 1.358e-05, eta: 4:00:04, time: 2.373, data_time: 0.052, memory: 20362, loss_cls: 0.0643, loss_bbox: 0.1951, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3235, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2070, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1984, loss: 1.9080, grad_norm: 28.3895
2025-06-18 22:41:03,314 - mmdet - INFO - Epoch [6][1350/7033]	lr: 1.358e-05, eta: 3:57:57, time: 2.359, data_time: 0.053, memory: 20362, loss_cls: 0.0627, loss_bbox: 0.1858, d0.loss_cls: 0.1486, d0.loss_bbox: 0.3064, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2055, d3.loss_cls: 0.0692, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1892, loss: 1.8292, grad_norm: 19.6988
2025-06-18 22:43:03,908 - mmdet - INFO - Epoch [6][1400/7033]	lr: 1.358e-05, eta: 3:55:51, time: 2.412, data_time: 0.051, memory: 20362, loss_cls: 0.0560, loss_bbox: 0.1955, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3141, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0656, d3.loss_bbox: 0.2042, d4.loss_cls: 0.0589, d4.loss_bbox: 0.1968, loss: 1.8669, grad_norm: 25.4315
2025-06-18 22:45:02,113 - mmdet - INFO - Epoch [6][1450/7033]	lr: 1.358e-05, eta: 3:53:44, time: 2.364, data_time: 0.053, memory: 20362, loss_cls: 0.0541, loss_bbox: 0.1824, d0.loss_cls: 0.1428, d0.loss_bbox: 0.2985, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2207, d2.loss_cls: 0.0721, d2.loss_bbox: 0.1990, d3.loss_cls: 0.0615, d3.loss_bbox: 0.1915, d4.loss_cls: 0.0556, d4.loss_bbox: 0.1849, loss: 1.7524, grad_norm: 15.2381
2025-06-18 22:47:00,026 - mmdet - INFO - Epoch [6][1500/7033]	lr: 1.358e-05, eta: 3:51:37, time: 2.358, data_time: 0.051, memory: 20362, loss_cls: 0.0602, loss_bbox: 0.1913, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3145, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2114, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1943, loss: 1.8559, grad_norm: 17.8777
2025-06-18 22:48:57,688 - mmdet - INFO - Epoch [6][1550/7033]	lr: 1.358e-05, eta: 3:49:30, time: 2.354, data_time: 0.051, memory: 20362, loss_cls: 0.0665, loss_bbox: 0.1862, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3093, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2035, d3.loss_cls: 0.0744, d3.loss_bbox: 0.1952, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1890, loss: 1.8740, grad_norm: 20.8601
2025-06-18 22:50:56,011 - mmdet - INFO - Epoch [6][1600/7033]	lr: 1.358e-05, eta: 3:47:24, time: 2.366, data_time: 0.052, memory: 20362, loss_cls: 0.0625, loss_bbox: 0.1885, d0.loss_cls: 0.1449, d0.loss_bbox: 0.3189, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1918, loss: 1.8617, grad_norm: 13.9162
2025-06-18 22:52:54,707 - mmdet - INFO - Epoch [6][1650/7033]	lr: 1.358e-05, eta: 3:45:17, time: 2.374, data_time: 0.076, memory: 20362, loss_cls: 0.0681, loss_bbox: 0.1938, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3196, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0704, d4.loss_bbox: 0.1969, loss: 1.9168, grad_norm: 18.7075
2025-06-18 22:54:52,375 - mmdet - INFO - Epoch [6][1700/7033]	lr: 1.358e-05, eta: 3:43:10, time: 2.353, data_time: 0.052, memory: 20362, loss_cls: 0.0585, loss_bbox: 0.1898, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2024, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1936, loss: 1.8641, grad_norm: 15.6374
2025-06-18 22:56:49,387 - mmdet - INFO - Epoch [6][1750/7033]	lr: 1.358e-05, eta: 3:41:04, time: 2.340, data_time: 0.048, memory: 20362, loss_cls: 0.0616, loss_bbox: 0.1835, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2038, d3.loss_cls: 0.0680, d3.loss_bbox: 0.1949, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1879, loss: 1.8291, grad_norm: 20.1260
2025-06-18 22:58:49,361 - mmdet - INFO - Epoch [6][1800/7033]	lr: 1.358e-05, eta: 3:38:57, time: 2.400, data_time: 0.048, memory: 20362, loss_cls: 0.0648, loss_bbox: 0.1895, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3173, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0737, d3.loss_bbox: 0.1994, d4.loss_cls: 0.0665, d4.loss_bbox: 0.1930, loss: 1.8822, grad_norm: 19.2891
2025-06-18 23:00:46,411 - mmdet - INFO - Epoch [6][1850/7033]	lr: 1.358e-05, eta: 3:36:51, time: 2.341, data_time: 0.050, memory: 20362, loss_cls: 0.0682, loss_bbox: 0.1935, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3215, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2036, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1980, loss: 1.9321, grad_norm: 19.0498
2025-06-18 23:02:43,256 - mmdet - INFO - Epoch [6][1900/7033]	lr: 1.358e-05, eta: 3:34:44, time: 2.337, data_time: 0.045, memory: 20362, loss_cls: 0.0601, loss_bbox: 0.1905, d0.loss_cls: 0.1492, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0955, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0778, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0682, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1934, loss: 1.8526, grad_norm: 18.5350
2025-06-18 23:04:41,869 - mmdet - INFO - Epoch [6][1950/7033]	lr: 1.358e-05, eta: 3:32:37, time: 2.372, data_time: 0.049, memory: 20362, loss_cls: 0.0605, loss_bbox: 0.1853, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3061, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2269, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2034, d3.loss_cls: 0.0675, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1876, loss: 1.8201, grad_norm: 21.1179
2025-06-18 23:06:36,551 - mmdet - INFO - Epoch [6][2000/7033]	lr: 1.358e-05, eta: 3:30:31, time: 2.294, data_time: 0.050, memory: 20362, loss_cls: 0.0598, loss_bbox: 0.1855, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3079, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2258, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2041, d3.loss_cls: 0.0657, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0594, d4.loss_bbox: 0.1890, loss: 1.8090, grad_norm: 17.2151
2025-06-18 23:08:33,284 - mmdet - INFO - Epoch [6][2050/7033]	lr: 1.358e-05, eta: 3:28:24, time: 2.335, data_time: 0.051, memory: 20362, loss_cls: 0.0570, loss_bbox: 0.1889, d0.loss_cls: 0.1461, d0.loss_bbox: 0.3042, d1.loss_cls: 0.0885, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0639, d3.loss_bbox: 0.1979, d4.loss_cls: 0.0585, d4.loss_bbox: 0.1905, loss: 1.8008, grad_norm: 44.9274
2025-06-18 23:10:28,976 - mmdet - INFO - Epoch [6][2100/7033]	lr: 1.358e-05, eta: 3:26:17, time: 2.314, data_time: 0.053, memory: 20362, loss_cls: 0.0630, loss_bbox: 0.1884, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2090, d3.loss_cls: 0.0724, d3.loss_bbox: 0.1987, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1907, loss: 1.8576, grad_norm: 36.5302
2025-06-18 23:12:29,886 - mmdet - INFO - Epoch [6][2150/7033]	lr: 1.358e-05, eta: 3:24:11, time: 2.418, data_time: 0.052, memory: 20362, loss_cls: 0.0617, loss_bbox: 0.1914, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3198, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2021, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1950, loss: 1.8931, grad_norm: 34.5767
2025-06-18 23:14:25,908 - mmdet - INFO - Epoch [6][2200/7033]	lr: 1.358e-05, eta: 3:22:04, time: 2.320, data_time: 0.056, memory: 20362, loss_cls: 0.0673, loss_bbox: 0.1876, d0.loss_cls: 0.1522, d0.loss_bbox: 0.3178, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2107, d3.loss_cls: 0.0742, d3.loss_bbox: 0.1987, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1911, loss: 1.8891, grad_norm: 16.4445
2025-06-18 23:16:21,680 - mmdet - INFO - Epoch [6][2250/7033]	lr: 1.358e-05, eta: 3:19:58, time: 2.315, data_time: 0.052, memory: 20362, loss_cls: 0.0586, loss_bbox: 0.1887, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3061, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2052, d3.loss_cls: 0.0667, d3.loss_bbox: 0.1975, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1911, loss: 1.8298, grad_norm: 60.2261
2025-06-18 23:18:15,941 - mmdet - INFO - Epoch [6][2300/7033]	lr: 1.358e-05, eta: 3:17:51, time: 2.285, data_time: 0.058, memory: 20362, loss_cls: 0.0616, loss_bbox: 0.1860, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3043, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2071, d3.loss_cls: 0.0676, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1894, loss: 1.8339, grad_norm: 24.7490
2025-06-18 23:20:13,243 - mmdet - INFO - Epoch [6][2350/7033]	lr: 1.358e-05, eta: 3:15:44, time: 2.346, data_time: 0.054, memory: 20362, loss_cls: 0.0602, loss_bbox: 0.1876, d0.loss_cls: 0.1452, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2288, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2071, d3.loss_cls: 0.0683, d3.loss_bbox: 0.1967, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1898, loss: 1.8250, grad_norm: 16.7285
2025-06-18 23:22:10,759 - mmdet - INFO - Epoch [6][2400/7033]	lr: 1.358e-05, eta: 3:13:38, time: 2.350, data_time: 0.053, memory: 20362, loss_cls: 0.0551, loss_bbox: 0.1838, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3001, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2211, d2.loss_cls: 0.0713, d2.loss_bbox: 0.2009, d3.loss_cls: 0.0634, d3.loss_bbox: 0.1925, d4.loss_cls: 0.0565, d4.loss_bbox: 0.1858, loss: 1.7728, grad_norm: 22.6270
2025-06-18 23:24:10,284 - mmdet - INFO - Epoch [6][2450/7033]	lr: 1.358e-05, eta: 3:11:32, time: 2.391, data_time: 0.056, memory: 20362, loss_cls: 0.0594, loss_bbox: 0.1873, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3026, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2274, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0678, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1892, loss: 1.8201, grad_norm: 48.4504
2025-06-18 23:26:07,521 - mmdet - INFO - Epoch [6][2500/7033]	lr: 1.358e-05, eta: 3:09:26, time: 2.345, data_time: 0.053, memory: 20362, loss_cls: 0.0646, loss_bbox: 0.1964, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2067, d4.loss_cls: 0.0651, d4.loss_bbox: 0.1997, loss: 1.9076, grad_norm: 34.8511
2025-06-18 23:28:04,469 - mmdet - INFO - Epoch [6][2550/7033]	lr: 1.358e-05, eta: 3:07:19, time: 2.339, data_time: 0.055, memory: 20362, loss_cls: 0.0625, loss_bbox: 0.1827, d0.loss_cls: 0.1525, d0.loss_bbox: 0.2971, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2230, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2012, d3.loss_cls: 0.0704, d3.loss_bbox: 0.1919, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1853, loss: 1.8120, grad_norm: 54.0397
2025-06-18 23:30:00,446 - mmdet - INFO - Epoch [6][2600/7033]	lr: 1.358e-05, eta: 3:05:13, time: 2.319, data_time: 0.056, memory: 20362, loss_cls: 0.0615, loss_bbox: 0.1877, d0.loss_cls: 0.1506, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0692, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1904, loss: 1.8411, grad_norm: 16.7652
2025-06-18 23:31:56,694 - mmdet - INFO - Epoch [6][2650/7033]	lr: 1.358e-05, eta: 3:03:06, time: 2.325, data_time: 0.052, memory: 20362, loss_cls: 0.0653, loss_bbox: 0.1872, d0.loss_cls: 0.1500, d0.loss_bbox: 0.3145, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0725, d3.loss_bbox: 0.1982, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1906, loss: 1.8694, grad_norm: 16.8262
2025-06-18 23:33:50,878 - mmdet - INFO - Epoch [6][2700/7033]	lr: 1.358e-05, eta: 3:01:00, time: 2.284, data_time: 0.051, memory: 20362, loss_cls: 0.0575, loss_bbox: 0.1829, d0.loss_cls: 0.1463, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0903, d1.loss_bbox: 0.2298, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2057, d3.loss_cls: 0.0652, d3.loss_bbox: 0.1947, d4.loss_cls: 0.0596, d4.loss_bbox: 0.1862, loss: 1.7959, grad_norm: 17.7441
2025-06-18 23:35:51,624 - mmdet - INFO - Epoch [6][2750/7033]	lr: 1.358e-05, eta: 2:58:54, time: 2.415, data_time: 0.050, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1894, d0.loss_cls: 0.1501, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2072, d3.loss_cls: 0.0716, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1913, loss: 1.8484, grad_norm: 16.3015
2025-06-18 23:37:49,435 - mmdet - INFO - Epoch [6][2800/7033]	lr: 1.358e-05, eta: 2:56:48, time: 2.356, data_time: 0.049, memory: 20362, loss_cls: 0.0592, loss_bbox: 0.1786, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0764, d2.loss_bbox: 0.1998, d3.loss_cls: 0.0653, d3.loss_bbox: 0.1916, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1828, loss: 1.7854, grad_norm: 24.5182
2025-06-18 23:39:45,980 - mmdet - INFO - Epoch [6][2850/7033]	lr: 1.358e-05, eta: 2:54:42, time: 2.331, data_time: 0.052, memory: 20362, loss_cls: 0.0574, loss_bbox: 0.1824, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0755, d2.loss_bbox: 0.2019, d3.loss_cls: 0.0646, d3.loss_bbox: 0.1946, d4.loss_cls: 0.0584, d4.loss_bbox: 0.1862, loss: 1.8015, grad_norm: 15.6172
2025-06-18 23:41:42,975 - mmdet - INFO - Epoch [6][2900/7033]	lr: 1.358e-05, eta: 2:52:35, time: 2.340, data_time: 0.055, memory: 20362, loss_cls: 0.0680, loss_bbox: 0.1977, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3191, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1996, loss: 1.9341, grad_norm: 156.8402
2025-06-18 23:43:40,962 - mmdet - INFO - Epoch [6][2950/7033]	lr: 1.358e-05, eta: 2:50:29, time: 2.360, data_time: 0.054, memory: 20362, loss_cls: 0.0549, loss_bbox: 0.1884, d0.loss_cls: 0.1429, d0.loss_bbox: 0.3111, d1.loss_cls: 0.0885, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0694, d2.loss_bbox: 0.2090, d3.loss_cls: 0.0609, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0558, d4.loss_bbox: 0.1923, loss: 1.8038, grad_norm: 17.3764
2025-06-18 23:45:40,368 - mmdet - INFO - Epoch [6][3000/7033]	lr: 1.358e-05, eta: 2:48:23, time: 2.388, data_time: 0.048, memory: 20362, loss_cls: 0.0628, loss_bbox: 0.1875, d0.loss_cls: 0.1424, d0.loss_bbox: 0.3013, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0690, d3.loss_bbox: 0.1989, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1917, loss: 1.8215, grad_norm: 22.5473
2025-06-18 23:47:40,298 - mmdet - INFO - Epoch [6][3050/7033]	lr: 1.358e-05, eta: 2:46:18, time: 2.398, data_time: 0.052, memory: 20362, loss_cls: 0.0548, loss_bbox: 0.1856, d0.loss_cls: 0.1394, d0.loss_bbox: 0.2977, d1.loss_cls: 0.0850, d1.loss_bbox: 0.2238, d2.loss_cls: 0.0685, d2.loss_bbox: 0.2043, d3.loss_cls: 0.0602, d3.loss_bbox: 0.1945, d4.loss_cls: 0.0568, d4.loss_bbox: 0.1888, loss: 1.7592, grad_norm: 50.7671
2025-06-18 23:49:37,666 - mmdet - INFO - Epoch [6][3100/7033]	lr: 1.358e-05, eta: 2:44:11, time: 2.348, data_time: 0.052, memory: 20362, loss_cls: 0.0586, loss_bbox: 0.1854, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3022, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0729, d2.loss_bbox: 0.2063, d3.loss_cls: 0.0647, d3.loss_bbox: 0.1960, d4.loss_cls: 0.0594, d4.loss_bbox: 0.1882, loss: 1.7997, grad_norm: 30.8460
2025-06-18 23:51:35,288 - mmdet - INFO - Epoch [6][3150/7033]	lr: 1.358e-05, eta: 2:42:05, time: 2.352, data_time: 0.054, memory: 20362, loss_cls: 0.0606, loss_bbox: 0.1877, d0.loss_cls: 0.1483, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0747, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0681, d3.loss_bbox: 0.1970, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1913, loss: 1.8266, grad_norm: 31.8602
2025-06-18 23:53:34,571 - mmdet - INFO - Epoch [6][3200/7033]	lr: 1.358e-05, eta: 2:40:00, time: 2.386, data_time: 0.054, memory: 20362, loss_cls: 0.0574, loss_bbox: 0.1854, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2025, d3.loss_cls: 0.0664, d3.loss_bbox: 0.1952, d4.loss_cls: 0.0583, d4.loss_bbox: 0.1879, loss: 1.8073, grad_norm: 16.5735
2025-06-18 23:55:32,447 - mmdet - INFO - Epoch [6][3250/7033]	lr: 1.358e-05, eta: 2:37:54, time: 2.357, data_time: 0.051, memory: 20362, loss_cls: 0.0619, loss_bbox: 0.1893, d0.loss_cls: 0.1474, d0.loss_bbox: 0.3148, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2011, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1938, loss: 1.8665, grad_norm: 28.1040
2025-06-18 23:57:32,200 - mmdet - INFO - Epoch [6][3300/7033]	lr: 1.358e-05, eta: 2:35:48, time: 2.395, data_time: 0.055, memory: 20362, loss_cls: 0.0624, loss_bbox: 0.1837, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3045, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2281, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2042, d3.loss_cls: 0.0688, d3.loss_bbox: 0.1953, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1869, loss: 1.8187, grad_norm: 15.5056
2025-06-18 23:59:30,949 - mmdet - INFO - Epoch [6][3350/7033]	lr: 1.358e-05, eta: 2:33:42, time: 2.375, data_time: 0.053, memory: 20362, loss_cls: 0.0672, loss_bbox: 0.1887, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3021, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0738, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0680, d4.loss_bbox: 0.1913, loss: 1.8642, grad_norm: 63.2141
2025-06-19 00:02:25,728 - mmdet - INFO - Epoch [6][3400/7033]	lr: 1.358e-05, eta: 2:31:42, time: 3.496, data_time: 1.341, memory: 20362, loss_cls: 0.0612, loss_bbox: 0.1899, d0.loss_cls: 0.1450, d0.loss_bbox: 0.3185, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0677, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1926, loss: 1.8484, grad_norm: 65.7606
2025-06-19 00:04:24,058 - mmdet - INFO - Epoch [6][3450/7033]	lr: 1.358e-05, eta: 2:29:36, time: 2.367, data_time: 0.056, memory: 20362, loss_cls: 0.0688, loss_bbox: 0.1948, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3169, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0695, d4.loss_bbox: 0.1987, loss: 1.9228, grad_norm: 16.2424
2025-06-19 00:06:23,376 - mmdet - INFO - Epoch [6][3500/7033]	lr: 1.358e-05, eta: 2:27:30, time: 2.386, data_time: 0.061, memory: 20362, loss_cls: 0.0581, loss_bbox: 0.1832, d0.loss_cls: 0.1509, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0775, d2.loss_bbox: 0.1997, d3.loss_cls: 0.0673, d3.loss_bbox: 0.1917, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1853, loss: 1.7943, grad_norm: 104.1796
2025-06-19 00:08:23,039 - mmdet - INFO - Epoch [6][3550/7033]	lr: 1.358e-05, eta: 2:25:24, time: 2.393, data_time: 0.061, memory: 20362, loss_cls: 0.0567, loss_bbox: 0.1854, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3037, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0698, d2.loss_bbox: 0.2052, d3.loss_cls: 0.0615, d3.loss_bbox: 0.1951, d4.loss_cls: 0.0577, d4.loss_bbox: 0.1877, loss: 1.7901, grad_norm: 22.3991
2025-06-19 00:10:24,468 - mmdet - INFO - Epoch [6][3600/7033]	lr: 1.358e-05, eta: 2:23:18, time: 2.428, data_time: 0.062, memory: 20362, loss_cls: 0.0589, loss_bbox: 0.1905, d0.loss_cls: 0.1497, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0747, d2.loss_bbox: 0.2112, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1943, loss: 1.8475, grad_norm: 20.1409
2025-06-19 00:12:27,720 - mmdet - INFO - Epoch [6][3650/7033]	lr: 1.358e-05, eta: 2:21:13, time: 2.465, data_time: 0.055, memory: 20362, loss_cls: 0.0685, loss_bbox: 0.1943, d0.loss_cls: 0.1520, d0.loss_bbox: 0.3113, d1.loss_cls: 0.1000, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2101, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0696, d4.loss_bbox: 0.1968, loss: 1.8966, grad_norm: 28.0471
2025-06-19 00:14:30,132 - mmdet - INFO - Epoch [6][3700/7033]	lr: 1.358e-05, eta: 2:19:08, time: 2.448, data_time: 0.055, memory: 20362, loss_cls: 0.0617, loss_bbox: 0.1875, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3090, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0674, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1908, loss: 1.8389, grad_norm: 15.0716
2025-06-19 00:16:33,009 - mmdet - INFO - Epoch [6][3750/7033]	lr: 1.358e-05, eta: 2:17:02, time: 2.458, data_time: 0.056, memory: 20362, loss_cls: 0.0615, loss_bbox: 0.1844, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3001, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2253, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2044, d3.loss_cls: 0.0671, d3.loss_bbox: 0.1967, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1880, loss: 1.8244, grad_norm: 30.3677
2025-06-19 00:18:32,518 - mmdet - INFO - Epoch [6][3800/7033]	lr: 1.358e-05, eta: 2:14:56, time: 2.390, data_time: 0.053, memory: 20362, loss_cls: 0.0643, loss_bbox: 0.1917, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3090, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2036, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1956, loss: 1.8721, grad_norm: 13.8259
2025-06-19 00:20:34,742 - mmdet - INFO - Epoch [6][3850/7033]	lr: 1.358e-05, eta: 2:12:51, time: 2.444, data_time: 0.051, memory: 20362, loss_cls: 0.0541, loss_bbox: 0.1780, d0.loss_cls: 0.1404, d0.loss_bbox: 0.2945, d1.loss_cls: 0.0868, d1.loss_bbox: 0.2185, d2.loss_cls: 0.0706, d2.loss_bbox: 0.1949, d3.loss_cls: 0.0612, d3.loss_bbox: 0.1872, d4.loss_cls: 0.0549, d4.loss_bbox: 0.1812, loss: 1.7224, grad_norm: 31.3576
2025-06-19 00:22:37,820 - mmdet - INFO - Epoch [6][3900/7033]	lr: 1.358e-05, eta: 2:10:46, time: 2.462, data_time: 0.084, memory: 20362, loss_cls: 0.0566, loss_bbox: 0.1871, d0.loss_cls: 0.1511, d0.loss_bbox: 0.3124, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2064, d3.loss_cls: 0.0630, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0589, d4.loss_bbox: 0.1908, loss: 1.8193, grad_norm: 18.4206
2025-06-19 00:24:38,716 - mmdet - INFO - Epoch [6][3950/7033]	lr: 1.358e-05, eta: 2:08:40, time: 2.418, data_time: 0.057, memory: 20362, loss_cls: 0.0629, loss_bbox: 0.1943, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2040, d4.loss_cls: 0.0639, d4.loss_bbox: 0.1979, loss: 1.8763, grad_norm: 18.0653
2025-06-19 00:26:42,714 - mmdet - INFO - Epoch [6][4000/7033]	lr: 1.358e-05, eta: 2:06:35, time: 2.480, data_time: 0.056, memory: 20362, loss_cls: 0.0584, loss_bbox: 0.1836, d0.loss_cls: 0.1487, d0.loss_bbox: 0.2992, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2036, d3.loss_cls: 0.0645, d3.loss_bbox: 0.1943, d4.loss_cls: 0.0601, d4.loss_bbox: 0.1875, loss: 1.7929, grad_norm: 28.7703
2025-06-19 00:28:44,299 - mmdet - INFO - Epoch [6][4050/7033]	lr: 1.358e-05, eta: 2:04:29, time: 2.431, data_time: 0.052, memory: 20362, loss_cls: 0.0540, loss_bbox: 0.1803, d0.loss_cls: 0.1474, d0.loss_bbox: 0.2993, d1.loss_cls: 0.0895, d1.loss_bbox: 0.2184, d2.loss_cls: 0.0730, d2.loss_bbox: 0.1979, d3.loss_cls: 0.0630, d3.loss_bbox: 0.1893, d4.loss_cls: 0.0564, d4.loss_bbox: 0.1830, loss: 1.7516, grad_norm: 16.2269
2025-06-19 00:30:46,844 - mmdet - INFO - Epoch [6][4100/7033]	lr: 1.358e-05, eta: 2:02:24, time: 2.451, data_time: 0.055, memory: 20362, loss_cls: 0.0600, loss_bbox: 0.1826, d0.loss_cls: 0.1448, d0.loss_bbox: 0.3003, d1.loss_cls: 0.0940, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2031, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1936, d4.loss_cls: 0.0624, d4.loss_bbox: 0.1847, loss: 1.7948, grad_norm: 18.4236
2025-06-19 00:32:48,771 - mmdet - INFO - Epoch [6][4150/7033]	lr: 1.358e-05, eta: 2:00:18, time: 2.439, data_time: 0.053, memory: 20362, loss_cls: 0.0574, loss_bbox: 0.1823, d0.loss_cls: 0.1480, d0.loss_bbox: 0.3027, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2213, d2.loss_cls: 0.0765, d2.loss_bbox: 0.1997, d3.loss_cls: 0.0654, d3.loss_bbox: 0.1922, d4.loss_cls: 0.0589, d4.loss_bbox: 0.1855, loss: 1.7836, grad_norm: 16.6781
2025-06-19 00:34:51,911 - mmdet - INFO - Epoch [6][4200/7033]	lr: 1.358e-05, eta: 1:58:13, time: 2.463, data_time: 0.051, memory: 20362, loss_cls: 0.0562, loss_bbox: 0.1782, d0.loss_cls: 0.1435, d0.loss_bbox: 0.2959, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2184, d2.loss_cls: 0.0733, d2.loss_bbox: 0.1957, d3.loss_cls: 0.0626, d3.loss_bbox: 0.1890, d4.loss_cls: 0.0569, d4.loss_bbox: 0.1809, loss: 1.7426, grad_norm: 17.9869
2025-06-19 00:36:55,209 - mmdet - INFO - Epoch [6][4250/7033]	lr: 1.358e-05, eta: 1:56:08, time: 2.466, data_time: 0.050, memory: 20362, loss_cls: 0.0563, loss_bbox: 0.1860, d0.loss_cls: 0.1473, d0.loss_bbox: 0.3022, d1.loss_cls: 0.0893, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0706, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0626, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0562, d4.loss_bbox: 0.1905, loss: 1.7943, grad_norm: 21.5804
2025-06-19 00:38:59,486 - mmdet - INFO - Epoch [6][4300/7033]	lr: 1.358e-05, eta: 1:54:02, time: 2.485, data_time: 0.050, memory: 20362, loss_cls: 0.0575, loss_bbox: 0.1890, d0.loss_cls: 0.1442, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0663, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1924, loss: 1.8240, grad_norm: 16.4847
2025-06-19 00:41:01,506 - mmdet - INFO - Epoch [6][4350/7033]	lr: 1.358e-05, eta: 1:51:57, time: 2.440, data_time: 0.050, memory: 20362, loss_cls: 0.0613, loss_bbox: 0.1971, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0630, d4.loss_bbox: 0.2003, loss: 1.8875, grad_norm: 34.7186
2025-06-19 00:43:04,616 - mmdet - INFO - Epoch [6][4400/7033]	lr: 1.358e-05, eta: 1:49:52, time: 2.463, data_time: 0.048, memory: 20362, loss_cls: 0.0587, loss_bbox: 0.1906, d0.loss_cls: 0.1475, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2106, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1944, loss: 1.8350, grad_norm: 25.8313
2025-06-19 00:45:08,289 - mmdet - INFO - Epoch [6][4450/7033]	lr: 1.358e-05, eta: 1:47:46, time: 2.473, data_time: 0.051, memory: 20362, loss_cls: 0.0678, loss_bbox: 0.1954, d0.loss_cls: 0.1481, d0.loss_bbox: 0.3129, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0685, d4.loss_bbox: 0.1995, loss: 1.9029, grad_norm: 13.4742
2025-06-19 00:47:13,047 - mmdet - INFO - Epoch [6][4500/7033]	lr: 1.358e-05, eta: 1:45:41, time: 2.496, data_time: 0.051, memory: 20362, loss_cls: 0.0614, loss_bbox: 0.1934, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0690, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1958, loss: 1.8706, grad_norm: 57.4941
2025-06-19 00:49:17,372 - mmdet - INFO - Epoch [6][4550/7033]	lr: 1.358e-05, eta: 1:43:36, time: 2.486, data_time: 0.050, memory: 20362, loss_cls: 0.0634, loss_bbox: 0.1913, d0.loss_cls: 0.1463, d0.loss_bbox: 0.3033, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0709, d3.loss_bbox: 0.1989, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1941, loss: 1.8443, grad_norm: 20.8232
2025-06-19 00:51:23,521 - mmdet - INFO - Epoch [6][4600/7033]	lr: 1.358e-05, eta: 1:41:31, time: 2.523, data_time: 0.055, memory: 20362, loss_cls: 0.0646, loss_bbox: 0.1873, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0719, d3.loss_bbox: 0.1973, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1906, loss: 1.8480, grad_norm: 22.2493
2025-06-19 00:53:27,272 - mmdet - INFO - Epoch [6][4650/7033]	lr: 1.358e-05, eta: 1:39:26, time: 2.475, data_time: 0.049, memory: 20362, loss_cls: 0.0592, loss_bbox: 0.1959, d0.loss_cls: 0.1524, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2047, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1977, loss: 1.8670, grad_norm: 157.6845
2025-06-19 00:55:31,847 - mmdet - INFO - Epoch [6][4700/7033]	lr: 1.358e-05, eta: 1:37:20, time: 2.492, data_time: 0.052, memory: 20362, loss_cls: 0.0636, loss_bbox: 0.1918, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1946, loss: 1.8757, grad_norm: 19.9628
2025-06-19 00:57:33,851 - mmdet - INFO - Epoch [6][4750/7033]	lr: 1.358e-05, eta: 1:35:15, time: 2.440, data_time: 0.050, memory: 20362, loss_cls: 0.0573, loss_bbox: 0.1898, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3049, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0636, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0592, d4.loss_bbox: 0.1933, loss: 1.8292, grad_norm: 17.6466
2025-06-19 00:59:38,495 - mmdet - INFO - Epoch [6][4800/7033]	lr: 1.358e-05, eta: 1:33:10, time: 2.493, data_time: 0.052, memory: 20362, loss_cls: 0.0599, loss_bbox: 0.1934, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3147, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2034, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1968, loss: 1.8723, grad_norm: 15.5268
2025-06-19 01:01:43,426 - mmdet - INFO - Epoch [6][4850/7033]	lr: 1.358e-05, eta: 1:31:05, time: 2.499, data_time: 0.053, memory: 20362, loss_cls: 0.0539, loss_bbox: 0.1862, d0.loss_cls: 0.1499, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2222, d2.loss_cls: 0.0705, d2.loss_bbox: 0.2029, d3.loss_cls: 0.0613, d3.loss_bbox: 0.1932, d4.loss_cls: 0.0554, d4.loss_bbox: 0.1886, loss: 1.7731, grad_norm: 14.3856
2025-06-19 01:03:47,334 - mmdet - INFO - Epoch [6][4900/7033]	lr: 1.358e-05, eta: 1:28:59, time: 2.478, data_time: 0.051, memory: 20362, loss_cls: 0.0655, loss_bbox: 0.1948, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2186, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1996, loss: 1.9424, grad_norm: 17.3275
2025-06-19 01:05:51,899 - mmdet - INFO - Epoch [6][4950/7033]	lr: 1.358e-05, eta: 1:26:54, time: 2.491, data_time: 0.048, memory: 20362, loss_cls: 0.0578, loss_bbox: 0.1888, d0.loss_cls: 0.1460, d0.loss_bbox: 0.3078, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0746, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0651, d3.loss_bbox: 0.1994, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1923, loss: 1.8216, grad_norm: 47.3634
2025-06-19 01:07:56,551 - mmdet - INFO - Epoch [6][5000/7033]	lr: 1.358e-05, eta: 1:24:49, time: 2.493, data_time: 0.052, memory: 20362, loss_cls: 0.0668, loss_bbox: 0.1961, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3248, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2169, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0666, d4.loss_bbox: 0.2000, loss: 1.9361, grad_norm: 30.4552
2025-06-19 01:10:00,060 - mmdet - INFO - Epoch [6][5050/7033]	lr: 1.358e-05, eta: 1:22:44, time: 2.470, data_time: 0.050, memory: 20362, loss_cls: 0.0580, loss_bbox: 0.1911, d0.loss_cls: 0.1482, d0.loss_bbox: 0.2992, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0733, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0639, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0587, d4.loss_bbox: 0.1944, loss: 1.8121, grad_norm: 61.1370
2025-06-19 01:12:05,547 - mmdet - INFO - Epoch [6][5100/7033]	lr: 1.358e-05, eta: 1:20:39, time: 2.510, data_time: 0.049, memory: 20362, loss_cls: 0.0617, loss_bbox: 0.1896, d0.loss_cls: 0.1535, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1935, loss: 1.8530, grad_norm: 21.9680
2025-06-19 01:14:09,325 - mmdet - INFO - Epoch [6][5150/7033]	lr: 1.358e-05, eta: 1:18:33, time: 2.475, data_time: 0.050, memory: 20362, loss_cls: 0.0587, loss_bbox: 0.1879, d0.loss_cls: 0.1498, d0.loss_bbox: 0.3003, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2264, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0646, d3.loss_bbox: 0.1989, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1907, loss: 1.8108, grad_norm: 25.9087
2025-06-19 01:16:15,966 - mmdet - INFO - Epoch [6][5200/7033]	lr: 1.358e-05, eta: 1:16:28, time: 2.533, data_time: 0.047, memory: 20362, loss_cls: 0.0596, loss_bbox: 0.1850, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3099, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2281, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2046, d3.loss_cls: 0.0657, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0616, d4.loss_bbox: 0.1885, loss: 1.8196, grad_norm: 16.9897
2025-06-19 01:18:21,424 - mmdet - INFO - Epoch [6][5250/7033]	lr: 1.358e-05, eta: 1:14:23, time: 2.509, data_time: 0.046, memory: 20362, loss_cls: 0.0579, loss_bbox: 0.1855, d0.loss_cls: 0.1466, d0.loss_bbox: 0.3054, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2047, d3.loss_cls: 0.0629, d3.loss_bbox: 0.1970, d4.loss_cls: 0.0596, d4.loss_bbox: 0.1880, loss: 1.8027, grad_norm: 74.5530
2025-06-19 01:20:25,319 - mmdet - INFO - Epoch [6][5300/7033]	lr: 1.358e-05, eta: 1:12:18, time: 2.478, data_time: 0.051, memory: 20362, loss_cls: 0.0558, loss_bbox: 0.1845, d0.loss_cls: 0.1414, d0.loss_bbox: 0.3009, d1.loss_cls: 0.0860, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0704, d2.loss_bbox: 0.2046, d3.loss_cls: 0.0611, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0562, d4.loss_bbox: 0.1886, loss: 1.7724, grad_norm: 18.1967
2025-06-19 01:22:29,071 - mmdet - INFO - Epoch [6][5350/7033]	lr: 1.358e-05, eta: 1:10:13, time: 2.475, data_time: 0.054, memory: 20362, loss_cls: 0.0588, loss_bbox: 0.1900, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3086, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0665, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1922, loss: 1.8373, grad_norm: 33.2768
2025-06-19 01:24:33,979 - mmdet - INFO - Epoch [6][5400/7033]	lr: 1.358e-05, eta: 1:08:07, time: 2.498, data_time: 0.049, memory: 20362, loss_cls: 0.0678, loss_bbox: 0.1951, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3110, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0768, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0699, d4.loss_bbox: 0.1973, loss: 1.9229, grad_norm: 16.2801
2025-06-19 01:26:41,480 - mmdet - INFO - Epoch [6][5450/7033]	lr: 1.358e-05, eta: 1:06:02, time: 2.550, data_time: 0.051, memory: 20362, loss_cls: 0.0540, loss_bbox: 0.1819, d0.loss_cls: 0.1439, d0.loss_bbox: 0.2938, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2184, d2.loss_cls: 0.0710, d2.loss_bbox: 0.2003, d3.loss_cls: 0.0614, d3.loss_bbox: 0.1920, d4.loss_cls: 0.0549, d4.loss_bbox: 0.1856, loss: 1.7471, grad_norm: 15.8741
2025-06-19 01:28:47,794 - mmdet - INFO - Epoch [6][5500/7033]	lr: 1.358e-05, eta: 1:03:57, time: 2.526, data_time: 0.054, memory: 20362, loss_cls: 0.0611, loss_bbox: 0.1913, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3188, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0627, d4.loss_bbox: 0.1951, loss: 1.8821, grad_norm: 20.3082
2025-06-19 01:30:52,299 - mmdet - INFO - Epoch [6][5550/7033]	lr: 1.358e-05, eta: 1:01:52, time: 2.490, data_time: 0.051, memory: 20362, loss_cls: 0.0669, loss_bbox: 0.1959, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3129, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0691, d4.loss_bbox: 0.1982, loss: 1.9168, grad_norm: 17.1691
2025-06-19 01:32:57,589 - mmdet - INFO - Epoch [6][5600/7033]	lr: 1.358e-05, eta: 0:59:47, time: 2.506, data_time: 0.049, memory: 20362, loss_cls: 0.0672, loss_bbox: 0.1959, d0.loss_cls: 0.1530, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2049, d4.loss_cls: 0.0685, d4.loss_bbox: 0.1998, loss: 1.9067, grad_norm: 18.5787
2025-06-19 01:35:02,480 - mmdet - INFO - Epoch [6][5650/7033]	lr: 1.358e-05, eta: 0:57:42, time: 2.498, data_time: 0.049, memory: 20362, loss_cls: 0.0614, loss_bbox: 0.1901, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3095, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2101, d3.loss_cls: 0.0697, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1926, loss: 1.8440, grad_norm: 18.6226
2025-06-19 01:37:08,273 - mmdet - INFO - Epoch [6][5700/7033]	lr: 1.358e-05, eta: 0:55:37, time: 2.516, data_time: 0.047, memory: 20362, loss_cls: 0.0566, loss_bbox: 0.1897, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3085, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0659, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0582, d4.loss_bbox: 0.1930, loss: 1.8282, grad_norm: 36.2345
2025-06-19 01:39:14,678 - mmdet - INFO - Epoch [6][5750/7033]	lr: 1.358e-05, eta: 0:53:31, time: 2.528, data_time: 0.050, memory: 20362, loss_cls: 0.0634, loss_bbox: 0.1884, d0.loss_cls: 0.1452, d0.loss_bbox: 0.3139, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2335, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2094, d3.loss_cls: 0.0702, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0639, d4.loss_bbox: 0.1913, loss: 1.8512, grad_norm: 16.9288
2025-06-19 01:41:21,174 - mmdet - INFO - Epoch [6][5800/7033]	lr: 1.358e-05, eta: 0:51:26, time: 2.530, data_time: 0.052, memory: 20362, loss_cls: 0.0611, loss_bbox: 0.1960, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3188, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1991, loss: 1.8852, grad_norm: 26.5683
2025-06-19 01:43:30,427 - mmdet - INFO - Epoch [6][5850/7033]	lr: 1.358e-05, eta: 0:49:21, time: 2.585, data_time: 0.052, memory: 20362, loss_cls: 0.0651, loss_bbox: 0.1922, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3155, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2346, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0665, d4.loss_bbox: 0.1961, loss: 1.8960, grad_norm: 15.2536
2025-06-19 01:45:36,492 - mmdet - INFO - Epoch [6][5900/7033]	lr: 1.358e-05, eta: 0:47:16, time: 2.521, data_time: 0.050, memory: 20362, loss_cls: 0.0623, loss_bbox: 0.1846, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3006, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2037, d3.loss_cls: 0.0681, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1884, loss: 1.8142, grad_norm: 18.5411
2025-06-19 01:47:42,484 - mmdet - INFO - Epoch [6][5950/7033]	lr: 1.358e-05, eta: 0:45:11, time: 2.520, data_time: 0.050, memory: 20362, loss_cls: 0.0612, loss_bbox: 0.1860, d0.loss_cls: 0.1494, d0.loss_bbox: 0.3020, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2250, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2058, d3.loss_cls: 0.0679, d3.loss_bbox: 0.1946, d4.loss_cls: 0.0628, d4.loss_bbox: 0.1878, loss: 1.8159, grad_norm: 16.8765
2025-06-19 01:49:48,999 - mmdet - INFO - Epoch [6][6000/7033]	lr: 1.358e-05, eta: 0:43:06, time: 2.530, data_time: 0.051, memory: 20362, loss_cls: 0.0611, loss_bbox: 0.1894, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2075, d3.loss_cls: 0.0678, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1921, loss: 1.8446, grad_norm: 20.0969
2025-06-19 01:51:58,836 - mmdet - INFO - Epoch [6][6050/7033]	lr: 1.358e-05, eta: 0:41:01, time: 2.597, data_time: 0.050, memory: 20362, loss_cls: 0.0611, loss_bbox: 0.1850, d0.loss_cls: 0.1483, d0.loss_bbox: 0.3070, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2261, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2033, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1953, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1881, loss: 1.8112, grad_norm: 13.7319
2025-06-19 01:54:05,072 - mmdet - INFO - Epoch [6][6100/7033]	lr: 1.358e-05, eta: 0:38:56, time: 2.525, data_time: 0.050, memory: 20362, loss_cls: 0.0580, loss_bbox: 0.1825, d0.loss_cls: 0.1470, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0884, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2045, d3.loss_cls: 0.0635, d3.loss_bbox: 0.1940, d4.loss_cls: 0.0586, d4.loss_bbox: 0.1865, loss: 1.7876, grad_norm: 55.7593
2025-06-19 01:56:11,413 - mmdet - INFO - Epoch [6][6150/7033]	lr: 1.358e-05, eta: 0:36:50, time: 2.526, data_time: 0.052, memory: 20362, loss_cls: 0.0605, loss_bbox: 0.1893, d0.loss_cls: 0.1433, d0.loss_bbox: 0.3109, d1.loss_cls: 0.0927, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0680, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1930, loss: 1.8357, grad_norm: 18.8344
2025-06-19 01:58:17,195 - mmdet - INFO - Epoch [6][6200/7033]	lr: 1.358e-05, eta: 0:34:45, time: 2.516, data_time: 0.048, memory: 20362, loss_cls: 0.0593, loss_bbox: 0.1882, d0.loss_cls: 0.1437, d0.loss_bbox: 0.3082, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2272, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2055, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1967, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1912, loss: 1.8143, grad_norm: 27.0076
2025-06-19 02:00:24,515 - mmdet - INFO - Epoch [6][6250/7033]	lr: 1.358e-05, eta: 0:32:40, time: 2.546, data_time: 0.051, memory: 20362, loss_cls: 0.0623, loss_bbox: 0.1868, d0.loss_cls: 0.1450, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2031, d3.loss_cls: 0.0687, d3.loss_bbox: 0.1967, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1890, loss: 1.8167, grad_norm: 69.5567
2025-06-19 02:02:29,227 - mmdet - INFO - Epoch [6][6300/7033]	lr: 1.358e-05, eta: 0:30:35, time: 2.494, data_time: 0.052, memory: 20362, loss_cls: 0.0586, loss_bbox: 0.1810, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0940, d1.loss_bbox: 0.2238, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2035, d3.loss_cls: 0.0651, d3.loss_bbox: 0.1926, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1851, loss: 1.7895, grad_norm: 54.7064
2025-06-19 02:04:35,430 - mmdet - INFO - Epoch [6][6350/7033]	lr: 1.358e-05, eta: 0:28:30, time: 2.523, data_time: 0.051, memory: 20362, loss_cls: 0.0618, loss_bbox: 0.1905, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0922, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1938, loss: 1.8547, grad_norm: 55.9235
2025-06-19 02:06:42,738 - mmdet - INFO - Epoch [6][6400/7033]	lr: 1.358e-05, eta: 0:26:25, time: 2.547, data_time: 0.060, memory: 20362, loss_cls: 0.0655, loss_bbox: 0.1973, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3164, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2002, loss: 1.9133, grad_norm: 17.5948
2025-06-19 02:08:50,672 - mmdet - INFO - Epoch [6][6450/7033]	lr: 1.358e-05, eta: 0:24:19, time: 2.558, data_time: 0.051, memory: 20362, loss_cls: 0.0567, loss_bbox: 0.1967, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3184, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2383, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0632, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0587, d4.loss_bbox: 0.1989, loss: 1.8699, grad_norm: 16.0360
2025-06-19 02:10:55,875 - mmdet - INFO - Epoch [6][6500/7033]	lr: 1.358e-05, eta: 0:22:14, time: 2.504, data_time: 0.051, memory: 20362, loss_cls: 0.0616, loss_bbox: 0.1936, d0.loss_cls: 0.1466, d0.loss_bbox: 0.3048, d1.loss_cls: 0.0926, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1962, loss: 1.8558, grad_norm: 14.5602
2025-06-19 02:13:03,125 - mmdet - INFO - Epoch [6][6550/7033]	lr: 1.358e-05, eta: 0:20:09, time: 2.545, data_time: 0.049, memory: 20362, loss_cls: 0.0647, loss_bbox: 0.1947, d0.loss_cls: 0.1511, d0.loss_bbox: 0.3177, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2042, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1976, loss: 1.8950, grad_norm: 61.6282
2025-06-19 02:15:09,803 - mmdet - INFO - Epoch [6][6600/7033]	lr: 1.358e-05, eta: 0:18:04, time: 2.534, data_time: 0.055, memory: 20362, loss_cls: 0.0684, loss_bbox: 0.1898, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3136, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2368, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2117, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2008, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1929, loss: 1.8938, grad_norm: 25.0194
2025-06-19 02:17:19,688 - mmdet - INFO - Epoch [6][6650/7033]	lr: 1.358e-05, eta: 0:15:59, time: 2.598, data_time: 0.051, memory: 20362, loss_cls: 0.0653, loss_bbox: 0.1946, d0.loss_cls: 0.1470, d0.loss_bbox: 0.3042, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2048, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1985, loss: 1.8712, grad_norm: 42.2851
2025-06-19 02:19:26,161 - mmdet - INFO - Epoch [6][6700/7033]	lr: 1.358e-05, eta: 0:13:53, time: 2.529, data_time: 0.049, memory: 20362, loss_cls: 0.0636, loss_bbox: 0.1884, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2339, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1936, loss: 1.8567, grad_norm: 26.9653
2025-06-19 02:21:33,692 - mmdet - INFO - Epoch [6][6750/7033]	lr: 1.358e-05, eta: 0:11:48, time: 2.551, data_time: 0.050, memory: 20362, loss_cls: 0.0655, loss_bbox: 0.1954, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1985, loss: 1.9011, grad_norm: 20.1174
2025-06-19 02:23:41,487 - mmdet - INFO - Epoch [6][6800/7033]	lr: 1.358e-05, eta: 0:09:43, time: 2.556, data_time: 0.050, memory: 20362, loss_cls: 0.0597, loss_bbox: 0.1901, d0.loss_cls: 0.1469, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0927, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0665, d3.loss_bbox: 0.1980, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1927, loss: 1.8210, grad_norm: 20.4670
2025-06-19 02:25:48,211 - mmdet - INFO - Epoch [6][6850/7033]	lr: 1.358e-05, eta: 0:07:38, time: 2.534, data_time: 0.053, memory: 20362, loss_cls: 0.0622, loss_bbox: 0.1919, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3143, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1948, loss: 1.8710, grad_norm: 20.4640
2025-06-19 02:27:56,937 - mmdet - INFO - Epoch [6][6900/7033]	lr: 1.358e-05, eta: 0:05:33, time: 2.575, data_time: 0.050, memory: 20362, loss_cls: 0.0574, loss_bbox: 0.1893, d0.loss_cls: 0.1461, d0.loss_bbox: 0.3071, d1.loss_cls: 0.0875, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0640, d3.loss_bbox: 0.2004, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1917, loss: 1.8159, grad_norm: 21.1700
2025-06-19 02:30:02,319 - mmdet - INFO - Epoch [6][6950/7033]	lr: 1.358e-05, eta: 0:03:27, time: 2.508, data_time: 0.051, memory: 20362, loss_cls: 0.0615, loss_bbox: 0.1916, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1947, loss: 1.8705, grad_norm: 17.2265
2025-06-19 02:32:09,100 - mmdet - INFO - Epoch [6][7000/7033]	lr: 1.358e-05, eta: 0:01:22, time: 2.536, data_time: 0.052, memory: 20362, loss_cls: 0.0592, loss_bbox: 0.1853, d0.loss_cls: 0.1509, d0.loss_bbox: 0.3074, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2054, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0596, d4.loss_bbox: 0.1889, loss: 1.8120, grad_norm: 40.4972
2025-06-19 02:33:34,285 - mmdet - INFO - Saving checkpoint at 6 epochs
2025-06-19 03:22:40,783 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 03:22:40,783 - mmdet - INFO - Epoch(val) [6][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7982, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8845, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9088, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9210, pts_bbox_NuScenes/car_trans_err: 0.1734, pts_bbox_NuScenes/car_scale_err: 0.1513, pts_bbox_NuScenes/car_orient_err: 0.0401, pts_bbox_NuScenes/car_vel_err: 0.2714, pts_bbox_NuScenes/car_attr_err: 0.1900, pts_bbox_NuScenes/mATE: 0.2841, pts_bbox_NuScenes/mASE: 0.2634, pts_bbox_NuScenes/mAOE: 0.2436, pts_bbox_NuScenes/mAVE: 0.2612, pts_bbox_NuScenes/mAAE: 0.1796, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4300, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6148, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7197, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7523, pts_bbox_NuScenes/truck_trans_err: 0.3375, pts_bbox_NuScenes/truck_scale_err: 0.1950, pts_bbox_NuScenes/truck_orient_err: 0.0400, pts_bbox_NuScenes/truck_vel_err: 0.2441, pts_bbox_NuScenes/truck_attr_err: 0.2148, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0549, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1953, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4029, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4673, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6683, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4382, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7958, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1121, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2967, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5352, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7602, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9081, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9268, pts_bbox_NuScenes/bus_trans_err: 0.3255, pts_bbox_NuScenes/bus_scale_err: 0.1920, pts_bbox_NuScenes/bus_orient_err: 0.0395, pts_bbox_NuScenes/bus_vel_err: 0.4458, pts_bbox_NuScenes/bus_attr_err: 0.2614, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1818, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4307, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5946, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6725, pts_bbox_NuScenes/trailer_trans_err: 0.4744, pts_bbox_NuScenes/trailer_scale_err: 0.2274, pts_bbox_NuScenes/trailer_orient_err: 0.4161, pts_bbox_NuScenes/trailer_vel_err: 0.2263, pts_bbox_NuScenes/trailer_attr_err: 0.1523, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6068, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7046, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7492, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7643, pts_bbox_NuScenes/barrier_trans_err: 0.2052, pts_bbox_NuScenes/barrier_scale_err: 0.2878, pts_bbox_NuScenes/barrier_orient_err: 0.0443, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6490, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7691, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7917, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8011, pts_bbox_NuScenes/motorcycle_trans_err: 0.2086, pts_bbox_NuScenes/motorcycle_scale_err: 0.2510, pts_bbox_NuScenes/motorcycle_orient_err: 0.2122, pts_bbox_NuScenes/motorcycle_vel_err: 0.3775, pts_bbox_NuScenes/motorcycle_attr_err: 0.2007, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5508, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5988, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6068, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6143, pts_bbox_NuScenes/bicycle_trans_err: 0.1718, pts_bbox_NuScenes/bicycle_scale_err: 0.2669, pts_bbox_NuScenes/bicycle_orient_err: 0.2836, pts_bbox_NuScenes/bicycle_vel_err: 0.1975, pts_bbox_NuScenes/bicycle_attr_err: 0.0063, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8137, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8555, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8771, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8884, pts_bbox_NuScenes/pedestrian_trans_err: 0.1449, pts_bbox_NuScenes/pedestrian_scale_err: 0.2950, pts_bbox_NuScenes/pedestrian_orient_err: 0.3208, pts_bbox_NuScenes/pedestrian_vel_err: 0.2149, pts_bbox_NuScenes/pedestrian_attr_err: 0.1144, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7301, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7691, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7916, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8121, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1318, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3293, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7131, pts_bbox_NuScenes/mAP: 0.6726
