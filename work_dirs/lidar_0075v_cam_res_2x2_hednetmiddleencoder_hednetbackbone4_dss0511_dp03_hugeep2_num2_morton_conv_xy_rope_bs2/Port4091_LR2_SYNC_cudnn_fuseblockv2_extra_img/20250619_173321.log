2025-06-19 17:33:21,181 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+3056288
spconv2.0: True
------------------------------------------------------------

2025-06-19 17:33:21,709 - mmdet - INFO - 分布式训练: True
2025-06-19 17:33:22,235 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4091_LR2_SYNC_cudnn_fuseblockv2_extra_img'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
cudnn_benchmark = True
gpu_ids = range(0, 2)

2025-06-19 17:33:22,236 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-19 17:33:23,109 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-19 17:33:23,378 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-19 17:33:23,450 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,451 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,504 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,556 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,606 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,646 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,697 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,748 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,798 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,849 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,898 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,949 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:23,991 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,032 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,074 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,116 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,167 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,208 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,257 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,301 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,343 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,383 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,425 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,467 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,511 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,551 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,593 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,634 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,676 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,718 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,770 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,815 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,866 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-19 17:33:24,928 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 768]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.A_log_h2t - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.D_h2t - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.A_log_t2h - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.D_t2h - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight - torch.Size([48, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.out_proj.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-19 17:33:25,013 - mmdet - INFO - 使用SyncBN
2025-06-19 17:33:25,100 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=768, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
                (camera_mixer): LidarCameraFusionMambaBlock(
                  (layers): ModuleList(
                    (0): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                    (1): LidarCameraFusionMambaV2(
                      (activation_fn): SiLU()
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (conv1d_h2t): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_h2t): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_h2t): Linear(in_features=16, out_features=512, bias=True)
                      (conv1d_t2h): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)
                      (x_proj_t2h): Linear(in_features=512, out_features=48, bias=False)
                      (dt_proj_t2h): Linear(in_features=16, out_features=512, bias=True)
                      (lidar_guidance_proj): Linear(in_features=512, out_features=512, bias=True)
                      (out_proj): Linear(in_features=512, out_features=256, bias=False)
                    )
                  )
                  (norm): ModuleList(
                    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (dropout): ModuleList(
                    (0): DropPath(drop_prob=0.100)
                    (1): DropPath(drop_prob=0.100)
                  )
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-19 17:33:44,028 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-19 17:33:53,802 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.camera_mixer.norm.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.camera_mixer.norm.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.camera_mixer.norm.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.camera_mixer.norm.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.camera_mixer.norm.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.A_log_h2t, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.D_h2t, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.A_log_t2h, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.D_t2h, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.in_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.0.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.A_log_h2t, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.D_h2t, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.A_log_t2h, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.D_t2h, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.in_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_h2t.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.x_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_h2t.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_h2t.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.conv1d_t2h.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.x_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_t2h.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.dt_proj_t2h.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.lidar_guidance_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.layers.1.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.camera_mixer.norm.1.bias

2025-06-19 17:33:53,812 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4091_LR2_SYNC_cudnn_fuseblockv2_extra_img
2025-06-19 17:33:53,885 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-19 17:33:53,885 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-19 17:33:53,886 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4091_LR2_SYNC_cudnn_fuseblockv2_extra_img by HardDiskBackend.
2025-06-19 17:35:20,010 - mmdet - INFO - Epoch [1][50/7033]	lr: 7.973e-05, eta: 20:04:50, time: 1.715, data_time: 0.105, memory: 18026, loss_cls: 0.9776, loss_bbox: 1.4182, d0.loss_cls: 1.0731, d0.loss_bbox: 1.5365, d1.loss_cls: 0.9806, d1.loss_bbox: 1.4471, d2.loss_cls: 0.9822, d2.loss_bbox: 1.4422, d3.loss_cls: 0.9961, d3.loss_bbox: 1.4437, d4.loss_cls: 0.9902, d4.loss_bbox: 1.4450, loss: 14.7326, grad_norm: 437.3174
2025-06-19 17:36:32,898 - mmdet - INFO - Epoch [1][100/7033]	lr: 9.307e-05, eta: 18:33:06, time: 1.458, data_time: 0.025, memory: 18026, loss_cls: 0.7321, loss_bbox: 1.1268, d0.loss_cls: 0.8341, d0.loss_bbox: 1.2800, d1.loss_cls: 0.7649, d1.loss_bbox: 1.1791, d2.loss_cls: 0.7454, d2.loss_bbox: 1.1744, d3.loss_cls: 0.7474, d3.loss_bbox: 1.1708, d4.loss_cls: 0.7386, d4.loss_bbox: 1.1471, loss: 11.6407, grad_norm: 23.9313
2025-06-19 17:37:45,769 - mmdet - INFO - Epoch [1][150/7033]	lr: 1.064e-04, eta: 18:01:38, time: 1.457, data_time: 0.026, memory: 18026, loss_cls: 0.6353, loss_bbox: 1.0857, d0.loss_cls: 0.7398, d0.loss_bbox: 1.2343, d1.loss_cls: 0.6639, d1.loss_bbox: 1.1327, d2.loss_cls: 0.6419, d2.loss_bbox: 1.1235, d3.loss_cls: 0.6502, d3.loss_bbox: 1.1099, d4.loss_cls: 0.6310, d4.loss_bbox: 1.0958, loss: 10.7440, grad_norm: 15.2166
2025-06-19 17:38:58,654 - mmdet - INFO - Epoch [1][200/7033]	lr: 1.197e-04, eta: 17:45:21, time: 1.458, data_time: 0.026, memory: 18026, loss_cls: 0.5892, loss_bbox: 1.0400, d0.loss_cls: 0.7030, d0.loss_bbox: 1.2054, d1.loss_cls: 0.6066, d1.loss_bbox: 1.0955, d2.loss_cls: 0.5907, d2.loss_bbox: 1.0701, d3.loss_cls: 0.5969, d3.loss_bbox: 1.0584, d4.loss_cls: 0.5843, d4.loss_bbox: 1.0424, loss: 10.1824, grad_norm: 12.3172
2025-06-19 17:40:11,643 - mmdet - INFO - Epoch [1][250/7033]	lr: 1.331e-04, eta: 17:35:22, time: 1.460, data_time: 0.026, memory: 18026, loss_cls: 0.5270, loss_bbox: 1.0001, d0.loss_cls: 0.6589, d0.loss_bbox: 1.1637, d1.loss_cls: 0.5206, d1.loss_bbox: 1.0456, d2.loss_cls: 0.5178, d2.loss_bbox: 1.0246, d3.loss_cls: 0.5261, d3.loss_bbox: 1.0171, d4.loss_cls: 0.5178, d4.loss_bbox: 1.0082, loss: 9.5274, grad_norm: 14.1672
2025-06-19 17:41:26,569 - mmdet - INFO - Epoch [1][300/7033]	lr: 1.464e-04, eta: 17:32:50, time: 1.499, data_time: 0.026, memory: 18026, loss_cls: 0.3935, loss_bbox: 0.9534, d0.loss_cls: 0.6261, d0.loss_bbox: 1.1351, d1.loss_cls: 0.3896, d1.loss_bbox: 1.0001, d2.loss_cls: 0.3767, d2.loss_bbox: 0.9808, d3.loss_cls: 0.3858, d3.loss_bbox: 0.9671, d4.loss_cls: 0.3849, d4.loss_bbox: 0.9591, loss: 8.5520, grad_norm: 13.7765
2025-06-19 17:42:39,582 - mmdet - INFO - Epoch [1][350/7033]	lr: 1.597e-04, eta: 17:26:50, time: 1.460, data_time: 0.025, memory: 18026, loss_cls: 0.3275, loss_bbox: 0.9332, d0.loss_cls: 0.5797, d0.loss_bbox: 1.1353, d1.loss_cls: 0.3347, d1.loss_bbox: 0.9666, d2.loss_cls: 0.3140, d2.loss_bbox: 0.9493, d3.loss_cls: 0.3153, d3.loss_bbox: 0.9382, d4.loss_cls: 0.3181, d4.loss_bbox: 0.9408, loss: 8.0528, grad_norm: 14.5289
2025-06-19 17:43:52,587 - mmdet - INFO - Epoch [1][400/7033]	lr: 1.731e-04, eta: 17:22:02, time: 1.460, data_time: 0.026, memory: 18026, loss_cls: 0.2721, loss_bbox: 0.8421, d0.loss_cls: 0.5015, d0.loss_bbox: 1.0921, d1.loss_cls: 0.3047, d1.loss_bbox: 0.8594, d2.loss_cls: 0.2727, d2.loss_bbox: 0.8472, d3.loss_cls: 0.2747, d3.loss_bbox: 0.8358, d4.loss_cls: 0.2737, d4.loss_bbox: 0.8405, loss: 7.2166, grad_norm: 34.7575
2025-06-19 17:45:05,625 - mmdet - INFO - Epoch [1][450/7033]	lr: 1.864e-04, eta: 17:18:05, time: 1.461, data_time: 0.026, memory: 18026, loss_cls: 0.2617, loss_bbox: 0.7244, d0.loss_cls: 0.4172, d0.loss_bbox: 1.0060, d1.loss_cls: 0.3049, d1.loss_bbox: 0.7072, d2.loss_cls: 0.2659, d2.loss_bbox: 0.7163, d3.loss_cls: 0.2591, d3.loss_bbox: 0.7077, d4.loss_cls: 0.2541, d4.loss_bbox: 0.7192, loss: 6.3436, grad_norm: 17.7001
2025-06-19 17:46:18,879 - mmdet - INFO - Epoch [1][500/7033]	lr: 1.997e-04, eta: 17:14:58, time: 1.465, data_time: 0.029, memory: 18026, loss_cls: 0.2188, loss_bbox: 0.6912, d0.loss_cls: 0.3395, d0.loss_bbox: 0.9299, d1.loss_cls: 0.2978, d1.loss_bbox: 0.6363, d2.loss_cls: 0.2398, d2.loss_bbox: 0.6487, d3.loss_cls: 0.2181, d3.loss_bbox: 0.6542, d4.loss_cls: 0.2140, d4.loss_bbox: 0.6783, loss: 5.7666, grad_norm: 30.0655
2025-06-19 17:47:31,940 - mmdet - INFO - Epoch [1][550/7033]	lr: 2.000e-04, eta: 17:11:57, time: 1.461, data_time: 0.028, memory: 18026, loss_cls: 0.2058, loss_bbox: 0.5966, d0.loss_cls: 0.2759, d0.loss_bbox: 0.7743, d1.loss_cls: 0.2502, d1.loss_bbox: 0.5549, d2.loss_cls: 0.2120, d2.loss_bbox: 0.5744, d3.loss_cls: 0.2033, d3.loss_bbox: 0.5741, d4.loss_cls: 0.1989, d4.loss_bbox: 0.5870, loss: 5.0072, grad_norm: 23.7199
2025-06-19 17:48:47,571 - mmdet - INFO - Epoch [1][600/7033]	lr: 2.000e-04, eta: 17:12:13, time: 1.513, data_time: 0.028, memory: 18196, loss_cls: 0.1767, loss_bbox: 0.5251, d0.loss_cls: 0.2552, d0.loss_bbox: 0.6904, d1.loss_cls: 0.2207, d1.loss_bbox: 0.4859, d2.loss_cls: 0.1858, d2.loss_bbox: 0.5010, d3.loss_cls: 0.1771, d3.loss_bbox: 0.5050, d4.loss_cls: 0.1769, d4.loss_bbox: 0.5152, loss: 4.4149, grad_norm: 23.9812
2025-06-19 17:50:00,763 - mmdet - INFO - Epoch [1][650/7033]	lr: 2.000e-04, eta: 17:09:39, time: 1.464, data_time: 0.029, memory: 18196, loss_cls: 0.1788, loss_bbox: 0.4913, d0.loss_cls: 0.2545, d0.loss_bbox: 0.6628, d1.loss_cls: 0.2158, d1.loss_bbox: 0.4495, d2.loss_cls: 0.1804, d2.loss_bbox: 0.4725, d3.loss_cls: 0.1720, d3.loss_bbox: 0.4751, d4.loss_cls: 0.1750, d4.loss_bbox: 0.4865, loss: 4.2143, grad_norm: 33.4803
2025-06-19 17:51:13,690 - mmdet - INFO - Epoch [1][700/7033]	lr: 2.000e-04, eta: 17:07:00, time: 1.459, data_time: 0.026, memory: 18196, loss_cls: 0.1707, loss_bbox: 0.4683, d0.loss_cls: 0.2457, d0.loss_bbox: 0.6017, d1.loss_cls: 0.2050, d1.loss_bbox: 0.4106, d2.loss_cls: 0.1741, d2.loss_bbox: 0.4292, d3.loss_cls: 0.1685, d3.loss_bbox: 0.4356, d4.loss_cls: 0.1667, d4.loss_bbox: 0.4478, loss: 3.9240, grad_norm: 41.9356
2025-06-19 17:52:27,081 - mmdet - INFO - Epoch [1][750/7033]	lr: 2.000e-04, eta: 17:04:58, time: 1.468, data_time: 0.026, memory: 18196, loss_cls: 0.1662, loss_bbox: 0.4471, d0.loss_cls: 0.2517, d0.loss_bbox: 0.5838, d1.loss_cls: 0.2046, d1.loss_bbox: 0.4071, d2.loss_cls: 0.1678, d2.loss_bbox: 0.4220, d3.loss_cls: 0.1644, d3.loss_bbox: 0.4290, d4.loss_cls: 0.1665, d4.loss_bbox: 0.4360, loss: 3.8461, grad_norm: 35.4117
2025-06-19 17:53:40,443 - mmdet - INFO - Epoch [1][800/7033]	lr: 2.000e-04, eta: 17:03:02, time: 1.467, data_time: 0.029, memory: 18196, loss_cls: 0.1660, loss_bbox: 0.4208, d0.loss_cls: 0.2458, d0.loss_bbox: 0.5634, d1.loss_cls: 0.2075, d1.loss_bbox: 0.3999, d2.loss_cls: 0.1707, d2.loss_bbox: 0.4033, d3.loss_cls: 0.1655, d3.loss_bbox: 0.4069, d4.loss_cls: 0.1667, d4.loss_bbox: 0.4117, loss: 3.7281, grad_norm: 75.4202
2025-06-19 17:54:53,443 - mmdet - INFO - Epoch [1][850/7033]	lr: 2.000e-04, eta: 17:00:52, time: 1.460, data_time: 0.026, memory: 18196, loss_cls: 0.1651, loss_bbox: 0.4010, d0.loss_cls: 0.2431, d0.loss_bbox: 0.5628, d1.loss_cls: 0.1981, d1.loss_bbox: 0.3754, d2.loss_cls: 0.1665, d2.loss_bbox: 0.3842, d3.loss_cls: 0.1619, d3.loss_bbox: 0.3894, d4.loss_cls: 0.1597, d4.loss_bbox: 0.3936, loss: 3.6009, grad_norm: 37.2043
2025-06-19 17:56:07,719 - mmdet - INFO - Epoch [1][900/7033]	lr: 2.000e-04, eta: 16:59:47, time: 1.485, data_time: 0.048, memory: 18196, loss_cls: 0.1554, loss_bbox: 0.3882, d0.loss_cls: 0.2359, d0.loss_bbox: 0.5535, d1.loss_cls: 0.1903, d1.loss_bbox: 0.3747, d2.loss_cls: 0.1648, d2.loss_bbox: 0.3744, d3.loss_cls: 0.1553, d3.loss_bbox: 0.3793, d4.loss_cls: 0.1552, d4.loss_bbox: 0.3810, loss: 3.5080, grad_norm: 38.9970
2025-06-19 17:57:20,708 - mmdet - INFO - Epoch [1][950/7033]	lr: 2.000e-04, eta: 16:57:46, time: 1.460, data_time: 0.026, memory: 18196, loss_cls: 0.1368, loss_bbox: 0.3727, d0.loss_cls: 0.2255, d0.loss_bbox: 0.5271, d1.loss_cls: 0.1805, d1.loss_bbox: 0.3592, d2.loss_cls: 0.1452, d2.loss_bbox: 0.3530, d3.loss_cls: 0.1355, d3.loss_bbox: 0.3602, d4.loss_cls: 0.1342, d4.loss_bbox: 0.3615, loss: 3.2914, grad_norm: 34.5085
2025-06-19 17:58:34,051 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 17:58:34,051 - mmdet - INFO - Epoch [1][1000/7033]	lr: 2.000e-04, eta: 16:56:04, time: 1.467, data_time: 0.029, memory: 18196, loss_cls: 0.1334, loss_bbox: 0.3499, d0.loss_cls: 0.2175, d0.loss_bbox: 0.5139, d1.loss_cls: 0.1684, d1.loss_bbox: 0.3404, d2.loss_cls: 0.1387, d2.loss_bbox: 0.3382, d3.loss_cls: 0.1325, d3.loss_bbox: 0.3410, d4.loss_cls: 0.1321, d4.loss_bbox: 0.3470, loss: 3.1532, grad_norm: 27.4052
2025-06-19 17:59:47,340 - mmdet - INFO - Epoch [1][1050/7033]	lr: 2.000e-04, eta: 16:54:22, time: 1.466, data_time: 0.028, memory: 18196, loss_cls: 0.1457, loss_bbox: 0.3558, d0.loss_cls: 0.2300, d0.loss_bbox: 0.5315, d1.loss_cls: 0.1830, d1.loss_bbox: 0.3528, d2.loss_cls: 0.1496, d2.loss_bbox: 0.3507, d3.loss_cls: 0.1457, d3.loss_bbox: 0.3580, d4.loss_cls: 0.1455, d4.loss_bbox: 0.3567, loss: 3.3050, grad_norm: 62.0049
2025-06-19 18:01:00,579 - mmdet - INFO - Epoch [1][1100/7033]	lr: 2.000e-04, eta: 16:52:42, time: 1.465, data_time: 0.028, memory: 18196, loss_cls: 0.1345, loss_bbox: 0.3446, d0.loss_cls: 0.2248, d0.loss_bbox: 0.5124, d1.loss_cls: 0.1776, d1.loss_bbox: 0.3465, d2.loss_cls: 0.1456, d2.loss_bbox: 0.3435, d3.loss_cls: 0.1397, d3.loss_bbox: 0.3442, d4.loss_cls: 0.1342, d4.loss_bbox: 0.3442, loss: 3.1919, grad_norm: 35.4847
2025-06-19 18:02:14,201 - mmdet - INFO - Epoch [1][1150/7033]	lr: 2.000e-04, eta: 16:51:17, time: 1.472, data_time: 0.029, memory: 18196, loss_cls: 0.1446, loss_bbox: 0.3191, d0.loss_cls: 0.2305, d0.loss_bbox: 0.4977, d1.loss_cls: 0.1785, d1.loss_bbox: 0.3299, d2.loss_cls: 0.1452, d2.loss_bbox: 0.3276, d3.loss_cls: 0.1410, d3.loss_bbox: 0.3284, d4.loss_cls: 0.1410, d4.loss_bbox: 0.3266, loss: 3.1102, grad_norm: 56.2826
2025-06-19 18:03:27,338 - mmdet - INFO - Epoch [1][1200/7033]	lr: 2.000e-04, eta: 16:49:37, time: 1.463, data_time: 0.029, memory: 18196, loss_cls: 0.1500, loss_bbox: 0.3325, d0.loss_cls: 0.2269, d0.loss_bbox: 0.5214, d1.loss_cls: 0.1933, d1.loss_bbox: 0.3481, d2.loss_cls: 0.1567, d2.loss_bbox: 0.3444, d3.loss_cls: 0.1500, d3.loss_bbox: 0.3447, d4.loss_cls: 0.1463, d4.loss_bbox: 0.3427, loss: 3.2570, grad_norm: 32.7833
2025-06-19 18:04:40,743 - mmdet - INFO - Epoch [1][1250/7033]	lr: 2.000e-04, eta: 16:48:07, time: 1.468, data_time: 0.029, memory: 18196, loss_cls: 0.1445, loss_bbox: 0.3292, d0.loss_cls: 0.2253, d0.loss_bbox: 0.4865, d1.loss_cls: 0.1842, d1.loss_bbox: 0.3389, d2.loss_cls: 0.1519, d2.loss_bbox: 0.3337, d3.loss_cls: 0.1451, d3.loss_bbox: 0.3385, d4.loss_cls: 0.1448, d4.loss_bbox: 0.3403, loss: 3.1628, grad_norm: 28.2757
2025-06-19 18:05:53,904 - mmdet - INFO - Epoch [1][1300/7033]	lr: 2.000e-04, eta: 16:46:31, time: 1.463, data_time: 0.028, memory: 18196, loss_cls: 0.1267, loss_bbox: 0.3118, d0.loss_cls: 0.2054, d0.loss_bbox: 0.4526, d1.loss_cls: 0.1566, d1.loss_bbox: 0.3217, d2.loss_cls: 0.1323, d2.loss_bbox: 0.3208, d3.loss_cls: 0.1288, d3.loss_bbox: 0.3263, d4.loss_cls: 0.1279, d4.loss_bbox: 0.3285, loss: 2.9395, grad_norm: 37.1787
2025-06-19 18:07:06,643 - mmdet - INFO - Epoch [1][1350/7033]	lr: 2.000e-04, eta: 16:44:44, time: 1.455, data_time: 0.027, memory: 18196, loss_cls: 0.1249, loss_bbox: 0.3115, d0.loss_cls: 0.2012, d0.loss_bbox: 0.4648, d1.loss_cls: 0.1588, d1.loss_bbox: 0.3282, d2.loss_cls: 0.1324, d2.loss_bbox: 0.3244, d3.loss_cls: 0.1252, d3.loss_bbox: 0.3317, d4.loss_cls: 0.1252, d4.loss_bbox: 0.3349, loss: 2.9634, grad_norm: 32.4764
2025-06-19 18:08:20,040 - mmdet - INFO - Epoch [1][1400/7033]	lr: 2.000e-04, eta: 16:43:19, time: 1.468, data_time: 0.027, memory: 18196, loss_cls: 0.1263, loss_bbox: 0.2904, d0.loss_cls: 0.2095, d0.loss_bbox: 0.4423, d1.loss_cls: 0.1603, d1.loss_bbox: 0.3124, d2.loss_cls: 0.1358, d2.loss_bbox: 0.3056, d3.loss_cls: 0.1270, d3.loss_bbox: 0.3092, d4.loss_cls: 0.1258, d4.loss_bbox: 0.3122, loss: 2.8567, grad_norm: 43.6723
2025-06-19 18:09:33,806 - mmdet - INFO - Epoch [1][1450/7033]	lr: 2.000e-04, eta: 16:42:05, time: 1.475, data_time: 0.033, memory: 18196, loss_cls: 0.1303, loss_bbox: 0.2936, d0.loss_cls: 0.2021, d0.loss_bbox: 0.4485, d1.loss_cls: 0.1532, d1.loss_bbox: 0.3216, d2.loss_cls: 0.1344, d2.loss_bbox: 0.3197, d3.loss_cls: 0.1284, d3.loss_bbox: 0.3214, d4.loss_cls: 0.1306, d4.loss_bbox: 0.3205, loss: 2.9043, grad_norm: 30.5065
2025-06-19 18:10:47,068 - mmdet - INFO - Epoch [1][1500/7033]	lr: 2.000e-04, eta: 16:40:37, time: 1.465, data_time: 0.030, memory: 18196, loss_cls: 0.1141, loss_bbox: 0.2858, d0.loss_cls: 0.2055, d0.loss_bbox: 0.4460, d1.loss_cls: 0.1494, d1.loss_bbox: 0.3133, d2.loss_cls: 0.1215, d2.loss_bbox: 0.3078, d3.loss_cls: 0.1163, d3.loss_bbox: 0.3121, d4.loss_cls: 0.1134, d4.loss_bbox: 0.3126, loss: 2.7976, grad_norm: 25.4574
2025-06-19 18:12:00,661 - mmdet - INFO - Epoch [1][1550/7033]	lr: 2.000e-04, eta: 16:39:19, time: 1.472, data_time: 0.037, memory: 18196, loss_cls: 0.1253, loss_bbox: 0.2828, d0.loss_cls: 0.2098, d0.loss_bbox: 0.4342, d1.loss_cls: 0.1543, d1.loss_bbox: 0.3114, d2.loss_cls: 0.1294, d2.loss_bbox: 0.3096, d3.loss_cls: 0.1255, d3.loss_bbox: 0.3095, d4.loss_cls: 0.1227, d4.loss_bbox: 0.3137, loss: 2.8282, grad_norm: 27.5927
2025-06-19 18:13:14,000 - mmdet - INFO - Epoch [1][1600/7033]	lr: 2.000e-04, eta: 16:37:55, time: 1.467, data_time: 0.030, memory: 18196, loss_cls: 0.1261, loss_bbox: 0.2907, d0.loss_cls: 0.2047, d0.loss_bbox: 0.4435, d1.loss_cls: 0.1524, d1.loss_bbox: 0.3163, d2.loss_cls: 0.1276, d2.loss_bbox: 0.3109, d3.loss_cls: 0.1265, d3.loss_bbox: 0.3103, d4.loss_cls: 0.1242, d4.loss_bbox: 0.3119, loss: 2.8453, grad_norm: 36.2206
2025-06-19 18:14:27,183 - mmdet - INFO - Epoch [1][1650/7033]	lr: 2.000e-04, eta: 16:36:27, time: 1.464, data_time: 0.029, memory: 18196, loss_cls: 0.1125, loss_bbox: 0.2704, d0.loss_cls: 0.2014, d0.loss_bbox: 0.4232, d1.loss_cls: 0.1472, d1.loss_bbox: 0.2980, d2.loss_cls: 0.1209, d2.loss_bbox: 0.2931, d3.loss_cls: 0.1167, d3.loss_bbox: 0.2926, d4.loss_cls: 0.1122, d4.loss_bbox: 0.2949, loss: 2.6831, grad_norm: 28.1695
2025-06-19 18:15:42,207 - mmdet - INFO - Epoch [1][1700/7033]	lr: 2.000e-04, eta: 16:35:44, time: 1.500, data_time: 0.030, memory: 18196, loss_cls: 0.1165, loss_bbox: 0.2936, d0.loss_cls: 0.2048, d0.loss_bbox: 0.4452, d1.loss_cls: 0.1461, d1.loss_bbox: 0.3238, d2.loss_cls: 0.1220, d2.loss_bbox: 0.3133, d3.loss_cls: 0.1152, d3.loss_bbox: 0.3188, d4.loss_cls: 0.1149, d4.loss_bbox: 0.3194, loss: 2.8335, grad_norm: 103.2210
2025-06-19 18:16:58,491 - mmdet - INFO - Epoch [1][1750/7033]	lr: 2.000e-04, eta: 16:35:29, time: 1.526, data_time: 0.097, memory: 18226, loss_cls: 0.1159, loss_bbox: 0.2732, d0.loss_cls: 0.1962, d0.loss_bbox: 0.4297, d1.loss_cls: 0.1414, d1.loss_bbox: 0.3083, d2.loss_cls: 0.1202, d2.loss_bbox: 0.2993, d3.loss_cls: 0.1146, d3.loss_bbox: 0.2995, d4.loss_cls: 0.1141, d4.loss_bbox: 0.2980, loss: 2.7103, grad_norm: 45.1400
2025-06-19 18:18:23,279 - mmdet - INFO - Epoch [1][1800/7033]	lr: 2.000e-04, eta: 16:38:21, time: 1.696, data_time: 0.071, memory: 18226, loss_cls: 0.1090, loss_bbox: 0.2776, d0.loss_cls: 0.1999, d0.loss_bbox: 0.4291, d1.loss_cls: 0.1430, d1.loss_bbox: 0.3098, d2.loss_cls: 0.1165, d2.loss_bbox: 0.3029, d3.loss_cls: 0.1115, d3.loss_bbox: 0.3064, d4.loss_cls: 0.1079, d4.loss_bbox: 0.3085, loss: 2.7223, grad_norm: 39.1941
2025-06-19 18:19:36,390 - mmdet - INFO - Epoch [1][1850/7033]	lr: 2.000e-04, eta: 16:36:44, time: 1.462, data_time: 0.030, memory: 18226, loss_cls: 0.1121, loss_bbox: 0.2653, d0.loss_cls: 0.1982, d0.loss_bbox: 0.4112, d1.loss_cls: 0.1477, d1.loss_bbox: 0.2996, d2.loss_cls: 0.1217, d2.loss_bbox: 0.2894, d3.loss_cls: 0.1136, d3.loss_bbox: 0.2904, d4.loss_cls: 0.1104, d4.loss_bbox: 0.2895, loss: 2.6490, grad_norm: 24.4949
2025-06-19 18:20:59,026 - mmdet - INFO - Epoch [1][1900/7033]	lr: 2.000e-04, eta: 16:38:31, time: 1.653, data_time: 0.028, memory: 18226, loss_cls: 0.1078, loss_bbox: 0.2674, d0.loss_cls: 0.1990, d0.loss_bbox: 0.4052, d1.loss_cls: 0.1375, d1.loss_bbox: 0.2980, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2920, d3.loss_cls: 0.1112, d3.loss_bbox: 0.2914, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2914, loss: 2.6252, grad_norm: 60.8954
2025-06-19 18:22:12,731 - mmdet - INFO - Epoch [1][1950/7033]	lr: 2.000e-04, eta: 16:37:04, time: 1.474, data_time: 0.030, memory: 18226, loss_cls: 0.1063, loss_bbox: 0.2768, d0.loss_cls: 0.1957, d0.loss_bbox: 0.4265, d1.loss_cls: 0.1371, d1.loss_bbox: 0.3061, d2.loss_cls: 0.1104, d2.loss_bbox: 0.3030, d3.loss_cls: 0.1054, d3.loss_bbox: 0.3039, d4.loss_cls: 0.1051, d4.loss_bbox: 0.3033, loss: 2.6797, grad_norm: 31.6689
2025-06-19 18:23:31,105 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 18:23:31,105 - mmdet - INFO - Epoch [1][2000/7033]	lr: 2.000e-04, eta: 16:37:11, time: 1.567, data_time: 0.028, memory: 18226, loss_cls: 0.1309, loss_bbox: 0.2895, d0.loss_cls: 0.2163, d0.loss_bbox: 0.4522, d1.loss_cls: 0.1608, d1.loss_bbox: 0.3320, d2.loss_cls: 0.1354, d2.loss_bbox: 0.3226, d3.loss_cls: 0.1329, d3.loss_bbox: 0.3219, d4.loss_cls: 0.1308, d4.loss_bbox: 0.3244, loss: 2.9496, grad_norm: 39.2032
2025-06-19 18:24:49,911 - mmdet - INFO - Epoch [1][2050/7033]	lr: 2.000e-04, eta: 16:37:22, time: 1.576, data_time: 0.027, memory: 18226, loss_cls: 0.1158, loss_bbox: 0.2865, d0.loss_cls: 0.1995, d0.loss_bbox: 0.4132, d1.loss_cls: 0.1451, d1.loss_bbox: 0.3074, d2.loss_cls: 0.1201, d2.loss_bbox: 0.3077, d3.loss_cls: 0.1179, d3.loss_bbox: 0.3105, d4.loss_cls: 0.1157, d4.loss_bbox: 0.3083, loss: 2.7478, grad_norm: 30.3519
2025-06-19 18:26:03,161 - mmdet - INFO - Epoch [1][2100/7033]	lr: 2.000e-04, eta: 16:35:43, time: 1.465, data_time: 0.030, memory: 18226, loss_cls: 0.1108, loss_bbox: 0.2794, d0.loss_cls: 0.1954, d0.loss_bbox: 0.4150, d1.loss_cls: 0.1371, d1.loss_bbox: 0.3123, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2987, d3.loss_cls: 0.1119, d3.loss_bbox: 0.2996, d4.loss_cls: 0.1108, d4.loss_bbox: 0.3013, loss: 2.6883, grad_norm: 33.7015
2025-06-19 18:27:16,250 - mmdet - INFO - Epoch [1][2150/7033]	lr: 2.000e-04, eta: 16:34:03, time: 1.462, data_time: 0.027, memory: 18226, loss_cls: 0.1196, loss_bbox: 0.2713, d0.loss_cls: 0.2034, d0.loss_bbox: 0.4168, d1.loss_cls: 0.1521, d1.loss_bbox: 0.3068, d2.loss_cls: 0.1302, d2.loss_bbox: 0.2957, d3.loss_cls: 0.1216, d3.loss_bbox: 0.2951, d4.loss_cls: 0.1203, d4.loss_bbox: 0.2943, loss: 2.7271, grad_norm: 27.4336
2025-06-19 18:28:29,514 - mmdet - INFO - Epoch [1][2200/7033]	lr: 2.000e-04, eta: 16:32:26, time: 1.465, data_time: 0.027, memory: 18226, loss_cls: 0.1185, loss_bbox: 0.2658, d0.loss_cls: 0.1972, d0.loss_bbox: 0.4091, d1.loss_cls: 0.1549, d1.loss_bbox: 0.3002, d2.loss_cls: 0.1270, d2.loss_bbox: 0.2905, d3.loss_cls: 0.1230, d3.loss_bbox: 0.2906, d4.loss_cls: 0.1179, d4.loss_bbox: 0.2925, loss: 2.6871, grad_norm: 54.4281
2025-06-19 18:29:42,636 - mmdet - INFO - Epoch [1][2250/7033]	lr: 2.000e-04, eta: 16:30:49, time: 1.462, data_time: 0.026, memory: 18226, loss_cls: 0.1175, loss_bbox: 0.2785, d0.loss_cls: 0.1957, d0.loss_bbox: 0.4177, d1.loss_cls: 0.1485, d1.loss_bbox: 0.3101, d2.loss_cls: 0.1251, d2.loss_bbox: 0.3039, d3.loss_cls: 0.1185, d3.loss_bbox: 0.3043, d4.loss_cls: 0.1147, d4.loss_bbox: 0.3039, loss: 2.7383, grad_norm: 34.8384
2025-06-19 18:30:59,402 - mmdet - INFO - Epoch [1][2300/7033]	lr: 2.000e-04, eta: 16:30:15, time: 1.535, data_time: 0.103, memory: 18226, loss_cls: 0.1168, loss_bbox: 0.2638, d0.loss_cls: 0.2016, d0.loss_bbox: 0.4103, d1.loss_cls: 0.1469, d1.loss_bbox: 0.2980, d2.loss_cls: 0.1259, d2.loss_bbox: 0.2911, d3.loss_cls: 0.1164, d3.loss_bbox: 0.2916, d4.loss_cls: 0.1154, d4.loss_bbox: 0.2911, loss: 2.6690, grad_norm: 41.4929
2025-06-19 18:32:12,860 - mmdet - INFO - Epoch [1][2350/7033]	lr: 2.000e-04, eta: 16:28:44, time: 1.469, data_time: 0.028, memory: 18311, loss_cls: 0.1133, loss_bbox: 0.2707, d0.loss_cls: 0.1973, d0.loss_bbox: 0.4143, d1.loss_cls: 0.1433, d1.loss_bbox: 0.3084, d2.loss_cls: 0.1177, d2.loss_bbox: 0.3037, d3.loss_cls: 0.1106, d3.loss_bbox: 0.3023, d4.loss_cls: 0.1111, d4.loss_bbox: 0.2982, loss: 2.6911, grad_norm: 35.4748
2025-06-19 18:33:27,982 - mmdet - INFO - Epoch [1][2400/7033]	lr: 2.000e-04, eta: 16:27:40, time: 1.502, data_time: 0.027, memory: 18311, loss_cls: 0.1127, loss_bbox: 0.2665, d0.loss_cls: 0.2008, d0.loss_bbox: 0.4122, d1.loss_cls: 0.1506, d1.loss_bbox: 0.3017, d2.loss_cls: 0.1257, d2.loss_bbox: 0.2967, d3.loss_cls: 0.1159, d3.loss_bbox: 0.2939, d4.loss_cls: 0.1115, d4.loss_bbox: 0.2941, loss: 2.6824, grad_norm: 48.0993
2025-06-19 18:34:41,238 - mmdet - INFO - Epoch [1][2450/7033]	lr: 2.000e-04, eta: 16:26:07, time: 1.465, data_time: 0.026, memory: 18311, loss_cls: 0.1129, loss_bbox: 0.2680, d0.loss_cls: 0.2018, d0.loss_bbox: 0.4107, d1.loss_cls: 0.1473, d1.loss_bbox: 0.2979, d2.loss_cls: 0.1239, d2.loss_bbox: 0.2897, d3.loss_cls: 0.1139, d3.loss_bbox: 0.2922, d4.loss_cls: 0.1108, d4.loss_bbox: 0.2916, loss: 2.6607, grad_norm: 36.1276
2025-06-19 18:35:56,034 - mmdet - INFO - Epoch [1][2500/7033]	lr: 2.000e-04, eta: 16:24:58, time: 1.496, data_time: 0.027, memory: 18311, loss_cls: 0.1146, loss_bbox: 0.2752, d0.loss_cls: 0.1988, d0.loss_bbox: 0.4230, d1.loss_cls: 0.1476, d1.loss_bbox: 0.3110, d2.loss_cls: 0.1237, d2.loss_bbox: 0.3026, d3.loss_cls: 0.1161, d3.loss_bbox: 0.2976, d4.loss_cls: 0.1144, d4.loss_bbox: 0.2936, loss: 2.7183, grad_norm: 31.6085
2025-06-19 18:37:09,051 - mmdet - INFO - Epoch [1][2550/7033]	lr: 2.000e-04, eta: 16:23:22, time: 1.460, data_time: 0.029, memory: 18311, loss_cls: 0.1051, loss_bbox: 0.2632, d0.loss_cls: 0.1911, d0.loss_bbox: 0.4003, d1.loss_cls: 0.1367, d1.loss_bbox: 0.2969, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2899, d3.loss_cls: 0.1063, d3.loss_bbox: 0.2896, d4.loss_cls: 0.1047, d4.loss_bbox: 0.2812, loss: 2.5782, grad_norm: 39.4200
2025-06-19 18:38:22,214 - mmdet - INFO - Epoch [1][2600/7033]	lr: 2.000e-04, eta: 16:21:48, time: 1.463, data_time: 0.027, memory: 18311, loss_cls: 0.1137, loss_bbox: 0.2703, d0.loss_cls: 0.2078, d0.loss_bbox: 0.4097, d1.loss_cls: 0.1489, d1.loss_bbox: 0.3050, d2.loss_cls: 0.1208, d2.loss_bbox: 0.2981, d3.loss_cls: 0.1153, d3.loss_bbox: 0.2972, d4.loss_cls: 0.1130, d4.loss_bbox: 0.2896, loss: 2.6894, grad_norm: 42.8101
2025-06-19 18:39:35,343 - mmdet - INFO - Epoch [1][2650/7033]	lr: 2.000e-04, eta: 16:20:15, time: 1.463, data_time: 0.026, memory: 18311, loss_cls: 0.1148, loss_bbox: 0.2804, d0.loss_cls: 0.2081, d0.loss_bbox: 0.4254, d1.loss_cls: 0.1504, d1.loss_bbox: 0.3186, d2.loss_cls: 0.1269, d2.loss_bbox: 0.3111, d3.loss_cls: 0.1155, d3.loss_bbox: 0.3092, d4.loss_cls: 0.1144, d4.loss_bbox: 0.3021, loss: 2.7767, grad_norm: 60.5861
2025-06-19 18:40:48,257 - mmdet - INFO - Epoch [1][2700/7033]	lr: 2.000e-04, eta: 16:18:40, time: 1.458, data_time: 0.027, memory: 18311, loss_cls: 0.1143, loss_bbox: 0.2589, d0.loss_cls: 0.2051, d0.loss_bbox: 0.4103, d1.loss_cls: 0.1512, d1.loss_bbox: 0.3005, d2.loss_cls: 0.1298, d2.loss_bbox: 0.2888, d3.loss_cls: 0.1182, d3.loss_bbox: 0.2840, d4.loss_cls: 0.1161, d4.loss_bbox: 0.2737, loss: 2.6509, grad_norm: 29.1491
2025-06-19 18:42:01,118 - mmdet - INFO - Epoch [1][2750/7033]	lr: 2.000e-04, eta: 16:17:04, time: 1.457, data_time: 0.027, memory: 18311, loss_cls: 0.1140, loss_bbox: 0.2745, d0.loss_cls: 0.2115, d0.loss_bbox: 0.4362, d1.loss_cls: 0.1480, d1.loss_bbox: 0.3149, d2.loss_cls: 0.1239, d2.loss_bbox: 0.3084, d3.loss_cls: 0.1154, d3.loss_bbox: 0.3002, d4.loss_cls: 0.1116, d4.loss_bbox: 0.2951, loss: 2.7536, grad_norm: 43.5147
2025-06-19 18:43:14,531 - mmdet - INFO - Epoch [1][2800/7033]	lr: 2.000e-04, eta: 16:15:37, time: 1.468, data_time: 0.030, memory: 18311, loss_cls: 0.1104, loss_bbox: 0.2551, d0.loss_cls: 0.2028, d0.loss_bbox: 0.4195, d1.loss_cls: 0.1462, d1.loss_bbox: 0.2960, d2.loss_cls: 0.1303, d2.loss_bbox: 0.2933, d3.loss_cls: 0.1206, d3.loss_bbox: 0.2842, d4.loss_cls: 0.1131, d4.loss_bbox: 0.2724, loss: 2.6439, grad_norm: 48.5312
2025-06-19 18:44:27,799 - mmdet - INFO - Epoch [1][2850/7033]	lr: 2.000e-04, eta: 16:14:09, time: 1.465, data_time: 0.026, memory: 18311, loss_cls: 0.1025, loss_bbox: 0.2641, d0.loss_cls: 0.2000, d0.loss_bbox: 0.4170, d1.loss_cls: 0.1403, d1.loss_bbox: 0.3033, d2.loss_cls: 0.1193, d2.loss_bbox: 0.2945, d3.loss_cls: 0.1100, d3.loss_bbox: 0.2846, d4.loss_cls: 0.1049, d4.loss_bbox: 0.2765, loss: 2.6169, grad_norm: 36.9686
2025-06-19 18:45:45,716 - mmdet - INFO - Epoch [1][2900/7033]	lr: 2.000e-04, eta: 16:13:44, time: 1.558, data_time: 0.043, memory: 18311, loss_cls: 0.0994, loss_bbox: 0.2473, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3930, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2829, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2778, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2714, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2605, loss: 2.4630, grad_norm: 37.9371
2025-06-19 18:46:58,619 - mmdet - INFO - Epoch [1][2950/7033]	lr: 2.000e-04, eta: 16:12:11, time: 1.458, data_time: 0.027, memory: 18311, loss_cls: 0.1195, loss_bbox: 0.2840, d0.loss_cls: 0.2035, d0.loss_bbox: 0.4271, d1.loss_cls: 0.1493, d1.loss_bbox: 0.3188, d2.loss_cls: 0.1301, d2.loss_bbox: 0.3112, d3.loss_cls: 0.1213, d3.loss_bbox: 0.3047, d4.loss_cls: 0.1185, d4.loss_bbox: 0.2991, loss: 2.7870, grad_norm: 33.6261
2025-06-19 18:48:11,667 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 18:48:11,667 - mmdet - INFO - Epoch [1][3000/7033]	lr: 2.000e-04, eta: 16:10:40, time: 1.461, data_time: 0.030, memory: 18311, loss_cls: 0.1198, loss_bbox: 0.2999, d0.loss_cls: 0.2018, d0.loss_bbox: 0.4438, d1.loss_cls: 0.1509, d1.loss_bbox: 0.3385, d2.loss_cls: 0.1328, d2.loss_bbox: 0.3303, d3.loss_cls: 0.1235, d3.loss_bbox: 0.3208, d4.loss_cls: 0.1228, d4.loss_bbox: 0.3119, loss: 2.8967, grad_norm: 50.9578
2025-06-19 18:49:24,799 - mmdet - INFO - Epoch [1][3050/7033]	lr: 2.000e-04, eta: 16:09:11, time: 1.463, data_time: 0.030, memory: 18311, loss_cls: 0.1105, loss_bbox: 0.2720, d0.loss_cls: 0.2026, d0.loss_bbox: 0.4248, d1.loss_cls: 0.1440, d1.loss_bbox: 0.3143, d2.loss_cls: 0.1239, d2.loss_bbox: 0.3034, d3.loss_cls: 0.1154, d3.loss_bbox: 0.2993, d4.loss_cls: 0.1130, d4.loss_bbox: 0.2894, loss: 2.7125, grad_norm: 46.7248
2025-06-19 18:50:38,008 - mmdet - INFO - Epoch [1][3100/7033]	lr: 2.000e-04, eta: 16:07:43, time: 1.464, data_time: 0.027, memory: 18311, loss_cls: 0.1009, loss_bbox: 0.2665, d0.loss_cls: 0.1951, d0.loss_bbox: 0.4132, d1.loss_cls: 0.1348, d1.loss_bbox: 0.3034, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2974, d3.loss_cls: 0.1067, d3.loss_bbox: 0.2860, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2768, loss: 2.5959, grad_norm: 26.6424
2025-06-19 18:51:51,202 - mmdet - INFO - Epoch [1][3150/7033]	lr: 2.000e-04, eta: 16:06:16, time: 1.464, data_time: 0.026, memory: 18311, loss_cls: 0.1118, loss_bbox: 0.2734, d0.loss_cls: 0.2039, d0.loss_bbox: 0.4333, d1.loss_cls: 0.1470, d1.loss_bbox: 0.3220, d2.loss_cls: 0.1241, d2.loss_bbox: 0.3086, d3.loss_cls: 0.1176, d3.loss_bbox: 0.3003, d4.loss_cls: 0.1124, d4.loss_bbox: 0.2850, loss: 2.7394, grad_norm: 37.4872
2025-06-19 18:53:04,309 - mmdet - INFO - Epoch [1][3200/7033]	lr: 2.000e-04, eta: 16:04:48, time: 1.462, data_time: 0.026, memory: 18311, loss_cls: 0.1029, loss_bbox: 0.2586, d0.loss_cls: 0.2039, d0.loss_bbox: 0.4056, d1.loss_cls: 0.1388, d1.loss_bbox: 0.3019, d2.loss_cls: 0.1169, d2.loss_bbox: 0.2885, d3.loss_cls: 0.1087, d3.loss_bbox: 0.2810, d4.loss_cls: 0.1064, d4.loss_bbox: 0.2689, loss: 2.5821, grad_norm: 29.2779
2025-06-19 18:54:17,014 - mmdet - INFO - Epoch [1][3250/7033]	lr: 2.000e-04, eta: 16:03:15, time: 1.454, data_time: 0.026, memory: 18311, loss_cls: 0.1014, loss_bbox: 0.2580, d0.loss_cls: 0.1974, d0.loss_bbox: 0.3959, d1.loss_cls: 0.1411, d1.loss_bbox: 0.3009, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2891, d3.loss_cls: 0.1061, d3.loss_bbox: 0.2836, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2692, loss: 2.5649, grad_norm: 40.5422
2025-06-19 18:55:30,158 - mmdet - INFO - Epoch [1][3300/7033]	lr: 2.000e-04, eta: 16:01:49, time: 1.463, data_time: 0.028, memory: 18311, loss_cls: 0.1058, loss_bbox: 0.2682, d0.loss_cls: 0.2004, d0.loss_bbox: 0.4105, d1.loss_cls: 0.1433, d1.loss_bbox: 0.3119, d2.loss_cls: 0.1218, d2.loss_bbox: 0.3038, d3.loss_cls: 0.1136, d3.loss_bbox: 0.2955, d4.loss_cls: 0.1075, d4.loss_bbox: 0.2806, loss: 2.6628, grad_norm: 42.7253
2025-06-19 18:56:43,213 - mmdet - INFO - Epoch [1][3350/7033]	lr: 2.000e-04, eta: 16:00:21, time: 1.461, data_time: 0.029, memory: 18311, loss_cls: 0.1065, loss_bbox: 0.2690, d0.loss_cls: 0.2106, d0.loss_bbox: 0.4164, d1.loss_cls: 0.1505, d1.loss_bbox: 0.3123, d2.loss_cls: 0.1273, d2.loss_bbox: 0.3010, d3.loss_cls: 0.1173, d3.loss_bbox: 0.2935, d4.loss_cls: 0.1094, d4.loss_bbox: 0.2789, loss: 2.6925, grad_norm: 56.4871
2025-06-19 18:57:56,588 - mmdet - INFO - Epoch [1][3400/7033]	lr: 2.000e-04, eta: 15:58:58, time: 1.468, data_time: 0.031, memory: 18311, loss_cls: 0.1056, loss_bbox: 0.2614, d0.loss_cls: 0.2013, d0.loss_bbox: 0.3975, d1.loss_cls: 0.1430, d1.loss_bbox: 0.2966, d2.loss_cls: 0.1222, d2.loss_bbox: 0.2897, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2800, d4.loss_cls: 0.1092, d4.loss_bbox: 0.2674, loss: 2.5860, grad_norm: 31.8606
2025-06-19 18:59:10,045 - mmdet - INFO - Epoch [1][3450/7033]	lr: 2.000e-04, eta: 15:57:36, time: 1.469, data_time: 0.030, memory: 18311, loss_cls: 0.0998, loss_bbox: 0.2494, d0.loss_cls: 0.1934, d0.loss_bbox: 0.3908, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2869, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2774, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2717, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2587, loss: 2.4746, grad_norm: 49.9280
2025-06-19 19:00:23,262 - mmdet - INFO - Epoch [1][3500/7033]	lr: 2.000e-04, eta: 15:56:12, time: 1.464, data_time: 0.028, memory: 18311, loss_cls: 0.0997, loss_bbox: 0.2467, d0.loss_cls: 0.2006, d0.loss_bbox: 0.3880, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2853, d2.loss_cls: 0.1159, d2.loss_bbox: 0.2754, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2684, d4.loss_cls: 0.1032, d4.loss_bbox: 0.2535, loss: 2.4826, grad_norm: 31.6984
2025-06-19 19:01:36,178 - mmdet - INFO - Epoch [1][3550/7033]	lr: 2.000e-04, eta: 15:54:45, time: 1.458, data_time: 0.027, memory: 18311, loss_cls: 0.0975, loss_bbox: 0.2420, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1373, d1.loss_bbox: 0.2787, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2696, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2663, d4.loss_cls: 0.1017, d4.loss_bbox: 0.2492, loss: 2.4320, grad_norm: 36.4751
2025-06-19 19:02:49,400 - mmdet - INFO - Epoch [1][3600/7033]	lr: 2.000e-04, eta: 15:53:21, time: 1.464, data_time: 0.029, memory: 18311, loss_cls: 0.0910, loss_bbox: 0.2457, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3752, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2808, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2738, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2674, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2521, loss: 2.3888, grad_norm: 37.7203
2025-06-19 19:04:02,554 - mmdet - INFO - Epoch [1][3650/7033]	lr: 2.000e-04, eta: 15:51:57, time: 1.463, data_time: 0.030, memory: 18311, loss_cls: 0.1063, loss_bbox: 0.2493, d0.loss_cls: 0.2118, d0.loss_bbox: 0.4021, d1.loss_cls: 0.1569, d1.loss_bbox: 0.3090, d2.loss_cls: 0.1253, d2.loss_bbox: 0.2963, d3.loss_cls: 0.1133, d3.loss_bbox: 0.2804, d4.loss_cls: 0.1091, d4.loss_bbox: 0.2627, loss: 2.6225, grad_norm: 28.2040
2025-06-19 19:05:15,635 - mmdet - INFO - Epoch [1][3700/7033]	lr: 2.000e-04, eta: 15:50:32, time: 1.462, data_time: 0.028, memory: 18311, loss_cls: 0.1086, loss_bbox: 0.2555, d0.loss_cls: 0.2003, d0.loss_bbox: 0.4056, d1.loss_cls: 0.1424, d1.loss_bbox: 0.3021, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2910, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2803, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2667, loss: 2.5898, grad_norm: 29.3735
2025-06-19 19:06:28,658 - mmdet - INFO - Epoch [1][3750/7033]	lr: 2.000e-04, eta: 15:49:07, time: 1.460, data_time: 0.028, memory: 18311, loss_cls: 0.1020, loss_bbox: 0.2435, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3749, d1.loss_cls: 0.1354, d1.loss_bbox: 0.2860, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2756, d3.loss_cls: 0.1088, d3.loss_bbox: 0.2631, d4.loss_cls: 0.1046, d4.loss_bbox: 0.2502, loss: 2.4578, grad_norm: 28.2984
2025-06-19 19:07:42,146 - mmdet - INFO - Epoch [1][3800/7033]	lr: 2.000e-04, eta: 15:47:48, time: 1.470, data_time: 0.029, memory: 18311, loss_cls: 0.1005, loss_bbox: 0.2542, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3822, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2995, d2.loss_cls: 0.1160, d2.loss_bbox: 0.2927, d3.loss_cls: 0.1056, d3.loss_bbox: 0.2776, d4.loss_cls: 0.1016, d4.loss_bbox: 0.2625, loss: 2.5262, grad_norm: 43.2786
2025-06-19 19:08:55,441 - mmdet - INFO - Epoch [1][3850/7033]	lr: 2.000e-04, eta: 15:46:26, time: 1.466, data_time: 0.029, memory: 18311, loss_cls: 0.0999, loss_bbox: 0.2506, d0.loss_cls: 0.1904, d0.loss_bbox: 0.3818, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2946, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2868, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2714, d4.loss_cls: 0.1003, d4.loss_bbox: 0.2572, loss: 2.4687, grad_norm: 45.6863
2025-06-19 19:10:08,988 - mmdet - INFO - Epoch [1][3900/7033]	lr: 2.000e-04, eta: 15:45:07, time: 1.471, data_time: 0.030, memory: 18311, loss_cls: 0.0999, loss_bbox: 0.2500, d0.loss_cls: 0.2030, d0.loss_bbox: 0.3763, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2870, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2833, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2669, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2551, loss: 2.4807, grad_norm: 37.9873
2025-06-19 19:11:24,018 - mmdet - INFO - Epoch [1][3950/7033]	lr: 2.000e-04, eta: 15:44:03, time: 1.501, data_time: 0.026, memory: 18311, loss_cls: 0.1068, loss_bbox: 0.2580, d0.loss_cls: 0.1990, d0.loss_bbox: 0.3961, d1.loss_cls: 0.1417, d1.loss_bbox: 0.3005, d2.loss_cls: 0.1207, d2.loss_bbox: 0.2914, d3.loss_cls: 0.1117, d3.loss_bbox: 0.2774, d4.loss_cls: 0.1113, d4.loss_bbox: 0.2639, loss: 2.5785, grad_norm: 40.9607
2025-06-19 19:12:36,937 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 19:12:36,937 - mmdet - INFO - Epoch [1][4000/7033]	lr: 2.000e-04, eta: 15:42:38, time: 1.458, data_time: 0.026, memory: 18311, loss_cls: 0.1062, loss_bbox: 0.2553, d0.loss_cls: 0.1954, d0.loss_bbox: 0.3852, d1.loss_cls: 0.1427, d1.loss_bbox: 0.2963, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2875, d3.loss_cls: 0.1134, d3.loss_bbox: 0.2729, d4.loss_cls: 0.1084, d4.loss_bbox: 0.2619, loss: 2.5488, grad_norm: 43.3988
2025-06-19 19:13:51,684 - mmdet - INFO - Epoch [1][4050/7033]	lr: 2.000e-04, eta: 15:41:31, time: 1.495, data_time: 0.029, memory: 18311, loss_cls: 0.1040, loss_bbox: 0.2611, d0.loss_cls: 0.1989, d0.loss_bbox: 0.3965, d1.loss_cls: 0.1411, d1.loss_bbox: 0.3021, d2.loss_cls: 0.1204, d2.loss_bbox: 0.2932, d3.loss_cls: 0.1107, d3.loss_bbox: 0.2775, d4.loss_cls: 0.1080, d4.loss_bbox: 0.2678, loss: 2.5813, grad_norm: 260.5945
2025-06-19 19:15:04,938 - mmdet - INFO - Epoch [1][4100/7033]	lr: 2.000e-04, eta: 15:40:09, time: 1.465, data_time: 0.028, memory: 18311, loss_cls: 0.0993, loss_bbox: 0.2614, d0.loss_cls: 0.2000, d0.loss_bbox: 0.3967, d1.loss_cls: 0.1330, d1.loss_bbox: 0.3010, d2.loss_cls: 0.1116, d2.loss_bbox: 0.2943, d3.loss_cls: 0.1064, d3.loss_bbox: 0.2790, d4.loss_cls: 0.0997, d4.loss_bbox: 0.2690, loss: 2.5514, grad_norm: 43.1164
2025-06-19 19:16:18,224 - mmdet - INFO - Epoch [1][4150/7033]	lr: 2.000e-04, eta: 15:38:48, time: 1.466, data_time: 0.026, memory: 18311, loss_cls: 0.0980, loss_bbox: 0.2441, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3821, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2821, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2772, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2613, d4.loss_cls: 0.0987, d4.loss_bbox: 0.2507, loss: 2.4223, grad_norm: 94.1999
2025-06-19 19:17:31,274 - mmdet - INFO - Epoch [1][4200/7033]	lr: 2.000e-04, eta: 15:37:25, time: 1.461, data_time: 0.026, memory: 18311, loss_cls: 0.0996, loss_bbox: 0.2539, d0.loss_cls: 0.2002, d0.loss_bbox: 0.3938, d1.loss_cls: 0.1337, d1.loss_bbox: 0.2975, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2847, d3.loss_cls: 0.1057, d3.loss_bbox: 0.2686, d4.loss_cls: 0.1029, d4.loss_bbox: 0.2591, loss: 2.5141, grad_norm: 32.8549
2025-06-19 19:18:44,279 - mmdet - INFO - Epoch [1][4250/7033]	lr: 2.000e-04, eta: 15:36:02, time: 1.460, data_time: 0.027, memory: 18311, loss_cls: 0.0958, loss_bbox: 0.2341, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3650, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2747, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2630, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2479, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2395, loss: 2.3472, grad_norm: 29.3796
2025-06-19 19:19:57,541 - mmdet - INFO - Epoch [1][4300/7033]	lr: 2.000e-04, eta: 15:34:42, time: 1.465, data_time: 0.028, memory: 18311, loss_cls: 0.0981, loss_bbox: 0.2464, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3978, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2945, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2822, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2654, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2534, loss: 2.4874, grad_norm: 111.6846
2025-06-19 19:21:11,061 - mmdet - INFO - Epoch [1][4350/7033]	lr: 2.000e-04, eta: 15:33:24, time: 1.470, data_time: 0.031, memory: 18311, loss_cls: 0.0930, loss_bbox: 0.2369, d0.loss_cls: 0.1952, d0.loss_bbox: 0.3794, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2826, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2732, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2570, d4.loss_cls: 0.0965, d4.loss_bbox: 0.2434, loss: 2.3939, grad_norm: 46.1262
2025-06-19 19:22:24,261 - mmdet - INFO - Epoch [1][4400/7033]	lr: 2.000e-04, eta: 15:32:03, time: 1.464, data_time: 0.026, memory: 18311, loss_cls: 0.0933, loss_bbox: 0.2395, d0.loss_cls: 0.1961, d0.loss_bbox: 0.3901, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2900, d2.loss_cls: 0.1108, d2.loss_bbox: 0.2765, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2569, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2446, loss: 2.4218, grad_norm: 47.3984
2025-06-19 19:23:37,242 - mmdet - INFO - Epoch [1][4450/7033]	lr: 2.000e-04, eta: 15:30:41, time: 1.460, data_time: 0.027, memory: 18311, loss_cls: 0.0933, loss_bbox: 0.2418, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3872, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2872, d2.loss_cls: 0.1057, d2.loss_bbox: 0.2761, d3.loss_cls: 0.0996, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2472, loss: 2.4064, grad_norm: 27.2994
2025-06-19 19:24:50,437 - mmdet - INFO - Epoch [1][4500/7033]	lr: 2.000e-04, eta: 15:29:20, time: 1.464, data_time: 0.027, memory: 18311, loss_cls: 0.0941, loss_bbox: 0.2463, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3785, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2828, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2722, d3.loss_cls: 0.0981, d3.loss_bbox: 0.2573, d4.loss_cls: 0.0968, d4.loss_bbox: 0.2502, loss: 2.4049, grad_norm: 94.1907
2025-06-19 19:26:03,420 - mmdet - INFO - Epoch [1][4550/7033]	lr: 2.000e-04, eta: 15:27:58, time: 1.460, data_time: 0.026, memory: 18311, loss_cls: 0.1040, loss_bbox: 0.2578, d0.loss_cls: 0.1984, d0.loss_bbox: 0.3965, d1.loss_cls: 0.1345, d1.loss_bbox: 0.3015, d2.loss_cls: 0.1175, d2.loss_bbox: 0.2901, d3.loss_cls: 0.1106, d3.loss_bbox: 0.2704, d4.loss_cls: 0.1055, d4.loss_bbox: 0.2632, loss: 2.5498, grad_norm: 43.1278
2025-06-19 19:27:16,773 - mmdet - INFO - Epoch [1][4600/7033]	lr: 2.000e-04, eta: 15:26:39, time: 1.467, data_time: 0.030, memory: 18311, loss_cls: 0.1034, loss_bbox: 0.2586, d0.loss_cls: 0.1967, d0.loss_bbox: 0.3950, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2957, d2.loss_cls: 0.1154, d2.loss_bbox: 0.2867, d3.loss_cls: 0.1078, d3.loss_bbox: 0.2734, d4.loss_cls: 0.1077, d4.loss_bbox: 0.2636, loss: 2.5420, grad_norm: 24.5397
2025-06-19 19:28:31,746 - mmdet - INFO - Epoch [1][4650/7033]	lr: 2.000e-04, eta: 15:25:34, time: 1.499, data_time: 0.027, memory: 18311, loss_cls: 0.1013, loss_bbox: 0.2526, d0.loss_cls: 0.1937, d0.loss_bbox: 0.3893, d1.loss_cls: 0.1376, d1.loss_bbox: 0.2883, d2.loss_cls: 0.1151, d2.loss_bbox: 0.2811, d3.loss_cls: 0.1068, d3.loss_bbox: 0.2633, d4.loss_cls: 0.1040, d4.loss_bbox: 0.2583, loss: 2.4913, grad_norm: 30.3679
2025-06-19 19:29:44,828 - mmdet - INFO - Epoch [1][4700/7033]	lr: 2.000e-04, eta: 15:24:13, time: 1.462, data_time: 0.026, memory: 18311, loss_cls: 0.0982, loss_bbox: 0.2602, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3985, d1.loss_cls: 0.1307, d1.loss_bbox: 0.3027, d2.loss_cls: 0.1126, d2.loss_bbox: 0.2901, d3.loss_cls: 0.1020, d3.loss_bbox: 0.2733, d4.loss_cls: 0.0985, d4.loss_bbox: 0.2670, loss: 2.5254, grad_norm: 30.9032
2025-06-19 19:30:57,761 - mmdet - INFO - Epoch [1][4750/7033]	lr: 2.000e-04, eta: 15:22:51, time: 1.459, data_time: 0.026, memory: 18311, loss_cls: 0.0912, loss_bbox: 0.2449, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2729, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2577, d4.loss_cls: 0.0945, d4.loss_bbox: 0.2516, loss: 2.3775, grad_norm: 23.6268
2025-06-19 19:32:11,051 - mmdet - INFO - Epoch [1][4800/7033]	lr: 2.000e-04, eta: 15:21:32, time: 1.466, data_time: 0.026, memory: 18311, loss_cls: 0.0918, loss_bbox: 0.2324, d0.loss_cls: 0.1929, d0.loss_bbox: 0.3731, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1068, d2.loss_bbox: 0.2619, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2349, loss: 2.3283, grad_norm: 38.9873
2025-06-19 19:33:24,246 - mmdet - INFO - Epoch [1][4850/7033]	lr: 2.000e-04, eta: 15:20:13, time: 1.464, data_time: 0.026, memory: 18311, loss_cls: 0.1016, loss_bbox: 0.2449, d0.loss_cls: 0.2060, d0.loss_bbox: 0.3984, d1.loss_cls: 0.1405, d1.loss_bbox: 0.2877, d2.loss_cls: 0.1159, d2.loss_bbox: 0.2760, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2570, d4.loss_cls: 0.1012, d4.loss_bbox: 0.2518, loss: 2.4881, grad_norm: 55.0697
2025-06-19 19:34:39,326 - mmdet - INFO - Epoch [1][4900/7033]	lr: 2.000e-04, eta: 15:19:08, time: 1.502, data_time: 0.027, memory: 18311, loss_cls: 0.1011, loss_bbox: 0.2444, d0.loss_cls: 0.2012, d0.loss_bbox: 0.3930, d1.loss_cls: 0.1468, d1.loss_bbox: 0.2891, d2.loss_cls: 0.1176, d2.loss_bbox: 0.2822, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2620, d4.loss_cls: 0.1019, d4.loss_bbox: 0.2526, loss: 2.5000, grad_norm: 46.1914
2025-06-19 19:35:52,527 - mmdet - INFO - Epoch [1][4950/7033]	lr: 2.000e-04, eta: 15:17:48, time: 1.464, data_time: 0.026, memory: 18311, loss_cls: 0.1031, loss_bbox: 0.2528, d0.loss_cls: 0.2030, d0.loss_bbox: 0.3926, d1.loss_cls: 0.1425, d1.loss_bbox: 0.2939, d2.loss_cls: 0.1212, d2.loss_bbox: 0.2810, d3.loss_cls: 0.1105, d3.loss_bbox: 0.2646, d4.loss_cls: 0.1043, d4.loss_bbox: 0.2583, loss: 2.5278, grad_norm: 219.8723
2025-06-19 19:37:05,562 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 19:37:05,562 - mmdet - INFO - Epoch [1][5000/7033]	lr: 2.000e-04, eta: 15:16:28, time: 1.461, data_time: 0.026, memory: 18311, loss_cls: 0.0883, loss_bbox: 0.2422, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2780, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2738, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2537, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2459, loss: 2.3512, grad_norm: 35.2337
2025-06-19 19:38:18,677 - mmdet - INFO - Epoch [1][5050/7033]	lr: 2.000e-04, eta: 15:15:08, time: 1.462, data_time: 0.027, memory: 18311, loss_cls: 0.1030, loss_bbox: 0.2486, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3931, d1.loss_cls: 0.1426, d1.loss_bbox: 0.2940, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2861, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2637, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2540, loss: 2.5106, grad_norm: 38.9747
2025-06-19 19:39:31,533 - mmdet - INFO - Epoch [1][5100/7033]	lr: 2.000e-04, eta: 15:13:46, time: 1.457, data_time: 0.026, memory: 18311, loss_cls: 0.0958, loss_bbox: 0.2506, d0.loss_cls: 0.1952, d0.loss_bbox: 0.3954, d1.loss_cls: 0.1365, d1.loss_bbox: 0.2949, d2.loss_cls: 0.1131, d2.loss_bbox: 0.2818, d3.loss_cls: 0.1020, d3.loss_bbox: 0.2624, d4.loss_cls: 0.0981, d4.loss_bbox: 0.2541, loss: 2.4800, grad_norm: 39.3023
2025-06-19 19:40:44,969 - mmdet - INFO - Epoch [1][5150/7033]	lr: 2.000e-04, eta: 15:12:29, time: 1.469, data_time: 0.030, memory: 18311, loss_cls: 0.1063, loss_bbox: 0.2473, d0.loss_cls: 0.2058, d0.loss_bbox: 0.3803, d1.loss_cls: 0.1456, d1.loss_bbox: 0.2893, d2.loss_cls: 0.1257, d2.loss_bbox: 0.2798, d3.loss_cls: 0.1138, d3.loss_bbox: 0.2606, d4.loss_cls: 0.1096, d4.loss_bbox: 0.2518, loss: 2.5160, grad_norm: 42.2275
2025-06-19 19:41:58,117 - mmdet - INFO - Epoch [1][5200/7033]	lr: 2.000e-04, eta: 15:11:10, time: 1.463, data_time: 0.028, memory: 18311, loss_cls: 0.0986, loss_bbox: 0.2409, d0.loss_cls: 0.1969, d0.loss_bbox: 0.3828, d1.loss_cls: 0.1373, d1.loss_bbox: 0.2848, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2747, d3.loss_cls: 0.1046, d3.loss_bbox: 0.2538, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2461, loss: 2.4356, grad_norm: 28.6711
2025-06-19 19:43:11,296 - mmdet - INFO - Epoch [1][5250/7033]	lr: 2.000e-04, eta: 15:09:51, time: 1.464, data_time: 0.030, memory: 18311, loss_cls: 0.0967, loss_bbox: 0.2364, d0.loss_cls: 0.2053, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2794, d2.loss_cls: 0.1157, d2.loss_bbox: 0.2647, d3.loss_cls: 0.1059, d3.loss_bbox: 0.2436, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2393, loss: 2.4000, grad_norm: 43.7967
2025-06-19 19:44:24,314 - mmdet - INFO - Epoch [1][5300/7033]	lr: 2.000e-04, eta: 15:08:31, time: 1.460, data_time: 0.028, memory: 18311, loss_cls: 0.0903, loss_bbox: 0.2252, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1332, d1.loss_bbox: 0.2752, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2654, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2435, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2339, loss: 2.3310, grad_norm: 44.3685
2025-06-19 19:45:37,526 - mmdet - INFO - Epoch [1][5350/7033]	lr: 2.000e-04, eta: 15:07:13, time: 1.464, data_time: 0.027, memory: 18311, loss_cls: 0.1052, loss_bbox: 0.2474, d0.loss_cls: 0.2120, d0.loss_bbox: 0.4120, d1.loss_cls: 0.1545, d1.loss_bbox: 0.3003, d2.loss_cls: 0.1271, d2.loss_bbox: 0.2885, d3.loss_cls: 0.1107, d3.loss_bbox: 0.2649, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2527, loss: 2.5820, grad_norm: 36.4737
2025-06-19 19:46:50,799 - mmdet - INFO - Epoch [1][5400/7033]	lr: 2.000e-04, eta: 15:05:55, time: 1.465, data_time: 0.028, memory: 18311, loss_cls: 0.0959, loss_bbox: 0.2427, d0.loss_cls: 0.2038, d0.loss_bbox: 0.3986, d1.loss_cls: 0.1401, d1.loss_bbox: 0.2922, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2773, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2551, d4.loss_cls: 0.0980, d4.loss_bbox: 0.2479, loss: 2.4756, grad_norm: 33.8526
2025-06-19 19:48:04,025 - mmdet - INFO - Epoch [1][5450/7033]	lr: 2.000e-04, eta: 15:04:37, time: 1.464, data_time: 0.028, memory: 18311, loss_cls: 0.0951, loss_bbox: 0.2452, d0.loss_cls: 0.1975, d0.loss_bbox: 0.3910, d1.loss_cls: 0.1429, d1.loss_bbox: 0.2954, d2.loss_cls: 0.1225, d2.loss_bbox: 0.2798, d3.loss_cls: 0.1033, d3.loss_bbox: 0.2594, d4.loss_cls: 0.0968, d4.loss_bbox: 0.2502, loss: 2.4790, grad_norm: 21.6032
2025-06-19 19:49:17,197 - mmdet - INFO - Epoch [1][5500/7033]	lr: 2.000e-04, eta: 15:03:18, time: 1.463, data_time: 0.026, memory: 18311, loss_cls: 0.0965, loss_bbox: 0.2334, d0.loss_cls: 0.1927, d0.loss_bbox: 0.3690, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2779, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2628, d3.loss_cls: 0.1055, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2366, loss: 2.3696, grad_norm: 36.8093
2025-06-19 19:50:30,125 - mmdet - INFO - Epoch [1][5550/7033]	lr: 2.000e-04, eta: 15:01:59, time: 1.459, data_time: 0.026, memory: 18311, loss_cls: 0.0943, loss_bbox: 0.2366, d0.loss_cls: 0.2024, d0.loss_bbox: 0.3930, d1.loss_cls: 0.1394, d1.loss_bbox: 0.2872, d2.loss_cls: 0.1124, d2.loss_bbox: 0.2768, d3.loss_cls: 0.1010, d3.loss_bbox: 0.2511, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2398, loss: 2.4291, grad_norm: 35.2226
2025-06-19 19:51:43,125 - mmdet - INFO - Epoch [1][5600/7033]	lr: 2.000e-04, eta: 15:00:39, time: 1.460, data_time: 0.027, memory: 18311, loss_cls: 0.0951, loss_bbox: 0.2418, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3852, d1.loss_cls: 0.1322, d1.loss_bbox: 0.2875, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2763, d3.loss_cls: 0.1019, d3.loss_bbox: 0.2575, d4.loss_cls: 0.0967, d4.loss_bbox: 0.2486, loss: 2.4292, grad_norm: 29.3094
2025-06-19 19:52:56,312 - mmdet - INFO - Epoch [1][5650/7033]	lr: 2.000e-04, eta: 14:59:21, time: 1.464, data_time: 0.029, memory: 18311, loss_cls: 0.0961, loss_bbox: 0.2447, d0.loss_cls: 0.1978, d0.loss_bbox: 0.3807, d1.loss_cls: 0.1307, d1.loss_bbox: 0.2890, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2764, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2573, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2483, loss: 2.4316, grad_norm: 76.7989
2025-06-19 19:54:09,469 - mmdet - INFO - Epoch [1][5700/7033]	lr: 2.000e-04, eta: 14:58:03, time: 1.463, data_time: 0.029, memory: 18311, loss_cls: 0.0888, loss_bbox: 0.2356, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3669, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2824, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2705, d3.loss_cls: 0.0952, d3.loss_bbox: 0.2497, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2408, loss: 2.3483, grad_norm: 27.3533
2025-06-19 19:55:22,553 - mmdet - INFO - Epoch [1][5750/7033]	lr: 2.000e-04, eta: 14:56:45, time: 1.462, data_time: 0.027, memory: 18311, loss_cls: 0.0845, loss_bbox: 0.2315, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2736, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2593, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2364, loss: 2.2770, grad_norm: 23.2664
2025-06-19 19:56:35,636 - mmdet - INFO - Epoch [1][5800/7033]	lr: 2.000e-04, eta: 14:55:26, time: 1.462, data_time: 0.026, memory: 18311, loss_cls: 0.0876, loss_bbox: 0.2255, d0.loss_cls: 0.1911, d0.loss_bbox: 0.3707, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2755, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2597, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2315, loss: 2.2947, grad_norm: 272.5137
2025-06-19 19:57:48,571 - mmdet - INFO - Epoch [1][5850/7033]	lr: 2.000e-04, eta: 14:54:07, time: 1.459, data_time: 0.026, memory: 18311, loss_cls: 0.0944, loss_bbox: 0.2337, d0.loss_cls: 0.2146, d0.loss_bbox: 0.3853, d1.loss_cls: 0.1425, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2659, d3.loss_cls: 0.1009, d3.loss_bbox: 0.2482, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2383, loss: 2.4120, grad_norm: 31.7388
2025-06-19 19:59:01,916 - mmdet - INFO - Epoch [1][5900/7033]	lr: 2.000e-04, eta: 14:52:51, time: 1.467, data_time: 0.028, memory: 18311, loss_cls: 0.0934, loss_bbox: 0.2326, d0.loss_cls: 0.2060, d0.loss_bbox: 0.3849, d1.loss_cls: 0.1369, d1.loss_bbox: 0.2821, d2.loss_cls: 0.1101, d2.loss_bbox: 0.2702, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2481, d4.loss_cls: 0.0957, d4.loss_bbox: 0.2376, loss: 2.3974, grad_norm: 74.6296
2025-06-19 20:00:14,993 - mmdet - INFO - Epoch [1][5950/7033]	lr: 2.000e-04, eta: 14:51:32, time: 1.462, data_time: 0.028, memory: 18311, loss_cls: 0.0985, loss_bbox: 0.2383, d0.loss_cls: 0.2152, d0.loss_bbox: 0.3892, d1.loss_cls: 0.1478, d1.loss_bbox: 0.2838, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2716, d3.loss_cls: 0.1071, d3.loss_bbox: 0.2482, d4.loss_cls: 0.1017, d4.loss_bbox: 0.2426, loss: 2.4611, grad_norm: 34.5438
2025-06-19 20:01:28,248 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 20:01:28,248 - mmdet - INFO - Epoch [1][6000/7033]	lr: 2.000e-04, eta: 14:50:15, time: 1.465, data_time: 0.026, memory: 18626, loss_cls: 0.0957, loss_bbox: 0.2430, d0.loss_cls: 0.2085, d0.loss_bbox: 0.3918, d1.loss_cls: 0.1416, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2700, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0971, d4.loss_bbox: 0.2449, loss: 2.4443, grad_norm: 33.8069
2025-06-19 20:02:41,295 - mmdet - INFO - Epoch [1][6050/7033]	lr: 2.000e-04, eta: 14:48:57, time: 1.461, data_time: 0.027, memory: 18626, loss_cls: 0.0943, loss_bbox: 0.2290, d0.loss_cls: 0.1956, d0.loss_bbox: 0.3741, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2770, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2650, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2407, d4.loss_cls: 0.0984, d4.loss_bbox: 0.2340, loss: 2.3518, grad_norm: 44.1005
2025-06-19 20:03:54,302 - mmdet - INFO - Epoch [1][6100/7033]	lr: 2.000e-04, eta: 14:47:39, time: 1.460, data_time: 0.027, memory: 18626, loss_cls: 0.0971, loss_bbox: 0.2339, d0.loss_cls: 0.2039, d0.loss_bbox: 0.3682, d1.loss_cls: 0.1483, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1214, d2.loss_bbox: 0.2639, d3.loss_cls: 0.1077, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2368, loss: 2.3984, grad_norm: 606.4457
2025-06-19 20:05:09,645 - mmdet - INFO - Epoch [1][6150/7033]	lr: 2.000e-04, eta: 14:46:34, time: 1.507, data_time: 0.029, memory: 18626, loss_cls: 0.0922, loss_bbox: 0.2433, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1302, d1.loss_bbox: 0.2860, d2.loss_cls: 0.1084, d2.loss_bbox: 0.2742, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2585, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2471, loss: 2.3939, grad_norm: 28.6325
2025-06-19 20:06:22,894 - mmdet - INFO - Epoch [1][6200/7033]	lr: 2.000e-04, eta: 14:45:17, time: 1.465, data_time: 0.027, memory: 18626, loss_cls: 0.0861, loss_bbox: 0.2268, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3641, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2732, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2604, d3.loss_cls: 0.0968, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2325, loss: 2.3006, grad_norm: 30.2403
2025-06-19 20:07:35,719 - mmdet - INFO - Epoch [1][6250/7033]	lr: 2.000e-04, eta: 14:43:58, time: 1.456, data_time: 0.025, memory: 18626, loss_cls: 0.0956, loss_bbox: 0.2350, d0.loss_cls: 0.2028, d0.loss_bbox: 0.3787, d1.loss_cls: 0.1444, d1.loss_bbox: 0.2838, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2706, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2543, d4.loss_cls: 0.0956, d4.loss_bbox: 0.2415, loss: 2.4205, grad_norm: 43.5596
2025-06-19 20:08:49,109 - mmdet - INFO - Epoch [1][6300/7033]	lr: 2.000e-04, eta: 14:42:42, time: 1.468, data_time: 0.027, memory: 18626, loss_cls: 0.0926, loss_bbox: 0.2466, d0.loss_cls: 0.1951, d0.loss_bbox: 0.3947, d1.loss_cls: 0.1366, d1.loss_bbox: 0.2921, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2769, d3.loss_cls: 0.0982, d3.loss_bbox: 0.2607, d4.loss_cls: 0.0958, d4.loss_bbox: 0.2509, loss: 2.4514, grad_norm: 45.7139
2025-06-19 20:10:02,238 - mmdet - INFO - Epoch [1][6350/7033]	lr: 2.000e-04, eta: 14:41:25, time: 1.463, data_time: 0.027, memory: 18626, loss_cls: 0.1021, loss_bbox: 0.2401, d0.loss_cls: 0.2027, d0.loss_bbox: 0.3871, d1.loss_cls: 0.1417, d1.loss_bbox: 0.2937, d2.loss_cls: 0.1176, d2.loss_bbox: 0.2775, d3.loss_cls: 0.1094, d3.loss_bbox: 0.2567, d4.loss_cls: 0.1050, d4.loss_bbox: 0.2468, loss: 2.4805, grad_norm: 76.5164
2025-06-19 20:11:15,537 - mmdet - INFO - Epoch [1][6400/7033]	lr: 2.000e-04, eta: 14:40:08, time: 1.466, data_time: 0.029, memory: 18626, loss_cls: 0.1030, loss_bbox: 0.2437, d0.loss_cls: 0.2066, d0.loss_bbox: 0.3918, d1.loss_cls: 0.1449, d1.loss_bbox: 0.2874, d2.loss_cls: 0.1226, d2.loss_bbox: 0.2792, d3.loss_cls: 0.1151, d3.loss_bbox: 0.2580, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2481, loss: 2.5075, grad_norm: 32.7947
2025-06-19 20:12:28,401 - mmdet - INFO - Epoch [1][6450/7033]	lr: 2.000e-04, eta: 14:38:50, time: 1.457, data_time: 0.024, memory: 18626, loss_cls: 0.0928, loss_bbox: 0.2311, d0.loss_cls: 0.1939, d0.loss_bbox: 0.3817, d1.loss_cls: 0.1345, d1.loss_bbox: 0.2737, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2632, d3.loss_cls: 0.1019, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0959, d4.loss_bbox: 0.2351, loss: 2.3591, grad_norm: 29.9495
2025-06-19 20:13:42,870 - mmdet - INFO - Epoch [1][6500/7033]	lr: 2.000e-04, eta: 14:37:40, time: 1.489, data_time: 0.024, memory: 18626, loss_cls: 0.0995, loss_bbox: 0.2370, d0.loss_cls: 0.2010, d0.loss_bbox: 0.3764, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2808, d2.loss_cls: 0.1134, d2.loss_bbox: 0.2685, d3.loss_cls: 0.1059, d3.loss_bbox: 0.2491, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2389, loss: 2.4120, grad_norm: 32.3893
2025-06-19 20:14:55,554 - mmdet - INFO - Epoch [1][6550/7033]	lr: 2.000e-04, eta: 14:36:20, time: 1.454, data_time: 0.025, memory: 18626, loss_cls: 0.0975, loss_bbox: 0.2355, d0.loss_cls: 0.1975, d0.loss_bbox: 0.3807, d1.loss_cls: 0.1439, d1.loss_bbox: 0.2769, d2.loss_cls: 0.1108, d2.loss_bbox: 0.2662, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2482, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2408, loss: 2.4002, grad_norm: 390.3688
2025-06-19 20:16:08,332 - mmdet - INFO - Epoch [1][6600/7033]	lr: 2.000e-04, eta: 14:35:01, time: 1.456, data_time: 0.026, memory: 18626, loss_cls: 0.0881, loss_bbox: 0.2282, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3717, d1.loss_cls: 0.1301, d1.loss_bbox: 0.2693, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2601, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0894, d4.loss_bbox: 0.2304, loss: 2.2995, grad_norm: 30.5591
2025-06-19 20:17:21,275 - mmdet - INFO - Epoch [1][6650/7033]	lr: 2.000e-04, eta: 14:33:43, time: 1.459, data_time: 0.025, memory: 18626, loss_cls: 0.0939, loss_bbox: 0.2391, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3843, d1.loss_cls: 0.1383, d1.loss_bbox: 0.2850, d2.loss_cls: 0.1091, d2.loss_bbox: 0.2736, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0950, d4.loss_bbox: 0.2424, loss: 2.4090, grad_norm: 30.9861
2025-06-19 20:18:34,129 - mmdet - INFO - Epoch [1][6700/7033]	lr: 2.000e-04, eta: 14:32:25, time: 1.457, data_time: 0.027, memory: 18626, loss_cls: 0.0890, loss_bbox: 0.2279, d0.loss_cls: 0.1970, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2720, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2590, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2409, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2310, loss: 2.3012, grad_norm: 34.3051
2025-06-19 20:19:47,025 - mmdet - INFO - Epoch [1][6750/7033]	lr: 2.000e-04, eta: 14:31:07, time: 1.458, data_time: 0.025, memory: 18626, loss_cls: 0.0850, loss_bbox: 0.2419, d0.loss_cls: 0.1950, d0.loss_bbox: 0.3816, d1.loss_cls: 0.1258, d1.loss_bbox: 0.2841, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2732, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2546, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2460, loss: 2.3713, grad_norm: 27.5518
2025-06-19 20:20:59,971 - mmdet - INFO - Epoch [1][6800/7033]	lr: 2.000e-04, eta: 14:29:49, time: 1.459, data_time: 0.027, memory: 18626, loss_cls: 0.0969, loss_bbox: 0.2363, d0.loss_cls: 0.2055, d0.loss_bbox: 0.3831, d1.loss_cls: 0.1401, d1.loss_bbox: 0.2827, d2.loss_cls: 0.1117, d2.loss_bbox: 0.2703, d3.loss_cls: 0.1023, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0973, d4.loss_bbox: 0.2418, loss: 2.4178, grad_norm: 27.1827
2025-06-19 20:22:14,429 - mmdet - INFO - Epoch [1][6850/7033]	lr: 2.000e-04, eta: 14:28:39, time: 1.489, data_time: 0.028, memory: 18626, loss_cls: 0.0897, loss_bbox: 0.2313, d0.loss_cls: 0.1992, d0.loss_bbox: 0.3671, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2758, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2649, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2452, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2370, loss: 2.3426, grad_norm: 38.7908
2025-06-19 20:23:27,281 - mmdet - INFO - Epoch [1][6900/7033]	lr: 2.000e-04, eta: 14:27:21, time: 1.457, data_time: 0.026, memory: 18626, loss_cls: 0.0951, loss_bbox: 0.2396, d0.loss_cls: 0.2024, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1352, d1.loss_bbox: 0.2920, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2789, d3.loss_cls: 0.1049, d3.loss_bbox: 0.2550, d4.loss_cls: 0.0981, d4.loss_bbox: 0.2455, loss: 2.4456, grad_norm: 293.5668
2025-06-19 20:24:40,294 - mmdet - INFO - Epoch [1][6950/7033]	lr: 2.000e-04, eta: 14:26:04, time: 1.460, data_time: 0.027, memory: 18626, loss_cls: 0.0891, loss_bbox: 0.2346, d0.loss_cls: 0.2016, d0.loss_bbox: 0.3868, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2840, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2680, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2501, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2393, loss: 2.3867, grad_norm: 49.3374
2025-06-19 20:25:53,339 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 20:25:53,339 - mmdet - INFO - Epoch [1][7000/7033]	lr: 2.000e-04, eta: 14:24:47, time: 1.461, data_time: 0.027, memory: 18626, loss_cls: 0.0939, loss_bbox: 0.2374, d0.loss_cls: 0.1992, d0.loss_bbox: 0.3878, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2897, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2769, d3.loss_cls: 0.1049, d3.loss_bbox: 0.2523, d4.loss_cls: 0.0976, d4.loss_bbox: 0.2426, loss: 2.4405, grad_norm: 60.8921
2025-06-19 20:26:41,516 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-19 20:49:52,191 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-19 20:49:52,191 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7830, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8776, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9061, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9207, pts_bbox_NuScenes/car_trans_err: 0.1883, pts_bbox_NuScenes/car_scale_err: 0.1551, pts_bbox_NuScenes/car_orient_err: 0.0510, pts_bbox_NuScenes/car_vel_err: 0.3345, pts_bbox_NuScenes/car_attr_err: 0.1744, pts_bbox_NuScenes/mATE: 0.3055, pts_bbox_NuScenes/mASE: 0.2647, pts_bbox_NuScenes/mAOE: 0.2592, pts_bbox_NuScenes/mAVE: 0.3295, pts_bbox_NuScenes/mAAE: 0.1765, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4025, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6000, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7049, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7444, pts_bbox_NuScenes/truck_trans_err: 0.3508, pts_bbox_NuScenes/truck_scale_err: 0.1917, pts_bbox_NuScenes/truck_orient_err: 0.0679, pts_bbox_NuScenes/truck_vel_err: 0.2906, pts_bbox_NuScenes/truck_attr_err: 0.2107, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0641, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2043, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4287, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4975, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6733, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4419, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8076, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1105, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2898, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4649, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7158, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8925, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9174, pts_bbox_NuScenes/bus_trans_err: 0.3935, pts_bbox_NuScenes/bus_scale_err: 0.2021, pts_bbox_NuScenes/bus_orient_err: 0.0389, pts_bbox_NuScenes/bus_vel_err: 0.5744, pts_bbox_NuScenes/bus_attr_err: 0.2283, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1773, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4319, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5938, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6973, pts_bbox_NuScenes/trailer_trans_err: 0.4921, pts_bbox_NuScenes/trailer_scale_err: 0.2155, pts_bbox_NuScenes/trailer_orient_err: 0.4149, pts_bbox_NuScenes/trailer_vel_err: 0.2550, pts_bbox_NuScenes/trailer_attr_err: 0.1580, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5747, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6732, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7224, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7394, pts_bbox_NuScenes/barrier_trans_err: 0.2289, pts_bbox_NuScenes/barrier_scale_err: 0.2910, pts_bbox_NuScenes/barrier_orient_err: 0.0523, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6328, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7756, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8086, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8147, pts_bbox_NuScenes/motorcycle_trans_err: 0.2291, pts_bbox_NuScenes/motorcycle_scale_err: 0.2513, pts_bbox_NuScenes/motorcycle_orient_err: 0.2296, pts_bbox_NuScenes/motorcycle_vel_err: 0.5877, pts_bbox_NuScenes/motorcycle_attr_err: 0.2292, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5506, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6089, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6219, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6292, pts_bbox_NuScenes/bicycle_trans_err: 0.1860, pts_bbox_NuScenes/bicycle_scale_err: 0.2647, pts_bbox_NuScenes/bicycle_orient_err: 0.3452, pts_bbox_NuScenes/bicycle_vel_err: 0.2545, pts_bbox_NuScenes/bicycle_attr_err: 0.0138, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7998, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8533, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8779, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8918, pts_bbox_NuScenes/pedestrian_trans_err: 0.1615, pts_bbox_NuScenes/pedestrian_scale_err: 0.2990, pts_bbox_NuScenes/pedestrian_orient_err: 0.3258, pts_bbox_NuScenes/pedestrian_vel_err: 0.2287, pts_bbox_NuScenes/pedestrian_attr_err: 0.1081, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7191, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7626, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7939, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8204, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1515, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3342, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7002, pts_bbox_NuScenes/mAP: 0.6674
2025-06-19 20:52:01,687 - mmdet - INFO - Epoch [2][50/7033]	lr: 1.866e-04, eta: 14:19:57, time: 1.774, data_time: 0.338, memory: 20458, loss_cls: 0.0966, loss_bbox: 0.2401, d0.loss_cls: 0.2004, d0.loss_bbox: 0.3836, d1.loss_cls: 0.1436, d1.loss_bbox: 0.2903, d2.loss_cls: 0.1187, d2.loss_bbox: 0.2777, d3.loss_cls: 0.1065, d3.loss_bbox: 0.2553, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2465, loss: 2.4591, grad_norm: 203.6125
2025-06-19 20:53:14,930 - mmdet - INFO - Epoch [2][100/7033]	lr: 1.866e-04, eta: 14:18:43, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.1084, loss_bbox: 0.2512, d0.loss_cls: 0.2167, d0.loss_bbox: 0.4019, d1.loss_cls: 0.1522, d1.loss_bbox: 0.3034, d2.loss_cls: 0.1288, d2.loss_bbox: 0.2900, d3.loss_cls: 0.1185, d3.loss_bbox: 0.2669, d4.loss_cls: 0.1095, d4.loss_bbox: 0.2571, loss: 2.6047, grad_norm: 27.2268
2025-06-19 20:54:27,905 - mmdet - INFO - Epoch [2][150/7033]	lr: 1.866e-04, eta: 14:17:27, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0827, loss_bbox: 0.2348, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3687, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2793, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2652, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2468, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2395, loss: 2.3064, grad_norm: 28.6945
2025-06-19 20:55:40,999 - mmdet - INFO - Epoch [2][200/7033]	lr: 1.866e-04, eta: 14:16:12, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0941, loss_bbox: 0.2429, d0.loss_cls: 0.1947, d0.loss_bbox: 0.3911, d1.loss_cls: 0.1313, d1.loss_bbox: 0.2929, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2794, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2589, d4.loss_cls: 0.0943, d4.loss_bbox: 0.2482, loss: 2.4432, grad_norm: 29.7545
2025-06-19 20:56:53,762 - mmdet - INFO - Epoch [2][250/7033]	lr: 1.866e-04, eta: 14:14:55, time: 1.455, data_time: 0.026, memory: 20458, loss_cls: 0.0918, loss_bbox: 0.2299, d0.loss_cls: 0.1944, d0.loss_bbox: 0.3570, d1.loss_cls: 0.1291, d1.loss_bbox: 0.2711, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2622, d3.loss_cls: 0.1027, d3.loss_bbox: 0.2428, d4.loss_cls: 0.0953, d4.loss_bbox: 0.2353, loss: 2.3265, grad_norm: 41.3957
2025-06-19 20:58:06,890 - mmdet - INFO - Epoch [2][300/7033]	lr: 1.866e-04, eta: 14:13:40, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0881, loss_bbox: 0.2296, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2725, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2626, d3.loss_cls: 0.0977, d3.loss_bbox: 0.2432, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2346, loss: 2.3128, grad_norm: 43.6975
2025-06-19 20:59:19,944 - mmdet - INFO - Epoch [2][350/7033]	lr: 1.866e-04, eta: 14:12:24, time: 1.461, data_time: 0.025, memory: 20458, loss_cls: 0.0970, loss_bbox: 0.2370, d0.loss_cls: 0.1997, d0.loss_bbox: 0.3851, d1.loss_cls: 0.1405, d1.loss_bbox: 0.2870, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2690, d3.loss_cls: 0.1028, d3.loss_bbox: 0.2508, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2406, loss: 2.4238, grad_norm: 42.9949
2025-06-19 21:00:32,877 - mmdet - INFO - Epoch [2][400/7033]	lr: 1.866e-04, eta: 14:11:08, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0844, loss_bbox: 0.2249, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3609, d1.loss_cls: 0.1354, d1.loss_bbox: 0.2800, d2.loss_cls: 0.1118, d2.loss_bbox: 0.2613, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0897, d4.loss_bbox: 0.2289, loss: 2.3042, grad_norm: 58.3063
2025-06-19 21:01:45,930 - mmdet - INFO - Epoch [2][450/7033]	lr: 1.866e-04, eta: 14:09:53, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0933, loss_bbox: 0.2420, d0.loss_cls: 0.1991, d0.loss_bbox: 0.3833, d1.loss_cls: 0.1347, d1.loss_bbox: 0.2964, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2785, d3.loss_cls: 0.1026, d3.loss_bbox: 0.2552, d4.loss_cls: 0.0977, d4.loss_bbox: 0.2466, loss: 2.4442, grad_norm: 38.1337
2025-06-19 21:02:59,531 - mmdet - INFO - Epoch [2][500/7033]	lr: 1.866e-04, eta: 14:08:40, time: 1.472, data_time: 0.035, memory: 20458, loss_cls: 0.0867, loss_bbox: 0.2261, d0.loss_cls: 0.2001, d0.loss_bbox: 0.3680, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2761, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2565, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2389, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2311, loss: 2.3081, grad_norm: 265.0958
2025-06-19 21:04:12,418 - mmdet - INFO - Epoch [2][550/7033]	lr: 1.866e-04, eta: 14:07:24, time: 1.458, data_time: 0.027, memory: 20458, loss_cls: 0.0909, loss_bbox: 0.2389, d0.loss_cls: 0.2032, d0.loss_bbox: 0.3800, d1.loss_cls: 0.1473, d1.loss_bbox: 0.3005, d2.loss_cls: 0.1171, d2.loss_bbox: 0.2742, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0942, d4.loss_bbox: 0.2443, loss: 2.4431, grad_norm: 27.5251
2025-06-19 21:05:25,826 - mmdet - INFO - Epoch [2][600/7033]	lr: 1.866e-04, eta: 14:06:11, time: 1.468, data_time: 0.033, memory: 20458, loss_cls: 0.0880, loss_bbox: 0.2401, d0.loss_cls: 0.1920, d0.loss_bbox: 0.3788, d1.loss_cls: 0.1423, d1.loss_bbox: 0.2895, d2.loss_cls: 0.1122, d2.loss_bbox: 0.2736, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2445, loss: 2.4022, grad_norm: 62.2521
2025-06-19 21:06:38,952 - mmdet - INFO - Epoch [2][650/7033]	lr: 1.866e-04, eta: 14:04:56, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0930, loss_bbox: 0.2419, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3834, d1.loss_cls: 0.1358, d1.loss_bbox: 0.2954, d2.loss_cls: 0.1164, d2.loss_bbox: 0.2794, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2589, d4.loss_cls: 0.0943, d4.loss_bbox: 0.2508, loss: 2.4456, grad_norm: 34.6728
2025-06-19 21:07:51,955 - mmdet - INFO - Epoch [2][700/7033]	lr: 1.866e-04, eta: 14:03:41, time: 1.460, data_time: 0.025, memory: 20458, loss_cls: 0.0963, loss_bbox: 0.2479, d0.loss_cls: 0.2032, d0.loss_bbox: 0.3840, d1.loss_cls: 0.1383, d1.loss_bbox: 0.3026, d2.loss_cls: 0.1169, d2.loss_bbox: 0.2817, d3.loss_cls: 0.1046, d3.loss_bbox: 0.2601, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2548, loss: 2.4897, grad_norm: 50.5131
2025-06-19 21:09:04,720 - mmdet - INFO - Epoch [2][750/7033]	lr: 1.866e-04, eta: 14:02:24, time: 1.455, data_time: 0.025, memory: 20458, loss_cls: 0.0914, loss_bbox: 0.2334, d0.loss_cls: 0.1979, d0.loss_bbox: 0.3691, d1.loss_cls: 0.1365, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2675, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2474, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2375, loss: 2.3490, grad_norm: 35.0290
2025-06-19 21:10:17,717 - mmdet - INFO - Epoch [2][800/7033]	lr: 1.866e-04, eta: 14:01:09, time: 1.460, data_time: 0.026, memory: 20458, loss_cls: 0.0973, loss_bbox: 0.2312, d0.loss_cls: 0.1987, d0.loss_bbox: 0.3646, d1.loss_cls: 0.1354, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1111, d2.loss_bbox: 0.2641, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2426, d4.loss_cls: 0.0992, d4.loss_bbox: 0.2348, loss: 2.3561, grad_norm: 33.6729
2025-06-19 21:11:30,608 - mmdet - INFO - Epoch [2][850/7033]	lr: 1.866e-04, eta: 13:59:53, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0910, loss_bbox: 0.2296, d0.loss_cls: 0.1991, d0.loss_bbox: 0.3623, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2632, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2430, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2356, loss: 2.3327, grad_norm: 50.8412
2025-06-19 21:12:43,539 - mmdet - INFO - Epoch [2][900/7033]	lr: 1.866e-04, eta: 13:58:37, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0867, loss_bbox: 0.2345, d0.loss_cls: 0.1908, d0.loss_bbox: 0.3660, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2783, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2639, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2488, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2408, loss: 2.3179, grad_norm: 181.8678
2025-06-19 21:13:56,580 - mmdet - INFO - Epoch [2][950/7033]	lr: 1.866e-04, eta: 13:57:22, time: 1.461, data_time: 0.025, memory: 20458, loss_cls: 0.0837, loss_bbox: 0.2222, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3539, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2539, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2255, loss: 2.2323, grad_norm: 59.5875
2025-06-19 21:15:09,608 - mmdet - INFO - Epoch [2][1000/7033]	lr: 1.866e-04, eta: 13:56:07, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0832, loss_bbox: 0.2273, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2710, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2578, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2405, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2318, loss: 2.2563, grad_norm: 29.4289
2025-06-19 21:16:22,645 - mmdet - INFO - Epoch [2][1050/7033]	lr: 1.866e-04, eta: 13:54:52, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0828, loss_bbox: 0.2367, d0.loss_cls: 0.1905, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1237, d1.loss_bbox: 0.2782, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2667, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2404, loss: 2.3099, grad_norm: 20.7815
2025-06-19 21:17:35,651 - mmdet - INFO - Epoch [2][1100/7033]	lr: 1.866e-04, eta: 13:53:37, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0897, loss_bbox: 0.2293, d0.loss_cls: 0.1994, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1301, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2605, d3.loss_cls: 0.0966, d3.loss_bbox: 0.2397, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2318, loss: 2.3051, grad_norm: 47.8504
2025-06-19 21:18:48,567 - mmdet - INFO - Epoch [2][1150/7033]	lr: 1.866e-04, eta: 13:52:22, time: 1.458, data_time: 0.028, memory: 20458, loss_cls: 0.0859, loss_bbox: 0.2211, d0.loss_cls: 0.1836, d0.loss_bbox: 0.3585, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2678, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2542, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2271, loss: 2.2321, grad_norm: 45.6773
2025-06-19 21:20:01,256 - mmdet - INFO - Epoch [2][1200/7033]	lr: 1.866e-04, eta: 13:51:05, time: 1.454, data_time: 0.026, memory: 20458, loss_cls: 0.0873, loss_bbox: 0.2251, d0.loss_cls: 0.1923, d0.loss_bbox: 0.3659, d1.loss_cls: 0.1309, d1.loss_bbox: 0.2726, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2571, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2378, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2320, loss: 2.2875, grad_norm: 30.1105
2025-06-19 21:21:14,565 - mmdet - INFO - Epoch [2][1250/7033]	lr: 1.866e-04, eta: 13:49:51, time: 1.466, data_time: 0.026, memory: 20458, loss_cls: 0.0814, loss_bbox: 0.2340, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3669, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2796, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2676, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2457, d4.loss_cls: 0.0841, d4.loss_bbox: 0.2386, loss: 2.2900, grad_norm: 25.8104
2025-06-19 21:22:30,081 - mmdet - INFO - Epoch [2][1300/7033]	lr: 1.866e-04, eta: 13:48:47, time: 1.510, data_time: 0.029, memory: 20458, loss_cls: 0.0860, loss_bbox: 0.2259, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3554, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2641, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2555, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2290, loss: 2.2469, grad_norm: 29.3358
2025-06-19 21:23:43,491 - mmdet - INFO - Epoch [2][1350/7033]	lr: 1.866e-04, eta: 13:47:33, time: 1.468, data_time: 0.035, memory: 20458, loss_cls: 0.0876, loss_bbox: 0.2263, d0.loss_cls: 0.1958, d0.loss_bbox: 0.3605, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2696, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2620, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0918, d4.loss_bbox: 0.2313, loss: 2.3032, grad_norm: 34.1456
2025-06-19 21:24:56,353 - mmdet - INFO - Epoch [2][1400/7033]	lr: 1.866e-04, eta: 13:46:17, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0907, loss_bbox: 0.2242, d0.loss_cls: 0.2022, d0.loss_bbox: 0.3631, d1.loss_cls: 0.1287, d1.loss_bbox: 0.2694, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2547, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0928, d4.loss_bbox: 0.2292, loss: 2.2926, grad_norm: 41.9727
2025-06-19 21:26:09,307 - mmdet - INFO - Epoch [2][1450/7033]	lr: 1.866e-04, eta: 13:45:02, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0899, loss_bbox: 0.2201, d0.loss_cls: 0.1903, d0.loss_bbox: 0.3627, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2694, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2553, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2332, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2249, loss: 2.2708, grad_norm: 25.4211
2025-06-19 21:27:22,536 - mmdet - INFO - Epoch [2][1500/7033]	lr: 1.866e-04, eta: 13:43:48, time: 1.465, data_time: 0.026, memory: 20458, loss_cls: 0.0871, loss_bbox: 0.2248, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3641, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2658, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2582, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2296, loss: 2.2490, grad_norm: 26.3871
2025-06-19 21:28:35,333 - mmdet - INFO - Epoch [2][1550/7033]	lr: 1.866e-04, eta: 13:42:32, time: 1.456, data_time: 0.028, memory: 20458, loss_cls: 0.0891, loss_bbox: 0.2281, d0.loss_cls: 0.1939, d0.loss_bbox: 0.3590, d1.loss_cls: 0.1304, d1.loss_bbox: 0.2705, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2633, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2400, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2327, loss: 2.2893, grad_norm: 33.6452
2025-06-19 21:29:54,635 - mmdet - INFO - Epoch [2][1600/7033]	lr: 1.866e-04, eta: 13:41:42, time: 1.586, data_time: 0.029, memory: 20458, loss_cls: 0.0875, loss_bbox: 0.2211, d0.loss_cls: 0.1945, d0.loss_bbox: 0.3541, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2633, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2532, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2248, loss: 2.2398, grad_norm: 27.4904
2025-06-19 21:31:07,551 - mmdet - INFO - Epoch [2][1650/7033]	lr: 1.866e-04, eta: 13:40:26, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0926, loss_bbox: 0.2287, d0.loss_cls: 0.1927, d0.loss_bbox: 0.3679, d1.loss_cls: 0.1288, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1057, d2.loss_bbox: 0.2626, d3.loss_cls: 0.0967, d3.loss_bbox: 0.2456, d4.loss_cls: 0.0941, d4.loss_bbox: 0.2344, loss: 2.3240, grad_norm: 111.5286
2025-06-19 21:32:20,482 - mmdet - INFO - Epoch [2][1700/7033]	lr: 1.866e-04, eta: 13:39:11, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0865, loss_bbox: 0.2263, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3675, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2577, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2295, loss: 2.2761, grad_norm: 31.9188
2025-06-19 21:33:33,439 - mmdet - INFO - Epoch [2][1750/7033]	lr: 1.866e-04, eta: 13:37:56, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0904, loss_bbox: 0.2266, d0.loss_cls: 0.1913, d0.loss_bbox: 0.3551, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2698, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2537, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2379, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2307, loss: 2.2741, grad_norm: 28.9916
2025-06-19 21:34:46,280 - mmdet - INFO - Epoch [2][1800/7033]	lr: 1.866e-04, eta: 13:36:40, time: 1.457, data_time: 0.026, memory: 20458, loss_cls: 0.0885, loss_bbox: 0.2212, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3659, d1.loss_cls: 0.1302, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2573, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2248, loss: 2.2582, grad_norm: 41.2034
2025-06-19 21:35:59,139 - mmdet - INFO - Epoch [2][1850/7033]	lr: 1.866e-04, eta: 13:35:24, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0874, loss_bbox: 0.2296, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3646, d1.loss_cls: 0.1361, d1.loss_bbox: 0.2693, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2583, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0894, d4.loss_bbox: 0.2325, loss: 2.2959, grad_norm: 36.9068
2025-06-19 21:37:12,217 - mmdet - INFO - Epoch [2][1900/7033]	lr: 1.866e-04, eta: 13:34:10, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0920, loss_bbox: 0.2243, d0.loss_cls: 0.1920, d0.loss_bbox: 0.3648, d1.loss_cls: 0.1359, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2560, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0943, d4.loss_bbox: 0.2273, loss: 2.2985, grad_norm: 51.9261
2025-06-19 21:38:24,978 - mmdet - INFO - Epoch [2][1950/7033]	lr: 1.866e-04, eta: 13:32:54, time: 1.455, data_time: 0.026, memory: 20458, loss_cls: 0.0881, loss_bbox: 0.2227, d0.loss_cls: 0.1947, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2649, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2524, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2275, loss: 2.2586, grad_norm: 62.0093
2025-06-19 21:39:38,160 - mmdet - INFO - Epoch [2][2000/7033]	lr: 1.866e-04, eta: 13:31:39, time: 1.464, data_time: 0.025, memory: 20458, loss_cls: 0.0830, loss_bbox: 0.2234, d0.loss_cls: 0.1978, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1283, d1.loss_bbox: 0.2643, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2512, d3.loss_cls: 0.0910, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2252, loss: 2.2423, grad_norm: 45.6232
2025-06-19 21:40:51,578 - mmdet - INFO - Epoch [2][2050/7033]	lr: 1.866e-04, eta: 13:30:26, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0829, loss_bbox: 0.2301, d0.loss_cls: 0.1906, d0.loss_bbox: 0.3676, d1.loss_cls: 0.1226, d1.loss_bbox: 0.2683, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2595, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2345, loss: 2.2719, grad_norm: 34.2290
2025-06-19 21:42:04,568 - mmdet - INFO - Epoch [2][2100/7033]	lr: 1.866e-04, eta: 13:29:11, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0834, loss_bbox: 0.2212, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3469, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2595, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2463, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2237, loss: 2.1940, grad_norm: 42.4626
2025-06-19 21:43:17,505 - mmdet - INFO - Epoch [2][2150/7033]	lr: 1.866e-04, eta: 13:27:56, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0898, loss_bbox: 0.2291, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3584, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2599, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2430, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2332, loss: 2.2941, grad_norm: 38.5266
2025-06-19 21:44:31,988 - mmdet - INFO - Epoch [2][2200/7033]	lr: 1.866e-04, eta: 13:26:46, time: 1.490, data_time: 0.027, memory: 20458, loss_cls: 0.0863, loss_bbox: 0.2270, d0.loss_cls: 0.1929, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2673, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2378, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2274, loss: 2.2583, grad_norm: 28.6800
2025-06-19 21:45:44,789 - mmdet - INFO - Epoch [2][2250/7033]	lr: 1.866e-04, eta: 13:25:31, time: 1.456, data_time: 0.028, memory: 20458, loss_cls: 0.0872, loss_bbox: 0.2326, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3707, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2767, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2658, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2450, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2355, loss: 2.3071, grad_norm: 52.2453
2025-06-19 21:46:57,439 - mmdet - INFO - Epoch [2][2300/7033]	lr: 1.866e-04, eta: 13:24:15, time: 1.453, data_time: 0.027, memory: 20458, loss_cls: 0.0905, loss_bbox: 0.2320, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3671, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1094, d2.loss_bbox: 0.2631, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2439, d4.loss_cls: 0.0929, d4.loss_bbox: 0.2366, loss: 2.3300, grad_norm: 67.9673
2025-06-19 21:48:10,038 - mmdet - INFO - Epoch [2][2350/7033]	lr: 1.866e-04, eta: 13:22:58, time: 1.452, data_time: 0.025, memory: 20458, loss_cls: 0.0906, loss_bbox: 0.2262, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2670, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2567, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2384, d4.loss_cls: 0.0942, d4.loss_bbox: 0.2289, loss: 2.2845, grad_norm: 46.5270
2025-06-19 21:49:22,922 - mmdet - INFO - Epoch [2][2400/7033]	lr: 1.866e-04, eta: 13:21:43, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0919, loss_bbox: 0.2310, d0.loss_cls: 0.2019, d0.loss_bbox: 0.3664, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2828, d2.loss_cls: 0.1107, d2.loss_bbox: 0.2680, d3.loss_cls: 0.0974, d3.loss_bbox: 0.2433, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2360, loss: 2.3617, grad_norm: 36.8705
2025-06-19 21:50:37,592 - mmdet - INFO - Epoch [2][2450/7033]	lr: 1.866e-04, eta: 13:20:34, time: 1.493, data_time: 0.028, memory: 20458, loss_cls: 0.0898, loss_bbox: 0.2217, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2707, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2572, d3.loss_cls: 0.0964, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0921, d4.loss_bbox: 0.2251, loss: 2.2770, grad_norm: 45.9449
2025-06-19 21:51:50,590 - mmdet - INFO - Epoch [2][2500/7033]	lr: 1.866e-04, eta: 13:19:19, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0889, loss_bbox: 0.2295, d0.loss_cls: 0.1884, d0.loss_bbox: 0.3603, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2742, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2587, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2385, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2312, loss: 2.2885, grad_norm: 40.0555
2025-06-19 21:53:03,783 - mmdet - INFO - Epoch [2][2550/7033]	lr: 1.866e-04, eta: 13:18:05, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0912, loss_bbox: 0.2315, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1306, d1.loss_bbox: 0.2763, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2635, d3.loss_cls: 0.0981, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2371, loss: 2.3311, grad_norm: 30.6008
2025-06-19 21:54:17,034 - mmdet - INFO - Epoch [2][2600/7033]	lr: 1.866e-04, eta: 13:16:51, time: 1.465, data_time: 0.026, memory: 20458, loss_cls: 0.1002, loss_bbox: 0.2383, d0.loss_cls: 0.1965, d0.loss_bbox: 0.3815, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2827, d2.loss_cls: 0.1142, d2.loss_bbox: 0.2720, d3.loss_cls: 0.1064, d3.loss_bbox: 0.2506, d4.loss_cls: 0.1027, d4.loss_bbox: 0.2432, loss: 2.4272, grad_norm: 28.0165
2025-06-19 21:55:29,828 - mmdet - INFO - Epoch [2][2650/7033]	lr: 1.866e-04, eta: 13:15:36, time: 1.456, data_time: 0.025, memory: 20458, loss_cls: 0.0887, loss_bbox: 0.2316, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3588, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2604, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0905, d4.loss_bbox: 0.2349, loss: 2.2853, grad_norm: 40.2615
2025-06-19 21:56:42,645 - mmdet - INFO - Epoch [2][2700/7033]	lr: 1.866e-04, eta: 13:14:20, time: 1.456, data_time: 0.028, memory: 20458, loss_cls: 0.0949, loss_bbox: 0.2335, d0.loss_cls: 0.1923, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2705, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2585, d3.loss_cls: 0.0980, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0941, d4.loss_bbox: 0.2383, loss: 2.3143, grad_norm: 550.4007
2025-06-19 21:57:55,651 - mmdet - INFO - Epoch [2][2750/7033]	lr: 1.866e-04, eta: 13:13:05, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0863, loss_bbox: 0.2267, d0.loss_cls: 0.1913, d0.loss_bbox: 0.3609, d1.loss_cls: 0.1268, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2557, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2383, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2318, loss: 2.2678, grad_norm: 48.6866
2025-06-19 21:59:08,465 - mmdet - INFO - Epoch [2][2800/7033]	lr: 1.866e-04, eta: 13:11:50, time: 1.456, data_time: 0.025, memory: 20458, loss_cls: 0.0881, loss_bbox: 0.2285, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3645, d1.loss_cls: 0.1264, d1.loss_bbox: 0.2722, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2607, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2438, d4.loss_cls: 0.0918, d4.loss_bbox: 0.2339, loss: 2.2969, grad_norm: 33.2469
2025-06-19 22:00:21,546 - mmdet - INFO - Epoch [2][2850/7033]	lr: 1.866e-04, eta: 13:10:36, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0931, loss_bbox: 0.2339, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1297, d1.loss_bbox: 0.2742, d2.loss_cls: 0.1106, d2.loss_bbox: 0.2640, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2467, d4.loss_cls: 0.0987, d4.loss_bbox: 0.2368, loss: 2.3487, grad_norm: 40.7799
2025-06-19 22:01:34,251 - mmdet - INFO - Epoch [2][2900/7033]	lr: 1.866e-04, eta: 13:09:20, time: 1.454, data_time: 0.026, memory: 20458, loss_cls: 0.0893, loss_bbox: 0.2321, d0.loss_cls: 0.1843, d0.loss_bbox: 0.3613, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2561, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2366, loss: 2.2788, grad_norm: 48.2891
2025-06-19 22:02:47,123 - mmdet - INFO - Epoch [2][2950/7033]	lr: 1.866e-04, eta: 13:08:05, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0855, loss_bbox: 0.2153, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2568, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2207, loss: 2.1777, grad_norm: 27.2918
2025-06-19 22:03:59,983 - mmdet - INFO - Epoch [2][3000/7033]	lr: 1.866e-04, eta: 13:06:50, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0771, loss_bbox: 0.2163, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2276, d4.loss_cls: 0.0818, d4.loss_bbox: 0.2180, loss: 2.1466, grad_norm: 31.3994
2025-06-19 22:05:14,458 - mmdet - INFO - Epoch [2][3050/7033]	lr: 1.866e-04, eta: 13:05:40, time: 1.490, data_time: 0.026, memory: 20458, loss_cls: 0.0893, loss_bbox: 0.2264, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2709, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2580, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2414, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2309, loss: 2.2879, grad_norm: 39.1085
2025-06-19 22:06:27,669 - mmdet - INFO - Epoch [2][3100/7033]	lr: 1.866e-04, eta: 13:04:26, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0905, loss_bbox: 0.2333, d0.loss_cls: 0.1979, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1308, d1.loss_bbox: 0.2760, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2653, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2380, loss: 2.3425, grad_norm: 25.1520
2025-06-19 22:07:40,511 - mmdet - INFO - Epoch [2][3150/7033]	lr: 1.866e-04, eta: 13:03:11, time: 1.457, data_time: 0.027, memory: 20458, loss_cls: 0.0906, loss_bbox: 0.2243, d0.loss_cls: 0.1924, d0.loss_bbox: 0.3616, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2692, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2541, d3.loss_cls: 0.0952, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2288, loss: 2.2804, grad_norm: 27.6711
2025-06-19 22:08:53,426 - mmdet - INFO - Epoch [2][3200/7033]	lr: 1.866e-04, eta: 13:01:56, time: 1.458, data_time: 0.028, memory: 20458, loss_cls: 0.0983, loss_bbox: 0.2420, d0.loss_cls: 0.2028, d0.loss_bbox: 0.3628, d1.loss_cls: 0.1378, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2682, d3.loss_cls: 0.1086, d3.loss_bbox: 0.2508, d4.loss_cls: 0.1035, d4.loss_bbox: 0.2450, loss: 2.4150, grad_norm: 121.9759
2025-06-19 22:10:06,286 - mmdet - INFO - Epoch [2][3250/7033]	lr: 1.866e-04, eta: 13:00:41, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0880, loss_bbox: 0.2370, d0.loss_cls: 0.1967, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1392, d1.loss_bbox: 0.2683, d2.loss_cls: 0.1109, d2.loss_bbox: 0.2577, d3.loss_cls: 0.1006, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2355, loss: 2.3252, grad_norm: 26.2625
2025-06-19 22:11:19,224 - mmdet - INFO - Epoch [2][3300/7033]	lr: 1.866e-04, eta: 12:59:26, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0836, loss_bbox: 0.2232, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1395, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2524, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2267, loss: 2.2680, grad_norm: 54.0193
2025-06-19 22:12:32,149 - mmdet - INFO - Epoch [2][3350/7033]	lr: 1.866e-04, eta: 12:58:11, time: 1.458, data_time: 0.026, memory: 20458, loss_cls: 0.0774, loss_bbox: 0.2209, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1265, d1.loss_bbox: 0.2617, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2493, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2256, loss: 2.2043, grad_norm: 25.8558
2025-06-19 22:13:45,504 - mmdet - INFO - Epoch [2][3400/7033]	lr: 1.866e-04, eta: 12:56:58, time: 1.467, data_time: 0.028, memory: 20458, loss_cls: 0.0887, loss_bbox: 0.2289, d0.loss_cls: 0.1986, d0.loss_bbox: 0.3653, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2781, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2597, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2392, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2325, loss: 2.3089, grad_norm: 36.8601
2025-06-19 22:14:59,781 - mmdet - INFO - Epoch [2][3450/7033]	lr: 1.866e-04, eta: 12:55:47, time: 1.486, data_time: 0.030, memory: 20458, loss_cls: 0.0882, loss_bbox: 0.2252, d0.loss_cls: 0.1941, d0.loss_bbox: 0.3541, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2676, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2557, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2383, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2299, loss: 2.2700, grad_norm: 60.2694
2025-06-19 22:16:12,531 - mmdet - INFO - Epoch [2][3500/7033]	lr: 1.866e-04, eta: 12:54:32, time: 1.455, data_time: 0.026, memory: 20458, loss_cls: 0.0858, loss_bbox: 0.2284, d0.loss_cls: 0.1939, d0.loss_bbox: 0.3571, d1.loss_cls: 0.1281, d1.loss_bbox: 0.2661, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2550, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2420, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2324, loss: 2.2786, grad_norm: 23.7839
2025-06-19 22:17:25,459 - mmdet - INFO - Epoch [2][3550/7033]	lr: 1.866e-04, eta: 12:53:17, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0902, loss_bbox: 0.2272, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3518, d1.loss_cls: 0.1260, d1.loss_bbox: 0.2652, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2320, loss: 2.2728, grad_norm: 151.7402
2025-06-19 22:18:38,214 - mmdet - INFO - Epoch [2][3600/7033]	lr: 1.866e-04, eta: 12:52:02, time: 1.455, data_time: 0.025, memory: 20458, loss_cls: 0.0865, loss_bbox: 0.2310, d0.loss_cls: 0.1903, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2379, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2305, loss: 2.2496, grad_norm: 32.4220
2025-06-19 22:19:51,234 - mmdet - INFO - Epoch [2][3650/7033]	lr: 1.866e-04, eta: 12:50:48, time: 1.460, data_time: 0.026, memory: 20458, loss_cls: 0.0817, loss_bbox: 0.2235, d0.loss_cls: 0.1989, d0.loss_bbox: 0.3602, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2675, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2562, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2375, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2276, loss: 2.2433, grad_norm: 42.3468
2025-06-19 22:21:05,997 - mmdet - INFO - Epoch [2][3700/7033]	lr: 1.866e-04, eta: 12:49:38, time: 1.495, data_time: 0.028, memory: 20458, loss_cls: 0.0952, loss_bbox: 0.2274, d0.loss_cls: 0.1944, d0.loss_bbox: 0.3743, d1.loss_cls: 0.1352, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2564, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2384, d4.loss_cls: 0.0960, d4.loss_bbox: 0.2309, loss: 2.3329, grad_norm: 26.1132
2025-06-19 22:22:18,846 - mmdet - INFO - Epoch [2][3750/7033]	lr: 1.866e-04, eta: 12:48:23, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0897, loss_bbox: 0.2206, d0.loss_cls: 0.2058, d0.loss_bbox: 0.3736, d1.loss_cls: 0.1373, d1.loss_bbox: 0.2665, d2.loss_cls: 0.1101, d2.loss_bbox: 0.2510, d3.loss_cls: 0.0969, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2261, loss: 2.3013, grad_norm: 40.9382
2025-06-19 22:23:31,653 - mmdet - INFO - Epoch [2][3800/7033]	lr: 1.866e-04, eta: 12:47:08, time: 1.456, data_time: 0.026, memory: 20458, loss_cls: 0.0833, loss_bbox: 0.2266, d0.loss_cls: 0.1945, d0.loss_bbox: 0.3700, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2709, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2281, loss: 2.2556, grad_norm: 211.8253
2025-06-19 22:24:44,736 - mmdet - INFO - Epoch [2][3850/7033]	lr: 1.866e-04, eta: 12:45:54, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0828, loss_bbox: 0.2312, d0.loss_cls: 0.1946, d0.loss_bbox: 0.3592, d1.loss_cls: 0.1254, d1.loss_bbox: 0.2670, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2561, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2396, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2340, loss: 2.2712, grad_norm: 58.2957
2025-06-19 22:25:58,167 - mmdet - INFO - Epoch [2][3900/7033]	lr: 1.866e-04, eta: 12:44:41, time: 1.469, data_time: 0.029, memory: 20458, loss_cls: 0.0870, loss_bbox: 0.2320, d0.loss_cls: 0.2078, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1325, d1.loss_bbox: 0.2666, d2.loss_cls: 0.1068, d2.loss_bbox: 0.2593, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2422, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2348, loss: 2.3173, grad_norm: 73.5241
2025-06-19 22:27:11,048 - mmdet - INFO - Epoch [2][3950/7033]	lr: 1.866e-04, eta: 12:43:26, time: 1.458, data_time: 0.029, memory: 20458, loss_cls: 0.0896, loss_bbox: 0.2338, d0.loss_cls: 0.1983, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1323, d1.loss_bbox: 0.2723, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2599, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2400, loss: 2.3319, grad_norm: 43.0715
2025-06-19 22:28:24,171 - mmdet - INFO - Epoch [2][4000/7033]	lr: 1.866e-04, eta: 12:42:12, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0962, loss_bbox: 0.2361, d0.loss_cls: 0.2127, d0.loss_bbox: 0.3871, d1.loss_cls: 0.1470, d1.loss_bbox: 0.2861, d2.loss_cls: 0.1131, d2.loss_bbox: 0.2697, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2510, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2407, loss: 2.4396, grad_norm: 70.2921
2025-06-19 22:29:37,261 - mmdet - INFO - Epoch [2][4050/7033]	lr: 1.866e-04, eta: 12:40:58, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0918, loss_bbox: 0.2311, d0.loss_cls: 0.2125, d0.loss_bbox: 0.3912, d1.loss_cls: 0.1381, d1.loss_bbox: 0.2857, d2.loss_cls: 0.1107, d2.loss_bbox: 0.2666, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2446, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2369, loss: 2.3992, grad_norm: 36.1744
2025-06-19 22:30:50,357 - mmdet - INFO - Epoch [2][4100/7033]	lr: 1.866e-04, eta: 12:39:44, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0933, loss_bbox: 0.2305, d0.loss_cls: 0.2080, d0.loss_bbox: 0.3884, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2860, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2663, d3.loss_cls: 0.0969, d3.loss_bbox: 0.2453, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2341, loss: 2.3981, grad_norm: 31.6357
2025-06-19 22:32:03,224 - mmdet - INFO - Epoch [2][4150/7033]	lr: 1.866e-04, eta: 12:38:29, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0944, loss_bbox: 0.2283, d0.loss_cls: 0.2143, d0.loss_bbox: 0.3863, d1.loss_cls: 0.1422, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2651, d3.loss_cls: 0.1009, d3.loss_bbox: 0.2417, d4.loss_cls: 0.0962, d4.loss_bbox: 0.2345, loss: 2.3959, grad_norm: 28.9935
2025-06-19 22:33:16,223 - mmdet - INFO - Epoch [2][4200/7033]	lr: 1.866e-04, eta: 12:37:15, time: 1.460, data_time: 0.026, memory: 20458, loss_cls: 0.0899, loss_bbox: 0.2362, d0.loss_cls: 0.2036, d0.loss_bbox: 0.3768, d1.loss_cls: 0.1423, d1.loss_bbox: 0.2819, d2.loss_cls: 0.1153, d2.loss_bbox: 0.2709, d3.loss_cls: 0.0968, d3.loss_bbox: 0.2483, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2396, loss: 2.3950, grad_norm: 28.6553
2025-06-19 22:34:29,034 - mmdet - INFO - Epoch [2][4250/7033]	lr: 1.866e-04, eta: 12:36:00, time: 1.456, data_time: 0.026, memory: 20458, loss_cls: 0.0948, loss_bbox: 0.2268, d0.loss_cls: 0.1991, d0.loss_bbox: 0.3714, d1.loss_cls: 0.1443, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2626, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2398, d4.loss_cls: 0.0961, d4.loss_bbox: 0.2313, loss: 2.3605, grad_norm: 56.7854
2025-06-19 22:35:41,728 - mmdet - INFO - Epoch [2][4300/7033]	lr: 1.866e-04, eta: 12:34:45, time: 1.454, data_time: 0.026, memory: 20458, loss_cls: 0.0888, loss_bbox: 0.2245, d0.loss_cls: 0.1958, d0.loss_bbox: 0.3737, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2742, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2602, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2374, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2297, loss: 2.3039, grad_norm: 27.3610
2025-06-19 22:36:54,946 - mmdet - INFO - Epoch [2][4350/7033]	lr: 1.866e-04, eta: 12:33:31, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0892, loss_bbox: 0.2291, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2759, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2593, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2323, loss: 2.2993, grad_norm: 40.1710
2025-06-19 22:38:07,985 - mmdet - INFO - Epoch [2][4400/7033]	lr: 1.866e-04, eta: 12:32:17, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0797, loss_bbox: 0.2192, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3516, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2655, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2513, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2232, loss: 2.1817, grad_norm: 37.5166
2025-06-19 22:39:22,854 - mmdet - INFO - Epoch [2][4450/7033]	lr: 1.866e-04, eta: 12:31:07, time: 1.497, data_time: 0.030, memory: 20458, loss_cls: 0.0893, loss_bbox: 0.2267, d0.loss_cls: 0.2017, d0.loss_bbox: 0.3685, d1.loss_cls: 0.1342, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2377, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2305, loss: 2.3145, grad_norm: 31.2784
2025-06-19 22:40:35,619 - mmdet - INFO - Epoch [2][4500/7033]	lr: 1.866e-04, eta: 12:29:52, time: 1.455, data_time: 0.028, memory: 20458, loss_cls: 0.0874, loss_bbox: 0.2311, d0.loss_cls: 0.2024, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1318, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2627, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2337, loss: 2.3317, grad_norm: 31.8912
2025-06-19 22:42:58,619 - mmdet - INFO - Epoch [2][4550/7033]	lr: 1.866e-04, eta: 12:31:43, time: 2.860, data_time: 1.398, memory: 20458, loss_cls: 0.0904, loss_bbox: 0.2239, d0.loss_cls: 0.1975, d0.loss_bbox: 0.3681, d1.loss_cls: 0.1339, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1091, d2.loss_bbox: 0.2571, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2390, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2305, loss: 2.3113, grad_norm: 37.4279
2025-06-19 22:44:11,636 - mmdet - INFO - Epoch [2][4600/7033]	lr: 1.866e-04, eta: 12:30:28, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0888, loss_bbox: 0.2223, d0.loss_cls: 0.1903, d0.loss_bbox: 0.3572, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2676, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2539, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2275, loss: 2.2584, grad_norm: 34.4595
2025-06-19 22:45:24,650 - mmdet - INFO - Epoch [2][4650/7033]	lr: 1.866e-04, eta: 12:29:12, time: 1.460, data_time: 0.030, memory: 20458, loss_cls: 0.0863, loss_bbox: 0.2207, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2659, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2527, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2247, loss: 2.2490, grad_norm: 35.7201
2025-06-19 22:46:37,949 - mmdet - INFO - Epoch [2][4700/7033]	lr: 1.866e-04, eta: 12:27:58, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0831, loss_bbox: 0.2258, d0.loss_cls: 0.1922, d0.loss_bbox: 0.3505, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2631, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2499, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2249, loss: 2.2173, grad_norm: 39.2625
2025-06-19 22:47:51,207 - mmdet - INFO - Epoch [2][4750/7033]	lr: 1.866e-04, eta: 12:26:43, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0865, loss_bbox: 0.2203, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3636, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2543, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2322, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2240, loss: 2.2333, grad_norm: 51.9318
2025-06-19 22:49:04,405 - mmdet - INFO - Epoch [2][4800/7033]	lr: 1.866e-04, eta: 12:25:28, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0849, loss_bbox: 0.2213, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2660, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2519, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2319, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2239, loss: 2.2148, grad_norm: 33.8990
2025-06-19 22:50:17,457 - mmdet - INFO - Epoch [2][4850/7033]	lr: 1.866e-04, eta: 12:24:13, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0772, loss_bbox: 0.2131, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3513, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2163, loss: 2.1352, grad_norm: 25.6760
2025-06-19 22:51:30,568 - mmdet - INFO - Epoch [2][4900/7033]	lr: 1.866e-04, eta: 12:22:58, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0928, loss_bbox: 0.2352, d0.loss_cls: 0.1943, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2768, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2625, d3.loss_cls: 0.0952, d3.loss_bbox: 0.2452, d4.loss_cls: 0.0931, d4.loss_bbox: 0.2366, loss: 2.3276, grad_norm: 37.3058
2025-06-19 22:52:43,773 - mmdet - INFO - Epoch [2][4950/7033]	lr: 1.866e-04, eta: 12:21:43, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0864, loss_bbox: 0.2304, d0.loss_cls: 0.1950, d0.loss_bbox: 0.3666, d1.loss_cls: 0.1258, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2550, d3.loss_cls: 0.0941, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0897, d4.loss_bbox: 0.2300, loss: 2.2832, grad_norm: 45.0918
2025-06-19 22:53:56,740 - mmdet - INFO - Epoch [2][5000/7033]	lr: 1.866e-04, eta: 12:20:28, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0881, loss_bbox: 0.2155, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3600, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2605, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0918, d4.loss_bbox: 0.2194, loss: 2.2295, grad_norm: 31.1306
2025-06-19 22:55:09,765 - mmdet - INFO - Epoch [2][5050/7033]	lr: 1.866e-04, eta: 12:19:13, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0938, loss_bbox: 0.2207, d0.loss_cls: 0.1994, d0.loss_bbox: 0.3639, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2651, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2547, d3.loss_cls: 0.0977, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2277, loss: 2.2813, grad_norm: 181.4907
2025-06-19 22:56:22,781 - mmdet - INFO - Epoch [2][5100/7033]	lr: 1.866e-04, eta: 12:17:57, time: 1.460, data_time: 0.025, memory: 20458, loss_cls: 0.0804, loss_bbox: 0.2184, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2615, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2221, loss: 2.1891, grad_norm: 34.3011
2025-06-19 22:57:35,729 - mmdet - INFO - Epoch [2][5150/7033]	lr: 1.866e-04, eta: 12:16:42, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0860, loss_bbox: 0.2342, d0.loss_cls: 0.1969, d0.loss_bbox: 0.3800, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2823, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2636, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2432, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2367, loss: 2.3294, grad_norm: 27.6964
2025-06-19 22:58:48,426 - mmdet - INFO - Epoch [2][5200/7033]	lr: 1.866e-04, eta: 12:15:26, time: 1.454, data_time: 0.025, memory: 20458, loss_cls: 0.0941, loss_bbox: 0.2247, d0.loss_cls: 0.1981, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2634, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2534, d3.loss_cls: 0.0977, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2256, loss: 2.2734, grad_norm: 93.6338
2025-06-19 23:00:03,458 - mmdet - INFO - Epoch [2][5250/7033]	lr: 1.866e-04, eta: 12:14:16, time: 1.501, data_time: 0.028, memory: 20458, loss_cls: 0.0863, loss_bbox: 0.2251, d0.loss_cls: 0.1955, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1284, d1.loss_bbox: 0.2645, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2529, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2280, loss: 2.2667, grad_norm: 30.7596
2025-06-19 23:01:17,050 - mmdet - INFO - Epoch [2][5300/7033]	lr: 1.866e-04, eta: 12:13:02, time: 1.472, data_time: 0.028, memory: 20458, loss_cls: 0.0850, loss_bbox: 0.2308, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3689, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2592, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2322, loss: 2.2880, grad_norm: 41.4066
2025-06-19 23:02:30,188 - mmdet - INFO - Epoch [2][5350/7033]	lr: 1.866e-04, eta: 12:11:47, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0885, loss_bbox: 0.2226, d0.loss_cls: 0.1929, d0.loss_bbox: 0.3604, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2630, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2495, d3.loss_cls: 0.0980, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2245, loss: 2.2525, grad_norm: 36.1029
2025-06-19 23:03:43,220 - mmdet - INFO - Epoch [2][5400/7033]	lr: 1.866e-04, eta: 12:10:32, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0796, loss_bbox: 0.2155, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3561, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2647, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2508, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2202, loss: 2.1989, grad_norm: 56.5496
2025-06-19 23:04:56,428 - mmdet - INFO - Epoch [2][5450/7033]	lr: 1.866e-04, eta: 12:09:18, time: 1.464, data_time: 0.026, memory: 20458, loss_cls: 0.0781, loss_bbox: 0.2257, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3642, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2724, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2589, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2289, loss: 2.2265, grad_norm: 42.5555
2025-06-19 23:06:09,466 - mmdet - INFO - Epoch [2][5500/7033]	lr: 1.866e-04, eta: 12:08:03, time: 1.461, data_time: 0.024, memory: 20458, loss_cls: 0.0834, loss_bbox: 0.2311, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1172, d1.loss_bbox: 0.2733, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2627, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2433, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2351, loss: 2.2622, grad_norm: 75.1885
2025-06-19 23:07:22,513 - mmdet - INFO - Epoch [2][5550/7033]	lr: 1.866e-04, eta: 12:06:48, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0829, loss_bbox: 0.2281, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3650, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2568, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2400, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2325, loss: 2.2630, grad_norm: 40.4974
2025-06-19 23:08:35,422 - mmdet - INFO - Epoch [2][5600/7033]	lr: 1.866e-04, eta: 12:05:32, time: 1.458, data_time: 0.026, memory: 20458, loss_cls: 0.0829, loss_bbox: 0.2195, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3554, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2612, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2504, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2234, loss: 2.2129, grad_norm: 42.7155
2025-06-19 23:09:48,333 - mmdet - INFO - Epoch [2][5650/7033]	lr: 1.866e-04, eta: 12:04:17, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0826, loss_bbox: 0.2299, d0.loss_cls: 0.1964, d0.loss_bbox: 0.3698, d1.loss_cls: 0.1275, d1.loss_bbox: 0.2757, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2586, d3.loss_cls: 0.0924, d3.loss_bbox: 0.2413, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2339, loss: 2.2978, grad_norm: 29.0990
2025-06-19 23:11:01,271 - mmdet - INFO - Epoch [2][5700/7033]	lr: 1.866e-04, eta: 12:03:02, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0743, loss_bbox: 0.2176, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3441, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2594, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2468, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2214, loss: 2.1408, grad_norm: 27.5808
2025-06-19 23:12:14,417 - mmdet - INFO - Epoch [2][5750/7033]	lr: 1.866e-04, eta: 12:01:47, time: 1.463, data_time: 0.026, memory: 20458, loss_cls: 0.0911, loss_bbox: 0.2278, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3622, d1.loss_cls: 0.1243, d1.loss_bbox: 0.2725, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2553, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2401, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2319, loss: 2.2970, grad_norm: 41.1576
2025-06-19 23:13:27,507 - mmdet - INFO - Epoch [2][5800/7033]	lr: 1.866e-04, eta: 12:00:32, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0798, loss_bbox: 0.2189, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2564, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2229, loss: 2.1545, grad_norm: 30.1477
2025-06-19 23:14:40,613 - mmdet - INFO - Epoch [2][5850/7033]	lr: 1.866e-04, eta: 11:59:18, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0862, loss_bbox: 0.2259, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2672, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2389, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2291, loss: 2.2431, grad_norm: 47.8342
2025-06-19 23:15:53,794 - mmdet - INFO - Epoch [2][5900/7033]	lr: 1.866e-04, eta: 11:58:03, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0833, loss_bbox: 0.2186, d0.loss_cls: 0.1884, d0.loss_bbox: 0.3538, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2621, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2482, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0863, d4.loss_bbox: 0.2217, loss: 2.2068, grad_norm: 25.9780
2025-06-19 23:17:06,905 - mmdet - INFO - Epoch [2][5950/7033]	lr: 1.866e-04, eta: 11:56:48, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0843, loss_bbox: 0.2300, d0.loss_cls: 0.2014, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2587, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2401, d4.loss_cls: 0.0863, d4.loss_bbox: 0.2332, loss: 2.3032, grad_norm: 47.2401
2025-06-19 23:18:21,818 - mmdet - INFO - Epoch [2][6000/7033]	lr: 1.866e-04, eta: 11:55:38, time: 1.498, data_time: 0.027, memory: 20458, loss_cls: 0.0751, loss_bbox: 0.2172, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3542, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2426, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2195, loss: 2.1550, grad_norm: 38.4239
2025-06-19 23:19:34,971 - mmdet - INFO - Epoch [2][6050/7033]	lr: 1.866e-04, eta: 11:54:23, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0821, loss_bbox: 0.2304, d0.loss_cls: 0.1977, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2737, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2592, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2348, loss: 2.2834, grad_norm: 272.3817
2025-06-19 23:20:48,161 - mmdet - INFO - Epoch [2][6100/7033]	lr: 1.866e-04, eta: 11:53:08, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0802, loss_bbox: 0.2176, d0.loss_cls: 0.1965, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2584, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2461, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2214, loss: 2.2244, grad_norm: 59.2860
2025-06-19 23:22:01,018 - mmdet - INFO - Epoch [2][6150/7033]	lr: 1.866e-04, eta: 11:51:53, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0851, loss_bbox: 0.2260, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3806, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2616, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2307, loss: 2.3113, grad_norm: 40.6002
2025-06-19 23:23:14,165 - mmdet - INFO - Epoch [2][6200/7033]	lr: 1.866e-04, eta: 11:50:39, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0825, loss_bbox: 0.2225, d0.loss_cls: 0.1953, d0.loss_bbox: 0.3708, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2705, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2565, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2275, loss: 2.2474, grad_norm: 36.5482
2025-06-19 23:24:27,345 - mmdet - INFO - Epoch [2][6250/7033]	lr: 1.866e-04, eta: 11:49:24, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0858, loss_bbox: 0.2253, d0.loss_cls: 0.1964, d0.loss_bbox: 0.3742, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2711, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2559, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2380, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2300, loss: 2.2826, grad_norm: 27.6171
2025-06-19 23:25:40,315 - mmdet - INFO - Epoch [2][6300/7033]	lr: 1.866e-04, eta: 11:48:09, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0840, loss_bbox: 0.2240, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3791, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2716, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2543, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2357, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2259, loss: 2.2657, grad_norm: 76.0109
2025-06-19 23:26:53,561 - mmdet - INFO - Epoch [2][6350/7033]	lr: 1.866e-04, eta: 11:46:55, time: 1.465, data_time: 0.032, memory: 20458, loss_cls: 0.0910, loss_bbox: 0.2264, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3741, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2745, d2.loss_cls: 0.1044, d2.loss_bbox: 0.2571, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2383, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2314, loss: 2.2980, grad_norm: 38.3982
2025-06-19 23:28:06,785 - mmdet - INFO - Epoch [2][6400/7033]	lr: 1.866e-04, eta: 11:45:40, time: 1.464, data_time: 0.026, memory: 20458, loss_cls: 0.0861, loss_bbox: 0.2276, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3604, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2674, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2541, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2325, loss: 2.2575, grad_norm: 31.1840
2025-06-19 23:29:19,874 - mmdet - INFO - Epoch [2][6450/7033]	lr: 1.866e-04, eta: 11:44:26, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0817, loss_bbox: 0.2290, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3699, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2688, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2559, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2370, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2325, loss: 2.2553, grad_norm: 34.8817
2025-06-19 23:30:33,099 - mmdet - INFO - Epoch [2][6500/7033]	lr: 1.866e-04, eta: 11:43:11, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0839, loss_bbox: 0.2197, d0.loss_cls: 0.2002, d0.loss_bbox: 0.3622, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2613, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2477, d3.loss_cls: 0.0922, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2213, loss: 2.2362, grad_norm: 31.2760
2025-06-19 23:31:46,132 - mmdet - INFO - Epoch [2][6550/7033]	lr: 1.866e-04, eta: 11:41:56, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0747, loss_bbox: 0.2213, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2427, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2232, loss: 2.1440, grad_norm: 21.4243
2025-06-19 23:32:59,022 - mmdet - INFO - Epoch [2][6600/7033]	lr: 1.866e-04, eta: 11:40:41, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0813, loss_bbox: 0.2175, d0.loss_cls: 0.1831, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2427, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2205, loss: 2.1791, grad_norm: 109.1242
2025-06-19 23:34:11,846 - mmdet - INFO - Epoch [2][6650/7033]	lr: 1.866e-04, eta: 11:39:26, time: 1.456, data_time: 0.027, memory: 20458, loss_cls: 0.0859, loss_bbox: 0.2239, d0.loss_cls: 0.1881, d0.loss_bbox: 0.3696, d1.loss_cls: 0.1228, d1.loss_bbox: 0.2735, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2283, loss: 2.2631, grad_norm: 29.6030
2025-06-19 23:35:26,737 - mmdet - INFO - Epoch [2][6700/7033]	lr: 1.866e-04, eta: 11:38:15, time: 1.498, data_time: 0.028, memory: 20458, loss_cls: 0.0780, loss_bbox: 0.2220, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3590, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2672, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2334, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2249, loss: 2.1936, grad_norm: 24.3853
2025-06-19 23:36:39,887 - mmdet - INFO - Epoch [2][6750/7033]	lr: 1.866e-04, eta: 11:37:01, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0870, loss_bbox: 0.2267, d0.loss_cls: 0.1860, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2658, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2572, d3.loss_cls: 0.0922, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2292, loss: 2.2336, grad_norm: 48.0556
2025-06-19 23:37:54,295 - mmdet - INFO - Epoch [2][6800/7033]	lr: 1.866e-04, eta: 11:35:49, time: 1.488, data_time: 0.026, memory: 20458, loss_cls: 0.0819, loss_bbox: 0.2241, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3674, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2671, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2517, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2269, loss: 2.2347, grad_norm: 48.1285
2025-06-19 23:39:07,404 - mmdet - INFO - Epoch [2][6850/7033]	lr: 1.866e-04, eta: 11:34:34, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0845, loss_bbox: 0.2302, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3755, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2397, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2320, loss: 2.2866, grad_norm: 98.9225
2025-06-19 23:40:20,357 - mmdet - INFO - Epoch [2][6900/7033]	lr: 1.866e-04, eta: 11:33:19, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0882, loss_bbox: 0.2278, d0.loss_cls: 0.1980, d0.loss_bbox: 0.3691, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2781, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2643, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2432, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2311, loss: 2.3142, grad_norm: 36.4583
2025-06-19 23:41:33,320 - mmdet - INFO - Epoch [2][6950/7033]	lr: 1.866e-04, eta: 11:32:05, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0789, loss_bbox: 0.2152, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3585, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2607, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2442, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0820, d4.loss_bbox: 0.2202, loss: 2.1858, grad_norm: 35.5141
2025-06-19 23:42:46,282 - mmdet - INFO - Epoch [2][7000/7033]	lr: 1.866e-04, eta: 11:30:50, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0818, loss_bbox: 0.2290, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3542, d1.loss_cls: 0.1197, d1.loss_bbox: 0.2602, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0841, d4.loss_bbox: 0.2282, loss: 2.2236, grad_norm: 35.2223
2025-06-19 23:43:35,106 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-20 00:06:17,527 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-20 00:06:17,527 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7844, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8804, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9092, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9226, pts_bbox_NuScenes/car_trans_err: 0.1876, pts_bbox_NuScenes/car_scale_err: 0.1504, pts_bbox_NuScenes/car_orient_err: 0.0496, pts_bbox_NuScenes/car_vel_err: 0.3334, pts_bbox_NuScenes/car_attr_err: 0.1560, pts_bbox_NuScenes/mATE: 0.2951, pts_bbox_NuScenes/mASE: 0.2631, pts_bbox_NuScenes/mAOE: 0.2710, pts_bbox_NuScenes/mAVE: 0.2875, pts_bbox_NuScenes/mAAE: 0.1794, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4353, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6252, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7254, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7606, pts_bbox_NuScenes/truck_trans_err: 0.3316, pts_bbox_NuScenes/truck_scale_err: 0.1887, pts_bbox_NuScenes/truck_orient_err: 0.0542, pts_bbox_NuScenes/truck_vel_err: 0.2370, pts_bbox_NuScenes/truck_attr_err: 0.2147, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0598, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2022, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4143, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4911, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6647, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4403, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8077, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1148, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3043, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5173, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7380, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8940, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9216, pts_bbox_NuScenes/bus_trans_err: 0.3442, pts_bbox_NuScenes/bus_scale_err: 0.1918, pts_bbox_NuScenes/bus_orient_err: 0.0415, pts_bbox_NuScenes/bus_vel_err: 0.4806, pts_bbox_NuScenes/bus_attr_err: 0.2208, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1715, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4191, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5970, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6847, pts_bbox_NuScenes/trailer_trans_err: 0.4978, pts_bbox_NuScenes/trailer_scale_err: 0.2146, pts_bbox_NuScenes/trailer_orient_err: 0.5424, pts_bbox_NuScenes/trailer_vel_err: 0.2199, pts_bbox_NuScenes/trailer_attr_err: 0.1885, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5939, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6966, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7515, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7682, pts_bbox_NuScenes/barrier_trans_err: 0.2253, pts_bbox_NuScenes/barrier_scale_err: 0.2938, pts_bbox_NuScenes/barrier_orient_err: 0.0559, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6290, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7560, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7971, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8053, pts_bbox_NuScenes/motorcycle_trans_err: 0.2262, pts_bbox_NuScenes/motorcycle_scale_err: 0.2435, pts_bbox_NuScenes/motorcycle_orient_err: 0.2211, pts_bbox_NuScenes/motorcycle_vel_err: 0.4683, pts_bbox_NuScenes/motorcycle_attr_err: 0.2461, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5337, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5925, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5999, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6075, pts_bbox_NuScenes/bicycle_trans_err: 0.1810, pts_bbox_NuScenes/bicycle_scale_err: 0.2812, pts_bbox_NuScenes/bicycle_orient_err: 0.3616, pts_bbox_NuScenes/bicycle_vel_err: 0.2267, pts_bbox_NuScenes/bicycle_attr_err: 0.0062, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8139, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8589, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8824, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8959, pts_bbox_NuScenes/pedestrian_trans_err: 0.1502, pts_bbox_NuScenes/pedestrian_scale_err: 0.2896, pts_bbox_NuScenes/pedestrian_orient_err: 0.3049, pts_bbox_NuScenes/pedestrian_vel_err: 0.2197, pts_bbox_NuScenes/pedestrian_attr_err: 0.0983, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7134, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7501, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7828, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8096, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1422, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3369, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7053, pts_bbox_NuScenes/mAP: 0.6698
2025-06-20 00:07:39,044 - mmdet - INFO - Epoch [3][50/7033]	lr: 1.501e-04, eta: 11:27:18, time: 1.545, data_time: 0.117, memory: 20458, loss_cls: 0.0863, loss_bbox: 0.2174, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2624, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2451, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2226, loss: 2.2064, grad_norm: 209.2266
2025-06-20 00:08:51,956 - mmdet - INFO - Epoch [3][100/7033]	lr: 1.501e-04, eta: 11:26:04, time: 1.458, data_time: 0.024, memory: 20458, loss_cls: 0.0792, loss_bbox: 0.2229, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1185, d1.loss_bbox: 0.2654, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2497, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2351, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2243, loss: 2.2042, grad_norm: 32.4150
2025-06-20 00:10:04,693 - mmdet - INFO - Epoch [3][150/7033]	lr: 1.501e-04, eta: 11:24:49, time: 1.455, data_time: 0.027, memory: 20458, loss_cls: 0.0745, loss_bbox: 0.2106, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2139, loss: 2.1005, grad_norm: 42.2368
2025-06-20 00:11:17,486 - mmdet - INFO - Epoch [3][200/7033]	lr: 1.501e-04, eta: 11:23:34, time: 1.456, data_time: 0.025, memory: 20458, loss_cls: 0.0796, loss_bbox: 0.2237, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2644, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2246, loss: 2.1938, grad_norm: 31.0601
2025-06-20 00:12:30,324 - mmdet - INFO - Epoch [3][250/7033]	lr: 1.501e-04, eta: 11:22:20, time: 1.457, data_time: 0.026, memory: 20458, loss_cls: 0.0783, loss_bbox: 0.2167, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0847, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2195, loss: 2.1274, grad_norm: 40.8416
2025-06-20 00:13:43,266 - mmdet - INFO - Epoch [3][300/7033]	lr: 1.501e-04, eta: 11:21:06, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0767, loss_bbox: 0.2188, d0.loss_cls: 0.1912, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2602, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2428, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2208, loss: 2.1586, grad_norm: 34.5622
2025-06-20 00:14:56,003 - mmdet - INFO - Epoch [3][350/7033]	lr: 1.501e-04, eta: 11:19:51, time: 1.455, data_time: 0.025, memory: 20458, loss_cls: 0.0792, loss_bbox: 0.2214, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3485, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2231, loss: 2.1618, grad_norm: 27.7860
2025-06-20 00:16:10,580 - mmdet - INFO - Epoch [3][400/7033]	lr: 1.501e-04, eta: 11:18:40, time: 1.492, data_time: 0.025, memory: 20458, loss_cls: 0.0817, loss_bbox: 0.2104, d0.loss_cls: 0.1942, d0.loss_bbox: 0.3337, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0872, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2148, loss: 2.1329, grad_norm: 39.6541
2025-06-20 00:17:23,256 - mmdet - INFO - Epoch [3][450/7033]	lr: 1.501e-04, eta: 11:17:25, time: 1.453, data_time: 0.027, memory: 20458, loss_cls: 0.0801, loss_bbox: 0.2118, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3404, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2529, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2130, loss: 2.1146, grad_norm: 68.3678
2025-06-20 00:18:36,193 - mmdet - INFO - Epoch [3][500/7033]	lr: 1.501e-04, eta: 11:16:10, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0838, loss_bbox: 0.2113, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2537, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0890, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2155, loss: 2.1591, grad_norm: 24.9843
2025-06-20 00:19:48,991 - mmdet - INFO - Epoch [3][550/7033]	lr: 1.501e-04, eta: 11:14:56, time: 1.456, data_time: 0.026, memory: 20458, loss_cls: 0.0707, loss_bbox: 0.2142, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2424, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2177, loss: 2.1063, grad_norm: 26.1167
2025-06-20 00:21:01,957 - mmdet - INFO - Epoch [3][600/7033]	lr: 1.501e-04, eta: 11:13:42, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0841, loss_bbox: 0.2259, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3469, d1.loss_cls: 0.1260, d1.loss_bbox: 0.2631, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2456, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2290, loss: 2.2292, grad_norm: 37.0830
2025-06-20 00:22:14,610 - mmdet - INFO - Epoch [3][650/7033]	lr: 1.501e-04, eta: 11:12:27, time: 1.453, data_time: 0.025, memory: 20458, loss_cls: 0.0788, loss_bbox: 0.2087, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3425, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2540, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2380, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2127, loss: 2.1217, grad_norm: 28.8947
2025-06-20 00:23:27,581 - mmdet - INFO - Epoch [3][700/7033]	lr: 1.501e-04, eta: 11:11:13, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0865, loss_bbox: 0.2140, d0.loss_cls: 0.2025, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2571, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0936, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2174, loss: 2.2082, grad_norm: 58.1011
2025-06-20 00:24:40,542 - mmdet - INFO - Epoch [3][750/7033]	lr: 1.501e-04, eta: 11:09:58, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0804, loss_bbox: 0.2221, d0.loss_cls: 0.1968, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2645, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2256, loss: 2.2187, grad_norm: 127.2485
2025-06-20 00:25:53,664 - mmdet - INFO - Epoch [3][800/7033]	lr: 1.501e-04, eta: 11:08:44, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0811, loss_bbox: 0.2160, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2431, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2199, loss: 2.1825, grad_norm: 62.6259
2025-06-20 00:27:06,786 - mmdet - INFO - Epoch [3][850/7033]	lr: 1.501e-04, eta: 11:07:31, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0758, loss_bbox: 0.2104, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2375, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2132, loss: 2.1115, grad_norm: 45.5280
2025-06-20 00:28:20,025 - mmdet - INFO - Epoch [3][900/7033]	lr: 1.501e-04, eta: 11:06:17, time: 1.465, data_time: 0.026, memory: 20458, loss_cls: 0.0818, loss_bbox: 0.2208, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3626, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2666, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2324, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2255, loss: 2.2255, grad_norm: 33.7247
2025-06-20 00:29:32,856 - mmdet - INFO - Epoch [3][950/7033]	lr: 1.501e-04, eta: 11:05:02, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0788, loss_bbox: 0.2139, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2408, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2170, loss: 2.1455, grad_norm: 37.4695
2025-06-20 00:30:45,940 - mmdet - INFO - Epoch [3][1000/7033]	lr: 1.501e-04, eta: 11:03:48, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0773, loss_bbox: 0.2164, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2195, loss: 2.1597, grad_norm: 38.8324
2025-06-20 00:31:58,906 - mmdet - INFO - Epoch [3][1050/7033]	lr: 1.501e-04, eta: 11:02:34, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0747, loss_bbox: 0.2147, d0.loss_cls: 0.1906, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2390, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2228, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2159, loss: 2.1099, grad_norm: 44.9795
2025-06-20 00:33:11,885 - mmdet - INFO - Epoch [3][1100/7033]	lr: 1.501e-04, eta: 11:01:20, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0797, loss_bbox: 0.2164, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2557, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0828, d4.loss_bbox: 0.2192, loss: 2.1511, grad_norm: 62.7401
2025-06-20 00:34:24,821 - mmdet - INFO - Epoch [3][1150/7033]	lr: 1.501e-04, eta: 11:00:06, time: 1.459, data_time: 0.025, memory: 20458, loss_cls: 0.0811, loss_bbox: 0.2285, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2710, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2554, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2396, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2316, loss: 2.2456, grad_norm: 38.5329
2025-06-20 00:35:37,936 - mmdet - INFO - Epoch [3][1200/7033]	lr: 1.501e-04, eta: 10:58:52, time: 1.462, data_time: 0.026, memory: 20458, loss_cls: 0.0750, loss_bbox: 0.2146, d0.loss_cls: 0.1859, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2583, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2406, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2178, loss: 2.1187, grad_norm: 19.3911
2025-06-20 00:36:51,116 - mmdet - INFO - Epoch [3][1250/7033]	lr: 1.501e-04, eta: 10:57:38, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0834, loss_bbox: 0.2237, d0.loss_cls: 0.1928, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2320, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2252, loss: 2.2216, grad_norm: 259.8503
2025-06-20 00:38:04,135 - mmdet - INFO - Epoch [3][1300/7033]	lr: 1.501e-04, eta: 10:56:24, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0786, loss_bbox: 0.2143, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2183, loss: 2.1242, grad_norm: 26.7461
2025-06-20 00:39:17,029 - mmdet - INFO - Epoch [3][1350/7033]	lr: 1.501e-04, eta: 10:55:10, time: 1.458, data_time: 0.027, memory: 20458, loss_cls: 0.0813, loss_bbox: 0.2185, d0.loss_cls: 0.1884, d0.loss_bbox: 0.3426, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2610, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2218, loss: 2.1756, grad_norm: 108.1368
2025-06-20 00:40:30,078 - mmdet - INFO - Epoch [3][1400/7033]	lr: 1.501e-04, eta: 10:53:56, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0842, loss_bbox: 0.2214, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3481, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2628, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2324, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2250, loss: 2.1965, grad_norm: 38.2343
2025-06-20 00:41:43,113 - mmdet - INFO - Epoch [3][1450/7033]	lr: 1.501e-04, eta: 10:52:42, time: 1.461, data_time: 0.025, memory: 20458, loss_cls: 0.0855, loss_bbox: 0.2173, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3566, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2591, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2450, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2289, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2222, loss: 2.2021, grad_norm: 28.3448
2025-06-20 00:42:56,330 - mmdet - INFO - Epoch [3][1500/7033]	lr: 1.501e-04, eta: 10:51:28, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0827, loss_bbox: 0.2167, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3432, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2450, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2206, loss: 2.1844, grad_norm: 45.8836
2025-06-20 00:44:09,388 - mmdet - INFO - Epoch [3][1550/7033]	lr: 1.501e-04, eta: 10:50:14, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0785, loss_bbox: 0.2186, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3485, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2629, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2227, loss: 2.1675, grad_norm: 39.0843
2025-06-20 00:45:22,269 - mmdet - INFO - Epoch [3][1600/7033]	lr: 1.501e-04, eta: 10:49:00, time: 1.458, data_time: 0.025, memory: 20458, loss_cls: 0.0752, loss_bbox: 0.2200, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3432, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0908, d2.loss_bbox: 0.2461, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2228, loss: 2.1489, grad_norm: 27.7465
2025-06-20 00:46:35,362 - mmdet - INFO - Epoch [3][1650/7033]	lr: 1.501e-04, eta: 10:47:46, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0764, loss_bbox: 0.2182, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2454, d3.loss_cls: 0.0826, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2224, loss: 2.1552, grad_norm: 55.3963
2025-06-20 00:47:50,579 - mmdet - INFO - Epoch [3][1700/7033]	lr: 1.501e-04, eta: 10:46:36, time: 1.504, data_time: 0.059, memory: 20458, loss_cls: 0.0790, loss_bbox: 0.2046, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2100, loss: 2.0756, grad_norm: 29.7183
2025-06-20 00:49:07,730 - mmdet - INFO - Epoch [3][1750/7033]	lr: 1.501e-04, eta: 10:45:29, time: 1.543, data_time: 0.108, memory: 20458, loss_cls: 0.0745, loss_bbox: 0.2163, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2193, loss: 2.1087, grad_norm: 29.5406
2025-06-20 00:50:20,590 - mmdet - INFO - Epoch [3][1800/7033]	lr: 1.501e-04, eta: 10:44:15, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0777, loss_bbox: 0.2165, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2590, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2435, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2272, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2191, loss: 2.1453, grad_norm: 42.4665
2025-06-20 00:51:33,599 - mmdet - INFO - Epoch [3][1850/7033]	lr: 1.501e-04, eta: 10:43:00, time: 1.460, data_time: 0.025, memory: 20458, loss_cls: 0.0783, loss_bbox: 0.2173, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3428, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2575, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2403, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2198, loss: 2.1273, grad_norm: 25.5800
2025-06-20 00:52:48,654 - mmdet - INFO - Epoch [3][1900/7033]	lr: 1.501e-04, eta: 10:41:50, time: 1.501, data_time: 0.034, memory: 20458, loss_cls: 0.0796, loss_bbox: 0.2209, d0.loss_cls: 0.1923, d0.loss_bbox: 0.3577, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2648, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2319, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2244, loss: 2.2016, grad_norm: 26.9359
2025-06-20 00:54:01,909 - mmdet - INFO - Epoch [3][1950/7033]	lr: 1.501e-04, eta: 10:40:36, time: 1.465, data_time: 0.032, memory: 20458, loss_cls: 0.0779, loss_bbox: 0.2217, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1172, d1.loss_bbox: 0.2670, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2504, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2328, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2246, loss: 2.1993, grad_norm: 26.6623
2025-06-20 00:55:15,025 - mmdet - INFO - Epoch [3][2000/7033]	lr: 1.501e-04, eta: 10:39:22, time: 1.462, data_time: 0.034, memory: 20458, loss_cls: 0.0787, loss_bbox: 0.2128, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3472, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2571, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2389, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2146, loss: 2.1554, grad_norm: 26.1104
2025-06-20 00:56:27,859 - mmdet - INFO - Epoch [3][2050/7033]	lr: 1.501e-04, eta: 10:38:08, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0724, loss_bbox: 0.2116, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2352, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2131, loss: 2.0840, grad_norm: 97.4811
2025-06-20 00:57:40,586 - mmdet - INFO - Epoch [3][2100/7033]	lr: 1.501e-04, eta: 10:36:53, time: 1.455, data_time: 0.026, memory: 20458, loss_cls: 0.0854, loss_bbox: 0.2233, d0.loss_cls: 0.1928, d0.loss_bbox: 0.3689, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2791, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2563, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2374, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2290, loss: 2.2789, grad_norm: 48.9768
2025-06-20 00:58:53,361 - mmdet - INFO - Epoch [3][2150/7033]	lr: 1.501e-04, eta: 10:35:39, time: 1.456, data_time: 0.025, memory: 20458, loss_cls: 0.0771, loss_bbox: 0.2068, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3364, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2330, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2102, loss: 2.0714, grad_norm: 19.8384
2025-06-20 01:00:06,499 - mmdet - INFO - Epoch [3][2200/7033]	lr: 1.501e-04, eta: 10:34:25, time: 1.463, data_time: 0.026, memory: 20458, loss_cls: 0.0825, loss_bbox: 0.2118, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3423, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2401, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2150, loss: 2.1388, grad_norm: 49.8806
2025-06-20 01:01:19,472 - mmdet - INFO - Epoch [3][2250/7033]	lr: 1.501e-04, eta: 10:33:11, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0797, loss_bbox: 0.2123, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2159, loss: 2.1297, grad_norm: 32.4222
2025-06-20 01:02:32,312 - mmdet - INFO - Epoch [3][2300/7033]	lr: 1.501e-04, eta: 10:31:57, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0852, loss_bbox: 0.2164, d0.loss_cls: 0.1927, d0.loss_bbox: 0.3440, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2412, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2196, loss: 2.1832, grad_norm: 62.1206
2025-06-20 01:03:45,179 - mmdet - INFO - Epoch [3][2350/7033]	lr: 1.501e-04, eta: 10:30:43, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0830, loss_bbox: 0.2291, d0.loss_cls: 0.1995, d0.loss_bbox: 0.3546, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2697, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2506, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2291, loss: 2.2520, grad_norm: 25.4258
2025-06-20 01:04:58,332 - mmdet - INFO - Epoch [3][2400/7033]	lr: 1.501e-04, eta: 10:29:29, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0811, loss_bbox: 0.2202, d0.loss_cls: 0.1917, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2636, d2.loss_cls: 0.0957, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2323, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2236, loss: 2.1894, grad_norm: 30.8921
2025-06-20 01:06:11,173 - mmdet - INFO - Epoch [3][2450/7033]	lr: 1.501e-04, eta: 10:28:15, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0765, loss_bbox: 0.2129, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2545, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2369, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2205, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2155, loss: 2.1297, grad_norm: 30.4004
2025-06-20 01:07:23,852 - mmdet - INFO - Epoch [3][2500/7033]	lr: 1.501e-04, eta: 10:27:00, time: 1.454, data_time: 0.028, memory: 20458, loss_cls: 0.0773, loss_bbox: 0.2120, d0.loss_cls: 0.1876, d0.loss_bbox: 0.3484, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2550, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2140, loss: 2.1367, grad_norm: 46.1476
2025-06-20 01:08:36,912 - mmdet - INFO - Epoch [3][2550/7033]	lr: 1.501e-04, eta: 10:25:46, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0745, loss_bbox: 0.2172, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3607, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2419, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2211, loss: 2.1626, grad_norm: 29.4156
2025-06-20 01:09:56,548 - mmdet - INFO - Epoch [3][2600/7033]	lr: 1.501e-04, eta: 10:24:43, time: 1.593, data_time: 0.028, memory: 20458, loss_cls: 0.0800, loss_bbox: 0.2196, d0.loss_cls: 0.1957, d0.loss_bbox: 0.3598, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2624, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2500, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2247, loss: 2.2148, grad_norm: 23.3716
2025-06-20 01:11:11,335 - mmdet - INFO - Epoch [3][2650/7033]	lr: 1.501e-04, eta: 10:23:31, time: 1.496, data_time: 0.029, memory: 20458, loss_cls: 0.0777, loss_bbox: 0.2175, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2563, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2177, loss: 2.1348, grad_norm: 46.8870
2025-06-20 01:12:24,462 - mmdet - INFO - Epoch [3][2700/7033]	lr: 1.501e-04, eta: 10:22:17, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0853, loss_bbox: 0.2246, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3711, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2687, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2508, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2346, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2275, loss: 2.2659, grad_norm: 33.2233
2025-06-20 01:13:37,691 - mmdet - INFO - Epoch [3][2750/7033]	lr: 1.501e-04, eta: 10:21:04, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0828, loss_bbox: 0.2136, d0.loss_cls: 0.2000, d0.loss_bbox: 0.3520, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2556, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2384, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2192, loss: 2.1817, grad_norm: 24.0323
2025-06-20 01:14:50,785 - mmdet - INFO - Epoch [3][2800/7033]	lr: 1.501e-04, eta: 10:19:50, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0849, loss_bbox: 0.2187, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3622, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2644, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2219, loss: 2.2153, grad_norm: 24.6928
2025-06-20 01:16:03,397 - mmdet - INFO - Epoch [3][2850/7033]	lr: 1.501e-04, eta: 10:18:35, time: 1.452, data_time: 0.024, memory: 20458, loss_cls: 0.0796, loss_bbox: 0.2229, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3580, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2666, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2507, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2267, loss: 2.2137, grad_norm: 26.2159
2025-06-20 01:17:16,630 - mmdet - INFO - Epoch [3][2900/7033]	lr: 1.501e-04, eta: 10:17:22, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0804, loss_bbox: 0.2138, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2162, loss: 2.1322, grad_norm: 44.3697
2025-06-20 01:18:29,674 - mmdet - INFO - Epoch [3][2950/7033]	lr: 1.501e-04, eta: 10:16:08, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0710, loss_bbox: 0.2096, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2126, loss: 2.0635, grad_norm: 35.2409
2025-06-20 01:19:42,580 - mmdet - INFO - Epoch [3][3000/7033]	lr: 1.501e-04, eta: 10:14:54, time: 1.458, data_time: 0.027, memory: 20458, loss_cls: 0.0756, loss_bbox: 0.2126, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2245, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2165, loss: 2.1318, grad_norm: 48.6283
2025-06-20 01:20:55,891 - mmdet - INFO - Epoch [3][3050/7033]	lr: 1.501e-04, eta: 10:13:40, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0794, loss_bbox: 0.2155, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2595, d2.loss_cls: 0.0974, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2187, loss: 2.1674, grad_norm: 57.9879
2025-06-20 01:22:08,664 - mmdet - INFO - Epoch [3][3100/7033]	lr: 1.501e-04, eta: 10:12:26, time: 1.455, data_time: 0.025, memory: 20458, loss_cls: 0.0839, loss_bbox: 0.2135, d0.loss_cls: 0.2024, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2585, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2164, loss: 2.1940, grad_norm: 79.0068
2025-06-20 01:23:21,618 - mmdet - INFO - Epoch [3][3150/7033]	lr: 1.501e-04, eta: 10:11:12, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0790, loss_bbox: 0.2104, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3402, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2345, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2145, loss: 2.1121, grad_norm: 38.1556
2025-06-20 01:24:34,343 - mmdet - INFO - Epoch [3][3200/7033]	lr: 1.501e-04, eta: 10:09:57, time: 1.454, data_time: 0.024, memory: 20458, loss_cls: 0.0815, loss_bbox: 0.2132, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3491, d1.loss_cls: 0.1197, d1.loss_bbox: 0.2560, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2386, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2152, loss: 2.1568, grad_norm: 45.9741
2025-06-20 01:25:47,521 - mmdet - INFO - Epoch [3][3250/7033]	lr: 1.501e-04, eta: 10:08:44, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0739, loss_bbox: 0.2088, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2112, loss: 2.0609, grad_norm: 33.5820
2025-06-20 01:27:00,506 - mmdet - INFO - Epoch [3][3300/7033]	lr: 1.501e-04, eta: 10:07:30, time: 1.460, data_time: 0.026, memory: 20458, loss_cls: 0.0799, loss_bbox: 0.2165, d0.loss_cls: 0.1954, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2419, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2203, loss: 2.1699, grad_norm: 28.1879
2025-06-20 01:28:13,596 - mmdet - INFO - Epoch [3][3350/7033]	lr: 1.501e-04, eta: 10:06:16, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0726, loss_bbox: 0.2124, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2538, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2161, loss: 2.1007, grad_norm: 64.5020
2025-06-20 01:29:26,848 - mmdet - INFO - Epoch [3][3400/7033]	lr: 1.501e-04, eta: 10:05:02, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0763, loss_bbox: 0.2099, d0.loss_cls: 0.1921, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2191, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2131, loss: 2.1163, grad_norm: 39.9365
2025-06-20 01:30:39,985 - mmdet - INFO - Epoch [3][3450/7033]	lr: 1.501e-04, eta: 10:03:48, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0808, loss_bbox: 0.2193, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3555, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2636, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2451, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2237, loss: 2.1895, grad_norm: 38.5725
2025-06-20 01:31:53,027 - mmdet - INFO - Epoch [3][3500/7033]	lr: 1.501e-04, eta: 10:02:35, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0743, loss_bbox: 0.2219, d0.loss_cls: 0.1851, d0.loss_bbox: 0.3439, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2593, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2210, loss: 2.1331, grad_norm: 47.7199
2025-06-20 01:33:05,887 - mmdet - INFO - Epoch [3][3550/7033]	lr: 1.501e-04, eta: 10:01:20, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0766, loss_bbox: 0.2086, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3431, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2154, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2095, loss: 2.0901, grad_norm: 38.9878
2025-06-20 01:34:18,815 - mmdet - INFO - Epoch [3][3600/7033]	lr: 1.501e-04, eta: 10:00:06, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0737, loss_bbox: 0.2110, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2540, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2160, loss: 2.1210, grad_norm: 33.7445
2025-06-20 01:35:31,743 - mmdet - INFO - Epoch [3][3650/7033]	lr: 1.501e-04, eta: 9:58:52, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0790, loss_bbox: 0.2182, d0.loss_cls: 0.1873, d0.loss_bbox: 0.3596, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0857, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2221, loss: 2.1933, grad_norm: 26.1640
2025-06-20 01:36:45,768 - mmdet - INFO - Epoch [3][3700/7033]	lr: 1.501e-04, eta: 9:57:40, time: 1.480, data_time: 0.038, memory: 20458, loss_cls: 0.0948, loss_bbox: 0.2139, d0.loss_cls: 0.1995, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1296, d1.loss_bbox: 0.2556, d2.loss_cls: 0.1105, d2.loss_bbox: 0.2427, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2230, d4.loss_cls: 0.0945, d4.loss_bbox: 0.2176, loss: 2.2269, grad_norm: 40.1564
2025-06-20 01:37:59,113 - mmdet - INFO - Epoch [3][3750/7033]	lr: 1.501e-04, eta: 9:56:26, time: 1.467, data_time: 0.029, memory: 20458, loss_cls: 0.0857, loss_bbox: 0.2190, d0.loss_cls: 0.1926, d0.loss_bbox: 0.3503, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2634, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2462, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2227, loss: 2.2248, grad_norm: 34.6024
2025-06-20 01:39:12,577 - mmdet - INFO - Epoch [3][3800/7033]	lr: 1.501e-04, eta: 9:55:13, time: 1.469, data_time: 0.026, memory: 20458, loss_cls: 0.0796, loss_bbox: 0.2135, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2575, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2407, d3.loss_cls: 0.0872, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2173, loss: 2.1598, grad_norm: 25.9651
2025-06-20 01:40:25,685 - mmdet - INFO - Epoch [3][3850/7033]	lr: 1.501e-04, eta: 9:53:59, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0866, loss_bbox: 0.2192, d0.loss_cls: 0.1955, d0.loss_bbox: 0.3511, d1.loss_cls: 0.1289, d1.loss_bbox: 0.2569, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2442, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2211, loss: 2.2232, grad_norm: 25.0289
2025-06-20 01:41:38,898 - mmdet - INFO - Epoch [3][3900/7033]	lr: 1.501e-04, eta: 9:52:46, time: 1.464, data_time: 0.031, memory: 20458, loss_cls: 0.0789, loss_bbox: 0.2096, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3446, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2118, loss: 2.1160, grad_norm: 27.5613
2025-06-20 01:42:52,026 - mmdet - INFO - Epoch [3][3950/7033]	lr: 1.501e-04, eta: 9:51:32, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0783, loss_bbox: 0.2163, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2217, loss: 2.1475, grad_norm: 66.7088
2025-06-20 01:44:05,429 - mmdet - INFO - Epoch [3][4000/7033]	lr: 1.501e-04, eta: 9:50:19, time: 1.468, data_time: 0.034, memory: 20458, loss_cls: 0.0771, loss_bbox: 0.2107, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2132, loss: 2.1147, grad_norm: 23.8640
2025-06-20 01:45:18,514 - mmdet - INFO - Epoch [3][4050/7033]	lr: 1.501e-04, eta: 9:49:05, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0718, loss_bbox: 0.2037, d0.loss_cls: 0.1926, d0.loss_bbox: 0.3315, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2047, loss: 2.0449, grad_norm: 54.6768
2025-06-20 01:46:31,865 - mmdet - INFO - Epoch [3][4100/7033]	lr: 1.501e-04, eta: 9:47:51, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0800, loss_bbox: 0.2201, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3621, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2665, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2235, loss: 2.2048, grad_norm: 43.5699
2025-06-20 01:47:48,930 - mmdet - INFO - Epoch [3][4150/7033]	lr: 1.501e-04, eta: 9:46:43, time: 1.541, data_time: 0.029, memory: 20458, loss_cls: 0.0767, loss_bbox: 0.2127, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2601, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2185, loss: 2.1450, grad_norm: 81.9173
2025-06-20 01:49:02,008 - mmdet - INFO - Epoch [3][4200/7033]	lr: 1.501e-04, eta: 9:45:29, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0799, loss_bbox: 0.2156, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3457, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2576, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2195, loss: 2.1483, grad_norm: 93.6629
2025-06-20 01:50:15,137 - mmdet - INFO - Epoch [3][4250/7033]	lr: 1.501e-04, eta: 9:44:15, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0851, loss_bbox: 0.2241, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2690, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2517, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2280, loss: 2.2434, grad_norm: 30.9354
2025-06-20 01:51:28,123 - mmdet - INFO - Epoch [3][4300/7033]	lr: 1.501e-04, eta: 9:43:01, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0810, loss_bbox: 0.2257, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3732, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2727, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2376, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2290, loss: 2.2453, grad_norm: 25.3717
2025-06-20 01:52:41,192 - mmdet - INFO - Epoch [3][4350/7033]	lr: 1.501e-04, eta: 9:41:48, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0814, loss_bbox: 0.2162, d0.loss_cls: 0.1922, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2561, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2403, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2182, loss: 2.1580, grad_norm: 25.1185
2025-06-20 01:53:54,382 - mmdet - INFO - Epoch [3][4400/7033]	lr: 1.501e-04, eta: 9:40:34, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0753, loss_bbox: 0.2130, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3415, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2561, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2166, loss: 2.1040, grad_norm: 42.6265
2025-06-20 01:55:07,704 - mmdet - INFO - Epoch [3][4450/7033]	lr: 1.501e-04, eta: 9:39:20, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0713, loss_bbox: 0.2070, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2087, loss: 2.0375, grad_norm: 23.5766
2025-06-20 01:56:20,583 - mmdet - INFO - Epoch [3][4500/7033]	lr: 1.501e-04, eta: 9:38:06, time: 1.458, data_time: 0.029, memory: 20458, loss_cls: 0.0778, loss_bbox: 0.2111, d0.loss_cls: 0.1843, d0.loss_bbox: 0.3425, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2143, loss: 2.1015, grad_norm: 25.6145
2025-06-20 01:57:33,578 - mmdet - INFO - Epoch [3][4550/7033]	lr: 1.501e-04, eta: 9:36:52, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0814, loss_bbox: 0.2193, d0.loss_cls: 0.1928, d0.loss_bbox: 0.3485, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2447, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2332, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2222, loss: 2.1879, grad_norm: 30.2258
2025-06-20 01:58:46,600 - mmdet - INFO - Epoch [3][4600/7033]	lr: 1.501e-04, eta: 9:35:39, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0758, loss_bbox: 0.2146, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3436, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2426, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2204, loss: 2.1213, grad_norm: 61.5764
2025-06-20 01:59:59,753 - mmdet - INFO - Epoch [3][4650/7033]	lr: 1.501e-04, eta: 9:34:25, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0832, loss_bbox: 0.2165, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2603, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2187, loss: 2.1699, grad_norm: 53.7079
2025-06-20 02:01:12,902 - mmdet - INFO - Epoch [3][4700/7033]	lr: 1.501e-04, eta: 9:33:11, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0835, loss_bbox: 0.2160, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2591, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2180, loss: 2.1520, grad_norm: 26.8504
2025-06-20 02:02:25,770 - mmdet - INFO - Epoch [3][4750/7033]	lr: 1.501e-04, eta: 9:31:57, time: 1.457, data_time: 0.027, memory: 20458, loss_cls: 0.0774, loss_bbox: 0.2050, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0915, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2087, loss: 2.0596, grad_norm: 33.7178
2025-06-20 02:03:38,726 - mmdet - INFO - Epoch [3][4800/7033]	lr: 1.501e-04, eta: 9:30:43, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0822, loss_bbox: 0.2114, d0.loss_cls: 0.1883, d0.loss_bbox: 0.3402, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0843, d4.loss_bbox: 0.2143, loss: 2.1245, grad_norm: 20.1697
2025-06-20 02:04:54,988 - mmdet - INFO - Epoch [3][4850/7033]	lr: 1.501e-04, eta: 9:29:33, time: 1.525, data_time: 0.027, memory: 20458, loss_cls: 0.0713, loss_bbox: 0.1999, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2036, loss: 1.9935, grad_norm: 41.1971
2025-06-20 02:06:07,978 - mmdet - INFO - Epoch [3][4900/7033]	lr: 1.501e-04, eta: 9:28:20, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0731, loss_bbox: 0.2085, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3373, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2110, loss: 2.0621, grad_norm: 36.7444
2025-06-20 02:07:21,005 - mmdet - INFO - Epoch [3][4950/7033]	lr: 1.501e-04, eta: 9:27:06, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0703, loss_bbox: 0.2038, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2098, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2043, loss: 2.0138, grad_norm: 40.8916
2025-06-20 02:08:34,157 - mmdet - INFO - Epoch [3][5000/7033]	lr: 1.501e-04, eta: 9:25:52, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0765, loss_bbox: 0.2161, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3375, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2392, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2191, loss: 2.1129, grad_norm: 26.9815
2025-06-20 02:09:47,059 - mmdet - INFO - Epoch [3][5050/7033]	lr: 1.501e-04, eta: 9:24:38, time: 1.458, data_time: 0.028, memory: 20458, loss_cls: 0.0846, loss_bbox: 0.2180, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3361, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2216, loss: 2.1638, grad_norm: 53.6418
2025-06-20 02:11:00,245 - mmdet - INFO - Epoch [3][5100/7033]	lr: 1.501e-04, eta: 9:23:24, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0703, loss_bbox: 0.2046, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2076, loss: 2.0304, grad_norm: 32.5199
2025-06-20 02:12:13,205 - mmdet - INFO - Epoch [3][5150/7033]	lr: 1.501e-04, eta: 9:22:11, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0721, loss_bbox: 0.2095, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2138, loss: 2.0863, grad_norm: 49.3762
2025-06-20 02:13:26,194 - mmdet - INFO - Epoch [3][5200/7033]	lr: 1.501e-04, eta: 9:20:57, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0759, loss_bbox: 0.2100, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3405, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2512, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2360, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2141, loss: 2.0923, grad_norm: 31.7735
2025-06-20 02:14:39,268 - mmdet - INFO - Epoch [3][5250/7033]	lr: 1.501e-04, eta: 9:19:43, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0772, loss_bbox: 0.2124, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3356, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2329, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2150, loss: 2.0975, grad_norm: 42.6203
2025-06-20 02:15:57,787 - mmdet - INFO - Epoch [3][5300/7033]	lr: 1.501e-04, eta: 9:18:36, time: 1.570, data_time: 0.142, memory: 20458, loss_cls: 0.0743, loss_bbox: 0.2184, d0.loss_cls: 0.1938, d0.loss_bbox: 0.3450, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2552, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2210, loss: 2.1412, grad_norm: 24.9028
2025-06-20 02:17:10,836 - mmdet - INFO - Epoch [3][5350/7033]	lr: 1.501e-04, eta: 9:17:22, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0725, loss_bbox: 0.2043, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2054, loss: 2.0263, grad_norm: 34.0721
2025-06-20 02:19:01,857 - mmdet - INFO - Epoch [3][5400/7033]	lr: 1.501e-04, eta: 9:16:52, time: 2.220, data_time: 0.788, memory: 20458, loss_cls: 0.0844, loss_bbox: 0.2176, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3467, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2542, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2403, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2201, loss: 2.1704, grad_norm: 136.0261
2025-06-20 02:20:14,690 - mmdet - INFO - Epoch [3][5450/7033]	lr: 1.501e-04, eta: 9:15:38, time: 1.457, data_time: 0.025, memory: 20458, loss_cls: 0.0857, loss_bbox: 0.2126, d0.loss_cls: 0.1954, d0.loss_bbox: 0.3381, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2512, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2147, loss: 2.1616, grad_norm: 21.6289
2025-06-20 02:21:27,582 - mmdet - INFO - Epoch [3][5500/7033]	lr: 1.501e-04, eta: 9:14:24, time: 1.458, data_time: 0.028, memory: 20458, loss_cls: 0.0756, loss_bbox: 0.2112, d0.loss_cls: 0.1952, d0.loss_bbox: 0.3455, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2165, loss: 2.1352, grad_norm: 30.2673
2025-06-20 02:22:40,537 - mmdet - INFO - Epoch [3][5550/7033]	lr: 1.501e-04, eta: 9:13:10, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0747, loss_bbox: 0.2075, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3345, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2122, loss: 2.0608, grad_norm: 29.6907
2025-06-20 02:23:53,697 - mmdet - INFO - Epoch [3][5600/7033]	lr: 1.501e-04, eta: 9:11:56, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0763, loss_bbox: 0.2106, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2130, loss: 2.1156, grad_norm: 37.5528
2025-06-20 02:25:06,769 - mmdet - INFO - Epoch [3][5650/7033]	lr: 1.501e-04, eta: 9:10:42, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0729, loss_bbox: 0.2065, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2102, loss: 2.0994, grad_norm: 70.0596
2025-06-20 02:26:20,072 - mmdet - INFO - Epoch [3][5700/7033]	lr: 1.501e-04, eta: 9:09:28, time: 1.466, data_time: 0.035, memory: 20458, loss_cls: 0.0751, loss_bbox: 0.2106, d0.loss_cls: 0.1857, d0.loss_bbox: 0.3451, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2140, loss: 2.1215, grad_norm: 23.8340
2025-06-20 02:27:33,178 - mmdet - INFO - Epoch [3][5750/7033]	lr: 1.501e-04, eta: 9:08:14, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0830, loss_bbox: 0.2036, d0.loss_cls: 0.1948, d0.loss_bbox: 0.3522, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2544, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2330, d3.loss_cls: 0.0887, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2079, loss: 2.1472, grad_norm: 54.9704
2025-06-20 02:28:47,160 - mmdet - INFO - Epoch [3][5800/7033]	lr: 1.501e-04, eta: 9:07:01, time: 1.480, data_time: 0.048, memory: 20458, loss_cls: 0.0807, loss_bbox: 0.2123, d0.loss_cls: 0.1974, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2575, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2147, loss: 2.1861, grad_norm: 31.5798
2025-06-20 02:30:00,336 - mmdet - INFO - Epoch [3][5850/7033]	lr: 1.501e-04, eta: 9:05:48, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0773, loss_bbox: 0.2181, d0.loss_cls: 0.1938, d0.loss_bbox: 0.3654, d1.loss_cls: 0.1234, d1.loss_bbox: 0.2674, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2435, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2215, loss: 2.2063, grad_norm: 44.5097
2025-06-20 02:31:13,569 - mmdet - INFO - Epoch [3][5900/7033]	lr: 1.501e-04, eta: 9:04:34, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0817, loss_bbox: 0.2146, d0.loss_cls: 0.2005, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2567, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2425, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2172, loss: 2.1821, grad_norm: 22.7893
2025-06-20 02:32:26,767 - mmdet - INFO - Epoch [3][5950/7033]	lr: 1.501e-04, eta: 9:03:20, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0840, loss_bbox: 0.2124, d0.loss_cls: 0.1977, d0.loss_bbox: 0.3621, d1.loss_cls: 0.1291, d1.loss_bbox: 0.2588, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2241, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2182, loss: 2.2027, grad_norm: 28.2489
2025-06-20 02:33:39,955 - mmdet - INFO - Epoch [3][6000/7033]	lr: 1.501e-04, eta: 9:02:06, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0820, loss_bbox: 0.2116, d0.loss_cls: 0.1932, d0.loss_bbox: 0.3522, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2610, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2383, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2159, loss: 2.1712, grad_norm: 59.2655
2025-06-20 02:34:52,940 - mmdet - INFO - Epoch [3][6050/7033]	lr: 1.501e-04, eta: 9:00:52, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0831, loss_bbox: 0.2026, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1226, d1.loss_bbox: 0.2524, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2283, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0846, d4.loss_bbox: 0.2052, loss: 2.1104, grad_norm: 26.2095
2025-06-20 02:36:05,965 - mmdet - INFO - Epoch [3][6100/7033]	lr: 1.501e-04, eta: 8:59:38, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0804, loss_bbox: 0.2070, d0.loss_cls: 0.1968, d0.loss_bbox: 0.3310, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2462, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2090, loss: 2.0978, grad_norm: 73.1043
2025-06-20 02:37:19,857 - mmdet - INFO - Epoch [3][6150/7033]	lr: 1.501e-04, eta: 8:58:25, time: 1.478, data_time: 0.049, memory: 20458, loss_cls: 0.0897, loss_bbox: 0.2198, d0.loss_cls: 0.1876, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2643, d2.loss_cls: 0.1091, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0964, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2247, loss: 2.2388, grad_norm: 30.4184
2025-06-20 02:38:32,735 - mmdet - INFO - Epoch [3][6200/7033]	lr: 1.501e-04, eta: 8:57:11, time: 1.458, data_time: 0.026, memory: 20458, loss_cls: 0.0777, loss_bbox: 0.2096, d0.loss_cls: 0.1983, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1220, d1.loss_bbox: 0.2494, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2178, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2121, loss: 2.1138, grad_norm: 38.5984
2025-06-20 02:39:45,838 - mmdet - INFO - Epoch [3][6250/7033]	lr: 1.501e-04, eta: 8:55:57, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0789, loss_bbox: 0.2173, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3462, d1.loss_cls: 0.1264, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0974, d2.loss_bbox: 0.2406, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2197, loss: 2.1700, grad_norm: 708.0131
2025-06-20 02:40:58,786 - mmdet - INFO - Epoch [3][6300/7033]	lr: 1.501e-04, eta: 8:54:43, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0744, loss_bbox: 0.2165, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2587, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0779, d4.loss_bbox: 0.2170, loss: 2.1286, grad_norm: 38.4523
2025-06-20 02:42:13,522 - mmdet - INFO - Epoch [3][6350/7033]	lr: 1.501e-04, eta: 8:53:31, time: 1.495, data_time: 0.028, memory: 20458, loss_cls: 0.0809, loss_bbox: 0.2224, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2616, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2443, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2247, loss: 2.2038, grad_norm: 22.8014
2025-06-20 02:43:28,491 - mmdet - INFO - Epoch [3][6400/7033]	lr: 1.501e-04, eta: 8:52:19, time: 1.499, data_time: 0.028, memory: 20458, loss_cls: 0.0740, loss_bbox: 0.2131, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2342, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2161, loss: 2.0825, grad_norm: 28.2131
2025-06-20 02:44:41,588 - mmdet - INFO - Epoch [3][6450/7033]	lr: 1.501e-04, eta: 8:51:05, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0794, loss_bbox: 0.2165, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3538, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2620, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2240, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2186, loss: 2.1835, grad_norm: 56.8614
2025-06-20 02:45:54,550 - mmdet - INFO - Epoch [3][6500/7033]	lr: 1.501e-04, eta: 8:49:51, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0813, loss_bbox: 0.2188, d0.loss_cls: 0.1861, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2681, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2477, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2227, loss: 2.1991, grad_norm: 36.8594
2025-06-20 02:47:07,479 - mmdet - INFO - Epoch [3][6550/7033]	lr: 1.501e-04, eta: 8:48:37, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0837, loss_bbox: 0.2266, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3572, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2693, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2501, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2294, loss: 2.2411, grad_norm: 33.5200
2025-06-20 02:48:20,481 - mmdet - INFO - Epoch [3][6600/7033]	lr: 1.501e-04, eta: 8:47:23, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0756, loss_bbox: 0.2181, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2213, loss: 2.1490, grad_norm: 60.8114
2025-06-20 02:49:33,440 - mmdet - INFO - Epoch [3][6650/7033]	lr: 1.501e-04, eta: 8:46:09, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0763, loss_bbox: 0.2099, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3468, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2124, loss: 2.1068, grad_norm: 131.7085
2025-06-20 02:50:46,528 - mmdet - INFO - Epoch [3][6700/7033]	lr: 1.501e-04, eta: 8:44:55, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0745, loss_bbox: 0.2103, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3303, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2138, loss: 2.0762, grad_norm: 28.3026
2025-06-20 02:51:59,427 - mmdet - INFO - Epoch [3][6750/7033]	lr: 1.501e-04, eta: 8:43:41, time: 1.458, data_time: 0.030, memory: 20458, loss_cls: 0.0810, loss_bbox: 0.2186, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3456, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2573, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2382, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0820, d4.loss_bbox: 0.2218, loss: 2.1661, grad_norm: 26.6046
2025-06-20 02:53:12,397 - mmdet - INFO - Epoch [3][6800/7033]	lr: 1.501e-04, eta: 8:42:27, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0779, loss_bbox: 0.2109, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2129, loss: 2.1092, grad_norm: 102.9771
2025-06-20 02:54:25,467 - mmdet - INFO - Epoch [3][6850/7033]	lr: 1.501e-04, eta: 8:41:13, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0834, loss_bbox: 0.2237, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2444, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2273, loss: 2.2175, grad_norm: 30.0810
2025-06-20 02:55:38,690 - mmdet - INFO - Epoch [3][6900/7033]	lr: 1.501e-04, eta: 8:40:00, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0815, loss_bbox: 0.2173, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3390, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2577, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2378, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2206, loss: 2.1570, grad_norm: 27.2642
2025-06-20 02:56:51,777 - mmdet - INFO - Epoch [3][6950/7033]	lr: 1.501e-04, eta: 8:38:46, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0746, loss_bbox: 0.2095, d0.loss_cls: 0.1860, d0.loss_bbox: 0.3342, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2191, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2143, loss: 2.0923, grad_norm: 34.9920
2025-06-20 02:58:04,867 - mmdet - INFO - Epoch [3][7000/7033]	lr: 1.501e-04, eta: 8:37:32, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0734, loss_bbox: 0.2089, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2106, loss: 2.0762, grad_norm: 26.4225
2025-06-20 02:58:53,512 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-20 03:21:58,285 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-20 03:21:58,285 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7924, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8845, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9093, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9218, pts_bbox_NuScenes/car_trans_err: 0.1803, pts_bbox_NuScenes/car_scale_err: 0.1544, pts_bbox_NuScenes/car_orient_err: 0.0443, pts_bbox_NuScenes/car_vel_err: 0.2847, pts_bbox_NuScenes/car_attr_err: 0.1803, pts_bbox_NuScenes/mATE: 0.2924, pts_bbox_NuScenes/mASE: 0.2627, pts_bbox_NuScenes/mAOE: 0.2500, pts_bbox_NuScenes/mAVE: 0.2703, pts_bbox_NuScenes/mAAE: 0.1889, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4325, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6165, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7287, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7640, pts_bbox_NuScenes/truck_trans_err: 0.3405, pts_bbox_NuScenes/truck_scale_err: 0.1966, pts_bbox_NuScenes/truck_orient_err: 0.0491, pts_bbox_NuScenes/truck_vel_err: 0.2411, pts_bbox_NuScenes/truck_attr_err: 0.2037, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0596, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2079, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4088, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4844, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6629, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4253, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7915, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1122, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2976, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5243, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7478, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8975, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9197, pts_bbox_NuScenes/bus_trans_err: 0.3357, pts_bbox_NuScenes/bus_scale_err: 0.1976, pts_bbox_NuScenes/bus_orient_err: 0.0323, pts_bbox_NuScenes/bus_vel_err: 0.5011, pts_bbox_NuScenes/bus_attr_err: 0.2673, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1959, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4354, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5951, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6834, pts_bbox_NuScenes/trailer_trans_err: 0.4778, pts_bbox_NuScenes/trailer_scale_err: 0.2214, pts_bbox_NuScenes/trailer_orient_err: 0.4083, pts_bbox_NuScenes/trailer_vel_err: 0.2115, pts_bbox_NuScenes/trailer_attr_err: 0.1757, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5825, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6814, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7359, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7537, pts_bbox_NuScenes/barrier_trans_err: 0.2270, pts_bbox_NuScenes/barrier_scale_err: 0.2831, pts_bbox_NuScenes/barrier_orient_err: 0.0464, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6418, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7681, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8063, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8133, pts_bbox_NuScenes/motorcycle_trans_err: 0.2175, pts_bbox_NuScenes/motorcycle_scale_err: 0.2452, pts_bbox_NuScenes/motorcycle_orient_err: 0.2031, pts_bbox_NuScenes/motorcycle_vel_err: 0.3736, pts_bbox_NuScenes/motorcycle_attr_err: 0.2613, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5388, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6002, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6103, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6188, pts_bbox_NuScenes/bicycle_trans_err: 0.1810, pts_bbox_NuScenes/bicycle_scale_err: 0.2747, pts_bbox_NuScenes/bicycle_orient_err: 0.3279, pts_bbox_NuScenes/bicycle_vel_err: 0.2179, pts_bbox_NuScenes/bicycle_attr_err: 0.0052, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7981, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8460, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8700, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8832, pts_bbox_NuScenes/pedestrian_trans_err: 0.1569, pts_bbox_NuScenes/pedestrian_scale_err: 0.3005, pts_bbox_NuScenes/pedestrian_orient_err: 0.3474, pts_bbox_NuScenes/pedestrian_vel_err: 0.2200, pts_bbox_NuScenes/pedestrian_attr_err: 0.1204, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7176, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7609, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7918, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8151, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1445, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3285, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7091, pts_bbox_NuScenes/mAP: 0.6711
2025-06-20 03:23:21,203 - mmdet - INFO - Epoch [4][50/7033]	lr: 1.001e-04, eta: 8:34:46, time: 1.565, data_time: 0.133, memory: 20458, loss_cls: 0.0789, loss_bbox: 0.2042, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3404, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2090, loss: 2.0818, grad_norm: 32.4314
2025-06-20 03:24:34,165 - mmdet - INFO - Epoch [4][100/7033]	lr: 1.001e-04, eta: 8:33:33, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0728, loss_bbox: 0.1986, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3175, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2026, loss: 1.9818, grad_norm: 28.7215
2025-06-20 03:25:47,104 - mmdet - INFO - Epoch [4][150/7033]	lr: 1.001e-04, eta: 8:32:19, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0711, loss_bbox: 0.1997, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2045, loss: 1.9886, grad_norm: 21.3395
2025-06-20 03:27:00,055 - mmdet - INFO - Epoch [4][200/7033]	lr: 1.001e-04, eta: 8:31:05, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0700, loss_bbox: 0.2084, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2104, loss: 2.0348, grad_norm: 40.9244
2025-06-20 03:28:13,012 - mmdet - INFO - Epoch [4][250/7033]	lr: 1.001e-04, eta: 8:29:51, time: 1.459, data_time: 0.026, memory: 20458, loss_cls: 0.0731, loss_bbox: 0.1986, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3221, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2203, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2010, loss: 2.0068, grad_norm: 28.3440
2025-06-20 03:29:26,171 - mmdet - INFO - Epoch [4][300/7033]	lr: 1.001e-04, eta: 8:28:38, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0734, loss_bbox: 0.2003, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3373, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2469, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2042, loss: 2.0374, grad_norm: 20.3640
2025-06-20 03:30:39,239 - mmdet - INFO - Epoch [4][350/7033]	lr: 1.001e-04, eta: 8:27:24, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0740, loss_bbox: 0.2047, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3239, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2074, loss: 2.0377, grad_norm: 57.9824
2025-06-20 03:31:52,545 - mmdet - INFO - Epoch [4][400/7033]	lr: 1.001e-04, eta: 8:26:11, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0781, loss_bbox: 0.2056, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2077, loss: 2.0978, grad_norm: 25.7647
2025-06-20 03:33:05,877 - mmdet - INFO - Epoch [4][450/7033]	lr: 1.001e-04, eta: 8:24:57, time: 1.467, data_time: 0.028, memory: 20458, loss_cls: 0.0680, loss_bbox: 0.2007, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2045, loss: 1.9903, grad_norm: 22.3781
2025-06-20 03:34:19,067 - mmdet - INFO - Epoch [4][500/7033]	lr: 1.001e-04, eta: 8:23:44, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0735, loss_bbox: 0.2063, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2503, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2085, loss: 2.0664, grad_norm: 61.7859
2025-06-20 03:35:32,155 - mmdet - INFO - Epoch [4][550/7033]	lr: 1.001e-04, eta: 8:22:30, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0728, loss_bbox: 0.2061, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1116, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2079, loss: 2.0495, grad_norm: 44.8489
2025-06-20 03:36:45,128 - mmdet - INFO - Epoch [4][600/7033]	lr: 1.001e-04, eta: 8:21:16, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0760, loss_bbox: 0.1968, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3172, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2001, loss: 1.9960, grad_norm: 25.8705
2025-06-20 03:37:58,588 - mmdet - INFO - Epoch [4][650/7033]	lr: 1.001e-04, eta: 8:20:03, time: 1.469, data_time: 0.031, memory: 20458, loss_cls: 0.0756, loss_bbox: 0.2079, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3321, d1.loss_cls: 0.1185, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2123, loss: 2.0752, grad_norm: 30.7295
2025-06-20 03:39:11,997 - mmdet - INFO - Epoch [4][700/7033]	lr: 1.001e-04, eta: 8:18:50, time: 1.468, data_time: 0.032, memory: 20458, loss_cls: 0.0751, loss_bbox: 0.2057, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2092, loss: 2.0585, grad_norm: 20.1171
2025-06-20 03:40:25,079 - mmdet - INFO - Epoch [4][750/7033]	lr: 1.001e-04, eta: 8:17:36, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0731, loss_bbox: 0.2037, d0.loss_cls: 0.1707, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2141, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2079, loss: 2.0177, grad_norm: 21.0218
2025-06-20 03:41:38,358 - mmdet - INFO - Epoch [4][800/7033]	lr: 1.001e-04, eta: 8:16:23, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0766, loss_bbox: 0.2032, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2263, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2069, loss: 2.0361, grad_norm: 40.2830
2025-06-20 03:42:53,422 - mmdet - INFO - Epoch [4][850/7033]	lr: 1.001e-04, eta: 8:15:11, time: 1.501, data_time: 0.028, memory: 20458, loss_cls: 0.0710, loss_bbox: 0.2061, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2092, loss: 2.0372, grad_norm: 20.7837
2025-06-20 03:44:06,679 - mmdet - INFO - Epoch [4][900/7033]	lr: 1.001e-04, eta: 8:13:57, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0661, loss_bbox: 0.1980, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0676, d4.loss_bbox: 0.2007, loss: 1.9497, grad_norm: 127.0701
2025-06-20 03:45:19,786 - mmdet - INFO - Epoch [4][950/7033]	lr: 1.001e-04, eta: 8:12:44, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2125, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3362, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2149, loss: 2.0641, grad_norm: 30.8514
2025-06-20 03:46:32,740 - mmdet - INFO - Epoch [4][1000/7033]	lr: 1.001e-04, eta: 8:11:30, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0743, loss_bbox: 0.2127, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2153, loss: 2.0636, grad_norm: 29.4832
2025-06-20 03:47:45,962 - mmdet - INFO - Epoch [4][1050/7033]	lr: 1.001e-04, eta: 8:10:17, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0611, loss_bbox: 0.1967, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3172, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1996, loss: 1.9204, grad_norm: 41.1752
2025-06-20 03:48:59,147 - mmdet - INFO - Epoch [4][1100/7033]	lr: 1.001e-04, eta: 8:09:03, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0720, loss_bbox: 0.2073, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2103, loss: 2.0166, grad_norm: 22.0003
2025-06-20 03:50:12,364 - mmdet - INFO - Epoch [4][1150/7033]	lr: 1.001e-04, eta: 8:07:50, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0747, loss_bbox: 0.2002, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3330, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2242, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2031, loss: 2.0396, grad_norm: 32.5386
2025-06-20 03:51:25,609 - mmdet - INFO - Epoch [4][1200/7033]	lr: 1.001e-04, eta: 8:06:36, time: 1.465, data_time: 0.027, memory: 20458, loss_cls: 0.0710, loss_bbox: 0.2001, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3272, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2031, loss: 2.0170, grad_norm: 31.9110
2025-06-20 03:52:38,637 - mmdet - INFO - Epoch [4][1250/7033]	lr: 1.001e-04, eta: 8:05:22, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0784, loss_bbox: 0.2071, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3418, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2107, loss: 2.1036, grad_norm: 27.9677
2025-06-20 03:53:51,815 - mmdet - INFO - Epoch [4][1300/7033]	lr: 1.001e-04, eta: 8:04:09, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0782, loss_bbox: 0.2075, d0.loss_cls: 0.1843, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2102, loss: 2.0923, grad_norm: 23.4686
2025-06-20 03:55:04,952 - mmdet - INFO - Epoch [4][1350/7033]	lr: 1.001e-04, eta: 8:02:55, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0726, loss_bbox: 0.2008, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2047, loss: 2.0211, grad_norm: 73.4204
2025-06-20 03:56:18,053 - mmdet - INFO - Epoch [4][1400/7033]	lr: 1.001e-04, eta: 8:01:42, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0717, loss_bbox: 0.2056, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3248, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2075, loss: 2.0202, grad_norm: 24.0753
2025-06-20 03:57:31,087 - mmdet - INFO - Epoch [4][1450/7033]	lr: 1.001e-04, eta: 8:00:28, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0714, loss_bbox: 0.1989, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3287, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2015, loss: 2.0108, grad_norm: 62.0728
2025-06-20 03:58:45,250 - mmdet - INFO - Epoch [4][1500/7033]	lr: 1.001e-04, eta: 7:59:15, time: 1.483, data_time: 0.049, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2051, d0.loss_cls: 0.1767, d0.loss_bbox: 0.3422, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2299, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2080, loss: 2.0442, grad_norm: 40.7872
2025-06-20 03:59:58,629 - mmdet - INFO - Epoch [4][1550/7033]	lr: 1.001e-04, eta: 7:58:02, time: 1.468, data_time: 0.028, memory: 20458, loss_cls: 0.0677, loss_bbox: 0.2065, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3223, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2079, loss: 1.9955, grad_norm: 27.3925
2025-06-20 04:01:11,818 - mmdet - INFO - Epoch [4][1600/7033]	lr: 1.001e-04, eta: 7:56:49, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2097, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2510, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2122, loss: 2.0616, grad_norm: 65.3452
2025-06-20 04:02:24,955 - mmdet - INFO - Epoch [4][1650/7033]	lr: 1.001e-04, eta: 7:55:35, time: 1.463, data_time: 0.026, memory: 20458, loss_cls: 0.0683, loss_bbox: 0.2017, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3356, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2046, loss: 2.0013, grad_norm: 20.1306
2025-06-20 04:03:38,022 - mmdet - INFO - Epoch [4][1700/7033]	lr: 1.001e-04, eta: 7:54:21, time: 1.461, data_time: 0.030, memory: 20458, loss_cls: 0.0723, loss_bbox: 0.2095, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3424, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2128, loss: 2.0824, grad_norm: 100.0681
2025-06-20 04:04:51,231 - mmdet - INFO - Epoch [4][1750/7033]	lr: 1.001e-04, eta: 7:53:08, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0729, loss_bbox: 0.2014, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2429, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2061, loss: 2.0085, grad_norm: 23.5162
2025-06-20 04:06:04,070 - mmdet - INFO - Epoch [4][1800/7033]	lr: 1.001e-04, eta: 7:51:54, time: 1.457, data_time: 0.027, memory: 20458, loss_cls: 0.0664, loss_bbox: 0.2001, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3266, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2037, loss: 1.9718, grad_norm: 19.5743
2025-06-20 04:07:17,179 - mmdet - INFO - Epoch [4][1850/7033]	lr: 1.001e-04, eta: 7:50:41, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0734, loss_bbox: 0.2033, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3275, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2071, loss: 2.0337, grad_norm: 47.7073
2025-06-20 04:08:30,227 - mmdet - INFO - Epoch [4][1900/7033]	lr: 1.001e-04, eta: 7:49:27, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0786, loss_bbox: 0.2068, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3329, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2462, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2101, loss: 2.0684, grad_norm: 72526.2770
2025-06-20 04:09:43,072 - mmdet - INFO - Epoch [4][1950/7033]	lr: 1.001e-04, eta: 7:48:13, time: 1.457, data_time: 0.027, memory: 20458, loss_cls: 0.0772, loss_bbox: 0.2104, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3415, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2122, loss: 2.0987, grad_norm: 27.3987
2025-06-20 04:10:56,366 - mmdet - INFO - Epoch [4][2000/7033]	lr: 1.001e-04, eta: 7:47:00, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0730, loss_bbox: 0.2083, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3423, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2126, loss: 2.0594, grad_norm: 46.3635
2025-06-20 04:12:09,309 - mmdet - INFO - Epoch [4][2050/7033]	lr: 1.001e-04, eta: 7:45:46, time: 1.459, data_time: 0.030, memory: 20458, loss_cls: 0.0712, loss_bbox: 0.2076, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1083, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2105, loss: 2.0449, grad_norm: 20.0846
2025-06-20 04:13:22,595 - mmdet - INFO - Epoch [4][2100/7033]	lr: 1.001e-04, eta: 7:44:33, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0689, loss_bbox: 0.1982, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2208, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2013, loss: 1.9771, grad_norm: 25.8824
2025-06-20 04:14:35,485 - mmdet - INFO - Epoch [4][2150/7033]	lr: 1.001e-04, eta: 7:43:19, time: 1.458, data_time: 0.027, memory: 20458, loss_cls: 0.0784, loss_bbox: 0.2065, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1228, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0850, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2093, loss: 2.0764, grad_norm: 24.9968
2025-06-20 04:15:48,659 - mmdet - INFO - Epoch [4][2200/7033]	lr: 1.001e-04, eta: 7:42:05, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0730, loss_bbox: 0.2049, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3362, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2161, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2100, loss: 2.0547, grad_norm: 55.7684
2025-06-20 04:17:01,842 - mmdet - INFO - Epoch [4][2250/7033]	lr: 1.001e-04, eta: 7:40:52, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.1955, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3124, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0706, d4.loss_bbox: 0.1978, loss: 1.9389, grad_norm: 54.3382
2025-06-20 04:18:16,028 - mmdet - INFO - Epoch [4][2300/7033]	lr: 1.001e-04, eta: 7:39:39, time: 1.484, data_time: 0.049, memory: 20458, loss_cls: 0.0773, loss_bbox: 0.2098, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3242, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2147, loss: 2.0734, grad_norm: 20.7248
2025-06-20 04:19:29,050 - mmdet - INFO - Epoch [4][2350/7033]	lr: 1.001e-04, eta: 7:38:26, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.1969, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0713, d4.loss_bbox: 0.1997, loss: 1.9633, grad_norm: 35.2132
2025-06-20 04:20:42,228 - mmdet - INFO - Epoch [4][2400/7033]	lr: 1.001e-04, eta: 7:37:12, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0717, loss_bbox: 0.2066, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2471, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2094, loss: 2.0505, grad_norm: 57.4313
2025-06-20 04:21:55,335 - mmdet - INFO - Epoch [4][2450/7033]	lr: 1.001e-04, eta: 7:35:59, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0723, loss_bbox: 0.2087, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2150, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2124, loss: 2.0567, grad_norm: 30.7981
2025-06-20 04:23:10,390 - mmdet - INFO - Epoch [4][2500/7033]	lr: 1.001e-04, eta: 7:34:47, time: 1.501, data_time: 0.030, memory: 20458, loss_cls: 0.0731, loss_bbox: 0.2090, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2317, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2136, loss: 2.0724, grad_norm: 28.6965
2025-06-20 04:24:23,328 - mmdet - INFO - Epoch [4][2550/7033]	lr: 1.001e-04, eta: 7:33:33, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0789, loss_bbox: 0.2079, d0.loss_cls: 0.1851, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2112, loss: 2.0980, grad_norm: 32.6843
2025-06-20 04:25:36,222 - mmdet - INFO - Epoch [4][2600/7033]	lr: 1.001e-04, eta: 7:32:19, time: 1.458, data_time: 0.028, memory: 20458, loss_cls: 0.0742, loss_bbox: 0.2078, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3314, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2496, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2113, loss: 2.0642, grad_norm: 44.9434
2025-06-20 04:26:49,330 - mmdet - INFO - Epoch [4][2650/7033]	lr: 1.001e-04, eta: 7:31:06, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0711, loss_bbox: 0.2106, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3312, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2145, loss: 2.0575, grad_norm: 40.8119
2025-06-20 04:28:02,670 - mmdet - INFO - Epoch [4][2700/7033]	lr: 1.001e-04, eta: 7:29:52, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0739, loss_bbox: 0.2024, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2055, loss: 2.0100, grad_norm: 21.0434
2025-06-20 04:29:15,813 - mmdet - INFO - Epoch [4][2750/7033]	lr: 1.001e-04, eta: 7:28:39, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0714, loss_bbox: 0.2037, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2154, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2087, loss: 2.0255, grad_norm: 55.0437
2025-06-20 04:30:28,826 - mmdet - INFO - Epoch [4][2800/7033]	lr: 1.001e-04, eta: 7:27:25, time: 1.460, data_time: 0.026, memory: 20458, loss_cls: 0.0724, loss_bbox: 0.2010, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3302, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2034, loss: 2.0133, grad_norm: 37.3634
2025-06-20 04:31:42,211 - mmdet - INFO - Epoch [4][2850/7033]	lr: 1.001e-04, eta: 7:26:12, time: 1.468, data_time: 0.028, memory: 20458, loss_cls: 0.0730, loss_bbox: 0.2117, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3465, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2529, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2369, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2223, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2139, loss: 2.0969, grad_norm: 25.2080
2025-06-20 04:32:55,211 - mmdet - INFO - Epoch [4][2900/7033]	lr: 1.001e-04, eta: 7:24:58, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0706, loss_bbox: 0.2074, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2495, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2107, loss: 2.0413, grad_norm: 29.5095
2025-06-20 04:34:08,325 - mmdet - INFO - Epoch [4][2950/7033]	lr: 1.001e-04, eta: 7:23:45, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0682, loss_bbox: 0.2045, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0690, d4.loss_bbox: 0.2071, loss: 1.9978, grad_norm: 1897.5516
2025-06-20 04:35:21,381 - mmdet - INFO - Epoch [4][3000/7033]	lr: 1.001e-04, eta: 7:22:31, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0720, loss_bbox: 0.2129, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3382, d1.loss_cls: 0.1107, d1.loss_bbox: 0.2546, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2154, loss: 2.0841, grad_norm: 349.5307
2025-06-20 04:36:41,828 - mmdet - INFO - Epoch [4][3050/7033]	lr: 1.001e-04, eta: 7:21:23, time: 1.609, data_time: 0.027, memory: 20458, loss_cls: 0.0801, loss_bbox: 0.2103, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1155, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2132, loss: 2.1094, grad_norm: 37.3762
2025-06-20 04:37:54,653 - mmdet - INFO - Epoch [4][3100/7033]	lr: 1.001e-04, eta: 7:20:09, time: 1.456, data_time: 0.029, memory: 20458, loss_cls: 0.0722, loss_bbox: 0.2097, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3350, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2133, loss: 2.0600, grad_norm: 24.8073
2025-06-20 04:39:07,988 - mmdet - INFO - Epoch [4][3150/7033]	lr: 1.001e-04, eta: 7:18:56, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0733, loss_bbox: 0.2109, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3337, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2147, loss: 2.0643, grad_norm: 27.2712
2025-06-20 04:40:21,085 - mmdet - INFO - Epoch [4][3200/7033]	lr: 1.001e-04, eta: 7:17:42, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0727, loss_bbox: 0.2069, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3283, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2117, loss: 2.0459, grad_norm: 40.5771
2025-06-20 04:41:34,289 - mmdet - INFO - Epoch [4][3250/7033]	lr: 1.001e-04, eta: 7:16:29, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0712, loss_bbox: 0.2140, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3455, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2596, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2179, loss: 2.0984, grad_norm: 21.3871
2025-06-20 04:42:47,134 - mmdet - INFO - Epoch [4][3300/7033]	lr: 1.001e-04, eta: 7:15:15, time: 1.457, data_time: 0.026, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.1955, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3152, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0703, d4.loss_bbox: 0.1989, loss: 1.9511, grad_norm: 56.5737
2025-06-20 04:44:00,300 - mmdet - INFO - Epoch [4][3350/7033]	lr: 1.001e-04, eta: 7:14:02, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0749, loss_bbox: 0.2071, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3318, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2090, loss: 2.0488, grad_norm: 240.8568
2025-06-20 04:45:13,468 - mmdet - INFO - Epoch [4][3400/7033]	lr: 1.001e-04, eta: 7:12:48, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0692, loss_bbox: 0.2041, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3297, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2060, loss: 2.0128, grad_norm: 19.9495
2025-06-20 04:46:26,794 - mmdet - INFO - Epoch [4][3450/7033]	lr: 1.001e-04, eta: 7:11:35, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0765, loss_bbox: 0.2076, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3378, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2108, loss: 2.0818, grad_norm: 29.1341
2025-06-20 04:47:39,801 - mmdet - INFO - Epoch [4][3500/7033]	lr: 1.001e-04, eta: 7:10:21, time: 1.460, data_time: 0.027, memory: 20458, loss_cls: 0.0733, loss_bbox: 0.2031, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2061, loss: 2.0307, grad_norm: 28.4008
2025-06-20 04:48:53,013 - mmdet - INFO - Epoch [4][3550/7033]	lr: 1.001e-04, eta: 7:09:08, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0795, loss_bbox: 0.2125, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0820, d4.loss_bbox: 0.2161, loss: 2.1263, grad_norm: 18.9665
2025-06-20 04:50:07,753 - mmdet - INFO - Epoch [4][3600/7033]	lr: 1.001e-04, eta: 7:07:55, time: 1.495, data_time: 0.029, memory: 20458, loss_cls: 0.0797, loss_bbox: 0.2131, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2146, loss: 2.1117, grad_norm: 42.5663
2025-06-20 04:51:20,924 - mmdet - INFO - Epoch [4][3650/7033]	lr: 1.001e-04, eta: 7:06:42, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0724, loss_bbox: 0.2001, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3256, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2042, loss: 2.0109, grad_norm: 25.2236
2025-06-20 04:52:34,071 - mmdet - INFO - Epoch [4][3700/7033]	lr: 1.001e-04, eta: 7:05:28, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0721, loss_bbox: 0.2062, d0.loss_cls: 0.1820, d0.loss_bbox: 0.3344, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2458, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2150, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2097, loss: 2.0415, grad_norm: 20.8806
2025-06-20 04:53:46,932 - mmdet - INFO - Epoch [4][3750/7033]	lr: 1.001e-04, eta: 7:04:15, time: 1.457, data_time: 0.027, memory: 20458, loss_cls: 0.0707, loss_bbox: 0.2028, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3299, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2244, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2057, loss: 2.0043, grad_norm: 32.4658
2025-06-20 04:55:00,013 - mmdet - INFO - Epoch [4][3800/7033]	lr: 1.001e-04, eta: 7:03:01, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0753, loss_bbox: 0.2060, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2096, loss: 2.0442, grad_norm: 33.1196
2025-06-20 04:56:13,102 - mmdet - INFO - Epoch [4][3850/7033]	lr: 1.001e-04, eta: 7:01:47, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1965, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3083, d1.loss_cls: 0.1011, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1980, loss: 1.9150, grad_norm: 20.0866
2025-06-20 04:57:26,603 - mmdet - INFO - Epoch [4][3900/7033]	lr: 1.001e-04, eta: 7:00:34, time: 1.470, data_time: 0.029, memory: 20458, loss_cls: 0.0683, loss_bbox: 0.1986, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2026, loss: 1.9671, grad_norm: 26.5160
2025-06-20 04:58:39,793 - mmdet - INFO - Epoch [4][3950/7033]	lr: 1.001e-04, eta: 6:59:21, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0760, loss_bbox: 0.2082, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3273, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2112, loss: 2.0603, grad_norm: 24.7960
2025-06-20 04:59:52,996 - mmdet - INFO - Epoch [4][4000/7033]	lr: 1.001e-04, eta: 6:58:07, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0778, loss_bbox: 0.2118, d0.loss_cls: 0.1874, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2554, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2148, loss: 2.1155, grad_norm: 25.4405
2025-06-20 05:01:06,045 - mmdet - INFO - Epoch [4][4050/7033]	lr: 1.001e-04, eta: 6:56:54, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0739, loss_bbox: 0.2046, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2081, loss: 2.0429, grad_norm: 25.5834
2025-06-20 05:02:19,020 - mmdet - INFO - Epoch [4][4100/7033]	lr: 1.001e-04, eta: 6:55:40, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0742, loss_bbox: 0.2066, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3316, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2091, loss: 2.0635, grad_norm: 41.2914
2025-06-20 05:03:34,204 - mmdet - INFO - Epoch [4][4150/7033]	lr: 1.001e-04, eta: 6:54:28, time: 1.504, data_time: 0.029, memory: 20458, loss_cls: 0.0725, loss_bbox: 0.2043, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3209, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2073, loss: 2.0189, grad_norm: 26.6986
2025-06-20 05:04:49,037 - mmdet - INFO - Epoch [4][4200/7033]	lr: 1.001e-04, eta: 6:53:16, time: 1.497, data_time: 0.030, memory: 20458, loss_cls: 0.0706, loss_bbox: 0.2148, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2148, loss: 2.0571, grad_norm: 26.9679
2025-06-20 05:06:02,348 - mmdet - INFO - Epoch [4][4250/7033]	lr: 1.001e-04, eta: 6:52:02, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0730, loss_bbox: 0.2073, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2094, loss: 2.0594, grad_norm: 20.4039
2025-06-20 05:07:15,390 - mmdet - INFO - Epoch [4][4300/7033]	lr: 1.001e-04, eta: 6:50:49, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0839, loss_bbox: 0.2093, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2119, loss: 2.1459, grad_norm: 22.8452
2025-06-20 05:08:28,530 - mmdet - INFO - Epoch [4][4350/7033]	lr: 1.001e-04, eta: 6:49:35, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0738, loss_bbox: 0.2067, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3366, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2105, loss: 2.0634, grad_norm: 66.2871
2025-06-20 05:09:42,024 - mmdet - INFO - Epoch [4][4400/7033]	lr: 1.001e-04, eta: 6:48:22, time: 1.470, data_time: 0.030, memory: 20458, loss_cls: 0.0785, loss_bbox: 0.2130, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3426, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2544, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2161, loss: 2.1261, grad_norm: 28.2806
2025-06-20 05:10:55,068 - mmdet - INFO - Epoch [4][4450/7033]	lr: 1.001e-04, eta: 6:47:08, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0671, loss_bbox: 0.1972, d0.loss_cls: 0.1790, d0.loss_bbox: 0.3219, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0684, d4.loss_bbox: 0.2013, loss: 1.9686, grad_norm: 20.2888
2025-06-20 05:12:08,372 - mmdet - INFO - Epoch [4][4500/7033]	lr: 1.001e-04, eta: 6:45:55, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0679, loss_bbox: 0.1995, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3320, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2104, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2033, loss: 1.9929, grad_norm: 31.8729
2025-06-20 05:13:21,496 - mmdet - INFO - Epoch [4][4550/7033]	lr: 1.001e-04, eta: 6:44:41, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0711, loss_bbox: 0.2068, d0.loss_cls: 0.1836, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2085, loss: 2.0622, grad_norm: 48.2084
2025-06-20 05:14:34,556 - mmdet - INFO - Epoch [4][4600/7033]	lr: 1.001e-04, eta: 6:43:28, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0764, loss_bbox: 0.2160, d0.loss_cls: 0.1854, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2378, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2180, loss: 2.1196, grad_norm: 29.2771
2025-06-20 05:15:47,497 - mmdet - INFO - Epoch [4][4650/7033]	lr: 1.001e-04, eta: 6:42:14, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0696, loss_bbox: 0.2041, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3304, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2070, loss: 2.0151, grad_norm: 72.7763
2025-06-20 05:17:00,722 - mmdet - INFO - Epoch [4][4700/7033]	lr: 1.001e-04, eta: 6:41:01, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0726, loss_bbox: 0.2078, d0.loss_cls: 0.1915, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2510, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2139, loss: 2.0786, grad_norm: 21.2596
2025-06-20 05:18:15,461 - mmdet - INFO - Epoch [4][4750/7033]	lr: 1.001e-04, eta: 6:39:48, time: 1.495, data_time: 0.031, memory: 20458, loss_cls: 0.0765, loss_bbox: 0.2060, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2102, loss: 2.0669, grad_norm: 24.2612
2025-06-20 05:19:28,769 - mmdet - INFO - Epoch [4][4800/7033]	lr: 1.001e-04, eta: 6:38:35, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0754, loss_bbox: 0.2093, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2134, loss: 2.0674, grad_norm: 42.5673
2025-06-20 05:20:41,798 - mmdet - INFO - Epoch [4][4850/7033]	lr: 1.001e-04, eta: 6:37:21, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0702, loss_bbox: 0.2003, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3188, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2030, loss: 1.9701, grad_norm: 47.2960
2025-06-20 05:21:55,017 - mmdet - INFO - Epoch [4][4900/7033]	lr: 1.001e-04, eta: 6:36:08, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0782, loss_bbox: 0.2074, d0.loss_cls: 0.1874, d0.loss_bbox: 0.3319, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2490, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2110, loss: 2.0839, grad_norm: 28.0793
2025-06-20 05:24:09,672 - mmdet - INFO - Epoch [4][4950/7033]	lr: 1.001e-04, eta: 6:35:32, time: 2.693, data_time: 1.255, memory: 20458, loss_cls: 0.0668, loss_bbox: 0.2029, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2076, loss: 2.0060, grad_norm: 19.2523
2025-06-20 05:25:22,470 - mmdet - INFO - Epoch [4][5000/7033]	lr: 1.001e-04, eta: 6:34:18, time: 1.456, data_time: 0.028, memory: 20458, loss_cls: 0.0761, loss_bbox: 0.2131, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2162, loss: 2.0990, grad_norm: 25.3053
2025-06-20 05:26:35,451 - mmdet - INFO - Epoch [4][5050/7033]	lr: 1.001e-04, eta: 6:33:05, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0761, loss_bbox: 0.2042, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3231, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2283, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2072, loss: 2.0276, grad_norm: 19.3320
2025-06-20 05:27:49,080 - mmdet - INFO - Epoch [4][5100/7033]	lr: 1.001e-04, eta: 6:31:51, time: 1.473, data_time: 0.035, memory: 20458, loss_cls: 0.0812, loss_bbox: 0.2221, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2672, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2467, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2325, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2251, loss: 2.2045, grad_norm: 35.3092
2025-06-20 05:29:03,426 - mmdet - INFO - Epoch [4][5150/7033]	lr: 1.001e-04, eta: 6:30:38, time: 1.487, data_time: 0.054, memory: 20458, loss_cls: 0.0714, loss_bbox: 0.1987, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0768, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2009, loss: 1.9993, grad_norm: 67.7322
2025-06-20 05:30:16,767 - mmdet - INFO - Epoch [4][5200/7033]	lr: 1.001e-04, eta: 6:29:25, time: 1.467, data_time: 0.035, memory: 20458, loss_cls: 0.0678, loss_bbox: 0.1977, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2021, loss: 1.9708, grad_norm: 46.6983
2025-06-20 05:31:29,839 - mmdet - INFO - Epoch [4][5250/7033]	lr: 1.001e-04, eta: 6:28:11, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0707, loss_bbox: 0.1982, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3104, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2009, loss: 1.9524, grad_norm: 27.0587
2025-06-20 05:32:43,492 - mmdet - INFO - Epoch [4][5300/7033]	lr: 1.001e-04, eta: 6:26:58, time: 1.473, data_time: 0.037, memory: 20458, loss_cls: 0.0749, loss_bbox: 0.2057, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2431, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2283, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2091, loss: 2.0393, grad_norm: 76.8275
2025-06-20 05:33:56,742 - mmdet - INFO - Epoch [4][5350/7033]	lr: 1.001e-04, eta: 6:25:44, time: 1.465, data_time: 0.034, memory: 20458, loss_cls: 0.0687, loss_bbox: 0.2020, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2053, loss: 1.9947, grad_norm: 19.8976
2025-06-20 05:35:09,906 - mmdet - INFO - Epoch [4][5400/7033]	lr: 1.001e-04, eta: 6:24:30, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0729, loss_bbox: 0.2057, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3260, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2105, loss: 2.0345, grad_norm: 36.7400
2025-06-20 05:36:23,192 - mmdet - INFO - Epoch [4][5450/7033]	lr: 1.001e-04, eta: 6:23:17, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0678, loss_bbox: 0.1923, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3165, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0712, d4.loss_bbox: 0.1950, loss: 1.9255, grad_norm: 26.0460
2025-06-20 05:37:36,504 - mmdet - INFO - Epoch [4][5500/7033]	lr: 1.001e-04, eta: 6:22:03, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0672, loss_bbox: 0.2009, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2405, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2092, d4.loss_cls: 0.0691, d4.loss_bbox: 0.2040, loss: 1.9894, grad_norm: 22.5589
2025-06-20 05:38:49,328 - mmdet - INFO - Epoch [4][5550/7033]	lr: 1.001e-04, eta: 6:20:49, time: 1.456, data_time: 0.027, memory: 20458, loss_cls: 0.0739, loss_bbox: 0.2027, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2045, loss: 2.0203, grad_norm: 48.8695
2025-06-20 05:40:02,498 - mmdet - INFO - Epoch [4][5600/7033]	lr: 1.001e-04, eta: 6:19:36, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0706, loss_bbox: 0.1948, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3170, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2030, d4.loss_cls: 0.0739, d4.loss_bbox: 0.1987, loss: 1.9480, grad_norm: 161.9611
2025-06-20 05:41:15,939 - mmdet - INFO - Epoch [4][5650/7033]	lr: 1.001e-04, eta: 6:18:22, time: 1.469, data_time: 0.031, memory: 20458, loss_cls: 0.0741, loss_bbox: 0.2105, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2542, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2124, loss: 2.0962, grad_norm: 27.0729
2025-06-20 05:42:30,861 - mmdet - INFO - Epoch [4][5700/7033]	lr: 1.001e-04, eta: 6:17:10, time: 1.498, data_time: 0.031, memory: 20458, loss_cls: 0.0769, loss_bbox: 0.2043, d0.loss_cls: 0.1791, d0.loss_bbox: 0.3347, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2090, loss: 2.0783, grad_norm: 43.1639
2025-06-20 05:43:43,997 - mmdet - INFO - Epoch [4][5750/7033]	lr: 1.001e-04, eta: 6:15:56, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0726, loss_bbox: 0.2115, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3365, d1.loss_cls: 0.1133, d1.loss_bbox: 0.2527, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2152, loss: 2.0768, grad_norm: 58.6097
2025-06-20 05:44:56,927 - mmdet - INFO - Epoch [4][5800/7033]	lr: 1.001e-04, eta: 6:14:42, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0763, loss_bbox: 0.2012, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2446, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2037, loss: 2.0322, grad_norm: 40.7902
2025-06-20 05:46:10,222 - mmdet - INFO - Epoch [4][5850/7033]	lr: 1.001e-04, eta: 6:13:28, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0761, loss_bbox: 0.2163, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2191, loss: 2.1251, grad_norm: 23.2301
2025-06-20 05:47:23,963 - mmdet - INFO - Epoch [4][5900/7033]	lr: 1.001e-04, eta: 6:12:15, time: 1.475, data_time: 0.039, memory: 20458, loss_cls: 0.0718, loss_bbox: 0.2033, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2242, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2135, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2061, loss: 2.0144, grad_norm: 33.0422
2025-06-20 05:48:37,302 - mmdet - INFO - Epoch [4][5950/7033]	lr: 1.001e-04, eta: 6:11:02, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0740, loss_bbox: 0.1975, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3301, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2021, loss: 2.0157, grad_norm: 30.3993
2025-06-20 05:49:50,481 - mmdet - INFO - Epoch [4][6000/7033]	lr: 1.001e-04, eta: 6:09:48, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0700, loss_bbox: 0.2004, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2051, loss: 1.9964, grad_norm: 28.8467
2025-06-20 05:51:03,539 - mmdet - INFO - Epoch [4][6050/7033]	lr: 1.001e-04, eta: 6:08:34, time: 1.461, data_time: 0.030, memory: 20458, loss_cls: 0.0731, loss_bbox: 0.1997, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0872, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2041, loss: 2.0219, grad_norm: 156.3930
2025-06-20 05:52:17,012 - mmdet - INFO - Epoch [4][6100/7033]	lr: 1.001e-04, eta: 6:07:21, time: 1.469, data_time: 0.031, memory: 20458, loss_cls: 0.0709, loss_bbox: 0.1992, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2033, loss: 1.9818, grad_norm: 35.0117
2025-06-20 05:53:30,186 - mmdet - INFO - Epoch [4][6150/7033]	lr: 1.001e-04, eta: 6:06:07, time: 1.463, data_time: 0.031, memory: 20458, loss_cls: 0.0754, loss_bbox: 0.2165, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1140, d1.loss_bbox: 0.2552, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2178, loss: 2.1098, grad_norm: 22.2339
2025-06-20 05:54:43,600 - mmdet - INFO - Epoch [4][6200/7033]	lr: 1.001e-04, eta: 6:04:54, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0738, loss_bbox: 0.2042, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2065, loss: 2.0221, grad_norm: 20.5080
2025-06-20 05:55:56,814 - mmdet - INFO - Epoch [4][6250/7033]	lr: 1.001e-04, eta: 6:03:40, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0734, loss_bbox: 0.2027, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3299, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2059, loss: 2.0172, grad_norm: 19.1079
2025-06-20 05:57:11,558 - mmdet - INFO - Epoch [4][6300/7033]	lr: 1.001e-04, eta: 6:02:27, time: 1.495, data_time: 0.029, memory: 20458, loss_cls: 0.0742, loss_bbox: 0.2081, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2123, loss: 2.0681, grad_norm: 31.6218
2025-06-20 05:58:25,103 - mmdet - INFO - Epoch [4][6350/7033]	lr: 1.001e-04, eta: 6:01:14, time: 1.471, data_time: 0.029, memory: 20458, loss_cls: 0.0699, loss_bbox: 0.2077, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3338, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2118, loss: 2.0418, grad_norm: 63.2154
2025-06-20 05:59:40,298 - mmdet - INFO - Epoch [4][6400/7033]	lr: 1.001e-04, eta: 6:00:01, time: 1.504, data_time: 0.029, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2068, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3221, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2243, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2074, loss: 2.0156, grad_norm: 22.7723
2025-06-20 06:00:53,471 - mmdet - INFO - Epoch [4][6450/7033]	lr: 1.001e-04, eta: 5:58:48, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0701, loss_bbox: 0.2030, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3185, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2061, loss: 1.9914, grad_norm: 51.9797
2025-06-20 06:02:06,498 - mmdet - INFO - Epoch [4][6500/7033]	lr: 1.001e-04, eta: 5:57:34, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2071, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2312, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2105, loss: 2.0405, grad_norm: 31.3142
2025-06-20 06:03:19,845 - mmdet - INFO - Epoch [4][6550/7033]	lr: 1.001e-04, eta: 5:56:20, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.2023, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3182, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2104, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2049, loss: 1.9657, grad_norm: 62.5757
2025-06-20 06:04:32,949 - mmdet - INFO - Epoch [4][6600/7033]	lr: 1.001e-04, eta: 5:55:07, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0754, loss_bbox: 0.2023, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3161, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2050, loss: 1.9970, grad_norm: 25.5611
2025-06-20 06:05:46,371 - mmdet - INFO - Epoch [4][6650/7033]	lr: 1.001e-04, eta: 5:53:53, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0676, loss_bbox: 0.1986, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3192, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2037, loss: 1.9571, grad_norm: 23.3609
2025-06-20 06:06:59,186 - mmdet - INFO - Epoch [4][6700/7033]	lr: 1.001e-04, eta: 5:52:39, time: 1.456, data_time: 0.027, memory: 20458, loss_cls: 0.0784, loss_bbox: 0.2060, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3257, d1.loss_cls: 0.1139, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2101, loss: 2.0677, grad_norm: 39.0975
2025-06-20 06:08:12,281 - mmdet - INFO - Epoch [4][6750/7033]	lr: 1.001e-04, eta: 5:51:26, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0777, loss_bbox: 0.2126, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3404, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2533, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2155, loss: 2.1195, grad_norm: 174.1635
2025-06-20 06:09:25,424 - mmdet - INFO - Epoch [4][6800/7033]	lr: 1.001e-04, eta: 5:50:12, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0675, loss_bbox: 0.2017, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3309, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2054, loss: 1.9932, grad_norm: 24.5733
2025-06-20 06:10:38,424 - mmdet - INFO - Epoch [4][6850/7033]	lr: 1.001e-04, eta: 5:48:58, time: 1.460, data_time: 0.031, memory: 20458, loss_cls: 0.0659, loss_bbox: 0.2012, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3225, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2050, loss: 1.9678, grad_norm: 31.7232
2025-06-20 06:11:51,550 - mmdet - INFO - Epoch [4][6900/7033]	lr: 1.001e-04, eta: 5:47:45, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0735, loss_bbox: 0.2063, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2317, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2092, loss: 2.0487, grad_norm: 61.0787
2025-06-20 06:13:04,524 - mmdet - INFO - Epoch [4][6950/7033]	lr: 1.001e-04, eta: 5:46:31, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0685, loss_bbox: 0.2086, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3296, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2109, loss: 2.0129, grad_norm: 25.0794
2025-06-20 06:14:18,114 - mmdet - INFO - Epoch [4][7000/7033]	lr: 1.001e-04, eta: 5:45:17, time: 1.472, data_time: 0.031, memory: 20458, loss_cls: 0.0706, loss_bbox: 0.1971, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3215, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0732, d4.loss_bbox: 0.1997, loss: 1.9708, grad_norm: 27.9852
2025-06-20 06:15:08,545 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-06-20 06:38:20,083 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-20 06:38:20,083 - mmdet - INFO - Epoch(val) [4][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7946, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8853, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9108, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9229, pts_bbox_NuScenes/car_trans_err: 0.1756, pts_bbox_NuScenes/car_scale_err: 0.1524, pts_bbox_NuScenes/car_orient_err: 0.0430, pts_bbox_NuScenes/car_vel_err: 0.2800, pts_bbox_NuScenes/car_attr_err: 0.1927, pts_bbox_NuScenes/mATE: 0.2866, pts_bbox_NuScenes/mASE: 0.2607, pts_bbox_NuScenes/mAOE: 0.2403, pts_bbox_NuScenes/mAVE: 0.2678, pts_bbox_NuScenes/mAAE: 0.1853, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4288, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6240, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7268, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7614, pts_bbox_NuScenes/truck_trans_err: 0.3379, pts_bbox_NuScenes/truck_scale_err: 0.1931, pts_bbox_NuScenes/truck_orient_err: 0.0475, pts_bbox_NuScenes/truck_vel_err: 0.2573, pts_bbox_NuScenes/truck_attr_err: 0.2089, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0630, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2093, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4101, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4824, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6496, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4292, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7604, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1068, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3037, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5072, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7385, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9053, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9274, pts_bbox_NuScenes/bus_trans_err: 0.3491, pts_bbox_NuScenes/bus_scale_err: 0.1908, pts_bbox_NuScenes/bus_orient_err: 0.0363, pts_bbox_NuScenes/bus_vel_err: 0.4686, pts_bbox_NuScenes/bus_attr_err: 0.2343, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1829, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4291, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5970, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6857, pts_bbox_NuScenes/trailer_trans_err: 0.4740, pts_bbox_NuScenes/trailer_scale_err: 0.2138, pts_bbox_NuScenes/trailer_orient_err: 0.4309, pts_bbox_NuScenes/trailer_vel_err: 0.2223, pts_bbox_NuScenes/trailer_attr_err: 0.1750, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6001, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6992, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7439, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7585, pts_bbox_NuScenes/barrier_trans_err: 0.2143, pts_bbox_NuScenes/barrier_scale_err: 0.2870, pts_bbox_NuScenes/barrier_orient_err: 0.0438, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6324, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7700, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8046, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8131, pts_bbox_NuScenes/motorcycle_trans_err: 0.2160, pts_bbox_NuScenes/motorcycle_scale_err: 0.2462, pts_bbox_NuScenes/motorcycle_orient_err: 0.1983, pts_bbox_NuScenes/motorcycle_vel_err: 0.3861, pts_bbox_NuScenes/motorcycle_attr_err: 0.2494, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5359, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5951, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6045, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6124, pts_bbox_NuScenes/bicycle_trans_err: 0.1736, pts_bbox_NuScenes/bicycle_scale_err: 0.2748, pts_bbox_NuScenes/bicycle_orient_err: 0.2735, pts_bbox_NuScenes/bicycle_vel_err: 0.2083, pts_bbox_NuScenes/bicycle_attr_err: 0.0049, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8146, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8562, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8757, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8873, pts_bbox_NuScenes/pedestrian_trans_err: 0.1474, pts_bbox_NuScenes/pedestrian_scale_err: 0.2931, pts_bbox_NuScenes/pedestrian_orient_err: 0.3288, pts_bbox_NuScenes/pedestrian_vel_err: 0.2134, pts_bbox_NuScenes/pedestrian_attr_err: 0.1135, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7362, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7705, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7973, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8198, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1290, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3267, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7124, pts_bbox_NuScenes/mAP: 0.6730
2025-06-20 06:39:43,449 - mmdet - INFO - Epoch [5][50/7033]	lr: 5.015e-05, eta: 5:42:54, time: 1.563, data_time: 0.126, memory: 20458, loss_cls: 0.0733, loss_bbox: 0.2036, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2066, loss: 2.0245, grad_norm: 27.9165
2025-06-20 06:40:56,809 - mmdet - INFO - Epoch [5][100/7033]	lr: 5.015e-05, eta: 5:41:40, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0685, loss_bbox: 0.1954, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3182, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2065, d4.loss_cls: 0.0704, d4.loss_bbox: 0.1980, loss: 1.9451, grad_norm: 18.4894
2025-06-20 06:42:10,072 - mmdet - INFO - Epoch [5][150/7033]	lr: 5.015e-05, eta: 5:40:27, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0773, loss_bbox: 0.2054, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0826, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2097, loss: 2.0644, grad_norm: 20.3394
2025-06-20 06:43:23,393 - mmdet - INFO - Epoch [5][200/7033]	lr: 5.015e-05, eta: 5:39:13, time: 1.466, data_time: 0.031, memory: 20458, loss_cls: 0.0690, loss_bbox: 0.2004, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3202, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2093, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2036, loss: 1.9729, grad_norm: 22.7866
2025-06-20 06:44:36,189 - mmdet - INFO - Epoch [5][250/7033]	lr: 5.015e-05, eta: 5:38:00, time: 1.456, data_time: 0.027, memory: 20458, loss_cls: 0.0676, loss_bbox: 0.2021, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3182, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2138, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2070, loss: 1.9749, grad_norm: 37.7316
2025-06-20 06:45:49,339 - mmdet - INFO - Epoch [5][300/7033]	lr: 5.015e-05, eta: 5:36:46, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0728, loss_bbox: 0.2032, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3200, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2130, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2062, loss: 1.9996, grad_norm: 43.7667
2025-06-20 06:47:02,381 - mmdet - INFO - Epoch [5][350/7033]	lr: 5.015e-05, eta: 5:35:33, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.2000, d0.loss_cls: 0.1820, d0.loss_bbox: 0.3197, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2023, loss: 1.9686, grad_norm: 23.9083
2025-06-20 06:48:15,678 - mmdet - INFO - Epoch [5][400/7033]	lr: 5.015e-05, eta: 5:34:19, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0590, loss_bbox: 0.1940, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3162, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0666, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1957, loss: 1.8885, grad_norm: 81.9549
2025-06-20 06:49:31,096 - mmdet - INFO - Epoch [5][450/7033]	lr: 5.015e-05, eta: 5:33:07, time: 1.508, data_time: 0.028, memory: 20458, loss_cls: 0.0710, loss_bbox: 0.2025, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3171, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2049, loss: 1.9881, grad_norm: 23.1770
2025-06-20 06:50:44,277 - mmdet - INFO - Epoch [5][500/7033]	lr: 5.015e-05, eta: 5:31:53, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0747, loss_bbox: 0.2062, d0.loss_cls: 0.1836, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2093, loss: 2.0592, grad_norm: 23.2940
2025-06-20 06:51:57,568 - mmdet - INFO - Epoch [5][550/7033]	lr: 5.015e-05, eta: 5:30:40, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0673, loss_bbox: 0.1965, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3119, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1996, loss: 1.9338, grad_norm: 24.0286
2025-06-20 06:53:10,905 - mmdet - INFO - Epoch [5][600/7033]	lr: 5.015e-05, eta: 5:29:26, time: 1.467, data_time: 0.032, memory: 20458, loss_cls: 0.0708, loss_bbox: 0.1970, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3226, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2001, loss: 1.9891, grad_norm: 37.2995
2025-06-20 06:54:24,281 - mmdet - INFO - Epoch [5][650/7033]	lr: 5.015e-05, eta: 5:28:13, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0690, loss_bbox: 0.2019, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3173, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2052, loss: 1.9698, grad_norm: 48.1387
2025-06-20 06:55:37,554 - mmdet - INFO - Epoch [5][700/7033]	lr: 5.015e-05, eta: 5:27:00, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1956, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3177, d1.loss_cls: 0.0955, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0763, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1986, loss: 1.9092, grad_norm: 32.1919
2025-06-20 06:56:50,722 - mmdet - INFO - Epoch [5][750/7033]	lr: 5.015e-05, eta: 5:25:46, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0673, loss_bbox: 0.1915, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2026, d4.loss_cls: 0.0695, d4.loss_bbox: 0.1943, loss: 1.9183, grad_norm: 60.2836
2025-06-20 06:58:04,053 - mmdet - INFO - Epoch [5][800/7033]	lr: 5.015e-05, eta: 5:24:33, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0680, loss_bbox: 0.2003, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2220, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2047, loss: 1.9694, grad_norm: 32.6878
2025-06-20 06:59:17,145 - mmdet - INFO - Epoch [5][850/7033]	lr: 5.015e-05, eta: 5:23:19, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0678, loss_bbox: 0.2037, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3256, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2070, loss: 1.9844, grad_norm: 84.3697
2025-06-20 07:00:30,349 - mmdet - INFO - Epoch [5][900/7033]	lr: 5.015e-05, eta: 5:22:06, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0671, loss_bbox: 0.1970, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3122, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2005, loss: 1.9359, grad_norm: 21.6989
2025-06-20 07:01:43,434 - mmdet - INFO - Epoch [5][950/7033]	lr: 5.015e-05, eta: 5:20:52, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0564, loss_bbox: 0.1868, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0713, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0617, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0588, d4.loss_bbox: 0.1903, loss: 1.8254, grad_norm: 30.1181
2025-06-20 07:02:56,548 - mmdet - INFO - Epoch [5][1000/7033]	lr: 5.015e-05, eta: 5:19:39, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0720, loss_bbox: 0.2006, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3136, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0785, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2039, loss: 1.9863, grad_norm: 21.4753
2025-06-20 07:04:09,633 - mmdet - INFO - Epoch [5][1050/7033]	lr: 5.015e-05, eta: 5:18:25, time: 1.462, data_time: 0.031, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1957, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2202, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0703, d4.loss_bbox: 0.1988, loss: 1.9399, grad_norm: 29.7731
2025-06-20 07:05:22,907 - mmdet - INFO - Epoch [5][1100/7033]	lr: 5.015e-05, eta: 5:17:12, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0646, loss_bbox: 0.1931, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3175, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1977, loss: 1.9078, grad_norm: 25.2825
2025-06-20 07:06:36,621 - mmdet - INFO - Epoch [5][1150/7033]	lr: 5.015e-05, eta: 5:15:58, time: 1.474, data_time: 0.032, memory: 20458, loss_cls: 0.0612, loss_bbox: 0.1897, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3036, d1.loss_cls: 0.0971, d1.loss_bbox: 0.2274, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0674, d3.loss_bbox: 0.1979, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1920, loss: 1.8447, grad_norm: 15.6131
2025-06-20 07:07:49,626 - mmdet - INFO - Epoch [5][1200/7033]	lr: 5.015e-05, eta: 5:14:45, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1897, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3094, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2000, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1920, loss: 1.8861, grad_norm: 70.3944
2025-06-20 07:09:02,721 - mmdet - INFO - Epoch [5][1250/7033]	lr: 5.015e-05, eta: 5:13:31, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0623, loss_bbox: 0.1967, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3153, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2066, d4.loss_cls: 0.0642, d4.loss_bbox: 0.2005, loss: 1.9197, grad_norm: 180.4154
2025-06-20 07:10:15,783 - mmdet - INFO - Epoch [5][1300/7033]	lr: 5.015e-05, eta: 5:12:18, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0638, loss_bbox: 0.1935, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3127, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2048, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1976, loss: 1.8961, grad_norm: 18.7193
2025-06-20 07:11:28,947 - mmdet - INFO - Epoch [5][1350/7033]	lr: 5.015e-05, eta: 5:11:04, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0725, loss_bbox: 0.2021, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2047, loss: 2.0057, grad_norm: 41.9082
2025-06-20 07:12:42,026 - mmdet - INFO - Epoch [5][1400/7033]	lr: 5.015e-05, eta: 5:09:51, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0635, loss_bbox: 0.1887, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3085, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1914, loss: 1.8775, grad_norm: 35.9772
2025-06-20 07:13:55,161 - mmdet - INFO - Epoch [5][1450/7033]	lr: 5.015e-05, eta: 5:08:37, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0708, loss_bbox: 0.2032, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2055, loss: 1.9989, grad_norm: 21.1171
2025-06-20 07:15:08,198 - mmdet - INFO - Epoch [5][1500/7033]	lr: 5.015e-05, eta: 5:07:24, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0700, loss_bbox: 0.1994, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3152, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2017, loss: 1.9552, grad_norm: 65.4273
2025-06-20 07:16:21,428 - mmdet - INFO - Epoch [5][1550/7033]	lr: 5.015e-05, eta: 5:06:10, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0683, loss_bbox: 0.2011, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3176, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2246, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2057, loss: 1.9750, grad_norm: 18.9264
2025-06-20 07:17:34,500 - mmdet - INFO - Epoch [5][1600/7033]	lr: 5.015e-05, eta: 5:04:57, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0682, loss_bbox: 0.1981, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3118, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2010, loss: 1.9491, grad_norm: 48.5613
2025-06-20 07:18:47,675 - mmdet - INFO - Epoch [5][1650/7033]	lr: 5.015e-05, eta: 5:03:43, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0660, loss_bbox: 0.2041, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2405, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0690, d4.loss_bbox: 0.2072, loss: 1.9784, grad_norm: 18.8920
2025-06-20 07:20:00,738 - mmdet - INFO - Epoch [5][1700/7033]	lr: 5.015e-05, eta: 5:02:30, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0646, loss_bbox: 0.1953, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3169, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1982, loss: 1.9341, grad_norm: 24.4306
2025-06-20 07:21:13,906 - mmdet - INFO - Epoch [5][1750/7033]	lr: 5.015e-05, eta: 5:01:16, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0708, loss_bbox: 0.1965, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3079, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0715, d4.loss_bbox: 0.1997, loss: 1.9441, grad_norm: 21.0256
2025-06-20 07:22:27,302 - mmdet - INFO - Epoch [5][1800/7033]	lr: 5.015e-05, eta: 5:00:03, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0690, loss_bbox: 0.2036, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3190, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2072, loss: 1.9830, grad_norm: 24.6883
2025-06-20 07:23:40,423 - mmdet - INFO - Epoch [5][1850/7033]	lr: 5.015e-05, eta: 4:58:49, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0630, loss_bbox: 0.1965, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2005, loss: 1.8982, grad_norm: 21.7410
2025-06-20 07:24:53,577 - mmdet - INFO - Epoch [5][1900/7033]	lr: 5.015e-05, eta: 4:57:36, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0687, loss_bbox: 0.2049, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3226, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2081, loss: 1.9996, grad_norm: 27.1429
2025-06-20 07:26:07,038 - mmdet - INFO - Epoch [5][1950/7033]	lr: 5.015e-05, eta: 4:56:22, time: 1.469, data_time: 0.029, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2029, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3188, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2072, loss: 1.9941, grad_norm: 122.6092
2025-06-20 07:27:20,438 - mmdet - INFO - Epoch [5][2000/7033]	lr: 5.015e-05, eta: 4:55:09, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0644, loss_bbox: 0.1999, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3270, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2243, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0657, d4.loss_bbox: 0.2030, loss: 1.9511, grad_norm: 23.8170
2025-06-20 07:28:35,585 - mmdet - INFO - Epoch [5][2050/7033]	lr: 5.015e-05, eta: 4:53:56, time: 1.503, data_time: 0.033, memory: 20458, loss_cls: 0.0704, loss_bbox: 0.1999, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3156, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2007, loss: 1.9626, grad_norm: 32.6083
2025-06-20 07:29:48,617 - mmdet - INFO - Epoch [5][2100/7033]	lr: 5.015e-05, eta: 4:52:43, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0677, loss_bbox: 0.1963, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0700, d4.loss_bbox: 0.1988, loss: 1.9644, grad_norm: 113.8955
2025-06-20 07:31:01,749 - mmdet - INFO - Epoch [5][2150/7033]	lr: 5.015e-05, eta: 4:51:29, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1991, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3239, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2100, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2026, loss: 1.9498, grad_norm: 33.9910
2025-06-20 07:32:14,898 - mmdet - INFO - Epoch [5][2200/7033]	lr: 5.015e-05, eta: 4:50:16, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.1954, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0678, d4.loss_bbox: 0.1988, loss: 1.9107, grad_norm: 28.3877
2025-06-20 07:33:28,319 - mmdet - INFO - Epoch [5][2250/7033]	lr: 5.015e-05, eta: 4:49:03, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1983, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3127, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2066, d4.loss_cls: 0.0656, d4.loss_bbox: 0.2007, loss: 1.9007, grad_norm: 19.1936
2025-06-20 07:34:41,745 - mmdet - INFO - Epoch [5][2300/7033]	lr: 5.015e-05, eta: 4:47:49, time: 1.469, data_time: 0.030, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.1975, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3263, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2008, loss: 1.9700, grad_norm: 27.9239
2025-06-20 07:35:55,027 - mmdet - INFO - Epoch [5][2350/7033]	lr: 5.015e-05, eta: 4:46:36, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0672, loss_bbox: 0.1974, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3169, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2003, loss: 1.9232, grad_norm: 28.2757
2025-06-20 07:37:08,256 - mmdet - INFO - Epoch [5][2400/7033]	lr: 5.015e-05, eta: 4:45:22, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0657, loss_bbox: 0.1925, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3198, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2339, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2024, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1957, loss: 1.9038, grad_norm: 17.8736
2025-06-20 07:38:21,668 - mmdet - INFO - Epoch [5][2450/7033]	lr: 5.015e-05, eta: 4:44:09, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0685, loss_bbox: 0.2028, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2066, loss: 1.9936, grad_norm: 27.2556
2025-06-20 07:39:35,183 - mmdet - INFO - Epoch [5][2500/7033]	lr: 5.015e-05, eta: 4:42:56, time: 1.470, data_time: 0.028, memory: 20458, loss_cls: 0.0605, loss_bbox: 0.1887, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3039, d1.loss_cls: 0.0905, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0649, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1923, loss: 1.8332, grad_norm: 33.6030
2025-06-20 07:40:47,952 - mmdet - INFO - Epoch [5][2550/7033]	lr: 5.015e-05, eta: 4:41:42, time: 1.455, data_time: 0.027, memory: 20458, loss_cls: 0.0693, loss_bbox: 0.1929, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3137, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2308, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0701, d4.loss_bbox: 0.1943, loss: 1.9123, grad_norm: 21.0317
2025-06-20 07:42:01,298 - mmdet - INFO - Epoch [5][2600/7033]	lr: 5.015e-05, eta: 4:40:29, time: 1.467, data_time: 0.028, memory: 20458, loss_cls: 0.0733, loss_bbox: 0.2017, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2045, loss: 1.9971, grad_norm: 24.3870
2025-06-20 07:43:14,971 - mmdet - INFO - Epoch [5][2650/7033]	lr: 5.015e-05, eta: 4:39:15, time: 1.473, data_time: 0.037, memory: 20458, loss_cls: 0.0662, loss_bbox: 0.1979, d0.loss_cls: 0.1711, d0.loss_bbox: 0.3164, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2368, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2013, loss: 1.9377, grad_norm: 28.0216
2025-06-20 07:44:28,125 - mmdet - INFO - Epoch [5][2700/7033]	lr: 5.015e-05, eta: 4:38:02, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0646, loss_bbox: 0.1962, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3130, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1998, loss: 1.9171, grad_norm: 28.2180
2025-06-20 07:45:41,255 - mmdet - INFO - Epoch [5][2750/7033]	lr: 5.015e-05, eta: 4:36:48, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0730, loss_bbox: 0.2030, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3197, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2098, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2050, loss: 1.9971, grad_norm: 37.2874
2025-06-20 07:46:54,565 - mmdet - INFO - Epoch [5][2800/7033]	lr: 5.015e-05, eta: 4:35:35, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0668, loss_bbox: 0.2005, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3184, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0683, d4.loss_bbox: 0.2032, loss: 1.9561, grad_norm: 45.7163
2025-06-20 07:48:07,992 - mmdet - INFO - Epoch [5][2850/7033]	lr: 5.015e-05, eta: 4:34:22, time: 1.469, data_time: 0.032, memory: 20458, loss_cls: 0.0740, loss_bbox: 0.2061, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2092, loss: 2.0217, grad_norm: 21.0225
2025-06-20 07:49:21,331 - mmdet - INFO - Epoch [5][2900/7033]	lr: 5.015e-05, eta: 4:33:08, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0623, loss_bbox: 0.1893, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2107, d3.loss_cls: 0.0679, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1923, loss: 1.8642, grad_norm: 24.3065
2025-06-20 07:50:34,292 - mmdet - INFO - Epoch [5][2950/7033]	lr: 5.015e-05, eta: 4:31:55, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0726, loss_bbox: 0.2061, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2456, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2150, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2087, loss: 2.0318, grad_norm: 24.8224
2025-06-20 07:51:47,590 - mmdet - INFO - Epoch [5][3000/7033]	lr: 5.015e-05, eta: 4:30:41, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0653, loss_bbox: 0.1927, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1985, loss: 1.9086, grad_norm: 28.1796
2025-06-20 07:53:00,807 - mmdet - INFO - Epoch [5][3050/7033]	lr: 5.015e-05, eta: 4:29:28, time: 1.464, data_time: 0.027, memory: 20458, loss_cls: 0.0618, loss_bbox: 0.2013, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3194, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0667, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0629, d4.loss_bbox: 0.2047, loss: 1.9231, grad_norm: 53.3313
2025-06-20 07:54:20,983 - mmdet - INFO - Epoch [5][3100/7033]	lr: 5.015e-05, eta: 4:28:17, time: 1.603, data_time: 0.030, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2069, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3287, d1.loss_cls: 0.1064, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2114, loss: 2.0446, grad_norm: 31.6522
2025-06-20 07:55:34,003 - mmdet - INFO - Epoch [5][3150/7033]	lr: 5.015e-05, eta: 4:27:03, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0644, loss_bbox: 0.1981, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2010, loss: 1.9125, grad_norm: 52.9660
2025-06-20 07:56:49,941 - mmdet - INFO - Epoch [5][3200/7033]	lr: 5.015e-05, eta: 4:25:51, time: 1.519, data_time: 0.049, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.1932, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3112, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2169, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1955, loss: 1.8969, grad_norm: 22.7327
2025-06-20 07:58:03,077 - mmdet - INFO - Epoch [5][3250/7033]	lr: 5.015e-05, eta: 4:24:37, time: 1.463, data_time: 0.026, memory: 20458, loss_cls: 0.0621, loss_bbox: 0.1991, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3179, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2195, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0647, d4.loss_bbox: 0.2018, loss: 1.9342, grad_norm: 83.6401
2025-06-20 07:59:16,507 - mmdet - INFO - Epoch [5][3300/7033]	lr: 5.015e-05, eta: 4:23:24, time: 1.469, data_time: 0.029, memory: 20458, loss_cls: 0.0713, loss_bbox: 0.2059, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2092, loss: 2.0090, grad_norm: 30.4170
2025-06-20 08:00:29,588 - mmdet - INFO - Epoch [5][3350/7033]	lr: 5.015e-05, eta: 4:22:10, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.2049, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3209, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0673, d4.loss_bbox: 0.2073, loss: 1.9657, grad_norm: 22.0628
2025-06-20 08:01:42,715 - mmdet - INFO - Epoch [5][3400/7033]	lr: 5.015e-05, eta: 4:20:57, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0657, loss_bbox: 0.1923, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3085, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0667, d4.loss_bbox: 0.1955, loss: 1.8893, grad_norm: 16.8267
2025-06-20 08:02:55,942 - mmdet - INFO - Epoch [5][3450/7033]	lr: 5.015e-05, eta: 4:19:43, time: 1.465, data_time: 0.027, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1933, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3120, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0667, d4.loss_bbox: 0.1952, loss: 1.8912, grad_norm: 48.8330
2025-06-20 08:04:09,244 - mmdet - INFO - Epoch [5][3500/7033]	lr: 5.015e-05, eta: 4:18:30, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0648, loss_bbox: 0.1935, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3105, d1.loss_cls: 0.0955, d1.loss_bbox: 0.2356, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2040, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1982, loss: 1.8994, grad_norm: 21.4220
2025-06-20 08:05:22,564 - mmdet - INFO - Epoch [5][3550/7033]	lr: 5.015e-05, eta: 4:17:16, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0635, loss_bbox: 0.1987, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3176, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2191, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0647, d4.loss_bbox: 0.2021, loss: 1.9291, grad_norm: 41.2953
2025-06-20 08:06:35,902 - mmdet - INFO - Epoch [5][3600/7033]	lr: 5.015e-05, eta: 4:16:03, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0666, loss_bbox: 0.1950, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3051, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2042, d4.loss_cls: 0.0678, d4.loss_bbox: 0.1988, loss: 1.9074, grad_norm: 47.2324
2025-06-20 08:07:49,283 - mmdet - INFO - Epoch [5][3650/7033]	lr: 5.015e-05, eta: 4:14:50, time: 1.468, data_time: 0.031, memory: 20458, loss_cls: 0.0683, loss_bbox: 0.1927, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3144, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0706, d4.loss_bbox: 0.1946, loss: 1.9170, grad_norm: 47.1916
2025-06-20 08:09:02,123 - mmdet - INFO - Epoch [5][3700/7033]	lr: 5.015e-05, eta: 4:13:36, time: 1.457, data_time: 0.028, memory: 20458, loss_cls: 0.0626, loss_bbox: 0.1924, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3125, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1956, loss: 1.8884, grad_norm: 24.4900
2025-06-20 08:10:15,588 - mmdet - INFO - Epoch [5][3750/7033]	lr: 5.015e-05, eta: 4:12:23, time: 1.469, data_time: 0.030, memory: 20458, loss_cls: 0.0659, loss_bbox: 0.2050, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3179, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2084, loss: 1.9642, grad_norm: 21.3800
2025-06-20 08:11:30,347 - mmdet - INFO - Epoch [5][3800/7033]	lr: 5.015e-05, eta: 4:11:10, time: 1.495, data_time: 0.029, memory: 20458, loss_cls: 0.0703, loss_bbox: 0.1951, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3081, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0737, d4.loss_bbox: 0.1974, loss: 1.9394, grad_norm: 25.3269
2025-06-20 08:12:43,694 - mmdet - INFO - Epoch [5][3850/7033]	lr: 5.015e-05, eta: 4:09:56, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0597, loss_bbox: 0.1899, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3008, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2263, d2.loss_cls: 0.0754, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0659, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1915, loss: 1.8333, grad_norm: 18.0270
2025-06-20 08:13:56,768 - mmdet - INFO - Epoch [5][3900/7033]	lr: 5.015e-05, eta: 4:08:43, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.1991, d0.loss_cls: 0.1717, d0.loss_bbox: 0.3180, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2182, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2015, loss: 1.9749, grad_norm: 20.8906
2025-06-20 08:15:09,996 - mmdet - INFO - Epoch [5][3950/7033]	lr: 5.015e-05, eta: 4:07:29, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0633, loss_bbox: 0.1945, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2036, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1976, loss: 1.8819, grad_norm: 20.2154
2025-06-20 08:16:23,245 - mmdet - INFO - Epoch [5][4000/7033]	lr: 5.015e-05, eta: 4:06:16, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0695, loss_bbox: 0.1929, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3084, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2298, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0707, d4.loss_bbox: 0.1957, loss: 1.9141, grad_norm: 27.7003
2025-06-20 08:17:36,538 - mmdet - INFO - Epoch [5][4050/7033]	lr: 5.015e-05, eta: 4:05:03, time: 1.466, data_time: 0.031, memory: 20458, loss_cls: 0.0677, loss_bbox: 0.1991, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2013, loss: 1.9451, grad_norm: 51.4430
2025-06-20 08:18:49,583 - mmdet - INFO - Epoch [5][4100/7033]	lr: 5.015e-05, eta: 4:03:49, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0674, loss_bbox: 0.1996, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3203, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2036, loss: 1.9600, grad_norm: 20.5295
2025-06-20 08:20:02,712 - mmdet - INFO - Epoch [5][4150/7033]	lr: 5.015e-05, eta: 4:02:36, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0675, loss_bbox: 0.1967, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2007, loss: 1.9608, grad_norm: 23.2906
2025-06-20 08:21:16,090 - mmdet - INFO - Epoch [5][4200/7033]	lr: 5.015e-05, eta: 4:01:22, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0723, loss_bbox: 0.1997, d0.loss_cls: 0.1767, d0.loss_bbox: 0.3222, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2031, loss: 2.0044, grad_norm: 18.0010
2025-06-20 08:22:31,504 - mmdet - INFO - Epoch [5][4250/7033]	lr: 5.015e-05, eta: 4:00:09, time: 1.508, data_time: 0.031, memory: 20458, loss_cls: 0.0702, loss_bbox: 0.2014, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3158, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2044, loss: 1.9743, grad_norm: 22.0267
2025-06-20 08:23:44,668 - mmdet - INFO - Epoch [5][4300/7033]	lr: 5.015e-05, eta: 3:58:56, time: 1.463, data_time: 0.031, memory: 20458, loss_cls: 0.0653, loss_bbox: 0.1888, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3094, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2092, d3.loss_cls: 0.0721, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1922, loss: 1.8816, grad_norm: 26.1477
2025-06-20 08:24:58,968 - mmdet - INFO - Epoch [5][4350/7033]	lr: 5.015e-05, eta: 3:57:43, time: 1.486, data_time: 0.051, memory: 20458, loss_cls: 0.0617, loss_bbox: 0.1856, d0.loss_cls: 0.1607, d0.loss_bbox: 0.2986, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2236, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0666, d3.loss_bbox: 0.1956, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1894, loss: 1.8209, grad_norm: 30.7759
2025-06-20 08:26:12,034 - mmdet - INFO - Epoch [5][4400/7033]	lr: 5.015e-05, eta: 3:56:29, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0719, loss_bbox: 0.1945, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2195, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0730, d4.loss_bbox: 0.1988, loss: 1.9341, grad_norm: 66.9809
2025-06-20 08:28:59,393 - mmdet - INFO - Epoch [5][4450/7033]	lr: 5.015e-05, eta: 3:55:44, time: 3.347, data_time: 1.906, memory: 20458, loss_cls: 0.0691, loss_bbox: 0.2010, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3162, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2034, loss: 1.9549, grad_norm: 18.4516
2025-06-20 08:30:12,762 - mmdet - INFO - Epoch [5][4500/7033]	lr: 5.015e-05, eta: 3:54:30, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0625, loss_bbox: 0.1890, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3087, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0671, d3.loss_bbox: 0.1987, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1909, loss: 1.8599, grad_norm: 28.9999
2025-06-20 08:31:25,971 - mmdet - INFO - Epoch [5][4550/7033]	lr: 5.015e-05, eta: 3:53:16, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0638, loss_bbox: 0.1986, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2018, loss: 1.9293, grad_norm: 76.5863
2025-06-20 08:32:39,477 - mmdet - INFO - Epoch [5][4600/7033]	lr: 5.015e-05, eta: 3:52:03, time: 1.470, data_time: 0.035, memory: 20458, loss_cls: 0.0638, loss_bbox: 0.1988, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3152, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0657, d4.loss_bbox: 0.1999, loss: 1.9156, grad_norm: 30.9688
2025-06-20 08:33:53,281 - mmdet - INFO - Epoch [5][4650/7033]	lr: 5.015e-05, eta: 3:50:49, time: 1.476, data_time: 0.034, memory: 20458, loss_cls: 0.0628, loss_bbox: 0.1945, d0.loss_cls: 0.1733, d0.loss_bbox: 0.3161, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1989, loss: 1.9076, grad_norm: 103.9525
2025-06-20 08:35:06,783 - mmdet - INFO - Epoch [5][4700/7033]	lr: 5.015e-05, eta: 3:49:36, time: 1.470, data_time: 0.034, memory: 20458, loss_cls: 0.0666, loss_bbox: 0.1971, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2368, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2066, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2005, loss: 1.9360, grad_norm: 20.8901
2025-06-20 08:36:20,346 - mmdet - INFO - Epoch [5][4750/7033]	lr: 5.015e-05, eta: 3:48:22, time: 1.471, data_time: 0.037, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1961, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0676, d4.loss_bbox: 0.1991, loss: 1.8986, grad_norm: 22.0876
2025-06-20 08:37:33,487 - mmdet - INFO - Epoch [5][4800/7033]	lr: 5.015e-05, eta: 3:47:09, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0677, loss_bbox: 0.1910, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0704, d4.loss_bbox: 0.1933, loss: 1.9016, grad_norm: 37.9915
2025-06-20 08:38:48,664 - mmdet - INFO - Epoch [5][4850/7033]	lr: 5.015e-05, eta: 3:45:55, time: 1.504, data_time: 0.036, memory: 20458, loss_cls: 0.0672, loss_bbox: 0.2006, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3211, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0690, d4.loss_bbox: 0.2035, loss: 1.9637, grad_norm: 57.4075
2025-06-20 08:40:01,996 - mmdet - INFO - Epoch [5][4900/7033]	lr: 5.015e-05, eta: 3:44:42, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0675, loss_bbox: 0.1934, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3121, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0692, d4.loss_bbox: 0.1977, loss: 1.9273, grad_norm: 26.8760
2025-06-20 08:41:15,203 - mmdet - INFO - Epoch [5][4950/7033]	lr: 5.015e-05, eta: 3:43:28, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0626, loss_bbox: 0.1881, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0680, d3.loss_bbox: 0.1972, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1923, loss: 1.8606, grad_norm: 27.7893
2025-06-20 08:42:28,117 - mmdet - INFO - Epoch [5][5000/7033]	lr: 5.015e-05, eta: 3:42:15, time: 1.458, data_time: 0.029, memory: 20458, loss_cls: 0.0570, loss_bbox: 0.1885, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3041, d1.loss_cls: 0.0881, d1.loss_bbox: 0.2270, d2.loss_cls: 0.0708, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0625, d3.loss_bbox: 0.1984, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1907, loss: 1.8143, grad_norm: 29.7631
2025-06-20 08:43:41,321 - mmdet - INFO - Epoch [5][5050/7033]	lr: 5.015e-05, eta: 3:41:01, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0597, loss_bbox: 0.1920, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3045, d1.loss_cls: 0.0914, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0657, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1956, loss: 1.8588, grad_norm: 66.5268
2025-06-20 08:44:55,016 - mmdet - INFO - Epoch [5][5100/7033]	lr: 5.015e-05, eta: 3:39:47, time: 1.474, data_time: 0.036, memory: 20458, loss_cls: 0.0694, loss_bbox: 0.2009, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2032, loss: 1.9761, grad_norm: 40.5175
2025-06-20 08:46:08,357 - mmdet - INFO - Epoch [5][5150/7033]	lr: 5.015e-05, eta: 3:38:34, time: 1.467, data_time: 0.029, memory: 20458, loss_cls: 0.0660, loss_bbox: 0.1970, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3126, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0663, d4.loss_bbox: 0.2011, loss: 1.9190, grad_norm: 81.3781
2025-06-20 08:47:21,407 - mmdet - INFO - Epoch [5][5200/7033]	lr: 5.015e-05, eta: 3:37:20, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0661, loss_bbox: 0.1981, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2196, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2065, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2010, loss: 1.9251, grad_norm: 17.8825
2025-06-20 08:48:35,068 - mmdet - INFO - Epoch [5][5250/7033]	lr: 5.015e-05, eta: 3:36:07, time: 1.473, data_time: 0.034, memory: 20458, loss_cls: 0.0599, loss_bbox: 0.1993, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3220, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0623, d4.loss_bbox: 0.2026, loss: 1.9356, grad_norm: 40.2003
2025-06-20 08:49:50,402 - mmdet - INFO - Epoch [5][5300/7033]	lr: 5.015e-05, eta: 3:34:54, time: 1.507, data_time: 0.036, memory: 20458, loss_cls: 0.0635, loss_bbox: 0.1912, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1943, loss: 1.8798, grad_norm: 43.1504
2025-06-20 08:51:03,430 - mmdet - INFO - Epoch [5][5350/7033]	lr: 5.015e-05, eta: 3:33:40, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0737, loss_bbox: 0.2075, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2122, loss: 2.0550, grad_norm: 29.8668
2025-06-20 08:52:17,582 - mmdet - INFO - Epoch [5][5400/7033]	lr: 5.015e-05, eta: 3:32:26, time: 1.483, data_time: 0.045, memory: 20458, loss_cls: 0.0625, loss_bbox: 0.1963, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1999, loss: 1.9104, grad_norm: 27.5383
2025-06-20 08:53:31,001 - mmdet - INFO - Epoch [5][5450/7033]	lr: 5.015e-05, eta: 3:31:13, time: 1.468, data_time: 0.033, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.2016, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3126, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2093, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2042, loss: 1.9574, grad_norm: 24.1114
2025-06-20 08:54:44,395 - mmdet - INFO - Epoch [5][5500/7033]	lr: 5.015e-05, eta: 3:29:59, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.2023, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2263, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2052, loss: 1.9935, grad_norm: 35.0401
2025-06-20 08:55:57,562 - mmdet - INFO - Epoch [5][5550/7033]	lr: 5.015e-05, eta: 3:28:46, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0658, loss_bbox: 0.1934, d0.loss_cls: 0.1733, d0.loss_bbox: 0.3097, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0670, d4.loss_bbox: 0.1963, loss: 1.8995, grad_norm: 26.6072
2025-06-20 08:57:10,666 - mmdet - INFO - Epoch [5][5600/7033]	lr: 5.015e-05, eta: 3:27:32, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0681, loss_bbox: 0.1951, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3175, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2055, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1992, loss: 1.9297, grad_norm: 20.4544
2025-06-20 08:58:23,991 - mmdet - INFO - Epoch [5][5650/7033]	lr: 5.015e-05, eta: 3:26:18, time: 1.467, data_time: 0.029, memory: 20458, loss_cls: 0.0663, loss_bbox: 0.1943, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3205, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2033, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1966, loss: 1.9200, grad_norm: 28.9693
2025-06-20 08:59:37,138 - mmdet - INFO - Epoch [5][5700/7033]	lr: 5.015e-05, eta: 3:25:05, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0666, loss_bbox: 0.2061, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3150, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2083, loss: 1.9693, grad_norm: 37.5128
2025-06-20 09:00:50,435 - mmdet - INFO - Epoch [5][5750/7033]	lr: 5.015e-05, eta: 3:23:51, time: 1.466, data_time: 0.033, memory: 20458, loss_cls: 0.0718, loss_bbox: 0.1988, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3160, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2069, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2016, loss: 1.9710, grad_norm: 30.0673
2025-06-20 09:02:03,602 - mmdet - INFO - Epoch [5][5800/7033]	lr: 5.015e-05, eta: 3:22:38, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0627, loss_bbox: 0.1862, d0.loss_cls: 0.1606, d0.loss_bbox: 0.3039, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0685, d3.loss_bbox: 0.1971, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1894, loss: 1.8440, grad_norm: 37.9860
2025-06-20 09:03:16,609 - mmdet - INFO - Epoch [5][5850/7033]	lr: 5.015e-05, eta: 3:21:24, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0691, loss_bbox: 0.2002, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2033, loss: 1.9845, grad_norm: 59.3386
2025-06-20 09:04:29,980 - mmdet - INFO - Epoch [5][5900/7033]	lr: 5.015e-05, eta: 3:20:10, time: 1.467, data_time: 0.029, memory: 20458, loss_cls: 0.0674, loss_bbox: 0.1962, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3159, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2360, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0694, d4.loss_bbox: 0.1990, loss: 1.9265, grad_norm: 20.4199
2025-06-20 09:05:43,024 - mmdet - INFO - Epoch [5][5950/7033]	lr: 5.015e-05, eta: 3:18:57, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0659, loss_bbox: 0.1940, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3119, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0691, d4.loss_bbox: 0.1964, loss: 1.9167, grad_norm: 25.6258
2025-06-20 09:06:57,897 - mmdet - INFO - Epoch [5][6000/7033]	lr: 5.015e-05, eta: 3:17:43, time: 1.497, data_time: 0.030, memory: 20458, loss_cls: 0.0676, loss_bbox: 0.1903, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2010, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1942, loss: 1.8839, grad_norm: 70.6608
2025-06-20 09:08:11,090 - mmdet - INFO - Epoch [5][6050/7033]	lr: 5.015e-05, eta: 3:16:30, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0603, loss_bbox: 0.1963, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3161, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2069, d4.loss_cls: 0.0625, d4.loss_bbox: 0.2003, loss: 1.8981, grad_norm: 20.2691
2025-06-20 09:09:24,375 - mmdet - INFO - Epoch [5][6100/7033]	lr: 5.015e-05, eta: 3:15:16, time: 1.466, data_time: 0.029, memory: 20458, loss_cls: 0.0673, loss_bbox: 0.2015, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2195, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2101, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2036, loss: 1.9352, grad_norm: 28.6747
2025-06-20 09:10:37,785 - mmdet - INFO - Epoch [5][6150/7033]	lr: 5.015e-05, eta: 3:14:03, time: 1.468, data_time: 0.028, memory: 20458, loss_cls: 0.0720, loss_bbox: 0.2025, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3194, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2431, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2065, loss: 1.9809, grad_norm: 34.2056
2025-06-20 09:11:50,799 - mmdet - INFO - Epoch [5][6200/7033]	lr: 5.015e-05, eta: 3:12:49, time: 1.460, data_time: 0.030, memory: 20458, loss_cls: 0.0647, loss_bbox: 0.1977, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3165, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2015, loss: 1.9273, grad_norm: 48.2076
2025-06-20 09:13:04,080 - mmdet - INFO - Epoch [5][6250/7033]	lr: 5.015e-05, eta: 3:11:35, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0650, loss_bbox: 0.1989, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3125, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2373, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0661, d4.loss_bbox: 0.2027, loss: 1.9253, grad_norm: 21.3144
2025-06-20 09:14:17,210 - mmdet - INFO - Epoch [5][6300/7033]	lr: 5.015e-05, eta: 3:10:22, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0685, loss_bbox: 0.1937, d0.loss_cls: 0.1665, d0.loss_bbox: 0.2985, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2305, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0694, d4.loss_bbox: 0.1963, loss: 1.8921, grad_norm: 24.4463
2025-06-20 09:15:30,383 - mmdet - INFO - Epoch [5][6350/7033]	lr: 5.015e-05, eta: 3:09:08, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0649, loss_bbox: 0.1946, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1988, loss: 1.9111, grad_norm: 16.0701
2025-06-20 09:16:45,549 - mmdet - INFO - Epoch [5][6400/7033]	lr: 5.015e-05, eta: 3:07:55, time: 1.503, data_time: 0.031, memory: 20458, loss_cls: 0.0654, loss_bbox: 0.1929, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1959, loss: 1.9013, grad_norm: 34.3125
2025-06-20 09:17:59,067 - mmdet - INFO - Epoch [5][6450/7033]	lr: 5.015e-05, eta: 3:06:41, time: 1.470, data_time: 0.030, memory: 20458, loss_cls: 0.0662, loss_bbox: 0.1912, d0.loss_cls: 0.1595, d0.loss_bbox: 0.3160, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2015, d4.loss_cls: 0.0672, d4.loss_bbox: 0.1950, loss: 1.8884, grad_norm: 24.9380
2025-06-20 09:19:12,377 - mmdet - INFO - Epoch [5][6500/7033]	lr: 5.015e-05, eta: 3:05:28, time: 1.466, data_time: 0.028, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.1946, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3227, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0672, d4.loss_bbox: 0.1978, loss: 1.9359, grad_norm: 24.7384
2025-06-20 09:20:25,320 - mmdet - INFO - Epoch [5][6550/7033]	lr: 5.015e-05, eta: 3:04:14, time: 1.459, data_time: 0.029, memory: 20458, loss_cls: 0.0622, loss_bbox: 0.1899, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2009, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1930, loss: 1.8764, grad_norm: 42.7959
2025-06-20 09:21:38,653 - mmdet - INFO - Epoch [5][6600/7033]	lr: 5.015e-05, eta: 3:03:00, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0626, loss_bbox: 0.2040, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3265, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0648, d4.loss_bbox: 0.2080, loss: 1.9648, grad_norm: 27.2758
2025-06-20 09:22:51,842 - mmdet - INFO - Epoch [5][6650/7033]	lr: 5.015e-05, eta: 3:01:47, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0636, loss_bbox: 0.1918, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3135, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1962, loss: 1.8917, grad_norm: 49.0085
2025-06-20 09:24:05,109 - mmdet - INFO - Epoch [5][6700/7033]	lr: 5.015e-05, eta: 3:00:33, time: 1.465, data_time: 0.027, memory: 20458, loss_cls: 0.0636, loss_bbox: 0.2004, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3179, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0690, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0650, d4.loss_bbox: 0.2036, loss: 1.9280, grad_norm: 20.6735
2025-06-20 09:25:18,456 - mmdet - INFO - Epoch [5][6750/7033]	lr: 5.015e-05, eta: 2:59:20, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0568, loss_bbox: 0.1907, d0.loss_cls: 0.1601, d0.loss_bbox: 0.2996, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0705, d2.loss_bbox: 0.2107, d3.loss_cls: 0.0608, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0581, d4.loss_bbox: 0.1944, loss: 1.8211, grad_norm: 79.7371
2025-06-20 09:26:31,668 - mmdet - INFO - Epoch [5][6800/7033]	lr: 5.015e-05, eta: 2:58:06, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0667, loss_bbox: 0.1986, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3101, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2011, loss: 1.9318, grad_norm: 19.6681
2025-06-20 09:27:44,909 - mmdet - INFO - Epoch [5][6850/7033]	lr: 5.015e-05, eta: 2:56:52, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0633, loss_bbox: 0.2049, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3178, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2071, loss: 1.9550, grad_norm: 20.2766
2025-06-20 09:28:57,923 - mmdet - INFO - Epoch [5][6900/7033]	lr: 5.015e-05, eta: 2:55:39, time: 1.460, data_time: 0.030, memory: 20458, loss_cls: 0.0638, loss_bbox: 0.1986, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2196, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2084, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2008, loss: 1.9318, grad_norm: 21.9003
2025-06-20 09:30:11,353 - mmdet - INFO - Epoch [5][6950/7033]	lr: 5.015e-05, eta: 2:54:25, time: 1.469, data_time: 0.030, memory: 20458, loss_cls: 0.0666, loss_bbox: 0.1983, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3176, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0683, d4.loss_bbox: 0.2015, loss: 1.9389, grad_norm: 23.0117
2025-06-20 09:31:24,604 - mmdet - INFO - Epoch [5][7000/7033]	lr: 5.015e-05, eta: 2:53:12, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.0704, loss_bbox: 0.1960, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3111, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2170, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0730, d4.loss_bbox: 0.1979, loss: 1.9486, grad_norm: 27.2294
2025-06-20 09:32:13,400 - mmdet - INFO - Saving checkpoint at 5 epochs
2025-06-20 09:55:11,627 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-20 09:55:11,627 - mmdet - INFO - Epoch(val) [5][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7951, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8841, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9091, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9212, pts_bbox_NuScenes/car_trans_err: 0.1749, pts_bbox_NuScenes/car_scale_err: 0.1497, pts_bbox_NuScenes/car_orient_err: 0.0412, pts_bbox_NuScenes/car_vel_err: 0.3220, pts_bbox_NuScenes/car_attr_err: 0.1834, pts_bbox_NuScenes/mATE: 0.2871, pts_bbox_NuScenes/mASE: 0.2609, pts_bbox_NuScenes/mAOE: 0.2430, pts_bbox_NuScenes/mAVE: 0.2801, pts_bbox_NuScenes/mAAE: 0.1816, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4203, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6073, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7128, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7523, pts_bbox_NuScenes/truck_trans_err: 0.3450, pts_bbox_NuScenes/truck_scale_err: 0.1910, pts_bbox_NuScenes/truck_orient_err: 0.0436, pts_bbox_NuScenes/truck_vel_err: 0.2586, pts_bbox_NuScenes/truck_attr_err: 0.1993, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0546, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2036, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4120, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4859, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6807, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4373, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7746, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1207, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3050, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5393, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7539, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9046, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9273, pts_bbox_NuScenes/bus_trans_err: 0.3248, pts_bbox_NuScenes/bus_scale_err: 0.1865, pts_bbox_NuScenes/bus_orient_err: 0.0335, pts_bbox_NuScenes/bus_vel_err: 0.4588, pts_bbox_NuScenes/bus_attr_err: 0.2504, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1801, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4273, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.6023, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6813, pts_bbox_NuScenes/trailer_trans_err: 0.4822, pts_bbox_NuScenes/trailer_scale_err: 0.2185, pts_bbox_NuScenes/trailer_orient_err: 0.4315, pts_bbox_NuScenes/trailer_vel_err: 0.2122, pts_bbox_NuScenes/trailer_attr_err: 0.1756, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6228, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7229, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7700, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7814, pts_bbox_NuScenes/barrier_trans_err: 0.2046, pts_bbox_NuScenes/barrier_scale_err: 0.2881, pts_bbox_NuScenes/barrier_orient_err: 0.0433, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6439, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7724, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8022, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8093, pts_bbox_NuScenes/motorcycle_trans_err: 0.2083, pts_bbox_NuScenes/motorcycle_scale_err: 0.2456, pts_bbox_NuScenes/motorcycle_orient_err: 0.1972, pts_bbox_NuScenes/motorcycle_vel_err: 0.4246, pts_bbox_NuScenes/motorcycle_attr_err: 0.2215, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5462, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5973, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6063, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6158, pts_bbox_NuScenes/bicycle_trans_err: 0.1703, pts_bbox_NuScenes/bicycle_scale_err: 0.2700, pts_bbox_NuScenes/bicycle_orient_err: 0.2948, pts_bbox_NuScenes/bicycle_vel_err: 0.2210, pts_bbox_NuScenes/bicycle_attr_err: 0.0071, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8197, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8582, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8787, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8905, pts_bbox_NuScenes/pedestrian_trans_err: 0.1443, pts_bbox_NuScenes/pedestrian_scale_err: 0.2933, pts_bbox_NuScenes/pedestrian_orient_err: 0.3270, pts_bbox_NuScenes/pedestrian_vel_err: 0.2224, pts_bbox_NuScenes/pedestrian_attr_err: 0.1102, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7414, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7762, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.8021, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8240, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1363, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3295, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7129, pts_bbox_NuScenes/mAP: 0.6764
2025-06-20 09:56:35,279 - mmdet - INFO - Epoch [6][50/7033]	lr: 1.358e-05, eta: 2:51:01, time: 1.567, data_time: 0.128, memory: 20458, loss_cls: 0.0674, loss_bbox: 0.1979, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3193, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2092, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2009, loss: 1.9476, grad_norm: 20.6027
2025-06-20 09:57:48,379 - mmdet - INFO - Epoch [6][100/7033]	lr: 1.358e-05, eta: 2:49:47, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0671, loss_bbox: 0.1983, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3121, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0691, d4.loss_bbox: 0.2014, loss: 1.9323, grad_norm: 62.3163
2025-06-20 09:59:01,559 - mmdet - INFO - Epoch [6][150/7033]	lr: 1.358e-05, eta: 2:48:34, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0640, loss_bbox: 0.1976, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0656, d4.loss_bbox: 0.2001, loss: 1.8971, grad_norm: 17.6319
2025-06-20 10:00:16,459 - mmdet - INFO - Epoch [6][200/7033]	lr: 1.358e-05, eta: 2:47:21, time: 1.498, data_time: 0.029, memory: 20458, loss_cls: 0.0709, loss_bbox: 0.1988, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3113, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2032, loss: 1.9682, grad_norm: 21.7571
2025-06-20 10:01:29,626 - mmdet - INFO - Epoch [6][250/7033]	lr: 1.358e-05, eta: 2:46:07, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0587, loss_bbox: 0.1875, d0.loss_cls: 0.1575, d0.loss_bbox: 0.2975, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2250, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2074, d3.loss_cls: 0.0651, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0609, d4.loss_bbox: 0.1903, loss: 1.8170, grad_norm: 20.0588
2025-06-20 10:02:42,681 - mmdet - INFO - Epoch [6][300/7033]	lr: 1.358e-05, eta: 2:44:54, time: 1.461, data_time: 0.027, memory: 20458, loss_cls: 0.0610, loss_bbox: 0.1891, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3043, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0681, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0624, d4.loss_bbox: 0.1916, loss: 1.8488, grad_norm: 17.0092
2025-06-20 10:03:55,895 - mmdet - INFO - Epoch [6][350/7033]	lr: 1.358e-05, eta: 2:43:40, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0610, loss_bbox: 0.1928, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3106, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0657, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1963, loss: 1.8768, grad_norm: 23.2653
2025-06-20 10:05:08,907 - mmdet - INFO - Epoch [6][400/7033]	lr: 1.358e-05, eta: 2:42:26, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0689, loss_bbox: 0.2018, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3158, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2051, loss: 1.9667, grad_norm: 23.0116
2025-06-20 10:06:22,172 - mmdet - INFO - Epoch [6][450/7033]	lr: 1.358e-05, eta: 2:41:13, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0716, loss_bbox: 0.1979, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3148, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2032, loss: 1.9770, grad_norm: 19.8164
2025-06-20 10:07:35,326 - mmdet - INFO - Epoch [6][500/7033]	lr: 1.358e-05, eta: 2:39:59, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0614, loss_bbox: 0.1929, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3127, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2303, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1957, loss: 1.8863, grad_norm: 24.1712
2025-06-20 10:08:48,600 - mmdet - INFO - Epoch [6][550/7033]	lr: 1.358e-05, eta: 2:38:46, time: 1.465, data_time: 0.027, memory: 20458, loss_cls: 0.0715, loss_bbox: 0.1980, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3258, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2031, loss: 2.0055, grad_norm: 16.6338
2025-06-20 10:10:01,672 - mmdet - INFO - Epoch [6][600/7033]	lr: 1.358e-05, eta: 2:37:32, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0671, loss_bbox: 0.1913, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3012, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2125, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2002, d4.loss_cls: 0.0675, d4.loss_bbox: 0.1947, loss: 1.8799, grad_norm: 55.4414
2025-06-20 10:11:14,817 - mmdet - INFO - Epoch [6][650/7033]	lr: 1.358e-05, eta: 2:36:19, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0618, loss_bbox: 0.1905, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3092, d1.loss_cls: 0.0905, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0676, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1949, loss: 1.8645, grad_norm: 36.4238
2025-06-20 10:12:28,203 - mmdet - INFO - Epoch [6][700/7033]	lr: 1.358e-05, eta: 2:35:05, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0601, loss_bbox: 0.1864, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3006, d1.loss_cls: 0.0907, d1.loss_bbox: 0.2242, d2.loss_cls: 0.0716, d2.loss_bbox: 0.2071, d3.loss_cls: 0.0645, d3.loss_bbox: 0.1959, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1894, loss: 1.8112, grad_norm: 55.9992
2025-06-20 10:13:41,422 - mmdet - INFO - Epoch [6][750/7033]	lr: 1.358e-05, eta: 2:33:52, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0601, loss_bbox: 0.1894, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0668, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0614, d4.loss_bbox: 0.1925, loss: 1.8541, grad_norm: 30.6946
2025-06-20 10:14:54,516 - mmdet - INFO - Epoch [6][800/7033]	lr: 1.358e-05, eta: 2:32:38, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0629, loss_bbox: 0.1971, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3153, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0660, d4.loss_bbox: 0.2005, loss: 1.9103, grad_norm: 70.8621
2025-06-20 10:16:14,785 - mmdet - INFO - Epoch [6][850/7033]	lr: 1.358e-05, eta: 2:31:26, time: 1.605, data_time: 0.030, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.1887, d0.loss_cls: 0.1682, d0.loss_bbox: 0.2991, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2256, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0717, d3.loss_bbox: 0.1974, d4.loss_cls: 0.0678, d4.loss_bbox: 0.1917, loss: 1.8663, grad_norm: 35.5701
2025-06-20 10:17:28,478 - mmdet - INFO - Epoch [6][900/7033]	lr: 1.358e-05, eta: 2:30:13, time: 1.474, data_time: 0.032, memory: 20458, loss_cls: 0.0583, loss_bbox: 0.1830, d0.loss_cls: 0.1590, d0.loss_bbox: 0.2956, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2210, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2051, d3.loss_cls: 0.0643, d3.loss_bbox: 0.1926, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1869, loss: 1.7933, grad_norm: 36.0005
2025-06-20 10:18:41,543 - mmdet - INFO - Epoch [6][950/7033]	lr: 1.358e-05, eta: 2:28:59, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1863, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3027, d1.loss_cls: 0.0926, d1.loss_bbox: 0.2267, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0678, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1898, loss: 1.8383, grad_norm: 20.0812
2025-06-20 10:19:54,847 - mmdet - INFO - Epoch [6][1000/7033]	lr: 1.358e-05, eta: 2:27:46, time: 1.466, data_time: 0.031, memory: 20458, loss_cls: 0.0553, loss_bbox: 0.1817, d0.loss_cls: 0.1606, d0.loss_bbox: 0.2922, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2203, d2.loss_cls: 0.0704, d2.loss_bbox: 0.1999, d3.loss_cls: 0.0605, d3.loss_bbox: 0.1906, d4.loss_cls: 0.0574, d4.loss_bbox: 0.1831, loss: 1.7616, grad_norm: 20.5516
2025-06-20 10:21:08,272 - mmdet - INFO - Epoch [6][1050/7033]	lr: 1.358e-05, eta: 2:26:32, time: 1.469, data_time: 0.032, memory: 20458, loss_cls: 0.0663, loss_bbox: 0.1960, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3135, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1997, loss: 1.9231, grad_norm: 38.9491
2025-06-20 10:22:21,689 - mmdet - INFO - Epoch [6][1100/7033]	lr: 1.358e-05, eta: 2:25:19, time: 1.468, data_time: 0.031, memory: 20458, loss_cls: 0.0606, loss_bbox: 0.1924, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3145, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0655, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1961, loss: 1.8714, grad_norm: 26.4564
2025-06-20 10:23:34,857 - mmdet - INFO - Epoch [6][1150/7033]	lr: 1.358e-05, eta: 2:24:05, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0648, loss_bbox: 0.1964, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1998, loss: 1.8969, grad_norm: 19.2901
2025-06-20 10:24:47,867 - mmdet - INFO - Epoch [6][1200/7033]	lr: 1.358e-05, eta: 2:22:51, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0661, loss_bbox: 0.1925, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3032, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2309, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1965, loss: 1.8926, grad_norm: 18.3941
2025-06-20 10:26:00,942 - mmdet - INFO - Epoch [6][1250/7033]	lr: 1.358e-05, eta: 2:21:38, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0635, loss_bbox: 0.1931, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1956, loss: 1.8837, grad_norm: 74.5661
2025-06-20 10:27:13,964 - mmdet - INFO - Epoch [6][1300/7033]	lr: 1.358e-05, eta: 2:20:24, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0633, loss_bbox: 0.1974, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3231, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0693, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0643, d4.loss_bbox: 0.2014, loss: 1.9268, grad_norm: 35.4578
2025-06-20 10:28:27,037 - mmdet - INFO - Epoch [6][1350/7033]	lr: 1.358e-05, eta: 2:19:11, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0656, loss_bbox: 0.1896, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2317, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0693, d3.loss_bbox: 0.2004, d4.loss_cls: 0.0664, d4.loss_bbox: 0.1933, loss: 1.8661, grad_norm: 82.2941
2025-06-20 10:29:40,645 - mmdet - INFO - Epoch [6][1400/7033]	lr: 1.358e-05, eta: 2:17:57, time: 1.472, data_time: 0.031, memory: 20458, loss_cls: 0.0582, loss_bbox: 0.1957, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0733, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0635, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1987, loss: 1.8687, grad_norm: 42.8854
2025-06-20 10:30:53,605 - mmdet - INFO - Epoch [6][1450/7033]	lr: 1.358e-05, eta: 2:16:44, time: 1.459, data_time: 0.028, memory: 20458, loss_cls: 0.0564, loss_bbox: 0.1851, d0.loss_cls: 0.1546, d0.loss_bbox: 0.3013, d1.loss_cls: 0.0902, d1.loss_bbox: 0.2230, d2.loss_cls: 0.0719, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0621, d3.loss_bbox: 0.1955, d4.loss_cls: 0.0583, d4.loss_bbox: 0.1883, loss: 1.7927, grad_norm: 23.2863
2025-06-20 10:32:07,010 - mmdet - INFO - Epoch [6][1500/7033]	lr: 1.358e-05, eta: 2:15:30, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0622, loss_bbox: 0.1975, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3108, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0637, d4.loss_bbox: 0.2010, loss: 1.9078, grad_norm: 20.5168
2025-06-20 10:33:20,209 - mmdet - INFO - Epoch [6][1550/7033]	lr: 1.358e-05, eta: 2:14:17, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0670, loss_bbox: 0.1884, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3068, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2330, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2112, d3.loss_cls: 0.0719, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1921, loss: 1.8902, grad_norm: 29.9576
2025-06-20 10:34:33,194 - mmdet - INFO - Epoch [6][1600/7033]	lr: 1.358e-05, eta: 2:13:03, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0635, loss_bbox: 0.1906, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2024, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1951, loss: 1.8768, grad_norm: 33.7954
2025-06-20 10:35:46,427 - mmdet - INFO - Epoch [6][1650/7033]	lr: 1.358e-05, eta: 2:11:50, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0711, loss_bbox: 0.1985, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2020, loss: 1.9625, grad_norm: 15.5777
2025-06-20 10:37:01,497 - mmdet - INFO - Epoch [6][1700/7033]	lr: 1.358e-05, eta: 2:10:37, time: 1.501, data_time: 0.031, memory: 20458, loss_cls: 0.0601, loss_bbox: 0.1907, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3138, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2360, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0655, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1958, loss: 1.8720, grad_norm: 18.4574
2025-06-20 10:38:14,589 - mmdet - INFO - Epoch [6][1750/7033]	lr: 1.358e-05, eta: 2:09:23, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0618, loss_bbox: 0.1887, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0692, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1917, loss: 1.8564, grad_norm: 55.3602
2025-06-20 10:39:28,130 - mmdet - INFO - Epoch [6][1800/7033]	lr: 1.358e-05, eta: 2:08:10, time: 1.471, data_time: 0.031, memory: 20458, loss_cls: 0.0670, loss_bbox: 0.1911, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2016, d4.loss_cls: 0.0683, d4.loss_bbox: 0.1963, loss: 1.8981, grad_norm: 167.4400
2025-06-20 10:40:41,368 - mmdet - INFO - Epoch [6][1850/7033]	lr: 1.358e-05, eta: 2:06:56, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0700, loss_bbox: 0.1970, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2007, loss: 1.9549, grad_norm: 32.7759
2025-06-20 10:41:54,597 - mmdet - INFO - Epoch [6][1900/7033]	lr: 1.358e-05, eta: 2:05:42, time: 1.465, data_time: 0.028, memory: 20458, loss_cls: 0.0628, loss_bbox: 0.1942, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1975, loss: 1.8847, grad_norm: 94.6761
2025-06-20 10:43:07,775 - mmdet - INFO - Epoch [6][1950/7033]	lr: 1.358e-05, eta: 2:04:29, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0616, loss_bbox: 0.1873, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2293, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0673, d3.loss_bbox: 0.1976, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1915, loss: 1.8498, grad_norm: 31.5307
2025-06-20 10:44:20,900 - mmdet - INFO - Epoch [6][2000/7033]	lr: 1.358e-05, eta: 2:03:15, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0599, loss_bbox: 0.1882, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0727, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0646, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1922, loss: 1.8396, grad_norm: 25.6762
2025-06-20 10:45:33,953 - mmdet - INFO - Epoch [6][2050/7033]	lr: 1.358e-05, eta: 2:02:02, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0595, loss_bbox: 0.1911, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3041, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2284, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0659, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1938, loss: 1.8420, grad_norm: 19.8745
2025-06-20 10:46:46,946 - mmdet - INFO - Epoch [6][2100/7033]	lr: 1.358e-05, eta: 2:00:48, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0634, loss_bbox: 0.1948, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1978, loss: 1.8950, grad_norm: 20.4552
2025-06-20 10:48:00,179 - mmdet - INFO - Epoch [6][2150/7033]	lr: 1.358e-05, eta: 1:59:35, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0627, loss_bbox: 0.1979, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3186, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0639, d4.loss_bbox: 0.2019, loss: 1.9277, grad_norm: 21.3932
2025-06-20 10:49:13,653 - mmdet - INFO - Epoch [6][2200/7033]	lr: 1.358e-05, eta: 1:58:21, time: 1.469, data_time: 0.030, memory: 20458, loss_cls: 0.0674, loss_bbox: 0.1938, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3167, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0685, d4.loss_bbox: 0.1967, loss: 1.9210, grad_norm: 24.9830
2025-06-20 10:50:26,740 - mmdet - INFO - Epoch [6][2250/7033]	lr: 1.358e-05, eta: 1:57:08, time: 1.462, data_time: 0.031, memory: 20458, loss_cls: 0.0602, loss_bbox: 0.1944, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3104, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1978, loss: 1.8814, grad_norm: 41.2959
2025-06-20 10:51:39,774 - mmdet - INFO - Epoch [6][2300/7033]	lr: 1.358e-05, eta: 1:55:54, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0616, loss_bbox: 0.1918, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3011, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2125, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1964, loss: 1.8625, grad_norm: 103.8587
2025-06-20 10:52:53,252 - mmdet - INFO - Epoch [6][2350/7033]	lr: 1.358e-05, eta: 1:54:41, time: 1.470, data_time: 0.031, memory: 20458, loss_cls: 0.0645, loss_bbox: 0.1945, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1993, loss: 1.8914, grad_norm: 23.6093
2025-06-20 10:54:08,375 - mmdet - INFO - Epoch [6][2400/7033]	lr: 1.358e-05, eta: 1:53:28, time: 1.502, data_time: 0.031, memory: 20458, loss_cls: 0.0585, loss_bbox: 0.1906, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3011, d1.loss_cls: 0.0908, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0725, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0649, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1926, loss: 1.8305, grad_norm: 27.0444
2025-06-20 10:55:21,887 - mmdet - INFO - Epoch [6][2450/7033]	lr: 1.358e-05, eta: 1:52:14, time: 1.470, data_time: 0.030, memory: 20458, loss_cls: 0.0602, loss_bbox: 0.1911, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2305, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0651, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1935, loss: 1.8550, grad_norm: 67.8052
2025-06-20 10:56:36,973 - mmdet - INFO - Epoch [6][2500/7033]	lr: 1.358e-05, eta: 1:51:01, time: 1.502, data_time: 0.030, memory: 20458, loss_cls: 0.0677, loss_bbox: 0.2043, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3163, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2076, loss: 1.9677, grad_norm: 22.7695
2025-06-20 10:57:49,816 - mmdet - INFO - Epoch [6][2550/7033]	lr: 1.358e-05, eta: 1:49:47, time: 1.457, data_time: 0.029, memory: 20458, loss_cls: 0.0655, loss_bbox: 0.1889, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3005, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0713, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0669, d4.loss_bbox: 0.1915, loss: 1.8589, grad_norm: 20.1929
2025-06-20 10:59:02,936 - mmdet - INFO - Epoch [6][2600/7033]	lr: 1.358e-05, eta: 1:48:34, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0655, loss_bbox: 0.1914, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3122, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2332, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1953, loss: 1.8851, grad_norm: 84.4225
2025-06-20 11:00:16,091 - mmdet - INFO - Epoch [6][2650/7033]	lr: 1.358e-05, eta: 1:47:20, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0657, loss_bbox: 0.1957, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3154, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2000, loss: 1.9247, grad_norm: 23.6288
2025-06-20 11:01:29,152 - mmdet - INFO - Epoch [6][2700/7033]	lr: 1.358e-05, eta: 1:46:07, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0588, loss_bbox: 0.1928, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0725, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0642, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1959, loss: 1.8503, grad_norm: 16.7101
2025-06-20 11:02:42,312 - mmdet - INFO - Epoch [6][2750/7033]	lr: 1.358e-05, eta: 1:44:53, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0642, loss_bbox: 0.1923, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3077, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1957, loss: 1.8814, grad_norm: 19.2120
2025-06-20 11:03:55,733 - mmdet - INFO - Epoch [6][2800/7033]	lr: 1.358e-05, eta: 1:43:40, time: 1.468, data_time: 0.029, memory: 20458, loss_cls: 0.0583, loss_bbox: 0.1885, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3043, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0654, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1914, loss: 1.8279, grad_norm: 27.0846
2025-06-20 11:05:09,004 - mmdet - INFO - Epoch [6][2850/7033]	lr: 1.358e-05, eta: 1:42:26, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.0618, loss_bbox: 0.1886, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2289, d2.loss_cls: 0.0773, d2.loss_bbox: 0.2085, d3.loss_cls: 0.0677, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1915, loss: 1.8526, grad_norm: 18.9420
2025-06-20 11:06:22,195 - mmdet - INFO - Epoch [6][2900/7033]	lr: 1.358e-05, eta: 1:41:13, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0698, loss_bbox: 0.2013, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3176, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2045, loss: 1.9692, grad_norm: 19.7096
2025-06-20 11:07:35,540 - mmdet - INFO - Epoch [6][2950/7033]	lr: 1.358e-05, eta: 1:39:59, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0563, loss_bbox: 0.1894, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3084, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0734, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0633, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0579, d4.loss_bbox: 0.1931, loss: 1.8348, grad_norm: 31.3562
2025-06-20 11:08:50,222 - mmdet - INFO - Epoch [6][3000/7033]	lr: 1.358e-05, eta: 1:38:46, time: 1.494, data_time: 0.053, memory: 20458, loss_cls: 0.0631, loss_bbox: 0.1934, d0.loss_cls: 0.1568, d0.loss_bbox: 0.2998, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0651, d4.loss_bbox: 0.1964, loss: 1.8591, grad_norm: 21.1040
2025-06-20 11:10:03,546 - mmdet - INFO - Epoch [6][3050/7033]	lr: 1.358e-05, eta: 1:37:32, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0580, loss_bbox: 0.1884, d0.loss_cls: 0.1572, d0.loss_bbox: 0.2972, d1.loss_cls: 0.0878, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0700, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0627, d3.loss_bbox: 0.1984, d4.loss_cls: 0.0600, d4.loss_bbox: 0.1914, loss: 1.8051, grad_norm: 25.1740
2025-06-20 11:11:16,945 - mmdet - INFO - Epoch [6][3100/7033]	lr: 1.358e-05, eta: 1:36:19, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0586, loss_bbox: 0.1873, d0.loss_cls: 0.1579, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0641, d3.loss_bbox: 0.1976, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1903, loss: 1.8134, grad_norm: 19.1072
2025-06-20 11:12:29,961 - mmdet - INFO - Epoch [6][3150/7033]	lr: 1.358e-05, eta: 1:35:05, time: 1.460, data_time: 0.029, memory: 20458, loss_cls: 0.0645, loss_bbox: 0.1951, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3097, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2313, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1981, loss: 1.8897, grad_norm: 32.8430
2025-06-20 11:13:42,986 - mmdet - INFO - Epoch [6][3200/7033]	lr: 1.358e-05, eta: 1:33:52, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0601, loss_bbox: 0.1911, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3017, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0662, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1937, loss: 1.8460, grad_norm: 26.5580
2025-06-20 11:14:56,152 - mmdet - INFO - Epoch [6][3250/7033]	lr: 1.358e-05, eta: 1:32:38, time: 1.463, data_time: 0.028, memory: 20458, loss_cls: 0.0638, loss_bbox: 0.1920, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3112, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1958, loss: 1.8910, grad_norm: 18.8052
2025-06-20 11:16:09,447 - mmdet - INFO - Epoch [6][3300/7033]	lr: 1.358e-05, eta: 1:31:25, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0662, loss_bbox: 0.1926, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3028, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2312, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1962, loss: 1.8786, grad_norm: 43.2903
2025-06-20 11:17:22,655 - mmdet - INFO - Epoch [6][3350/7033]	lr: 1.358e-05, eta: 1:30:11, time: 1.464, data_time: 0.031, memory: 20458, loss_cls: 0.0679, loss_bbox: 0.1970, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3021, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0714, d4.loss_bbox: 0.1977, loss: 1.9140, grad_norm: 16.1848
2025-06-20 11:18:35,500 - mmdet - INFO - Epoch [6][3400/7033]	lr: 1.358e-05, eta: 1:28:58, time: 1.457, data_time: 0.029, memory: 20458, loss_cls: 0.0640, loss_bbox: 0.1925, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0667, d4.loss_bbox: 0.1951, loss: 1.8961, grad_norm: 37.8401
2025-06-20 11:19:48,942 - mmdet - INFO - Epoch [6][3450/7033]	lr: 1.358e-05, eta: 1:27:44, time: 1.469, data_time: 0.027, memory: 20458, loss_cls: 0.0690, loss_bbox: 0.1977, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3094, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2007, loss: 1.9412, grad_norm: 20.2641
2025-06-20 11:21:02,195 - mmdet - INFO - Epoch [6][3500/7033]	lr: 1.358e-05, eta: 1:26:31, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0603, loss_bbox: 0.1885, d0.loss_cls: 0.1667, d0.loss_bbox: 0.2995, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0764, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0668, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1915, loss: 1.8388, grad_norm: 18.3806
2025-06-20 11:22:15,238 - mmdet - INFO - Epoch [6][3550/7033]	lr: 1.358e-05, eta: 1:25:17, time: 1.461, data_time: 0.030, memory: 20458, loss_cls: 0.0576, loss_bbox: 0.1877, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2261, d2.loss_cls: 0.0717, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0637, d3.loss_bbox: 0.1973, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1904, loss: 1.8122, grad_norm: 17.9903
2025-06-20 11:23:30,269 - mmdet - INFO - Epoch [6][3600/7033]	lr: 1.358e-05, eta: 1:24:04, time: 1.501, data_time: 0.029, memory: 20458, loss_cls: 0.0602, loss_bbox: 0.1934, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3079, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0665, d3.loss_bbox: 0.2054, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1980, loss: 1.8736, grad_norm: 31.0065
2025-06-20 11:24:43,321 - mmdet - INFO - Epoch [6][3650/7033]	lr: 1.358e-05, eta: 1:22:51, time: 1.461, data_time: 0.028, memory: 20458, loss_cls: 0.0689, loss_bbox: 0.1943, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3050, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0708, d4.loss_bbox: 0.1973, loss: 1.9168, grad_norm: 24.5561
2025-06-20 11:25:56,425 - mmdet - INFO - Epoch [6][3700/7033]	lr: 1.358e-05, eta: 1:21:37, time: 1.462, data_time: 0.030, memory: 20458, loss_cls: 0.0614, loss_bbox: 0.1917, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3034, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2003, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1951, loss: 1.8618, grad_norm: 24.2013
2025-06-20 11:27:10,441 - mmdet - INFO - Epoch [6][3750/7033]	lr: 1.358e-05, eta: 1:20:24, time: 1.480, data_time: 0.035, memory: 20458, loss_cls: 0.0637, loss_bbox: 0.1897, d0.loss_cls: 0.1661, d0.loss_bbox: 0.2973, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2266, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0692, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1926, loss: 1.8578, grad_norm: 18.4301
2025-06-20 11:28:23,714 - mmdet - INFO - Epoch [6][3800/7033]	lr: 1.358e-05, eta: 1:19:10, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.0686, loss_bbox: 0.1963, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3099, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2005, loss: 1.9151, grad_norm: 49.4968
2025-06-20 11:29:36,882 - mmdet - INFO - Epoch [6][3850/7033]	lr: 1.358e-05, eta: 1:17:57, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0567, loss_bbox: 0.1846, d0.loss_cls: 0.1535, d0.loss_bbox: 0.2937, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2212, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2048, d3.loss_cls: 0.0616, d3.loss_bbox: 0.1931, d4.loss_cls: 0.0579, d4.loss_bbox: 0.1877, loss: 1.7772, grad_norm: 65.8890
2025-06-20 11:30:49,602 - mmdet - INFO - Epoch [6][3900/7033]	lr: 1.358e-05, eta: 1:16:43, time: 1.454, data_time: 0.027, memory: 20458, loss_cls: 0.0608, loss_bbox: 0.1905, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0651, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0624, d4.loss_bbox: 0.1939, loss: 1.8498, grad_norm: 65.0400
2025-06-20 11:32:04,215 - mmdet - INFO - Epoch [6][3950/7033]	lr: 1.358e-05, eta: 1:15:30, time: 1.492, data_time: 0.026, memory: 20458, loss_cls: 0.0623, loss_bbox: 0.1966, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2054, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1990, loss: 1.8845, grad_norm: 33.4006
2025-06-20 11:33:54,581 - mmdet - INFO - Epoch [6][4000/7033]	lr: 1.358e-05, eta: 1:14:19, time: 2.207, data_time: 0.777, memory: 20458, loss_cls: 0.0590, loss_bbox: 0.1876, d0.loss_cls: 0.1610, d0.loss_bbox: 0.2994, d1.loss_cls: 0.0903, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2076, d3.loss_cls: 0.0652, d3.loss_bbox: 0.1972, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1910, loss: 1.8205, grad_norm: 24.8765
2025-06-20 11:35:07,674 - mmdet - INFO - Epoch [6][4050/7033]	lr: 1.358e-05, eta: 1:13:06, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0567, loss_bbox: 0.1827, d0.loss_cls: 0.1655, d0.loss_bbox: 0.2917, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2206, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2029, d3.loss_cls: 0.0629, d3.loss_bbox: 0.1925, d4.loss_cls: 0.0583, d4.loss_bbox: 0.1861, loss: 1.7860, grad_norm: 43.5210
2025-06-20 11:36:21,056 - mmdet - INFO - Epoch [6][4100/7033]	lr: 1.358e-05, eta: 1:11:52, time: 1.468, data_time: 0.036, memory: 20458, loss_cls: 0.0612, loss_bbox: 0.1860, d0.loss_cls: 0.1556, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0922, d1.loss_bbox: 0.2247, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2074, d3.loss_cls: 0.0672, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1895, loss: 1.8176, grad_norm: 22.2444
2025-06-20 11:37:34,623 - mmdet - INFO - Epoch [6][4150/7033]	lr: 1.358e-05, eta: 1:10:39, time: 1.471, data_time: 0.036, memory: 20458, loss_cls: 0.0603, loss_bbox: 0.1854, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3010, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2234, d2.loss_cls: 0.0744, d2.loss_bbox: 0.2055, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1954, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1886, loss: 1.8167, grad_norm: 17.3883
2025-06-20 11:38:49,606 - mmdet - INFO - Epoch [6][4200/7033]	lr: 1.358e-05, eta: 1:09:25, time: 1.500, data_time: 0.037, memory: 20458, loss_cls: 0.0588, loss_bbox: 0.1801, d0.loss_cls: 0.1515, d0.loss_bbox: 0.2915, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2192, d2.loss_cls: 0.0727, d2.loss_bbox: 0.2022, d3.loss_cls: 0.0640, d3.loss_bbox: 0.1917, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1845, loss: 1.7649, grad_norm: 953.8795
2025-06-20 11:40:03,021 - mmdet - INFO - Epoch [6][4250/7033]	lr: 1.358e-05, eta: 1:08:12, time: 1.468, data_time: 0.035, memory: 20458, loss_cls: 0.0568, loss_bbox: 0.1889, d0.loss_cls: 0.1623, d0.loss_bbox: 0.2995, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2280, d2.loss_cls: 0.0727, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0623, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0582, d4.loss_bbox: 0.1911, loss: 1.8200, grad_norm: 17.7488
2025-06-20 11:41:16,307 - mmdet - INFO - Epoch [6][4300/7033]	lr: 1.358e-05, eta: 1:06:58, time: 1.466, data_time: 0.031, memory: 20458, loss_cls: 0.0621, loss_bbox: 0.1932, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0907, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0744, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1961, loss: 1.8709, grad_norm: 25.2243
2025-06-20 11:42:29,866 - mmdet - INFO - Epoch [6][4350/7033]	lr: 1.358e-05, eta: 1:05:45, time: 1.471, data_time: 0.035, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1980, d0.loss_cls: 0.1630, d0.loss_bbox: 0.3075, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0632, d4.loss_bbox: 0.2023, loss: 1.9018, grad_norm: 22.9226
2025-06-20 11:43:43,509 - mmdet - INFO - Epoch [6][4400/7033]	lr: 1.358e-05, eta: 1:04:31, time: 1.473, data_time: 0.037, memory: 20458, loss_cls: 0.0603, loss_bbox: 0.1905, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3066, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0729, d2.loss_bbox: 0.2098, d3.loss_cls: 0.0643, d3.loss_bbox: 0.1991, d4.loss_cls: 0.0609, d4.loss_bbox: 0.1943, loss: 1.8377, grad_norm: 17.7789
2025-06-20 11:44:56,917 - mmdet - INFO - Epoch [6][4450/7033]	lr: 1.358e-05, eta: 1:03:18, time: 1.468, data_time: 0.030, memory: 20458, loss_cls: 0.0668, loss_bbox: 0.1991, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3147, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0662, d4.loss_bbox: 0.2043, loss: 1.9336, grad_norm: 16.4103
2025-06-20 11:46:10,775 - mmdet - INFO - Epoch [6][4500/7033]	lr: 1.358e-05, eta: 1:02:04, time: 1.477, data_time: 0.036, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1974, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0639, d4.loss_bbox: 0.2012, loss: 1.9018, grad_norm: 21.2256
2025-06-20 11:47:24,488 - mmdet - INFO - Epoch [6][4550/7033]	lr: 1.358e-05, eta: 1:00:51, time: 1.474, data_time: 0.038, memory: 20458, loss_cls: 0.0647, loss_bbox: 0.1922, d0.loss_cls: 0.1612, d0.loss_bbox: 0.2990, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0703, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1955, loss: 1.8676, grad_norm: 43.7130
2025-06-20 11:48:37,955 - mmdet - INFO - Epoch [6][4600/7033]	lr: 1.358e-05, eta: 0:59:37, time: 1.469, data_time: 0.032, memory: 20458, loss_cls: 0.0660, loss_bbox: 0.1911, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2011, d4.loss_cls: 0.0688, d4.loss_bbox: 0.1938, loss: 1.8812, grad_norm: 74.7692
2025-06-20 11:49:53,090 - mmdet - INFO - Epoch [6][4650/7033]	lr: 1.358e-05, eta: 0:58:24, time: 1.503, data_time: 0.031, memory: 20458, loss_cls: 0.0609, loss_bbox: 0.1964, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3110, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0743, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0657, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0628, d4.loss_bbox: 0.2006, loss: 1.8938, grad_norm: 23.8747
2025-06-20 11:51:06,205 - mmdet - INFO - Epoch [6][4700/7033]	lr: 1.358e-05, eta: 0:57:10, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0636, loss_bbox: 0.1961, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1989, loss: 1.9022, grad_norm: 27.2224
2025-06-20 11:52:19,522 - mmdet - INFO - Epoch [6][4750/7033]	lr: 1.358e-05, eta: 0:55:56, time: 1.466, data_time: 0.031, memory: 20458, loss_cls: 0.0595, loss_bbox: 0.1919, d0.loss_cls: 0.1669, d0.loss_bbox: 0.2998, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0749, d2.loss_bbox: 0.2128, d3.loss_cls: 0.0646, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1956, loss: 1.8513, grad_norm: 155.9778
2025-06-20 11:53:32,862 - mmdet - INFO - Epoch [6][4800/7033]	lr: 1.358e-05, eta: 0:54:43, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0651, loss_bbox: 0.1943, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3147, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0791, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0670, d4.loss_bbox: 0.1979, loss: 1.9163, grad_norm: 254.0717
2025-06-20 11:54:45,993 - mmdet - INFO - Epoch [6][4850/7033]	lr: 1.358e-05, eta: 0:53:29, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0575, loss_bbox: 0.1884, d0.loss_cls: 0.1569, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2243, d2.loss_cls: 0.0729, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0634, d3.loss_bbox: 0.1983, d4.loss_cls: 0.0593, d4.loss_bbox: 0.1909, loss: 1.8096, grad_norm: 74.0030
2025-06-20 11:55:59,199 - mmdet - INFO - Epoch [6][4900/7033]	lr: 1.358e-05, eta: 0:52:16, time: 1.464, data_time: 0.031, memory: 20458, loss_cls: 0.0654, loss_bbox: 0.1979, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3233, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2024, loss: 1.9470, grad_norm: 34.1490
2025-06-20 11:57:12,441 - mmdet - INFO - Epoch [6][4950/7033]	lr: 1.358e-05, eta: 0:51:02, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0604, loss_bbox: 0.1919, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0907, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2020, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1964, loss: 1.8477, grad_norm: 26.6227
2025-06-20 11:58:25,520 - mmdet - INFO - Epoch [6][5000/7033]	lr: 1.358e-05, eta: 0:49:49, time: 1.462, data_time: 0.029, memory: 20458, loss_cls: 0.0694, loss_bbox: 0.2020, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2067, loss: 1.9956, grad_norm: 25.3534
2025-06-20 11:59:38,886 - mmdet - INFO - Epoch [6][5050/7033]	lr: 1.358e-05, eta: 0:48:35, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0590, loss_bbox: 0.1921, d0.loss_cls: 0.1557, d0.loss_bbox: 0.2997, d1.loss_cls: 0.0877, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0725, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0631, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0593, d4.loss_bbox: 0.1953, loss: 1.8238, grad_norm: 44.3684
2025-06-20 12:00:52,054 - mmdet - INFO - Epoch [6][5100/7033]	lr: 1.358e-05, eta: 0:47:22, time: 1.463, data_time: 0.030, memory: 20458, loss_cls: 0.0630, loss_bbox: 0.1932, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3116, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2335, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2040, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1967, loss: 1.8839, grad_norm: 85.6385
2025-06-20 12:02:05,301 - mmdet - INFO - Epoch [6][5150/7033]	lr: 1.358e-05, eta: 0:46:08, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0610, loss_bbox: 0.1915, d0.loss_cls: 0.1588, d0.loss_bbox: 0.2974, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2015, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1948, loss: 1.8427, grad_norm: 31.4824
2025-06-20 12:03:18,326 - mmdet - INFO - Epoch [6][5200/7033]	lr: 1.358e-05, eta: 0:44:55, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0616, loss_bbox: 0.1905, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3108, d1.loss_cls: 0.0971, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2004, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1950, loss: 1.8722, grad_norm: 52.9727
2025-06-20 12:04:31,814 - mmdet - INFO - Epoch [6][5250/7033]	lr: 1.358e-05, eta: 0:43:41, time: 1.470, data_time: 0.031, memory: 20458, loss_cls: 0.0577, loss_bbox: 0.1906, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0901, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0722, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0627, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0594, d4.loss_bbox: 0.1927, loss: 1.8331, grad_norm: 20.9490
2025-06-20 12:05:44,841 - mmdet - INFO - Epoch [6][5300/7033]	lr: 1.358e-05, eta: 0:42:28, time: 1.461, data_time: 0.030, memory: 20458, loss_cls: 0.0562, loss_bbox: 0.1870, d0.loss_cls: 0.1532, d0.loss_bbox: 0.2985, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2245, d2.loss_cls: 0.0718, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0618, d3.loss_bbox: 0.1952, d4.loss_cls: 0.0577, d4.loss_bbox: 0.1893, loss: 1.7912, grad_norm: 32.1394
2025-06-20 12:06:58,081 - mmdet - INFO - Epoch [6][5350/7033]	lr: 1.358e-05, eta: 0:41:14, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.0603, loss_bbox: 0.1896, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3033, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2290, d2.loss_cls: 0.0754, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0656, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1938, loss: 1.8506, grad_norm: 17.8046
2025-06-20 12:08:12,305 - mmdet - INFO - Epoch [6][5400/7033]	lr: 1.358e-05, eta: 0:40:01, time: 1.484, data_time: 0.049, memory: 20458, loss_cls: 0.0708, loss_bbox: 0.1977, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3093, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2005, loss: 1.9534, grad_norm: 78.9366
2025-06-20 12:09:25,802 - mmdet - INFO - Epoch [6][5450/7033]	lr: 1.358e-05, eta: 0:38:47, time: 1.470, data_time: 0.031, memory: 20458, loss_cls: 0.0601, loss_bbox: 0.1856, d0.loss_cls: 0.1563, d0.loss_bbox: 0.2942, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2180, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0652, d3.loss_bbox: 0.1946, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1882, loss: 1.7938, grad_norm: 32.3385
2025-06-20 12:10:39,461 - mmdet - INFO - Epoch [6][5500/7033]	lr: 1.358e-05, eta: 0:37:34, time: 1.473, data_time: 0.030, memory: 20458, loss_cls: 0.0647, loss_bbox: 0.1980, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3143, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0670, d4.loss_bbox: 0.2013, loss: 1.9235, grad_norm: 25.3405
2025-06-20 12:11:52,819 - mmdet - INFO - Epoch [6][5550/7033]	lr: 1.358e-05, eta: 0:36:20, time: 1.467, data_time: 0.033, memory: 20458, loss_cls: 0.0693, loss_bbox: 0.2011, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3145, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2051, loss: 1.9659, grad_norm: 36.3142
2025-06-20 12:13:05,917 - mmdet - INFO - Epoch [6][5600/7033]	lr: 1.358e-05, eta: 0:35:07, time: 1.462, data_time: 0.028, memory: 20458, loss_cls: 0.0674, loss_bbox: 0.1981, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3125, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2009, loss: 1.9330, grad_norm: 42.9271
2025-06-20 12:14:18,941 - mmdet - INFO - Epoch [6][5650/7033]	lr: 1.358e-05, eta: 0:33:53, time: 1.460, data_time: 0.031, memory: 20458, loss_cls: 0.0617, loss_bbox: 0.1921, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1964, loss: 1.8642, grad_norm: 34.2700
2025-06-20 12:15:32,250 - mmdet - INFO - Epoch [6][5700/7033]	lr: 1.358e-05, eta: 0:32:39, time: 1.466, data_time: 0.030, memory: 20458, loss_cls: 0.0589, loss_bbox: 0.1908, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0658, d3.loss_bbox: 0.1999, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1945, loss: 1.8462, grad_norm: 40.7578
2025-06-20 12:16:47,294 - mmdet - INFO - Epoch [6][5750/7033]	lr: 1.358e-05, eta: 0:31:26, time: 1.501, data_time: 0.031, memory: 20458, loss_cls: 0.0663, loss_bbox: 0.1940, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0678, d4.loss_bbox: 0.1997, loss: 1.9022, grad_norm: 48.4381
2025-06-20 12:18:00,518 - mmdet - INFO - Epoch [6][5800/7033]	lr: 1.358e-05, eta: 0:30:12, time: 1.464, data_time: 0.028, memory: 20458, loss_cls: 0.0636, loss_bbox: 0.2000, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3168, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0650, d4.loss_bbox: 0.2035, loss: 1.9257, grad_norm: 45.2268
2025-06-20 12:19:13,772 - mmdet - INFO - Epoch [6][5850/7033]	lr: 1.358e-05, eta: 0:28:59, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0665, loss_bbox: 0.1961, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3137, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2070, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1993, loss: 1.9316, grad_norm: 25.0033
2025-06-20 12:20:26,901 - mmdet - INFO - Epoch [6][5900/7033]	lr: 1.358e-05, eta: 0:27:45, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0639, loss_bbox: 0.1860, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3028, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0689, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1901, loss: 1.8411, grad_norm: 14.3426
2025-06-20 12:21:40,418 - mmdet - INFO - Epoch [6][5950/7033]	lr: 1.358e-05, eta: 0:26:32, time: 1.470, data_time: 0.032, memory: 20458, loss_cls: 0.0632, loss_bbox: 0.1878, d0.loss_cls: 0.1595, d0.loss_bbox: 0.3005, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2264, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0685, d3.loss_bbox: 0.1970, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1904, loss: 1.8377, grad_norm: 29.4567
2025-06-20 12:22:53,806 - mmdet - INFO - Epoch [6][6000/7033]	lr: 1.358e-05, eta: 0:25:18, time: 1.468, data_time: 0.032, memory: 20458, loss_cls: 0.0653, loss_bbox: 0.1953, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3064, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0664, d4.loss_bbox: 0.1993, loss: 1.8956, grad_norm: 26.5490
2025-06-20 12:24:06,929 - mmdet - INFO - Epoch [6][6050/7033]	lr: 1.358e-05, eta: 0:24:05, time: 1.462, data_time: 0.027, memory: 20458, loss_cls: 0.0606, loss_bbox: 0.1891, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2260, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0667, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1943, loss: 1.8434, grad_norm: 43.6389
2025-06-20 12:25:20,271 - mmdet - INFO - Epoch [6][6100/7033]	lr: 1.358e-05, eta: 0:22:51, time: 1.467, data_time: 0.030, memory: 20458, loss_cls: 0.0589, loss_bbox: 0.1876, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3009, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0733, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0637, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1905, loss: 1.8184, grad_norm: 63.7749
2025-06-20 12:26:35,028 - mmdet - INFO - Epoch [6][6150/7033]	lr: 1.358e-05, eta: 0:21:38, time: 1.495, data_time: 0.028, memory: 20458, loss_cls: 0.0594, loss_bbox: 0.1906, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3050, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2008, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1945, loss: 1.8456, grad_norm: 22.4178
2025-06-20 12:27:48,244 - mmdet - INFO - Epoch [6][6200/7033]	lr: 1.358e-05, eta: 0:20:24, time: 1.464, data_time: 0.030, memory: 20458, loss_cls: 0.0621, loss_bbox: 0.1912, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3052, d1.loss_cls: 0.0892, d1.loss_bbox: 0.2308, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2114, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1951, loss: 1.8437, grad_norm: 57.6833
2025-06-20 12:29:01,617 - mmdet - INFO - Epoch [6][6250/7033]	lr: 1.358e-05, eta: 0:19:11, time: 1.467, data_time: 0.031, memory: 20458, loss_cls: 0.0627, loss_bbox: 0.1901, d0.loss_cls: 0.1607, d0.loss_bbox: 0.2982, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2242, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0687, d3.loss_bbox: 0.1982, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1924, loss: 1.8400, grad_norm: 19.7308
2025-06-20 12:30:14,865 - mmdet - INFO - Epoch [6][6300/7033]	lr: 1.358e-05, eta: 0:17:57, time: 1.465, data_time: 0.031, memory: 20458, loss_cls: 0.0621, loss_bbox: 0.1849, d0.loss_cls: 0.1582, d0.loss_bbox: 0.2997, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2245, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0687, d3.loss_bbox: 0.1939, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1889, loss: 1.8239, grad_norm: 47.1534
2025-06-20 12:31:28,036 - mmdet - INFO - Epoch [6][6350/7033]	lr: 1.358e-05, eta: 0:16:44, time: 1.463, data_time: 0.031, memory: 20458, loss_cls: 0.0616, loss_bbox: 0.1958, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3049, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1990, loss: 1.8733, grad_norm: 16.5009
2025-06-20 12:32:41,589 - mmdet - INFO - Epoch [6][6400/7033]	lr: 1.358e-05, eta: 0:15:30, time: 1.471, data_time: 0.031, memory: 20458, loss_cls: 0.0658, loss_bbox: 0.2024, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3135, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0703, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2051, loss: 1.9424, grad_norm: 31.7530
2025-06-20 12:33:56,580 - mmdet - INFO - Epoch [6][6450/7033]	lr: 1.358e-05, eta: 0:14:17, time: 1.500, data_time: 0.031, memory: 20458, loss_cls: 0.0600, loss_bbox: 0.1997, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3141, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0657, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0617, d4.loss_bbox: 0.2029, loss: 1.9044, grad_norm: 49.3564
2025-06-20 12:35:09,824 - mmdet - INFO - Epoch [6][6500/7033]	lr: 1.358e-05, eta: 0:13:03, time: 1.465, data_time: 0.029, memory: 20458, loss_cls: 0.0630, loss_bbox: 0.1976, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2293, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2146, d3.loss_cls: 0.0666, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0635, d4.loss_bbox: 0.2010, loss: 1.8738, grad_norm: 23.7075
2025-06-20 12:36:22,888 - mmdet - INFO - Epoch [6][6550/7033]	lr: 1.358e-05, eta: 0:11:50, time: 1.461, data_time: 0.029, memory: 20458, loss_cls: 0.0648, loss_bbox: 0.1934, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3110, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2026, d4.loss_cls: 0.0665, d4.loss_bbox: 0.1970, loss: 1.9010, grad_norm: 16.3369
2025-06-20 12:37:36,058 - mmdet - INFO - Epoch [6][6600/7033]	lr: 1.358e-05, eta: 0:10:36, time: 1.463, data_time: 0.029, memory: 20458, loss_cls: 0.0698, loss_bbox: 0.1925, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0706, d4.loss_bbox: 0.1981, loss: 1.9142, grad_norm: 17.8432
2025-06-20 12:38:49,293 - mmdet - INFO - Epoch [6][6650/7033]	lr: 1.358e-05, eta: 0:09:23, time: 1.465, data_time: 0.030, memory: 20458, loss_cls: 0.0654, loss_bbox: 0.1959, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3029, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1985, loss: 1.8916, grad_norm: 24.4061
2025-06-20 12:40:02,317 - mmdet - INFO - Epoch [6][6700/7033]	lr: 1.358e-05, eta: 0:08:09, time: 1.460, data_time: 0.028, memory: 20458, loss_cls: 0.0626, loss_bbox: 0.1936, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3078, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2128, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1963, loss: 1.8723, grad_norm: 25.6219
2025-06-20 12:41:15,442 - mmdet - INFO - Epoch [6][6750/7033]	lr: 1.358e-05, eta: 0:06:56, time: 1.462, data_time: 0.025, memory: 20458, loss_cls: 0.0688, loss_bbox: 0.1957, d0.loss_cls: 0.1643, d0.loss_bbox: 0.3133, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0700, d4.loss_bbox: 0.1999, loss: 1.9268, grad_norm: 62.9217
2025-06-20 12:42:28,501 - mmdet - INFO - Epoch [6][6800/7033]	lr: 1.358e-05, eta: 0:05:42, time: 1.461, data_time: 0.026, memory: 20458, loss_cls: 0.0609, loss_bbox: 0.1882, d0.loss_cls: 0.1576, d0.loss_bbox: 0.2981, d1.loss_cls: 0.0872, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0723, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1914, loss: 1.8184, grad_norm: 34.2983
2025-06-20 12:43:43,436 - mmdet - INFO - Epoch [6][6850/7033]	lr: 1.358e-05, eta: 0:04:29, time: 1.499, data_time: 0.026, memory: 20458, loss_cls: 0.0634, loss_bbox: 0.1946, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1977, loss: 1.8911, grad_norm: 51.5278
2025-06-20 12:44:56,645 - mmdet - INFO - Epoch [6][6900/7033]	lr: 1.358e-05, eta: 0:03:15, time: 1.464, data_time: 0.029, memory: 20458, loss_cls: 0.0594, loss_bbox: 0.1938, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3036, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0628, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1962, loss: 1.8531, grad_norm: 39.0285
2025-06-20 12:46:09,606 - mmdet - INFO - Epoch [6][6950/7033]	lr: 1.358e-05, eta: 0:02:02, time: 1.459, data_time: 0.027, memory: 20458, loss_cls: 0.0639, loss_bbox: 0.1962, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3094, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2054, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1997, loss: 1.9084, grad_norm: 27.2998
2025-06-20 12:47:22,767 - mmdet - INFO - Epoch [6][7000/7033]	lr: 1.358e-05, eta: 0:00:48, time: 1.463, data_time: 0.027, memory: 20458, loss_cls: 0.0583, loss_bbox: 0.1875, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3004, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0633, d3.loss_bbox: 0.1988, d4.loss_cls: 0.0601, d4.loss_bbox: 0.1911, loss: 1.8246, grad_norm: 19.2833
2025-06-20 12:48:11,544 - mmdet - INFO - Saving checkpoint at 6 epochs
2025-06-20 13:11:02,112 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-20 13:11:02,112 - mmdet - INFO - Epoch(val) [6][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7958, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8856, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9105, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9226, pts_bbox_NuScenes/car_trans_err: 0.1739, pts_bbox_NuScenes/car_scale_err: 0.1511, pts_bbox_NuScenes/car_orient_err: 0.0404, pts_bbox_NuScenes/car_vel_err: 0.2906, pts_bbox_NuScenes/car_attr_err: 0.1869, pts_bbox_NuScenes/mATE: 0.2824, pts_bbox_NuScenes/mASE: 0.2605, pts_bbox_NuScenes/mAOE: 0.2364, pts_bbox_NuScenes/mAVE: 0.2607, pts_bbox_NuScenes/mAAE: 0.1849, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4290, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6157, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7220, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7556, pts_bbox_NuScenes/truck_trans_err: 0.3411, pts_bbox_NuScenes/truck_scale_err: 0.1928, pts_bbox_NuScenes/truck_orient_err: 0.0444, pts_bbox_NuScenes/truck_vel_err: 0.2346, pts_bbox_NuScenes/truck_attr_err: 0.2115, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0563, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2027, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4096, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4818, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6656, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4320, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7610, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1069, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3056, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5447, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7535, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9085, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9290, pts_bbox_NuScenes/bus_trans_err: 0.3186, pts_bbox_NuScenes/bus_scale_err: 0.1923, pts_bbox_NuScenes/bus_orient_err: 0.0350, pts_bbox_NuScenes/bus_vel_err: 0.4390, pts_bbox_NuScenes/bus_attr_err: 0.2383, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1811, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4472, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.6026, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6810, pts_bbox_NuScenes/trailer_trans_err: 0.4709, pts_bbox_NuScenes/trailer_scale_err: 0.2237, pts_bbox_NuScenes/trailer_orient_err: 0.3855, pts_bbox_NuScenes/trailer_vel_err: 0.2119, pts_bbox_NuScenes/trailer_attr_err: 0.1719, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6126, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7134, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7627, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7767, pts_bbox_NuScenes/barrier_trans_err: 0.2064, pts_bbox_NuScenes/barrier_scale_err: 0.2843, pts_bbox_NuScenes/barrier_orient_err: 0.0426, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6563, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7789, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8042, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8117, pts_bbox_NuScenes/motorcycle_trans_err: 0.2080, pts_bbox_NuScenes/motorcycle_scale_err: 0.2411, pts_bbox_NuScenes/motorcycle_orient_err: 0.1987, pts_bbox_NuScenes/motorcycle_vel_err: 0.3841, pts_bbox_NuScenes/motorcycle_attr_err: 0.2463, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5574, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6038, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6127, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6205, pts_bbox_NuScenes/bicycle_trans_err: 0.1675, pts_bbox_NuScenes/bicycle_scale_err: 0.2698, pts_bbox_NuScenes/bicycle_orient_err: 0.2933, pts_bbox_NuScenes/bicycle_vel_err: 0.2061, pts_bbox_NuScenes/bicycle_attr_err: 0.0062, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8171, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8582, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8781, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8901, pts_bbox_NuScenes/pedestrian_trans_err: 0.1445, pts_bbox_NuScenes/pedestrian_scale_err: 0.2907, pts_bbox_NuScenes/pedestrian_orient_err: 0.3268, pts_bbox_NuScenes/pedestrian_vel_err: 0.2126, pts_bbox_NuScenes/pedestrian_attr_err: 0.1126, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7452, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7767, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7991, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8216, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1270, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3268, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7167, pts_bbox_NuScenes/mAP: 0.6783
