2025-06-10 22:44:29,320 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+91e369b
spconv2.0: True
------------------------------------------------------------

2025-06-10 22:44:30,862 - mmdet - INFO - 分布式训练: True
2025-06-10 22:44:32,339 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/lr2'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
gpu_ids = range(0, 2)

2025-06-10 22:44:32,340 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-10 22:44:40,900 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-10 22:44:41,447 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-10 22:44:41,789 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,791 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,791 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,793 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,795 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,796 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,797 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,801 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,813 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,819 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,826 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,832 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,838 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,845 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,851 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,857 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,864 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,870 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,876 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,883 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,889 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,900 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,906 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,912 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,919 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,925 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,931 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,938 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,944 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,950 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:41,990 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:42,017 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:42,044 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 22:44:42,138 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-10 22:44:42,196 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-10 22:46:37,166 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-10 22:46:55,431 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias

2025-06-10 22:46:55,457 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/lr2
2025-06-10 22:46:55,458 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-10 22:46:55,458 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-10 22:46:55,459 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/lr2 by HardDiskBackend.
2025-06-10 22:48:47,351 - mmdet - INFO - Epoch [1][50/7033]	lr: 7.973e-05, eta: 1 day, 2:02:13, time: 2.224, data_time: 0.219, memory: 16751, loss_cls: 1.0392, loss_bbox: 1.4716, d0.loss_cls: 1.0288, d0.loss_bbox: 1.5379, d1.loss_cls: 1.0663, d1.loss_bbox: 1.4884, d2.loss_cls: 0.9766, d2.loss_bbox: 1.4546, d3.loss_cls: 0.9473, d3.loss_bbox: 1.4680, d4.loss_cls: 0.9500, d4.loss_bbox: 1.4360, loss: 14.8647, grad_norm: 15.3360
2025-06-10 22:50:08,209 - mmdet - INFO - Epoch [1][100/7033]	lr: 9.307e-05, eta: 22:28:04, time: 1.619, data_time: 0.055, memory: 16901, loss_cls: 0.7778, loss_bbox: 1.1572, d0.loss_cls: 0.8518, d0.loss_bbox: 1.2807, d1.loss_cls: 0.7923, d1.loss_bbox: 1.2165, d2.loss_cls: 0.7641, d2.loss_bbox: 1.1899, d3.loss_cls: 0.7616, d3.loss_bbox: 1.1820, d4.loss_cls: 0.7557, d4.loss_bbox: 1.1549, loss: 11.8846, grad_norm: 11.8266
2025-06-10 22:51:32,048 - mmdet - INFO - Epoch [1][150/7033]	lr: 1.064e-04, eta: 21:29:18, time: 1.677, data_time: 0.076, memory: 16901, loss_cls: 0.6659, loss_bbox: 1.0734, d0.loss_cls: 0.7731, d0.loss_bbox: 1.2371, d1.loss_cls: 0.6990, d1.loss_bbox: 1.1469, d2.loss_cls: 0.6848, d2.loss_bbox: 1.1073, d3.loss_cls: 0.6771, d3.loss_bbox: 1.0942, d4.loss_cls: 0.6535, d4.loss_bbox: 1.0857, loss: 10.8980, grad_norm: 10.4625
2025-06-10 22:52:55,949 - mmdet - INFO - Epoch [1][200/7033]	lr: 1.197e-04, eta: 20:59:07, time: 1.676, data_time: 0.052, memory: 16901, loss_cls: 0.6049, loss_bbox: 1.0469, d0.loss_cls: 0.7231, d0.loss_bbox: 1.2149, d1.loss_cls: 0.6589, d1.loss_bbox: 1.1244, d2.loss_cls: 0.6431, d2.loss_bbox: 1.0793, d3.loss_cls: 0.6346, d3.loss_bbox: 1.0699, d4.loss_cls: 0.5982, d4.loss_bbox: 1.0592, loss: 10.4573, grad_norm: 10.0608
2025-06-10 22:54:21,446 - mmdet - INFO - Epoch [1][250/7033]	lr: 1.331e-04, eta: 20:45:27, time: 1.712, data_time: 0.050, memory: 16901, loss_cls: 0.5578, loss_bbox: 1.0168, d0.loss_cls: 0.6864, d0.loss_bbox: 1.1780, d1.loss_cls: 0.6200, d1.loss_bbox: 1.0777, d2.loss_cls: 0.6129, d2.loss_bbox: 1.0449, d3.loss_cls: 0.5997, d3.loss_bbox: 1.0328, d4.loss_cls: 0.5522, d4.loss_bbox: 1.0257, loss: 10.0048, grad_norm: 10.5622
2025-06-10 22:55:44,053 - mmdet - INFO - Epoch [1][300/7033]	lr: 1.464e-04, eta: 20:28:55, time: 1.652, data_time: 0.048, memory: 16901, loss_cls: 0.5076, loss_bbox: 0.9656, d0.loss_cls: 0.6606, d0.loss_bbox: 1.1402, d1.loss_cls: 0.5701, d1.loss_bbox: 1.0326, d2.loss_cls: 0.5560, d2.loss_bbox: 0.9928, d3.loss_cls: 0.5508, d3.loss_bbox: 0.9800, d4.loss_cls: 0.4973, d4.loss_bbox: 0.9821, loss: 9.4358, grad_norm: 10.5014
2025-06-10 22:57:08,259 - mmdet - INFO - Epoch [1][350/7033]	lr: 1.597e-04, eta: 20:19:54, time: 1.684, data_time: 0.107, memory: 16901, loss_cls: 0.4619, loss_bbox: 0.9803, d0.loss_cls: 0.6316, d0.loss_bbox: 1.1556, d1.loss_cls: 0.5253, d1.loss_bbox: 1.0389, d2.loss_cls: 0.5201, d2.loss_bbox: 0.9911, d3.loss_cls: 0.5057, d3.loss_bbox: 0.9886, d4.loss_cls: 0.4572, d4.loss_bbox: 0.9880, loss: 9.2444, grad_norm: 11.4333
2025-06-10 22:58:36,378 - mmdet - INFO - Epoch [1][400/7033]	lr: 1.731e-04, eta: 20:19:36, time: 1.762, data_time: 0.051, memory: 16939, loss_cls: 0.4421, loss_bbox: 0.9269, d0.loss_cls: 0.5914, d0.loss_bbox: 1.1137, d1.loss_cls: 0.4875, d1.loss_bbox: 0.9993, d2.loss_cls: 0.4736, d2.loss_bbox: 0.9480, d3.loss_cls: 0.4654, d3.loss_bbox: 0.9385, d4.loss_cls: 0.4347, d4.loss_bbox: 0.9335, loss: 8.7545, grad_norm: 11.2744
2025-06-10 23:00:08,320 - mmdet - INFO - Epoch [1][450/7033]	lr: 1.864e-04, eta: 20:24:55, time: 1.839, data_time: 0.090, memory: 16939, loss_cls: 0.4290, loss_bbox: 0.9008, d0.loss_cls: 0.5588, d0.loss_bbox: 1.0953, d1.loss_cls: 0.4608, d1.loss_bbox: 0.9696, d2.loss_cls: 0.4597, d2.loss_bbox: 0.9153, d3.loss_cls: 0.4481, d3.loss_bbox: 0.9038, d4.loss_cls: 0.4317, d4.loss_bbox: 0.8978, loss: 8.4707, grad_norm: 11.3188
2025-06-10 23:01:32,036 - mmdet - INFO - Epoch [1][500/7033]	lr: 1.997e-04, eta: 20:17:29, time: 1.675, data_time: 0.095, memory: 16939, loss_cls: 0.3943, loss_bbox: 0.8634, d0.loss_cls: 0.5541, d0.loss_bbox: 1.0909, d1.loss_cls: 0.3969, d1.loss_bbox: 0.9444, d2.loss_cls: 0.3987, d2.loss_bbox: 0.8794, d3.loss_cls: 0.3954, d3.loss_bbox: 0.8611, d4.loss_cls: 0.3847, d4.loss_bbox: 0.8612, loss: 8.0246, grad_norm: 12.3520
2025-06-10 23:02:56,315 - mmdet - INFO - Epoch [1][550/7033]	lr: 2.000e-04, eta: 20:11:50, time: 1.686, data_time: 0.071, memory: 16986, loss_cls: 0.3405, loss_bbox: 0.8746, d0.loss_cls: 0.5446, d0.loss_bbox: 1.0808, d1.loss_cls: 0.3539, d1.loss_bbox: 0.9162, d2.loss_cls: 0.3423, d2.loss_bbox: 0.8621, d3.loss_cls: 0.3305, d3.loss_bbox: 0.8534, d4.loss_cls: 0.3264, d4.loss_bbox: 0.8602, loss: 7.6856, grad_norm: 15.4977
2025-06-10 23:04:21,007 - mmdet - INFO - Epoch [1][600/7033]	lr: 2.000e-04, eta: 20:07:22, time: 1.694, data_time: 0.097, memory: 17172, loss_cls: 0.2847, loss_bbox: 0.8308, d0.loss_cls: 0.5033, d0.loss_bbox: 1.0636, d1.loss_cls: 0.3205, d1.loss_bbox: 0.8806, d2.loss_cls: 0.2944, d2.loss_bbox: 0.8248, d3.loss_cls: 0.2805, d3.loss_bbox: 0.8150, d4.loss_cls: 0.2774, d4.loss_bbox: 0.8133, loss: 7.1889, grad_norm: 14.6937
2025-06-10 23:05:43,586 - mmdet - INFO - Epoch [1][650/7033]	lr: 2.000e-04, eta: 20:01:08, time: 1.652, data_time: 0.081, memory: 17172, loss_cls: 0.2842, loss_bbox: 0.7800, d0.loss_cls: 0.5001, d0.loss_bbox: 1.0533, d1.loss_cls: 0.3120, d1.loss_bbox: 0.8363, d2.loss_cls: 0.2874, d2.loss_bbox: 0.7715, d3.loss_cls: 0.2841, d3.loss_bbox: 0.7611, d4.loss_cls: 0.2730, d4.loss_bbox: 0.7646, loss: 6.9075, grad_norm: 17.3764
2025-06-10 23:07:06,640 - mmdet - INFO - Epoch [1][700/7033]	lr: 2.000e-04, eta: 19:56:02, time: 1.661, data_time: 0.060, memory: 17172, loss_cls: 0.2719, loss_bbox: 0.7056, d0.loss_cls: 0.4948, d0.loss_bbox: 1.0338, d1.loss_cls: 0.3291, d1.loss_bbox: 0.7560, d2.loss_cls: 0.2870, d2.loss_bbox: 0.6851, d3.loss_cls: 0.2699, d3.loss_bbox: 0.6829, d4.loss_cls: 0.2689, d4.loss_bbox: 0.6846, loss: 6.4698, grad_norm: 17.4860
2025-06-10 23:08:32,259 - mmdet - INFO - Epoch [1][750/7033]	lr: 2.000e-04, eta: 19:53:50, time: 1.713, data_time: 0.099, memory: 17172, loss_cls: 0.2583, loss_bbox: 0.6609, d0.loss_cls: 0.4806, d0.loss_bbox: 1.0358, d1.loss_cls: 0.3187, d1.loss_bbox: 0.6891, d2.loss_cls: 0.2757, d2.loss_bbox: 0.6334, d3.loss_cls: 0.2523, d3.loss_bbox: 0.6421, d4.loss_cls: 0.2504, d4.loss_bbox: 0.6452, loss: 6.1427, grad_norm: 16.6577
2025-06-10 23:09:54,875 - mmdet - INFO - Epoch [1][800/7033]	lr: 2.000e-04, eta: 19:49:05, time: 1.651, data_time: 0.074, memory: 17172, loss_cls: 0.2340, loss_bbox: 0.6218, d0.loss_cls: 0.4584, d0.loss_bbox: 0.9895, d1.loss_cls: 0.3222, d1.loss_bbox: 0.5728, d2.loss_cls: 0.2619, d2.loss_bbox: 0.5501, d3.loss_cls: 0.2399, d3.loss_bbox: 0.5638, d4.loss_cls: 0.2348, d4.loss_bbox: 0.5844, loss: 5.6337, grad_norm: 22.9534
2025-06-10 23:11:17,937 - mmdet - INFO - Epoch [1][850/7033]	lr: 2.000e-04, eta: 19:45:05, time: 1.660, data_time: 0.077, memory: 17172, loss_cls: 0.2235, loss_bbox: 0.5676, d0.loss_cls: 0.4395, d0.loss_bbox: 0.9538, d1.loss_cls: 0.3156, d1.loss_bbox: 0.5380, d2.loss_cls: 0.2519, d2.loss_bbox: 0.5244, d3.loss_cls: 0.2274, d3.loss_bbox: 0.5382, d4.loss_cls: 0.2218, d4.loss_bbox: 0.5450, loss: 5.3468, grad_norm: 20.7871
2025-06-10 23:12:43,472 - mmdet - INFO - Epoch [1][900/7033]	lr: 2.000e-04, eta: 19:43:21, time: 1.712, data_time: 0.053, memory: 17172, loss_cls: 0.2345, loss_bbox: 0.5246, d0.loss_cls: 0.4274, d0.loss_bbox: 0.9209, d1.loss_cls: 0.3090, d1.loss_bbox: 0.5017, d2.loss_cls: 0.2555, d2.loss_bbox: 0.4927, d3.loss_cls: 0.2312, d3.loss_bbox: 0.5052, d4.loss_cls: 0.2305, d4.loss_bbox: 0.5135, loss: 5.1467, grad_norm: 37.4055
2025-06-10 23:14:10,225 - mmdet - INFO - Epoch [1][950/7033]	lr: 2.000e-04, eta: 19:42:31, time: 1.736, data_time: 0.070, memory: 17172, loss_cls: 0.1882, loss_bbox: 0.4859, d0.loss_cls: 0.3633, d0.loss_bbox: 0.8424, d1.loss_cls: 0.2756, d1.loss_bbox: 0.4477, d2.loss_cls: 0.2175, d2.loss_bbox: 0.4344, d3.loss_cls: 0.1912, d3.loss_bbox: 0.4498, d4.loss_cls: 0.1873, d4.loss_bbox: 0.4629, loss: 4.5461, grad_norm: 22.5510
2025-06-10 23:15:33,615 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 23:15:33,616 - mmdet - INFO - Epoch [1][1000/7033]	lr: 2.000e-04, eta: 19:39:16, time: 1.667, data_time: 0.107, memory: 17172, loss_cls: 0.1682, loss_bbox: 0.4426, d0.loss_cls: 0.3298, d0.loss_bbox: 0.7259, d1.loss_cls: 0.2599, d1.loss_bbox: 0.4049, d2.loss_cls: 0.2005, d2.loss_bbox: 0.3960, d3.loss_cls: 0.1762, d3.loss_bbox: 0.4066, d4.loss_cls: 0.1699, d4.loss_bbox: 0.4206, loss: 4.1011, grad_norm: 28.7325
2025-06-10 23:16:59,734 - mmdet - INFO - Epoch [1][1050/7033]	lr: 2.000e-04, eta: 19:38:01, time: 1.723, data_time: 0.054, memory: 17172, loss_cls: 0.1754, loss_bbox: 0.4321, d0.loss_cls: 0.3236, d0.loss_bbox: 0.6642, d1.loss_cls: 0.2570, d1.loss_bbox: 0.4112, d2.loss_cls: 0.2011, d2.loss_bbox: 0.3951, d3.loss_cls: 0.1796, d3.loss_bbox: 0.4063, d4.loss_cls: 0.1784, d4.loss_bbox: 0.4147, loss: 4.0387, grad_norm: 32.6120
2025-06-10 23:18:22,659 - mmdet - INFO - Epoch [1][1100/7033]	lr: 2.000e-04, eta: 19:34:41, time: 1.657, data_time: 0.081, memory: 17172, loss_cls: 0.1649, loss_bbox: 0.4046, d0.loss_cls: 0.2757, d0.loss_bbox: 0.5669, d1.loss_cls: 0.2187, d1.loss_bbox: 0.3712, d2.loss_cls: 0.1831, d2.loss_bbox: 0.3643, d3.loss_cls: 0.1663, d3.loss_bbox: 0.3736, d4.loss_cls: 0.1618, d4.loss_bbox: 0.3896, loss: 3.6408, grad_norm: 39.9644
2025-06-10 23:19:44,219 - mmdet - INFO - Epoch [1][1150/7033]	lr: 2.000e-04, eta: 19:30:49, time: 1.633, data_time: 0.089, memory: 17172, loss_cls: 0.1669, loss_bbox: 0.3936, d0.loss_cls: 0.2699, d0.loss_bbox: 0.5554, d1.loss_cls: 0.2313, d1.loss_bbox: 0.3773, d2.loss_cls: 0.1875, d2.loss_bbox: 0.3643, d3.loss_cls: 0.1716, d3.loss_bbox: 0.3682, d4.loss_cls: 0.1631, d4.loss_bbox: 0.3778, loss: 3.6269, grad_norm: 44.5864
2025-06-10 23:21:06,699 - mmdet - INFO - Epoch [1][1200/7033]	lr: 2.000e-04, eta: 19:27:37, time: 1.649, data_time: 0.074, memory: 17172, loss_cls: 0.1636, loss_bbox: 0.4114, d0.loss_cls: 0.2553, d0.loss_bbox: 0.5396, d1.loss_cls: 0.2153, d1.loss_bbox: 0.3781, d2.loss_cls: 0.1792, d2.loss_bbox: 0.3690, d3.loss_cls: 0.1673, d3.loss_bbox: 0.3770, d4.loss_cls: 0.1623, d4.loss_bbox: 0.3910, loss: 3.6092, grad_norm: 31.2919
2025-06-10 23:22:32,854 - mmdet - INFO - Epoch [1][1250/7033]	lr: 2.000e-04, eta: 19:26:35, time: 1.723, data_time: 0.092, memory: 17172, loss_cls: 0.1501, loss_bbox: 0.3867, d0.loss_cls: 0.2479, d0.loss_bbox: 0.5098, d1.loss_cls: 0.1939, d1.loss_bbox: 0.3609, d2.loss_cls: 0.1679, d2.loss_bbox: 0.3531, d3.loss_cls: 0.1532, d3.loss_bbox: 0.3579, d4.loss_cls: 0.1490, d4.loss_bbox: 0.3707, loss: 3.4011, grad_norm: 30.7275
2025-06-10 23:24:01,557 - mmdet - INFO - Epoch [1][1300/7033]	lr: 2.000e-04, eta: 19:26:51, time: 1.774, data_time: 0.105, memory: 17172, loss_cls: 0.1403, loss_bbox: 0.3639, d0.loss_cls: 0.2446, d0.loss_bbox: 0.4812, d1.loss_cls: 0.1866, d1.loss_bbox: 0.3406, d2.loss_cls: 0.1530, d2.loss_bbox: 0.3287, d3.loss_cls: 0.1411, d3.loss_bbox: 0.3389, d4.loss_cls: 0.1362, d4.loss_bbox: 0.3495, loss: 3.2047, grad_norm: 569.3379
2025-06-10 23:25:27,028 - mmdet - INFO - Epoch [1][1350/7033]	lr: 2.000e-04, eta: 19:25:20, time: 1.708, data_time: 0.077, memory: 17172, loss_cls: 0.1342, loss_bbox: 0.3691, d0.loss_cls: 0.2332, d0.loss_bbox: 0.4841, d1.loss_cls: 0.1765, d1.loss_bbox: 0.3425, d2.loss_cls: 0.1428, d2.loss_bbox: 0.3321, d3.loss_cls: 0.1343, d3.loss_bbox: 0.3413, d4.loss_cls: 0.1318, d4.loss_bbox: 0.3544, loss: 3.1764, grad_norm: 36.1816
2025-06-10 23:26:52,862 - mmdet - INFO - Epoch [1][1400/7033]	lr: 2.000e-04, eta: 19:24:04, time: 1.718, data_time: 0.063, memory: 17172, loss_cls: 0.1362, loss_bbox: 0.3525, d0.loss_cls: 0.2307, d0.loss_bbox: 0.4671, d1.loss_cls: 0.1764, d1.loss_bbox: 0.3287, d2.loss_cls: 0.1479, d2.loss_bbox: 0.3213, d3.loss_cls: 0.1358, d3.loss_bbox: 0.3279, d4.loss_cls: 0.1335, d4.loss_bbox: 0.3372, loss: 3.0952, grad_norm: 32.2478
2025-06-10 23:28:15,649 - mmdet - INFO - Epoch [1][1450/7033]	lr: 2.000e-04, eta: 19:21:19, time: 1.656, data_time: 0.085, memory: 17172, loss_cls: 0.1384, loss_bbox: 0.3659, d0.loss_cls: 0.2263, d0.loss_bbox: 0.4631, d1.loss_cls: 0.1670, d1.loss_bbox: 0.3372, d2.loss_cls: 0.1470, d2.loss_bbox: 0.3303, d3.loss_cls: 0.1384, d3.loss_bbox: 0.3416, d4.loss_cls: 0.1383, d4.loss_bbox: 0.3516, loss: 3.1451, grad_norm: 58.7762
2025-06-10 23:29:40,803 - mmdet - INFO - Epoch [1][1500/7033]	lr: 2.000e-04, eta: 19:19:44, time: 1.703, data_time: 0.091, memory: 17172, loss_cls: 0.1307, loss_bbox: 0.3416, d0.loss_cls: 0.2384, d0.loss_bbox: 0.4538, d1.loss_cls: 0.1673, d1.loss_bbox: 0.3273, d2.loss_cls: 0.1436, d2.loss_bbox: 0.3154, d3.loss_cls: 0.1318, d3.loss_bbox: 0.3246, d4.loss_cls: 0.1279, d4.loss_bbox: 0.3313, loss: 3.0337, grad_norm: 44.2604
2025-06-10 23:31:05,855 - mmdet - INFO - Epoch [1][1550/7033]	lr: 2.000e-04, eta: 19:18:06, time: 1.701, data_time: 0.075, memory: 17172, loss_cls: 0.1338, loss_bbox: 0.3476, d0.loss_cls: 0.2303, d0.loss_bbox: 0.4506, d1.loss_cls: 0.1704, d1.loss_bbox: 0.3248, d2.loss_cls: 0.1441, d2.loss_bbox: 0.3185, d3.loss_cls: 0.1319, d3.loss_bbox: 0.3255, d4.loss_cls: 0.1325, d4.loss_bbox: 0.3335, loss: 3.0436, grad_norm: 29.7882
2025-06-10 23:32:36,984 - mmdet - INFO - Epoch [1][1600/7033]	lr: 2.000e-04, eta: 19:19:05, time: 1.823, data_time: 0.144, memory: 17172, loss_cls: 0.1415, loss_bbox: 0.3502, d0.loss_cls: 0.2161, d0.loss_bbox: 0.4508, d1.loss_cls: 0.1740, d1.loss_bbox: 0.3296, d2.loss_cls: 0.1446, d2.loss_bbox: 0.3214, d3.loss_cls: 0.1386, d3.loss_bbox: 0.3289, d4.loss_cls: 0.1366, d4.loss_bbox: 0.3396, loss: 3.0719, grad_norm: 46.6015
2025-06-10 23:34:05,087 - mmdet - INFO - Epoch [1][1650/7033]	lr: 2.000e-04, eta: 19:18:38, time: 1.760, data_time: 0.116, memory: 17172, loss_cls: 0.1273, loss_bbox: 0.3368, d0.loss_cls: 0.2175, d0.loss_bbox: 0.4506, d1.loss_cls: 0.1692, d1.loss_bbox: 0.3149, d2.loss_cls: 0.1418, d2.loss_bbox: 0.3054, d3.loss_cls: 0.1299, d3.loss_bbox: 0.3128, d4.loss_cls: 0.1237, d4.loss_bbox: 0.3236, loss: 2.9537, grad_norm: 40.6896
2025-06-10 23:35:32,489 - mmdet - INFO - Epoch [1][1700/7033]	lr: 2.000e-04, eta: 19:17:54, time: 1.750, data_time: 0.110, memory: 17172, loss_cls: 0.1299, loss_bbox: 0.3533, d0.loss_cls: 0.2141, d0.loss_bbox: 0.4599, d1.loss_cls: 0.1616, d1.loss_bbox: 0.3324, d2.loss_cls: 0.1354, d2.loss_bbox: 0.3217, d3.loss_cls: 0.1275, d3.loss_bbox: 0.3300, d4.loss_cls: 0.1240, d4.loss_bbox: 0.3426, loss: 3.0323, grad_norm: 30.9684
2025-06-10 23:36:56,427 - mmdet - INFO - Epoch [1][1750/7033]	lr: 2.000e-04, eta: 19:15:45, time: 1.679, data_time: 0.072, memory: 17206, loss_cls: 0.1271, loss_bbox: 0.3304, d0.loss_cls: 0.2132, d0.loss_bbox: 0.4381, d1.loss_cls: 0.1600, d1.loss_bbox: 0.3175, d2.loss_cls: 0.1318, d2.loss_bbox: 0.3067, d3.loss_cls: 0.1232, d3.loss_bbox: 0.3134, d4.loss_cls: 0.1244, d4.loss_bbox: 0.3188, loss: 2.9047, grad_norm: 37.0610
2025-06-10 23:38:20,331 - mmdet - INFO - Epoch [1][1800/7033]	lr: 2.000e-04, eta: 19:13:37, time: 1.676, data_time: 0.066, memory: 17206, loss_cls: 0.1233, loss_bbox: 0.3436, d0.loss_cls: 0.2221, d0.loss_bbox: 0.4465, d1.loss_cls: 0.1666, d1.loss_bbox: 0.3196, d2.loss_cls: 0.1349, d2.loss_bbox: 0.3112, d3.loss_cls: 0.1259, d3.loss_bbox: 0.3173, d4.loss_cls: 0.1260, d4.loss_bbox: 0.3281, loss: 2.9652, grad_norm: 40.9012
2025-06-10 23:39:48,404 - mmdet - INFO - Epoch [1][1850/7033]	lr: 2.000e-04, eta: 19:13:06, time: 1.763, data_time: 0.090, memory: 17206, loss_cls: 0.1234, loss_bbox: 0.3354, d0.loss_cls: 0.2226, d0.loss_bbox: 0.4236, d1.loss_cls: 0.1660, d1.loss_bbox: 0.3049, d2.loss_cls: 0.1354, d2.loss_bbox: 0.3004, d3.loss_cls: 0.1251, d3.loss_bbox: 0.3101, d4.loss_cls: 0.1212, d4.loss_bbox: 0.3209, loss: 2.8889, grad_norm: 34.9964
2025-06-10 23:41:11,254 - mmdet - INFO - Epoch [1][1900/7033]	lr: 2.000e-04, eta: 19:10:39, time: 1.657, data_time: 0.076, memory: 17206, loss_cls: 0.1210, loss_bbox: 0.3230, d0.loss_cls: 0.2133, d0.loss_bbox: 0.4239, d1.loss_cls: 0.1565, d1.loss_bbox: 0.3030, d2.loss_cls: 0.1298, d2.loss_bbox: 0.2924, d3.loss_cls: 0.1213, d3.loss_bbox: 0.2987, d4.loss_cls: 0.1188, d4.loss_bbox: 0.3082, loss: 2.8098, grad_norm: 102.8419
2025-06-10 23:42:37,038 - mmdet - INFO - Epoch [1][1950/7033]	lr: 2.000e-04, eta: 19:09:15, time: 1.716, data_time: 0.097, memory: 17206, loss_cls: 0.1197, loss_bbox: 0.3282, d0.loss_cls: 0.2246, d0.loss_bbox: 0.4383, d1.loss_cls: 0.1583, d1.loss_bbox: 0.3203, d2.loss_cls: 0.1301, d2.loss_bbox: 0.3056, d3.loss_cls: 0.1225, d3.loss_bbox: 0.3125, d4.loss_cls: 0.1176, d4.loss_bbox: 0.3182, loss: 2.8959, grad_norm: 34.2790
2025-06-10 23:44:06,115 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 23:44:06,115 - mmdet - INFO - Epoch [1][2000/7033]	lr: 2.000e-04, eta: 19:08:57, time: 1.780, data_time: 0.136, memory: 17206, loss_cls: 0.1394, loss_bbox: 0.3540, d0.loss_cls: 0.2326, d0.loss_bbox: 0.4585, d1.loss_cls: 0.1748, d1.loss_bbox: 0.3365, d2.loss_cls: 0.1470, d2.loss_bbox: 0.3257, d3.loss_cls: 0.1425, d3.loss_bbox: 0.3308, d4.loss_cls: 0.1393, d4.loss_bbox: 0.3397, loss: 3.1207, grad_norm: 89.4136
2025-06-10 23:45:32,965 - mmdet - INFO - Epoch [1][2050/7033]	lr: 2.000e-04, eta: 19:07:53, time: 1.737, data_time: 0.082, memory: 17206, loss_cls: 0.1227, loss_bbox: 0.3387, d0.loss_cls: 0.2215, d0.loss_bbox: 0.4208, d1.loss_cls: 0.1651, d1.loss_bbox: 0.3104, d2.loss_cls: 0.1342, d2.loss_bbox: 0.3052, d3.loss_cls: 0.1264, d3.loss_bbox: 0.3126, d4.loss_cls: 0.1222, d4.loss_bbox: 0.3248, loss: 2.9047, grad_norm: 47.0695
2025-06-10 23:47:01,265 - mmdet - INFO - Epoch [1][2100/7033]	lr: 2.000e-04, eta: 19:07:16, time: 1.767, data_time: 0.147, memory: 17206, loss_cls: 0.1258, loss_bbox: 0.3526, d0.loss_cls: 0.2182, d0.loss_bbox: 0.4459, d1.loss_cls: 0.1653, d1.loss_bbox: 0.3230, d2.loss_cls: 0.1369, d2.loss_bbox: 0.3146, d3.loss_cls: 0.1249, d3.loss_bbox: 0.3251, d4.loss_cls: 0.1230, d4.loss_bbox: 0.3363, loss: 2.9915, grad_norm: 45.1422
2025-06-10 23:48:22,703 - mmdet - INFO - Epoch [1][2150/7033]	lr: 2.000e-04, eta: 19:04:29, time: 1.629, data_time: 0.071, memory: 17206, loss_cls: 0.1277, loss_bbox: 0.3445, d0.loss_cls: 0.2219, d0.loss_bbox: 0.4256, d1.loss_cls: 0.1570, d1.loss_bbox: 0.3071, d2.loss_cls: 0.1319, d2.loss_bbox: 0.3003, d3.loss_cls: 0.1283, d3.loss_bbox: 0.3091, d4.loss_cls: 0.1260, d4.loss_bbox: 0.3265, loss: 2.9059, grad_norm: 39.3915
2025-06-10 23:49:49,586 - mmdet - INFO - Epoch [1][2200/7033]	lr: 2.000e-04, eta: 19:03:24, time: 1.738, data_time: 0.079, memory: 17206, loss_cls: 0.1245, loss_bbox: 0.3348, d0.loss_cls: 0.2148, d0.loss_bbox: 0.4181, d1.loss_cls: 0.1566, d1.loss_bbox: 0.3152, d2.loss_cls: 0.1384, d2.loss_bbox: 0.2994, d3.loss_cls: 0.1298, d3.loss_bbox: 0.3072, d4.loss_cls: 0.1243, d4.loss_bbox: 0.3180, loss: 2.8812, grad_norm: 32.4261
2025-06-10 23:51:13,962 - mmdet - INFO - Epoch [1][2250/7033]	lr: 2.000e-04, eta: 19:01:32, time: 1.686, data_time: 0.076, memory: 17206, loss_cls: 0.1257, loss_bbox: 0.3389, d0.loss_cls: 0.2150, d0.loss_bbox: 0.4203, d1.loss_cls: 0.1536, d1.loss_bbox: 0.3159, d2.loss_cls: 0.1345, d2.loss_bbox: 0.3094, d3.loss_cls: 0.1287, d3.loss_bbox: 0.3182, d4.loss_cls: 0.1241, d4.loss_bbox: 0.3278, loss: 2.9121, grad_norm: 41.2675
2025-06-10 23:52:38,931 - mmdet - INFO - Epoch [1][2300/7033]	lr: 2.000e-04, eta: 18:59:54, time: 1.701, data_time: 0.093, memory: 17206, loss_cls: 0.1249, loss_bbox: 0.3412, d0.loss_cls: 0.2108, d0.loss_bbox: 0.4150, d1.loss_cls: 0.1549, d1.loss_bbox: 0.3128, d2.loss_cls: 0.1358, d2.loss_bbox: 0.3068, d3.loss_cls: 0.1267, d3.loss_bbox: 0.3148, d4.loss_cls: 0.1270, d4.loss_bbox: 0.3250, loss: 2.8955, grad_norm: 40.7348
2025-06-10 23:54:04,221 - mmdet - INFO - Epoch [1][2350/7033]	lr: 2.000e-04, eta: 18:58:20, time: 1.704, data_time: 0.117, memory: 17291, loss_cls: 0.1215, loss_bbox: 0.3292, d0.loss_cls: 0.2166, d0.loss_bbox: 0.4187, d1.loss_cls: 0.1483, d1.loss_bbox: 0.3132, d2.loss_cls: 0.1262, d2.loss_bbox: 0.3039, d3.loss_cls: 0.1207, d3.loss_bbox: 0.3113, d4.loss_cls: 0.1191, d4.loss_bbox: 0.3170, loss: 2.8457, grad_norm: 31.7654
2025-06-10 23:55:31,407 - mmdet - INFO - Epoch [1][2400/7033]	lr: 2.000e-04, eta: 18:57:21, time: 1.746, data_time: 0.057, memory: 17291, loss_cls: 0.1174, loss_bbox: 0.3253, d0.loss_cls: 0.2180, d0.loss_bbox: 0.4246, d1.loss_cls: 0.1516, d1.loss_bbox: 0.3075, d2.loss_cls: 0.1275, d2.loss_bbox: 0.2975, d3.loss_cls: 0.1193, d3.loss_bbox: 0.3023, d4.loss_cls: 0.1175, d4.loss_bbox: 0.3112, loss: 2.8198, grad_norm: 36.3236
2025-06-10 23:56:52,860 - mmdet - INFO - Epoch [1][2450/7033]	lr: 2.000e-04, eta: 18:54:45, time: 1.629, data_time: 0.078, memory: 17291, loss_cls: 0.1138, loss_bbox: 0.3214, d0.loss_cls: 0.2091, d0.loss_bbox: 0.4062, d1.loss_cls: 0.1466, d1.loss_bbox: 0.2972, d2.loss_cls: 0.1212, d2.loss_bbox: 0.2899, d3.loss_cls: 0.1101, d3.loss_bbox: 0.3004, d4.loss_cls: 0.1110, d4.loss_bbox: 0.3102, loss: 2.7370, grad_norm: 40.5035
2025-06-10 23:58:22,488 - mmdet - INFO - Epoch [1][2500/7033]	lr: 2.000e-04, eta: 18:54:23, time: 1.792, data_time: 0.055, memory: 17291, loss_cls: 0.1162, loss_bbox: 0.3106, d0.loss_cls: 0.2096, d0.loss_bbox: 0.4108, d1.loss_cls: 0.1418, d1.loss_bbox: 0.2971, d2.loss_cls: 0.1220, d2.loss_bbox: 0.2857, d3.loss_cls: 0.1148, d3.loss_bbox: 0.2909, d4.loss_cls: 0.1148, d4.loss_bbox: 0.2983, loss: 2.7127, grad_norm: 28.7895
2025-06-10 23:59:46,929 - mmdet - INFO - Epoch [1][2550/7033]	lr: 2.000e-04, eta: 18:52:36, time: 1.687, data_time: 0.090, memory: 17291, loss_cls: 0.1117, loss_bbox: 0.3240, d0.loss_cls: 0.2086, d0.loss_bbox: 0.4224, d1.loss_cls: 0.1462, d1.loss_bbox: 0.3025, d2.loss_cls: 0.1207, d2.loss_bbox: 0.2927, d3.loss_cls: 0.1132, d3.loss_bbox: 0.2987, d4.loss_cls: 0.1108, d4.loss_bbox: 0.3085, loss: 2.7599, grad_norm: 911.5198
2025-06-11 00:01:10,737 - mmdet - INFO - Epoch [1][2600/7033]	lr: 2.000e-04, eta: 18:50:43, time: 1.678, data_time: 0.104, memory: 17291, loss_cls: 0.1205, loss_bbox: 0.3160, d0.loss_cls: 0.2102, d0.loss_bbox: 0.4130, d1.loss_cls: 0.1475, d1.loss_bbox: 0.3040, d2.loss_cls: 0.1279, d2.loss_bbox: 0.2895, d3.loss_cls: 0.1195, d3.loss_bbox: 0.2927, d4.loss_cls: 0.1188, d4.loss_bbox: 0.3025, loss: 2.7621, grad_norm: 24.7606
2025-06-11 00:02:42,016 - mmdet - INFO - Epoch [1][2650/7033]	lr: 2.000e-04, eta: 18:50:41, time: 1.826, data_time: 0.078, memory: 17291, loss_cls: 0.1157, loss_bbox: 0.3475, d0.loss_cls: 0.2111, d0.loss_bbox: 0.4183, d1.loss_cls: 0.1527, d1.loss_bbox: 0.3191, d2.loss_cls: 0.1305, d2.loss_bbox: 0.3111, d3.loss_cls: 0.1216, d3.loss_bbox: 0.3193, d4.loss_cls: 0.1159, d4.loss_bbox: 0.3293, loss: 2.8920, grad_norm: 70.2500
2025-06-11 00:04:06,979 - mmdet - INFO - Epoch [1][2700/7033]	lr: 2.000e-04, eta: 18:49:03, time: 1.699, data_time: 0.102, memory: 17291, loss_cls: 0.1166, loss_bbox: 0.3088, d0.loss_cls: 0.2016, d0.loss_bbox: 0.3939, d1.loss_cls: 0.1395, d1.loss_bbox: 0.2979, d2.loss_cls: 0.1254, d2.loss_bbox: 0.2837, d3.loss_cls: 0.1191, d3.loss_bbox: 0.2889, d4.loss_cls: 0.1168, d4.loss_bbox: 0.2989, loss: 2.6912, grad_norm: 46.8551
2025-06-11 00:05:29,795 - mmdet - INFO - Epoch [1][2750/7033]	lr: 2.000e-04, eta: 18:46:55, time: 1.656, data_time: 0.087, memory: 17291, loss_cls: 0.1098, loss_bbox: 0.3126, d0.loss_cls: 0.2106, d0.loss_bbox: 0.4143, d1.loss_cls: 0.1456, d1.loss_bbox: 0.3026, d2.loss_cls: 0.1189, d2.loss_bbox: 0.2918, d3.loss_cls: 0.1115, d3.loss_bbox: 0.2950, d4.loss_cls: 0.1085, d4.loss_bbox: 0.3003, loss: 2.7216, grad_norm: 30.0126
2025-06-11 00:06:53,652 - mmdet - INFO - Epoch [1][2800/7033]	lr: 2.000e-04, eta: 18:45:04, time: 1.677, data_time: 0.092, memory: 17291, loss_cls: 0.1112, loss_bbox: 0.3043, d0.loss_cls: 0.2020, d0.loss_bbox: 0.3987, d1.loss_cls: 0.1448, d1.loss_bbox: 0.2952, d2.loss_cls: 0.1206, d2.loss_bbox: 0.2816, d3.loss_cls: 0.1123, d3.loss_bbox: 0.2855, d4.loss_cls: 0.1100, d4.loss_bbox: 0.2916, loss: 2.6578, grad_norm: 34.3277
2025-06-11 00:08:21,155 - mmdet - INFO - Epoch [1][2850/7033]	lr: 2.000e-04, eta: 18:44:03, time: 1.750, data_time: 0.059, memory: 17291, loss_cls: 0.1056, loss_bbox: 0.3136, d0.loss_cls: 0.2019, d0.loss_bbox: 0.3996, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2938, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2847, d3.loss_cls: 0.1075, d3.loss_bbox: 0.2898, d4.loss_cls: 0.1051, d4.loss_bbox: 0.2992, loss: 2.6587, grad_norm: 29.4008
2025-06-11 00:09:42,620 - mmdet - INFO - Epoch [1][2900/7033]	lr: 2.000e-04, eta: 18:41:40, time: 1.630, data_time: 0.104, memory: 17291, loss_cls: 0.1020, loss_bbox: 0.2944, d0.loss_cls: 0.1972, d0.loss_bbox: 0.3873, d1.loss_cls: 0.1367, d1.loss_bbox: 0.2800, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2675, d3.loss_cls: 0.1052, d3.loss_bbox: 0.2727, d4.loss_cls: 0.1011, d4.loss_bbox: 0.2803, loss: 2.5407, grad_norm: 97.4766
2025-06-11 00:11:06,232 - mmdet - INFO - Epoch [1][2950/7033]	lr: 2.000e-04, eta: 18:39:47, time: 1.671, data_time: 0.075, memory: 17291, loss_cls: 0.1128, loss_bbox: 0.3161, d0.loss_cls: 0.2110, d0.loss_bbox: 0.4058, d1.loss_cls: 0.1459, d1.loss_bbox: 0.2996, d2.loss_cls: 0.1240, d2.loss_bbox: 0.2868, d3.loss_cls: 0.1114, d3.loss_bbox: 0.2961, d4.loss_cls: 0.1097, d4.loss_bbox: 0.3047, loss: 2.7240, grad_norm: 27.0998
2025-06-11 00:12:29,395 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 00:12:29,396 - mmdet - INFO - Epoch [1][3000/7033]	lr: 2.000e-04, eta: 18:37:50, time: 1.664, data_time: 0.058, memory: 17291, loss_cls: 0.1218, loss_bbox: 0.3193, d0.loss_cls: 0.2131, d0.loss_bbox: 0.4064, d1.loss_cls: 0.1527, d1.loss_bbox: 0.3028, d2.loss_cls: 0.1283, d2.loss_bbox: 0.2920, d3.loss_cls: 0.1210, d3.loss_bbox: 0.2973, d4.loss_cls: 0.1208, d4.loss_bbox: 0.3072, loss: 2.7826, grad_norm: 30.2072
2025-06-11 00:13:56,607 - mmdet - INFO - Epoch [1][3050/7033]	lr: 2.000e-04, eta: 18:36:46, time: 1.744, data_time: 0.085, memory: 17291, loss_cls: 0.1098, loss_bbox: 0.3049, d0.loss_cls: 0.2052, d0.loss_bbox: 0.3993, d1.loss_cls: 0.1386, d1.loss_bbox: 0.2983, d2.loss_cls: 0.1179, d2.loss_bbox: 0.2846, d3.loss_cls: 0.1098, d3.loss_bbox: 0.2888, d4.loss_cls: 0.1066, d4.loss_bbox: 0.2962, loss: 2.6600, grad_norm: 335.4117
2025-06-11 00:15:23,163 - mmdet - INFO - Epoch [1][3100/7033]	lr: 2.000e-04, eta: 18:35:33, time: 1.731, data_time: 0.109, memory: 17291, loss_cls: 0.1060, loss_bbox: 0.3094, d0.loss_cls: 0.2015, d0.loss_bbox: 0.4088, d1.loss_cls: 0.1368, d1.loss_bbox: 0.2995, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2849, d3.loss_cls: 0.1065, d3.loss_bbox: 0.2884, d4.loss_cls: 0.1064, d4.loss_bbox: 0.2951, loss: 2.6607, grad_norm: 72.0815
2025-06-11 00:16:48,023 - mmdet - INFO - Epoch [1][3150/7033]	lr: 2.000e-04, eta: 18:33:58, time: 1.697, data_time: 0.058, memory: 17291, loss_cls: 0.1112, loss_bbox: 0.3201, d0.loss_cls: 0.2113, d0.loss_bbox: 0.4270, d1.loss_cls: 0.1478, d1.loss_bbox: 0.3168, d2.loss_cls: 0.1245, d2.loss_bbox: 0.3004, d3.loss_cls: 0.1133, d3.loss_bbox: 0.3035, d4.loss_cls: 0.1098, d4.loss_bbox: 0.3091, loss: 2.7951, grad_norm: 32.1392
2025-06-11 00:18:26,827 - mmdet - INFO - Epoch [1][3200/7033]	lr: 2.000e-04, eta: 18:35:12, time: 1.974, data_time: 0.153, memory: 17291, loss_cls: 0.1090, loss_bbox: 0.3047, d0.loss_cls: 0.2072, d0.loss_bbox: 0.4089, d1.loss_cls: 0.1425, d1.loss_bbox: 0.3000, d2.loss_cls: 0.1242, d2.loss_bbox: 0.2846, d3.loss_cls: 0.1104, d3.loss_bbox: 0.2894, d4.loss_cls: 0.1101, d4.loss_bbox: 0.2924, loss: 2.6833, grad_norm: 24.2311
2025-06-11 00:19:49,278 - mmdet - INFO - Epoch [1][3250/7033]	lr: 2.000e-04, eta: 18:33:08, time: 1.651, data_time: 0.080, memory: 17291, loss_cls: 0.1077, loss_bbox: 0.3024, d0.loss_cls: 0.1944, d0.loss_bbox: 0.3956, d1.loss_cls: 0.1378, d1.loss_bbox: 0.2977, d2.loss_cls: 0.1184, d2.loss_bbox: 0.2784, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2826, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2896, loss: 2.6245, grad_norm: 32.6109
2025-06-11 00:21:12,807 - mmdet - INFO - Epoch [1][3300/7033]	lr: 2.000e-04, eta: 18:31:16, time: 1.671, data_time: 0.078, memory: 17291, loss_cls: 0.1058, loss_bbox: 0.3098, d0.loss_cls: 0.1993, d0.loss_bbox: 0.3993, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2961, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2802, d3.loss_cls: 0.1115, d3.loss_bbox: 0.2828, d4.loss_cls: 0.1058, d4.loss_bbox: 0.2921, loss: 2.6337, grad_norm: 39.1250
2025-06-11 00:22:35,170 - mmdet - INFO - Epoch [1][3350/7033]	lr: 2.000e-04, eta: 18:29:11, time: 1.647, data_time: 0.071, memory: 17291, loss_cls: 0.1089, loss_bbox: 0.3227, d0.loss_cls: 0.1976, d0.loss_bbox: 0.4031, d1.loss_cls: 0.1421, d1.loss_bbox: 0.3023, d2.loss_cls: 0.1216, d2.loss_bbox: 0.2909, d3.loss_cls: 0.1109, d3.loss_bbox: 0.2994, d4.loss_cls: 0.1087, d4.loss_bbox: 0.3097, loss: 2.7177, grad_norm: 40.3511
2025-06-11 00:23:58,023 - mmdet - INFO - Epoch [1][3400/7033]	lr: 2.000e-04, eta: 18:27:14, time: 1.657, data_time: 0.077, memory: 17291, loss_cls: 0.1161, loss_bbox: 0.3193, d0.loss_cls: 0.2017, d0.loss_bbox: 0.4018, d1.loss_cls: 0.1469, d1.loss_bbox: 0.2971, d2.loss_cls: 0.1253, d2.loss_bbox: 0.2848, d3.loss_cls: 0.1154, d3.loss_bbox: 0.2934, d4.loss_cls: 0.1129, d4.loss_bbox: 0.3037, loss: 2.7183, grad_norm: 281.0758
2025-06-11 00:25:18,631 - mmdet - INFO - Epoch [1][3450/7033]	lr: 2.000e-04, eta: 18:24:52, time: 1.612, data_time: 0.055, memory: 17291, loss_cls: 0.1018, loss_bbox: 0.3024, d0.loss_cls: 0.1904, d0.loss_bbox: 0.3853, d1.loss_cls: 0.1331, d1.loss_bbox: 0.2887, d2.loss_cls: 0.1155, d2.loss_bbox: 0.2783, d3.loss_cls: 0.1033, d3.loss_bbox: 0.2811, d4.loss_cls: 0.1018, d4.loss_bbox: 0.2905, loss: 2.5723, grad_norm: 56.7981
2025-06-11 00:26:41,976 - mmdet - INFO - Epoch [1][3500/7033]	lr: 2.000e-04, eta: 18:23:01, time: 1.665, data_time: 0.059, memory: 17291, loss_cls: 0.1090, loss_bbox: 0.3076, d0.loss_cls: 0.2089, d0.loss_bbox: 0.3880, d1.loss_cls: 0.1468, d1.loss_bbox: 0.2882, d2.loss_cls: 0.1275, d2.loss_bbox: 0.2788, d3.loss_cls: 0.1127, d3.loss_bbox: 0.2872, d4.loss_cls: 0.1095, d4.loss_bbox: 0.2967, loss: 2.6609, grad_norm: 32.2123
2025-06-11 00:28:06,020 - mmdet - INFO - Epoch [1][3550/7033]	lr: 2.000e-04, eta: 18:21:20, time: 1.682, data_time: 0.104, memory: 17291, loss_cls: 0.1024, loss_bbox: 0.2985, d0.loss_cls: 0.2096, d0.loss_bbox: 0.3801, d1.loss_cls: 0.1400, d1.loss_bbox: 0.2871, d2.loss_cls: 0.1167, d2.loss_bbox: 0.2744, d3.loss_cls: 0.1069, d3.loss_bbox: 0.2801, d4.loss_cls: 0.1027, d4.loss_bbox: 0.2880, loss: 2.5863, grad_norm: 31.8212
2025-06-11 00:29:29,002 - mmdet - INFO - Epoch [1][3600/7033]	lr: 2.000e-04, eta: 18:19:28, time: 1.660, data_time: 0.103, memory: 17291, loss_cls: 0.0981, loss_bbox: 0.2975, d0.loss_cls: 0.1969, d0.loss_bbox: 0.3743, d1.loss_cls: 0.1296, d1.loss_bbox: 0.2808, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2681, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2741, d4.loss_cls: 0.0990, d4.loss_bbox: 0.2811, loss: 2.5094, grad_norm: 178.9666
2025-06-11 00:30:51,099 - mmdet - INFO - Epoch [1][3650/7033]	lr: 2.000e-04, eta: 18:17:27, time: 1.642, data_time: 0.062, memory: 17291, loss_cls: 0.1045, loss_bbox: 0.2922, d0.loss_cls: 0.1995, d0.loss_bbox: 0.3810, d1.loss_cls: 0.1403, d1.loss_bbox: 0.2873, d2.loss_cls: 0.1197, d2.loss_bbox: 0.2741, d3.loss_cls: 0.1079, d3.loss_bbox: 0.2761, d4.loss_cls: 0.1052, d4.loss_bbox: 0.2801, loss: 2.5677, grad_norm: 125.8617
2025-06-11 00:32:12,506 - mmdet - INFO - Epoch [1][3700/7033]	lr: 2.000e-04, eta: 18:15:20, time: 1.628, data_time: 0.100, memory: 17291, loss_cls: 0.1015, loss_bbox: 0.3035, d0.loss_cls: 0.2012, d0.loss_bbox: 0.3952, d1.loss_cls: 0.1376, d1.loss_bbox: 0.2999, d2.loss_cls: 0.1197, d2.loss_bbox: 0.2802, d3.loss_cls: 0.1096, d3.loss_bbox: 0.2844, d4.loss_cls: 0.1041, d4.loss_bbox: 0.2919, loss: 2.6289, grad_norm: 50.0180
2025-06-11 00:33:33,925 - mmdet - INFO - Epoch [1][3750/7033]	lr: 2.000e-04, eta: 18:13:14, time: 1.629, data_time: 0.116, memory: 17291, loss_cls: 0.1093, loss_bbox: 0.3057, d0.loss_cls: 0.1980, d0.loss_bbox: 0.3802, d1.loss_cls: 0.1454, d1.loss_bbox: 0.2925, d2.loss_cls: 0.1232, d2.loss_bbox: 0.2763, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2845, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2914, loss: 2.6273, grad_norm: 44.8077
2025-06-11 00:34:55,148 - mmdet - INFO - Epoch [1][3800/7033]	lr: 2.000e-04, eta: 18:11:08, time: 1.624, data_time: 0.095, memory: 17291, loss_cls: 0.1031, loss_bbox: 0.3007, d0.loss_cls: 0.2046, d0.loss_bbox: 0.3887, d1.loss_cls: 0.1404, d1.loss_bbox: 0.2918, d2.loss_cls: 0.1168, d2.loss_bbox: 0.2799, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2843, d4.loss_cls: 0.1021, d4.loss_bbox: 0.2896, loss: 2.6064, grad_norm: 24.4249
2025-06-11 00:36:15,682 - mmdet - INFO - Epoch [1][3850/7033]	lr: 2.000e-04, eta: 18:08:55, time: 1.611, data_time: 0.077, memory: 17291, loss_cls: 0.0978, loss_bbox: 0.3034, d0.loss_cls: 0.1914, d0.loss_bbox: 0.3877, d1.loss_cls: 0.1324, d1.loss_bbox: 0.2927, d2.loss_cls: 0.1106, d2.loss_bbox: 0.2807, d3.loss_cls: 0.1009, d3.loss_bbox: 0.2850, d4.loss_cls: 0.0987, d4.loss_bbox: 0.2919, loss: 2.5731, grad_norm: 37.5251
2025-06-11 00:37:48,141 - mmdet - INFO - Epoch [1][3900/7033]	lr: 2.000e-04, eta: 18:08:42, time: 1.849, data_time: 0.256, memory: 17291, loss_cls: 0.1018, loss_bbox: 0.2889, d0.loss_cls: 0.1965, d0.loss_bbox: 0.3713, d1.loss_cls: 0.1293, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2687, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2732, d4.loss_cls: 0.1005, d4.loss_bbox: 0.2794, loss: 2.5005, grad_norm: 43.7293
2025-06-11 00:39:09,167 - mmdet - INFO - Epoch [1][3950/7033]	lr: 2.000e-04, eta: 18:06:35, time: 1.621, data_time: 0.087, memory: 17291, loss_cls: 0.1014, loss_bbox: 0.2919, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3901, d1.loss_cls: 0.1318, d1.loss_bbox: 0.2940, d2.loss_cls: 0.1136, d2.loss_bbox: 0.2764, d3.loss_cls: 0.1033, d3.loss_bbox: 0.2790, d4.loss_cls: 0.1007, d4.loss_bbox: 0.2829, loss: 2.5610, grad_norm: 26.4950
2025-06-11 00:40:28,258 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 00:40:28,259 - mmdet - INFO - Epoch [1][4000/7033]	lr: 2.000e-04, eta: 18:04:11, time: 1.580, data_time: 0.065, memory: 17291, loss_cls: 0.1030, loss_bbox: 0.3045, d0.loss_cls: 0.1965, d0.loss_bbox: 0.3773, d1.loss_cls: 0.1305, d1.loss_bbox: 0.2856, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2711, d3.loss_cls: 0.1043, d3.loss_bbox: 0.2773, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2880, loss: 2.5546, grad_norm: 36.5205
2025-06-11 00:41:47,128 - mmdet - INFO - Epoch [1][4050/7033]	lr: 2.000e-04, eta: 18:01:47, time: 1.579, data_time: 0.057, memory: 17291, loss_cls: 0.1043, loss_bbox: 0.3036, d0.loss_cls: 0.1982, d0.loss_bbox: 0.3849, d1.loss_cls: 0.1357, d1.loss_bbox: 0.2895, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2745, d3.loss_cls: 0.1069, d3.loss_bbox: 0.2820, d4.loss_cls: 0.1047, d4.loss_bbox: 0.2909, loss: 2.5928, grad_norm: 69.6635
2025-06-11 00:43:07,664 - mmdet - INFO - Epoch [1][4100/7033]	lr: 2.000e-04, eta: 17:59:40, time: 1.611, data_time: 0.070, memory: 17291, loss_cls: 0.0973, loss_bbox: 0.3020, d0.loss_cls: 0.1985, d0.loss_bbox: 0.3882, d1.loss_cls: 0.1295, d1.loss_bbox: 0.3006, d2.loss_cls: 0.1115, d2.loss_bbox: 0.2821, d3.loss_cls: 0.0994, d3.loss_bbox: 0.2866, d4.loss_cls: 0.0984, d4.loss_bbox: 0.2910, loss: 2.5850, grad_norm: 33.7948
2025-06-11 00:44:29,082 - mmdet - INFO - Epoch [1][4150/7033]	lr: 2.000e-04, eta: 17:57:42, time: 1.628, data_time: 0.087, memory: 17291, loss_cls: 0.0995, loss_bbox: 0.3033, d0.loss_cls: 0.2016, d0.loss_bbox: 0.3755, d1.loss_cls: 0.1314, d1.loss_bbox: 0.2879, d2.loss_cls: 0.1132, d2.loss_bbox: 0.2777, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2834, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2901, loss: 2.5630, grad_norm: 31.0284
2025-06-11 00:45:49,362 - mmdet - INFO - Epoch [1][4200/7033]	lr: 2.000e-04, eta: 17:55:34, time: 1.606, data_time: 0.082, memory: 17291, loss_cls: 0.0978, loss_bbox: 0.2872, d0.loss_cls: 0.2011, d0.loss_bbox: 0.3857, d1.loss_cls: 0.1343, d1.loss_bbox: 0.2891, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2703, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2733, d4.loss_cls: 0.0996, d4.loss_bbox: 0.2777, loss: 2.5298, grad_norm: 32.5097
2025-06-11 00:47:19,312 - mmdet - INFO - Epoch [1][4250/7033]	lr: 2.000e-04, eta: 17:54:54, time: 1.799, data_time: 0.088, memory: 17291, loss_cls: 0.0955, loss_bbox: 0.2855, d0.loss_cls: 0.1953, d0.loss_bbox: 0.3689, d1.loss_cls: 0.1330, d1.loss_bbox: 0.2775, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2606, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2649, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2717, loss: 2.4577, grad_norm: 40.2607
2025-06-11 00:48:45,359 - mmdet - INFO - Epoch [1][4300/7033]	lr: 2.000e-04, eta: 17:53:39, time: 1.721, data_time: 0.063, memory: 17291, loss_cls: 0.1012, loss_bbox: 0.2921, d0.loss_cls: 0.2089, d0.loss_bbox: 0.4026, d1.loss_cls: 0.1433, d1.loss_bbox: 0.2960, d2.loss_cls: 0.1185, d2.loss_bbox: 0.2764, d3.loss_cls: 0.1060, d3.loss_bbox: 0.2786, d4.loss_cls: 0.1006, d4.loss_bbox: 0.2824, loss: 2.6066, grad_norm: 40.3898
2025-06-11 00:50:07,574 - mmdet - INFO - Epoch [1][4350/7033]	lr: 2.000e-04, eta: 17:51:50, time: 1.644, data_time: 0.072, memory: 17291, loss_cls: 0.0940, loss_bbox: 0.2970, d0.loss_cls: 0.1979, d0.loss_bbox: 0.3811, d1.loss_cls: 0.1313, d1.loss_bbox: 0.2828, d2.loss_cls: 0.1084, d2.loss_bbox: 0.2733, d3.loss_cls: 0.0974, d3.loss_bbox: 0.2789, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2850, loss: 2.5218, grad_norm: 53.1434
2025-06-11 00:51:33,841 - mmdet - INFO - Epoch [1][4400/7033]	lr: 2.000e-04, eta: 17:50:36, time: 1.725, data_time: 0.121, memory: 17291, loss_cls: 0.0954, loss_bbox: 0.2862, d0.loss_cls: 0.1996, d0.loss_bbox: 0.3918, d1.loss_cls: 0.1360, d1.loss_bbox: 0.2870, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2703, d3.loss_cls: 0.1016, d3.loss_bbox: 0.2713, d4.loss_cls: 0.0967, d4.loss_bbox: 0.2766, loss: 2.5256, grad_norm: 35.7672
2025-06-11 00:52:55,270 - mmdet - INFO - Epoch [1][4450/7033]	lr: 2.000e-04, eta: 17:48:41, time: 1.628, data_time: 0.071, memory: 17291, loss_cls: 0.1021, loss_bbox: 0.2829, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3811, d1.loss_cls: 0.1355, d1.loss_bbox: 0.2871, d2.loss_cls: 0.1151, d2.loss_bbox: 0.2676, d3.loss_cls: 0.1061, d3.loss_bbox: 0.2685, d4.loss_cls: 0.1032, d4.loss_bbox: 0.2725, loss: 2.5144, grad_norm: 31.6589
2025-06-11 00:54:16,335 - mmdet - INFO - Epoch [1][4500/7033]	lr: 2.000e-04, eta: 17:46:43, time: 1.621, data_time: 0.058, memory: 17291, loss_cls: 0.1001, loss_bbox: 0.2765, d0.loss_cls: 0.1943, d0.loss_bbox: 0.3622, d1.loss_cls: 0.1298, d1.loss_bbox: 0.2777, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2623, d3.loss_cls: 0.1018, d3.loss_bbox: 0.2667, d4.loss_cls: 0.1000, d4.loss_bbox: 0.2686, loss: 2.4493, grad_norm: 27.1611
2025-06-11 00:55:37,589 - mmdet - INFO - Epoch [1][4550/7033]	lr: 2.000e-04, eta: 17:44:48, time: 1.625, data_time: 0.080, memory: 17291, loss_cls: 0.1014, loss_bbox: 0.2792, d0.loss_cls: 0.2095, d0.loss_bbox: 0.3888, d1.loss_cls: 0.1379, d1.loss_bbox: 0.2822, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2685, d3.loss_cls: 0.1039, d3.loss_bbox: 0.2673, d4.loss_cls: 0.1028, d4.loss_bbox: 0.2692, loss: 2.5245, grad_norm: 36.6202
2025-06-11 00:56:57,664 - mmdet - INFO - Epoch [1][4600/7033]	lr: 2.000e-04, eta: 17:42:44, time: 1.602, data_time: 0.067, memory: 17291, loss_cls: 0.1060, loss_bbox: 0.2842, d0.loss_cls: 0.2013, d0.loss_bbox: 0.3751, d1.loss_cls: 0.1370, d1.loss_bbox: 0.2869, d2.loss_cls: 0.1170, d2.loss_bbox: 0.2730, d3.loss_cls: 0.1097, d3.loss_bbox: 0.2761, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2733, loss: 2.5464, grad_norm: 33.8560
2025-06-11 00:58:17,989 - mmdet - INFO - Epoch [1][4650/7033]	lr: 2.000e-04, eta: 17:40:44, time: 1.607, data_time: 0.054, memory: 17291, loss_cls: 0.1086, loss_bbox: 0.2890, d0.loss_cls: 0.2006, d0.loss_bbox: 0.3730, d1.loss_cls: 0.1384, d1.loss_bbox: 0.2828, d2.loss_cls: 0.1200, d2.loss_bbox: 0.2684, d3.loss_cls: 0.1120, d3.loss_bbox: 0.2708, d4.loss_cls: 0.1092, d4.loss_bbox: 0.2760, loss: 2.5488, grad_norm: 39.6171
2025-06-11 00:59:39,614 - mmdet - INFO - Epoch [1][4700/7033]	lr: 2.000e-04, eta: 17:38:54, time: 1.633, data_time: 0.083, memory: 17291, loss_cls: 0.0963, loss_bbox: 0.3063, d0.loss_cls: 0.1951, d0.loss_bbox: 0.3825, d1.loss_cls: 0.1393, d1.loss_bbox: 0.3010, d2.loss_cls: 0.1150, d2.loss_bbox: 0.2835, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2905, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2932, loss: 2.6020, grad_norm: 67.6809
2025-06-11 01:01:00,052 - mmdet - INFO - Epoch [1][4750/7033]	lr: 2.000e-04, eta: 17:36:56, time: 1.609, data_time: 0.062, memory: 17291, loss_cls: 0.0957, loss_bbox: 0.2785, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3655, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2815, d2.loss_cls: 0.1137, d2.loss_bbox: 0.2673, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2713, d4.loss_cls: 0.0969, d4.loss_bbox: 0.2693, loss: 2.4607, grad_norm: 26.2807
2025-06-11 01:02:22,469 - mmdet - INFO - Epoch [1][4800/7033]	lr: 2.000e-04, eta: 17:35:13, time: 1.648, data_time: 0.077, memory: 17291, loss_cls: 0.1016, loss_bbox: 0.2813, d0.loss_cls: 0.1958, d0.loss_bbox: 0.3745, d1.loss_cls: 0.1331, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2649, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2708, d4.loss_cls: 0.1025, d4.loss_bbox: 0.2712, loss: 2.4964, grad_norm: 36.9856
2025-06-11 01:03:44,219 - mmdet - INFO - Epoch [1][4850/7033]	lr: 2.000e-04, eta: 17:33:26, time: 1.635, data_time: 0.063, memory: 17291, loss_cls: 0.1091, loss_bbox: 0.2875, d0.loss_cls: 0.2059, d0.loss_bbox: 0.3907, d1.loss_cls: 0.1439, d1.loss_bbox: 0.2940, d2.loss_cls: 0.1263, d2.loss_bbox: 0.2739, d3.loss_cls: 0.1125, d3.loss_bbox: 0.2773, d4.loss_cls: 0.1104, d4.loss_bbox: 0.2751, loss: 2.6067, grad_norm: 51.0474
2025-06-11 01:05:02,378 - mmdet - INFO - Epoch [1][4900/7033]	lr: 2.000e-04, eta: 17:31:12, time: 1.561, data_time: 0.061, memory: 17291, loss_cls: 0.1047, loss_bbox: 0.2940, d0.loss_cls: 0.1938, d0.loss_bbox: 0.3845, d1.loss_cls: 0.1352, d1.loss_bbox: 0.2957, d2.loss_cls: 0.1181, d2.loss_bbox: 0.2760, d3.loss_cls: 0.1074, d3.loss_bbox: 0.2796, d4.loss_cls: 0.1034, d4.loss_bbox: 0.2827, loss: 2.5750, grad_norm: 24.5705
2025-06-11 01:06:20,998 - mmdet - INFO - Epoch [1][4950/7033]	lr: 2.000e-04, eta: 17:29:03, time: 1.575, data_time: 0.074, memory: 17291, loss_cls: 0.1089, loss_bbox: 0.2827, d0.loss_cls: 0.2049, d0.loss_bbox: 0.3894, d1.loss_cls: 0.1388, d1.loss_bbox: 0.2916, d2.loss_cls: 0.1208, d2.loss_bbox: 0.2728, d3.loss_cls: 0.1084, d3.loss_bbox: 0.2751, d4.loss_cls: 0.1083, d4.loss_bbox: 0.2750, loss: 2.5767, grad_norm: 29.8755
2025-06-11 01:07:42,258 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 01:07:42,259 - mmdet - INFO - Epoch [1][5000/7033]	lr: 2.000e-04, eta: 17:27:14, time: 1.624, data_time: 0.065, memory: 17291, loss_cls: 0.0867, loss_bbox: 0.2648, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3625, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2751, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2572, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2598, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2582, loss: 2.3478, grad_norm: 43.9257
2025-06-11 01:09:02,010 - mmdet - INFO - Epoch [1][5050/7033]	lr: 2.000e-04, eta: 17:25:15, time: 1.595, data_time: 0.057, memory: 17291, loss_cls: 0.1010, loss_bbox: 0.2717, d0.loss_cls: 0.1992, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1374, d1.loss_bbox: 0.2791, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2610, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2630, d4.loss_cls: 0.1026, d4.loss_bbox: 0.2620, loss: 2.4672, grad_norm: 39.7137
2025-06-11 01:10:21,434 - mmdet - INFO - Epoch [1][5100/7033]	lr: 2.000e-04, eta: 17:23:15, time: 1.589, data_time: 0.058, memory: 17291, loss_cls: 0.1040, loss_bbox: 0.2739, d0.loss_cls: 0.1991, d0.loss_bbox: 0.3840, d1.loss_cls: 0.1378, d1.loss_bbox: 0.2903, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2681, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2716, d4.loss_cls: 0.1052, d4.loss_bbox: 0.2653, loss: 2.5209, grad_norm: 39.6410
2025-06-11 01:11:43,301 - mmdet - INFO - Epoch [1][5150/7033]	lr: 2.000e-04, eta: 17:21:33, time: 1.638, data_time: 0.058, memory: 17291, loss_cls: 0.1083, loss_bbox: 0.2772, d0.loss_cls: 0.2025, d0.loss_bbox: 0.3770, d1.loss_cls: 0.1370, d1.loss_bbox: 0.2879, d2.loss_cls: 0.1214, d2.loss_bbox: 0.2683, d3.loss_cls: 0.1097, d3.loss_bbox: 0.2709, d4.loss_cls: 0.1077, d4.loss_bbox: 0.2670, loss: 2.5351, grad_norm: 63.7865
2025-06-11 01:13:04,664 - mmdet - INFO - Epoch [1][5200/7033]	lr: 2.000e-04, eta: 17:19:47, time: 1.627, data_time: 0.059, memory: 17291, loss_cls: 0.0971, loss_bbox: 0.2722, d0.loss_cls: 0.2004, d0.loss_bbox: 0.3812, d1.loss_cls: 0.1384, d1.loss_bbox: 0.2854, d2.loss_cls: 0.1109, d2.loss_bbox: 0.2667, d3.loss_cls: 0.1006, d3.loss_bbox: 0.2684, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2649, loss: 2.4833, grad_norm: 39.1431
2025-06-11 01:14:24,107 - mmdet - INFO - Epoch [1][5250/7033]	lr: 2.000e-04, eta: 17:17:48, time: 1.589, data_time: 0.063, memory: 17291, loss_cls: 0.0987, loss_bbox: 0.2703, d0.loss_cls: 0.2040, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1395, d1.loss_bbox: 0.2804, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2646, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2690, d4.loss_cls: 0.0989, d4.loss_bbox: 0.2636, loss: 2.4865, grad_norm: 42.3556
2025-06-11 01:15:42,748 - mmdet - INFO - Epoch [1][5300/7033]	lr: 2.000e-04, eta: 17:15:45, time: 1.572, data_time: 0.065, memory: 17291, loss_cls: 0.0936, loss_bbox: 0.2621, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2695, d2.loss_cls: 0.1131, d2.loss_bbox: 0.2481, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2513, d4.loss_cls: 0.0959, d4.loss_bbox: 0.2522, loss: 2.3845, grad_norm: 52.6307
2025-06-11 01:17:04,301 - mmdet - INFO - Epoch [1][5350/7033]	lr: 2.000e-04, eta: 17:14:02, time: 1.631, data_time: 0.078, memory: 17291, loss_cls: 0.1053, loss_bbox: 0.2754, d0.loss_cls: 0.2121, d0.loss_bbox: 0.4012, d1.loss_cls: 0.1491, d1.loss_bbox: 0.3034, d2.loss_cls: 0.1293, d2.loss_bbox: 0.2794, d3.loss_cls: 0.1089, d3.loss_bbox: 0.2828, d4.loss_cls: 0.1061, d4.loss_bbox: 0.2730, loss: 2.6261, grad_norm: 39.8888
2025-06-11 01:18:26,571 - mmdet - INFO - Epoch [1][5400/7033]	lr: 2.000e-04, eta: 17:12:25, time: 1.646, data_time: 0.076, memory: 17291, loss_cls: 0.0991, loss_bbox: 0.2681, d0.loss_cls: 0.2037, d0.loss_bbox: 0.3875, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2879, d2.loss_cls: 0.1220, d2.loss_bbox: 0.2653, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2676, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2621, loss: 2.5077, grad_norm: 34.9213
2025-06-11 01:19:50,058 - mmdet - INFO - Epoch [1][5450/7033]	lr: 2.000e-04, eta: 17:10:56, time: 1.669, data_time: 0.066, memory: 17291, loss_cls: 0.0986, loss_bbox: 0.2777, d0.loss_cls: 0.2030, d0.loss_bbox: 0.3812, d1.loss_cls: 0.1359, d1.loss_bbox: 0.2867, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2708, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2704, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2672, loss: 2.5102, grad_norm: 47.2141
2025-06-11 01:21:12,138 - mmdet - INFO - Epoch [1][5500/7033]	lr: 2.000e-04, eta: 17:09:18, time: 1.642, data_time: 0.049, memory: 17291, loss_cls: 0.0966, loss_bbox: 0.2681, d0.loss_cls: 0.1954, d0.loss_bbox: 0.3582, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2749, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2553, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2583, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2569, loss: 2.4005, grad_norm: 26.8512
2025-06-11 01:22:33,773 - mmdet - INFO - Epoch [1][5550/7033]	lr: 2.000e-04, eta: 17:07:38, time: 1.633, data_time: 0.052, memory: 17291, loss_cls: 0.0938, loss_bbox: 0.2670, d0.loss_cls: 0.1995, d0.loss_bbox: 0.3796, d1.loss_cls: 0.1340, d1.loss_bbox: 0.2854, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2636, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2654, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2598, loss: 2.4478, grad_norm: 37.6147
2025-06-11 01:23:53,501 - mmdet - INFO - Epoch [1][5600/7033]	lr: 2.000e-04, eta: 17:05:45, time: 1.595, data_time: 0.057, memory: 17291, loss_cls: 0.0971, loss_bbox: 0.2743, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3785, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2851, d2.loss_cls: 0.1117, d2.loss_bbox: 0.2646, d3.loss_cls: 0.0993, d3.loss_bbox: 0.2670, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2635, loss: 2.4541, grad_norm: 37.3514
2025-06-11 01:25:12,489 - mmdet - INFO - Epoch [1][5650/7033]	lr: 2.000e-04, eta: 17:03:48, time: 1.580, data_time: 0.066, memory: 17291, loss_cls: 0.0993, loss_bbox: 0.2749, d0.loss_cls: 0.1969, d0.loss_bbox: 0.3814, d1.loss_cls: 0.1347, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2672, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2704, d4.loss_cls: 0.1017, d4.loss_bbox: 0.2665, loss: 2.4942, grad_norm: 58.0644
2025-06-11 01:26:34,651 - mmdet - INFO - Epoch [1][5700/7033]	lr: 2.000e-04, eta: 17:02:12, time: 1.643, data_time: 0.096, memory: 17291, loss_cls: 0.0908, loss_bbox: 0.2631, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3681, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2764, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2578, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2578, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2563, loss: 2.3791, grad_norm: 28.8903
2025-06-11 01:27:54,985 - mmdet - INFO - Epoch [1][5750/7033]	lr: 2.000e-04, eta: 17:00:24, time: 1.607, data_time: 0.062, memory: 17291, loss_cls: 0.0877, loss_bbox: 0.2598, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3661, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2709, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2542, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2601, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2518, loss: 2.3492, grad_norm: 29.4235
2025-06-11 01:29:14,075 - mmdet - INFO - Epoch [1][5800/7033]	lr: 2.000e-04, eta: 16:58:29, time: 1.581, data_time: 0.076, memory: 17291, loss_cls: 0.0912, loss_bbox: 0.2625, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3716, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2754, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2547, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2578, d4.loss_cls: 0.0919, d4.loss_bbox: 0.2538, loss: 2.3720, grad_norm: 34.3962
2025-06-11 01:30:33,938 - mmdet - INFO - Epoch [1][5850/7033]	lr: 2.000e-04, eta: 16:56:40, time: 1.599, data_time: 0.064, memory: 17291, loss_cls: 0.0961, loss_bbox: 0.2612, d0.loss_cls: 0.2018, d0.loss_bbox: 0.3721, d1.loss_cls: 0.1303, d1.loss_bbox: 0.2769, d2.loss_cls: 0.1102, d2.loss_bbox: 0.2584, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2635, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2551, loss: 2.4185, grad_norm: 32.6000
2025-06-11 01:31:53,148 - mmdet - INFO - Epoch [1][5900/7033]	lr: 2.000e-04, eta: 16:54:47, time: 1.584, data_time: 0.055, memory: 17291, loss_cls: 0.0880, loss_bbox: 0.2679, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3748, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2780, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2596, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2617, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2582, loss: 2.3868, grad_norm: 48.5766
2025-06-11 01:33:13,459 - mmdet - INFO - Epoch [1][5950/7033]	lr: 2.000e-04, eta: 16:53:02, time: 1.606, data_time: 0.080, memory: 17291, loss_cls: 0.0901, loss_bbox: 0.2699, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2609, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2643, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2611, loss: 2.3921, grad_norm: 58.2679
2025-06-11 01:34:34,458 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 01:34:34,459 - mmdet - INFO - Epoch [1][6000/7033]	lr: 2.000e-04, eta: 16:51:21, time: 1.620, data_time: 0.067, memory: 17624, loss_cls: 0.0967, loss_bbox: 0.2662, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3718, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2817, d2.loss_cls: 0.1082, d2.loss_bbox: 0.2578, d3.loss_cls: 0.1000, d3.loss_bbox: 0.2579, d4.loss_cls: 0.0975, d4.loss_bbox: 0.2555, loss: 2.4102, grad_norm: 32.7995
2025-06-11 01:36:00,216 - mmdet - INFO - Epoch [1][6050/7033]	lr: 2.000e-04, eta: 16:50:09, time: 1.715, data_time: 0.062, memory: 17624, loss_cls: 0.0900, loss_bbox: 0.2613, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3582, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1071, d2.loss_bbox: 0.2536, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2558, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2542, loss: 2.3597, grad_norm: 41.5651
2025-06-11 01:37:22,014 - mmdet - INFO - Epoch [1][6100/7033]	lr: 2.000e-04, eta: 16:48:33, time: 1.635, data_time: 0.056, memory: 17624, loss_cls: 0.1056, loss_bbox: 0.2612, d0.loss_cls: 0.2020, d0.loss_bbox: 0.3656, d1.loss_cls: 0.1346, d1.loss_bbox: 0.2760, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2554, d3.loss_cls: 0.1093, d3.loss_bbox: 0.2567, d4.loss_cls: 0.1071, d4.loss_bbox: 0.2520, loss: 2.4468, grad_norm: 28.3732
2025-06-11 01:38:39,715 - mmdet - INFO - Epoch [1][6150/7033]	lr: 2.000e-04, eta: 16:46:33, time: 1.554, data_time: 0.058, memory: 17624, loss_cls: 0.0983, loss_bbox: 0.2744, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3744, d1.loss_cls: 0.1342, d1.loss_bbox: 0.2843, d2.loss_cls: 0.1128, d2.loss_bbox: 0.2670, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2717, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2668, loss: 2.4760, grad_norm: 48.5601
2025-06-11 01:40:00,406 - mmdet - INFO - Epoch [1][6200/7033]	lr: 2.000e-04, eta: 16:44:51, time: 1.613, data_time: 0.055, memory: 17624, loss_cls: 0.0832, loss_bbox: 0.2684, d0.loss_cls: 0.1957, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2674, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2496, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2515, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2526, loss: 2.3195, grad_norm: 42.2297
2025-06-11 01:41:22,681 - mmdet - INFO - Epoch [1][6250/7033]	lr: 2.000e-04, eta: 16:43:19, time: 1.646, data_time: 0.049, memory: 17624, loss_cls: 0.0952, loss_bbox: 0.2613, d0.loss_cls: 0.1931, d0.loss_bbox: 0.3772, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2808, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2653, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2687, d4.loss_cls: 0.0970, d4.loss_bbox: 0.2575, loss: 2.4364, grad_norm: 36.5426
2025-06-11 01:42:46,165 - mmdet - INFO - Epoch [1][6300/7033]	lr: 2.000e-04, eta: 16:41:54, time: 1.669, data_time: 0.060, memory: 17624, loss_cls: 0.0921, loss_bbox: 0.2792, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3848, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2923, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2732, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2741, d4.loss_cls: 0.0929, d4.loss_bbox: 0.2691, loss: 2.4705, grad_norm: 42.3520
2025-06-11 01:44:07,146 - mmdet - INFO - Epoch [1][6350/7033]	lr: 2.000e-04, eta: 16:40:15, time: 1.620, data_time: 0.064, memory: 17624, loss_cls: 0.1004, loss_bbox: 0.2647, d0.loss_cls: 0.1952, d0.loss_bbox: 0.3727, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1168, d2.loss_bbox: 0.2636, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2675, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2564, loss: 2.4613, grad_norm: 73.1908
2025-06-11 01:45:26,936 - mmdet - INFO - Epoch [1][6400/7033]	lr: 2.000e-04, eta: 16:38:29, time: 1.596, data_time: 0.068, memory: 17624, loss_cls: 0.0991, loss_bbox: 0.2613, d0.loss_cls: 0.2002, d0.loss_bbox: 0.3833, d1.loss_cls: 0.1321, d1.loss_bbox: 0.2817, d2.loss_cls: 0.1151, d2.loss_bbox: 0.2609, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2643, d4.loss_cls: 0.1015, d4.loss_bbox: 0.2567, loss: 2.4585, grad_norm: 31.1313
2025-06-11 01:46:48,223 - mmdet - INFO - Epoch [1][6450/7033]	lr: 2.000e-04, eta: 16:36:52, time: 1.624, data_time: 0.087, memory: 17624, loss_cls: 0.0897, loss_bbox: 0.2576, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3527, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2552, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2487, loss: 2.3108, grad_norm: 21.9133
2025-06-11 01:48:08,632 - mmdet - INFO - Epoch [1][6500/7033]	lr: 2.000e-04, eta: 16:35:11, time: 1.610, data_time: 0.067, memory: 17624, loss_cls: 0.1018, loss_bbox: 0.2648, d0.loss_cls: 0.2090, d0.loss_bbox: 0.3757, d1.loss_cls: 0.1434, d1.loss_bbox: 0.2769, d2.loss_cls: 0.1203, d2.loss_bbox: 0.2583, d3.loss_cls: 0.1050, d3.loss_bbox: 0.2584, d4.loss_cls: 0.1040, d4.loss_bbox: 0.2526, loss: 2.4703, grad_norm: 32.9337
2025-06-11 01:49:29,911 - mmdet - INFO - Epoch [1][6550/7033]	lr: 2.000e-04, eta: 16:33:34, time: 1.625, data_time: 0.068, memory: 17624, loss_cls: 0.0962, loss_bbox: 0.2583, d0.loss_cls: 0.1966, d0.loss_bbox: 0.3798, d1.loss_cls: 0.1340, d1.loss_bbox: 0.2769, d2.loss_cls: 0.1105, d2.loss_bbox: 0.2565, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2573, d4.loss_cls: 0.0981, d4.loss_bbox: 0.2524, loss: 2.4179, grad_norm: 18.7702
2025-06-11 01:50:50,439 - mmdet - INFO - Epoch [1][6600/7033]	lr: 2.000e-04, eta: 16:31:54, time: 1.611, data_time: 0.064, memory: 17624, loss_cls: 0.0845, loss_bbox: 0.2482, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3627, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2674, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2496, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2410, loss: 2.2875, grad_norm: 28.1158
2025-06-11 01:52:11,397 - mmdet - INFO - Epoch [1][6650/7033]	lr: 2.000e-04, eta: 16:30:16, time: 1.618, data_time: 0.065, memory: 17624, loss_cls: 0.0929, loss_bbox: 0.2650, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3669, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2582, d3.loss_cls: 0.0951, d3.loss_bbox: 0.2628, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2550, loss: 2.3800, grad_norm: 44.7506
2025-06-11 01:53:31,805 - mmdet - INFO - Epoch [1][6700/7033]	lr: 2.000e-04, eta: 16:28:36, time: 1.609, data_time: 0.076, memory: 17624, loss_cls: 0.0931, loss_bbox: 0.2487, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3626, d1.loss_cls: 0.1318, d1.loss_bbox: 0.2692, d2.loss_cls: 0.1113, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2514, d4.loss_cls: 0.0956, d4.loss_bbox: 0.2429, loss: 2.3491, grad_norm: 57.7582
2025-06-11 01:54:54,444 - mmdet - INFO - Epoch [1][6750/7033]	lr: 2.000e-04, eta: 16:27:07, time: 1.652, data_time: 0.070, memory: 17624, loss_cls: 0.0885, loss_bbox: 0.2611, d0.loss_cls: 0.1955, d0.loss_bbox: 0.3756, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2767, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2557, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2515, loss: 2.3718, grad_norm: 44.7876
2025-06-11 01:56:14,528 - mmdet - INFO - Epoch [1][6800/7033]	lr: 2.000e-04, eta: 16:25:26, time: 1.602, data_time: 0.072, memory: 17624, loss_cls: 0.1000, loss_bbox: 0.2584, d0.loss_cls: 0.2002, d0.loss_bbox: 0.3689, d1.loss_cls: 0.1356, d1.loss_bbox: 0.2745, d2.loss_cls: 0.1132, d2.loss_bbox: 0.2572, d3.loss_cls: 0.1054, d3.loss_bbox: 0.2572, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2516, loss: 2.4247, grad_norm: 36.2288
2025-06-11 01:57:33,182 - mmdet - INFO - Epoch [1][6850/7033]	lr: 2.000e-04, eta: 16:23:37, time: 1.573, data_time: 0.058, memory: 17624, loss_cls: 0.0830, loss_bbox: 0.2529, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3637, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2729, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2555, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2558, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2469, loss: 2.3003, grad_norm: 29.8346
2025-06-11 01:58:52,206 - mmdet - INFO - Epoch [1][6900/7033]	lr: 2.000e-04, eta: 16:21:51, time: 1.578, data_time: 0.061, memory: 17624, loss_cls: 0.0929, loss_bbox: 0.2615, d0.loss_cls: 0.2011, d0.loss_bbox: 0.3785, d1.loss_cls: 0.1318, d1.loss_bbox: 0.2856, d2.loss_cls: 0.1095, d2.loss_bbox: 0.2662, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2664, d4.loss_cls: 0.0923, d4.loss_bbox: 0.2562, loss: 2.4390, grad_norm: 34.0090
2025-06-11 02:00:10,841 - mmdet - INFO - Epoch [1][6950/7033]	lr: 2.000e-04, eta: 16:20:03, time: 1.575, data_time: 0.060, memory: 17624, loss_cls: 0.0886, loss_bbox: 0.2612, d0.loss_cls: 0.1877, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2755, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2580, d3.loss_cls: 0.0910, d3.loss_bbox: 0.2584, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2510, loss: 2.3481, grad_norm: 36.0631
2025-06-11 02:01:35,037 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 02:01:35,038 - mmdet - INFO - Epoch [1][7000/7033]	lr: 2.000e-04, eta: 16:18:44, time: 1.684, data_time: 0.059, memory: 17624, loss_cls: 0.0950, loss_bbox: 0.2609, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3707, d1.loss_cls: 0.1301, d1.loss_bbox: 0.2774, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2560, d3.loss_cls: 0.0969, d3.loss_bbox: 0.2567, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2510, loss: 2.3915, grad_norm: 45.8943
2025-06-11 02:02:27,400 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-11 02:39:55,326 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 02:39:55,327 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7301, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8762, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9076, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9221, pts_bbox_NuScenes/car_trans_err: 0.2456, pts_bbox_NuScenes/car_scale_err: 0.1558, pts_bbox_NuScenes/car_orient_err: 0.0603, pts_bbox_NuScenes/car_vel_err: 0.3814, pts_bbox_NuScenes/car_attr_err: 0.1753, pts_bbox_NuScenes/mATE: 0.3932, pts_bbox_NuScenes/mASE: 0.2701, pts_bbox_NuScenes/mAOE: 0.2861, pts_bbox_NuScenes/mAVE: 0.3678, pts_bbox_NuScenes/mAAE: 0.1843, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.3815, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.5908, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7061, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7475, pts_bbox_NuScenes/truck_trans_err: 0.3868, pts_bbox_NuScenes/truck_scale_err: 0.2108, pts_bbox_NuScenes/truck_orient_err: 0.0952, pts_bbox_NuScenes/truck_vel_err: 0.3913, pts_bbox_NuScenes/truck_attr_err: 0.2169, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0495, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1936, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3764, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4487, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6687, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4578, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8483, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1183, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2981, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4220, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7107, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8861, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9130, pts_bbox_NuScenes/bus_trans_err: 0.4192, pts_bbox_NuScenes/bus_scale_err: 0.2087, pts_bbox_NuScenes/bus_orient_err: 0.0536, pts_bbox_NuScenes/bus_vel_err: 0.6933, pts_bbox_NuScenes/bus_attr_err: 0.2398, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1500, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4187, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5867, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6778, pts_bbox_NuScenes/trailer_trans_err: 0.5172, pts_bbox_NuScenes/trailer_scale_err: 0.2322, pts_bbox_NuScenes/trailer_orient_err: 0.4280, pts_bbox_NuScenes/trailer_vel_err: 0.2273, pts_bbox_NuScenes/trailer_attr_err: 0.1810, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.4795, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6896, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7429, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7591, pts_bbox_NuScenes/barrier_trans_err: 0.3871, pts_bbox_NuScenes/barrier_scale_err: 0.2943, pts_bbox_NuScenes/barrier_orient_err: 0.0781, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5191, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7418, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7883, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7968, pts_bbox_NuScenes/motorcycle_trans_err: 0.3341, pts_bbox_NuScenes/motorcycle_scale_err: 0.2467, pts_bbox_NuScenes/motorcycle_orient_err: 0.2278, pts_bbox_NuScenes/motorcycle_vel_err: 0.6619, pts_bbox_NuScenes/motorcycle_attr_err: 0.2347, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.4957, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6011, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6203, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6305, pts_bbox_NuScenes/bicycle_trans_err: 0.3159, pts_bbox_NuScenes/bicycle_scale_err: 0.2684, pts_bbox_NuScenes/bicycle_orient_err: 0.4358, pts_bbox_NuScenes/bicycle_vel_err: 0.2172, pts_bbox_NuScenes/bicycle_attr_err: 0.0035, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7555, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8444, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8734, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8868, pts_bbox_NuScenes/pedestrian_trans_err: 0.2877, pts_bbox_NuScenes/pedestrian_scale_err: 0.2952, pts_bbox_NuScenes/pedestrian_orient_err: 0.3483, pts_bbox_NuScenes/pedestrian_vel_err: 0.2520, pts_bbox_NuScenes/pedestrian_attr_err: 0.1249, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.5947, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7497, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7872, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8109, pts_bbox_NuScenes/traffic_cone_trans_err: 0.3699, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3310, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6731, pts_bbox_NuScenes/mAP: 0.6466
2025-06-11 02:41:37,527 - mmdet - INFO - Epoch [2][50/7033]	lr: 1.866e-04, eta: 16:13:07, time: 1.969, data_time: 0.363, memory: 17624, loss_cls: 0.0862, loss_bbox: 0.2530, d0.loss_cls: 0.1840, d0.loss_bbox: 0.3631, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2718, d2.loss_cls: 0.1053, d2.loss_bbox: 0.2550, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2479, loss: 2.3301, grad_norm: 29.7202
2025-06-11 02:42:57,347 - mmdet - INFO - Epoch [2][100/7033]	lr: 1.866e-04, eta: 16:11:27, time: 1.596, data_time: 0.064, memory: 17624, loss_cls: 0.0944, loss_bbox: 0.2592, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3724, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2795, d2.loss_cls: 0.1092, d2.loss_bbox: 0.2591, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2608, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2499, loss: 2.4093, grad_norm: 34.4154
2025-06-11 02:44:19,007 - mmdet - INFO - Epoch [2][150/7033]	lr: 1.866e-04, eta: 16:09:57, time: 1.633, data_time: 0.063, memory: 17624, loss_cls: 0.0816, loss_bbox: 0.2456, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2652, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2494, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2511, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2399, loss: 2.2440, grad_norm: 25.0151
2025-06-11 02:45:40,247 - mmdet - INFO - Epoch [2][200/7033]	lr: 1.866e-04, eta: 16:08:25, time: 1.625, data_time: 0.075, memory: 17624, loss_cls: 0.0916, loss_bbox: 0.2536, d0.loss_cls: 0.1901, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2741, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2563, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2483, loss: 2.3584, grad_norm: 26.0567
2025-06-11 02:46:59,955 - mmdet - INFO - Epoch [2][250/7033]	lr: 1.866e-04, eta: 16:06:46, time: 1.594, data_time: 0.072, memory: 17624, loss_cls: 0.0877, loss_bbox: 0.2477, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1275, d1.loss_bbox: 0.2638, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2500, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2428, loss: 2.2909, grad_norm: 24.9523
2025-06-11 02:48:21,043 - mmdet - INFO - Epoch [2][300/7033]	lr: 1.866e-04, eta: 16:05:13, time: 1.622, data_time: 0.068, memory: 17624, loss_cls: 0.0851, loss_bbox: 0.2400, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3566, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2662, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2463, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2358, loss: 2.2636, grad_norm: 37.0040
2025-06-11 02:49:41,028 - mmdet - INFO - Epoch [2][350/7033]	lr: 1.866e-04, eta: 16:03:35, time: 1.600, data_time: 0.066, memory: 17624, loss_cls: 0.0964, loss_bbox: 0.2511, d0.loss_cls: 0.1966, d0.loss_bbox: 0.3759, d1.loss_cls: 0.1378, d1.loss_bbox: 0.2742, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2549, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2559, d4.loss_cls: 0.0977, d4.loss_bbox: 0.2476, loss: 2.4028, grad_norm: 65.2490
2025-06-11 02:51:03,199 - mmdet - INFO - Epoch [2][400/7033]	lr: 1.866e-04, eta: 16:02:08, time: 1.643, data_time: 0.069, memory: 17624, loss_cls: 0.0850, loss_bbox: 0.2344, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3529, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2604, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2386, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2396, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2307, loss: 2.2149, grad_norm: 31.5886
2025-06-11 02:52:27,114 - mmdet - INFO - Epoch [2][450/7033]	lr: 1.866e-04, eta: 16:00:49, time: 1.678, data_time: 0.061, memory: 17624, loss_cls: 0.0907, loss_bbox: 0.2558, d0.loss_cls: 0.1953, d0.loss_bbox: 0.3827, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2790, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2595, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2584, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2513, loss: 2.4067, grad_norm: 28.9900
2025-06-11 02:53:57,370 - mmdet - INFO - Epoch [2][500/7033]	lr: 1.866e-04, eta: 16:00:00, time: 1.805, data_time: 0.205, memory: 17624, loss_cls: 0.0915, loss_bbox: 0.2398, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2633, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2418, d4.loss_cls: 0.0945, d4.loss_bbox: 0.2341, loss: 2.2772, grad_norm: 80.5984
2025-06-11 02:55:16,375 - mmdet - INFO - Epoch [2][550/7033]	lr: 1.866e-04, eta: 15:58:18, time: 1.580, data_time: 0.064, memory: 17624, loss_cls: 0.0960, loss_bbox: 0.2564, d0.loss_cls: 0.2042, d0.loss_bbox: 0.3770, d1.loss_cls: 0.1382, d1.loss_bbox: 0.2833, d2.loss_cls: 0.1189, d2.loss_bbox: 0.2644, d3.loss_cls: 0.1042, d3.loss_bbox: 0.2625, d4.loss_cls: 0.1001, d4.loss_bbox: 0.2539, loss: 2.4591, grad_norm: 55.0180
2025-06-11 02:56:36,095 - mmdet - INFO - Epoch [2][600/7033]	lr: 1.866e-04, eta: 15:56:40, time: 1.595, data_time: 0.069, memory: 17624, loss_cls: 0.0933, loss_bbox: 0.2609, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3708, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2823, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2651, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2651, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2564, loss: 2.4154, grad_norm: 38.0170
2025-06-11 02:57:56,964 - mmdet - INFO - Epoch [2][650/7033]	lr: 1.866e-04, eta: 15:55:07, time: 1.617, data_time: 0.072, memory: 17624, loss_cls: 0.0946, loss_bbox: 0.2629, d0.loss_cls: 0.1923, d0.loss_bbox: 0.3751, d1.loss_cls: 0.1320, d1.loss_bbox: 0.2885, d2.loss_cls: 0.1160, d2.loss_bbox: 0.2676, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2706, d4.loss_cls: 0.0969, d4.loss_bbox: 0.2582, loss: 2.4550, grad_norm: 36.7416
2025-06-11 02:59:15,426 - mmdet - INFO - Epoch [2][700/7033]	lr: 1.866e-04, eta: 15:53:24, time: 1.569, data_time: 0.069, memory: 17624, loss_cls: 0.0987, loss_bbox: 0.2637, d0.loss_cls: 0.1980, d0.loss_bbox: 0.3826, d1.loss_cls: 0.1386, d1.loss_bbox: 0.2961, d2.loss_cls: 0.1235, d2.loss_bbox: 0.2693, d3.loss_cls: 0.1026, d3.loss_bbox: 0.2708, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2605, loss: 2.5043, grad_norm: 29.2505
2025-06-11 03:00:33,027 - mmdet - INFO - Epoch [2][750/7033]	lr: 1.866e-04, eta: 15:51:37, time: 1.552, data_time: 0.068, memory: 17624, loss_cls: 0.0946, loss_bbox: 0.2456, d0.loss_cls: 0.1968, d0.loss_bbox: 0.3684, d1.loss_cls: 0.1334, d1.loss_bbox: 0.2751, d2.loss_cls: 0.1150, d2.loss_bbox: 0.2529, d3.loss_cls: 0.1010, d3.loss_bbox: 0.2533, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2418, loss: 2.3750, grad_norm: 28.6357
2025-06-11 03:01:56,673 - mmdet - INFO - Epoch [2][800/7033]	lr: 1.866e-04, eta: 15:50:17, time: 1.673, data_time: 0.069, memory: 17624, loss_cls: 0.0975, loss_bbox: 0.2390, d0.loss_cls: 0.1920, d0.loss_bbox: 0.3577, d1.loss_cls: 0.1325, d1.loss_bbox: 0.2653, d2.loss_cls: 0.1158, d2.loss_bbox: 0.2463, d3.loss_cls: 0.1036, d3.loss_bbox: 0.2462, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2369, loss: 2.3326, grad_norm: 32.0201
2025-06-11 03:03:18,695 - mmdet - INFO - Epoch [2][850/7033]	lr: 1.866e-04, eta: 15:48:50, time: 1.640, data_time: 0.088, memory: 17624, loss_cls: 0.0903, loss_bbox: 0.2415, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2683, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2476, d4.loss_cls: 0.0923, d4.loss_bbox: 0.2390, loss: 2.2838, grad_norm: 82.0134
2025-06-11 03:04:42,069 - mmdet - INFO - Epoch [2][900/7033]	lr: 1.866e-04, eta: 15:47:29, time: 1.668, data_time: 0.073, memory: 17624, loss_cls: 0.0861, loss_bbox: 0.2459, d0.loss_cls: 0.1926, d0.loss_bbox: 0.3493, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2484, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2534, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2418, loss: 2.2912, grad_norm: 23.2686
2025-06-11 03:06:05,317 - mmdet - INFO - Epoch [2][950/7033]	lr: 1.866e-04, eta: 15:46:07, time: 1.665, data_time: 0.065, memory: 17624, loss_cls: 0.0829, loss_bbox: 0.2371, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3432, d1.loss_cls: 0.1208, d1.loss_bbox: 0.2579, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2447, d4.loss_cls: 0.0857, d4.loss_bbox: 0.2324, loss: 2.2188, grad_norm: 29.8221
2025-06-11 03:07:28,917 - mmdet - INFO - Epoch [2][1000/7033]	lr: 1.866e-04, eta: 15:44:47, time: 1.672, data_time: 0.065, memory: 17624, loss_cls: 0.0864, loss_bbox: 0.2435, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3623, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2690, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2480, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2510, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2411, loss: 2.2905, grad_norm: 29.4918
2025-06-11 03:08:51,417 - mmdet - INFO - Epoch [2][1050/7033]	lr: 1.866e-04, eta: 15:43:22, time: 1.649, data_time: 0.057, memory: 17624, loss_cls: 0.0868, loss_bbox: 0.2427, d0.loss_cls: 0.1903, d0.loss_bbox: 0.3704, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2513, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2508, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2403, loss: 2.3221, grad_norm: 29.3766
2025-06-11 03:10:10,512 - mmdet - INFO - Epoch [2][1100/7033]	lr: 1.866e-04, eta: 15:41:43, time: 1.582, data_time: 0.067, memory: 17624, loss_cls: 0.0902, loss_bbox: 0.2422, d0.loss_cls: 0.1908, d0.loss_bbox: 0.3612, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2684, d2.loss_cls: 0.1107, d2.loss_bbox: 0.2517, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2532, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2384, loss: 2.3217, grad_norm: 43.1948
2025-06-11 03:11:30,319 - mmdet - INFO - Epoch [2][1150/7033]	lr: 1.866e-04, eta: 15:40:07, time: 1.597, data_time: 0.063, memory: 17624, loss_cls: 0.0804, loss_bbox: 0.2356, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2629, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2465, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2323, loss: 2.2166, grad_norm: 30.7985
2025-06-11 03:12:51,129 - mmdet - INFO - Epoch [2][1200/7033]	lr: 1.866e-04, eta: 15:38:35, time: 1.616, data_time: 0.078, memory: 17624, loss_cls: 0.0881, loss_bbox: 0.2319, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3530, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2608, d2.loss_cls: 0.1036, d2.loss_bbox: 0.2407, d3.loss_cls: 0.0916, d3.loss_bbox: 0.2392, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2292, loss: 2.2332, grad_norm: 30.2651
2025-06-11 03:14:15,513 - mmdet - INFO - Epoch [2][1250/7033]	lr: 1.866e-04, eta: 15:37:18, time: 1.688, data_time: 0.093, memory: 17624, loss_cls: 0.0812, loss_bbox: 0.2419, d0.loss_cls: 0.1842, d0.loss_bbox: 0.3580, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2701, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2523, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2405, loss: 2.2489, grad_norm: 41.4923
2025-06-11 03:15:38,868 - mmdet - INFO - Epoch [2][1300/7033]	lr: 1.866e-04, eta: 15:35:57, time: 1.667, data_time: 0.081, memory: 17624, loss_cls: 0.0837, loss_bbox: 0.2377, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3550, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2649, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2450, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2477, d4.loss_cls: 0.0873, d4.loss_bbox: 0.2348, loss: 2.2522, grad_norm: 28.9444
2025-06-11 03:17:01,722 - mmdet - INFO - Epoch [2][1350/7033]	lr: 1.866e-04, eta: 15:34:34, time: 1.657, data_time: 0.086, memory: 17624, loss_cls: 0.0843, loss_bbox: 0.2389, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3493, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2665, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2487, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2488, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2367, loss: 2.2530, grad_norm: 34.6503
2025-06-11 03:18:25,335 - mmdet - INFO - Epoch [2][1400/7033]	lr: 1.866e-04, eta: 15:33:14, time: 1.672, data_time: 0.078, memory: 17624, loss_cls: 0.0922, loss_bbox: 0.2370, d0.loss_cls: 0.1943, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2659, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2454, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0943, d4.loss_bbox: 0.2344, loss: 2.2959, grad_norm: 24.9765
2025-06-11 03:19:46,470 - mmdet - INFO - Epoch [2][1450/7033]	lr: 1.866e-04, eta: 15:31:44, time: 1.623, data_time: 0.074, memory: 17624, loss_cls: 0.0924, loss_bbox: 0.2251, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3529, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2588, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2398, d4.loss_cls: 0.0937, d4.loss_bbox: 0.2244, loss: 2.2366, grad_norm: 37.4214
2025-06-11 03:21:14,087 - mmdet - INFO - Epoch [2][1500/7033]	lr: 1.866e-04, eta: 15:30:40, time: 1.752, data_time: 0.126, memory: 17624, loss_cls: 0.0869, loss_bbox: 0.2332, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3581, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2423, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2427, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2324, loss: 2.2465, grad_norm: 23.6660
2025-06-11 03:22:36,844 - mmdet - INFO - Epoch [2][1550/7033]	lr: 1.866e-04, eta: 15:29:16, time: 1.654, data_time: 0.073, memory: 17624, loss_cls: 0.0903, loss_bbox: 0.2384, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3487, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2375, loss: 2.2742, grad_norm: 22.3756
2025-06-11 03:24:00,554 - mmdet - INFO - Epoch [2][1600/7033]	lr: 1.866e-04, eta: 15:27:56, time: 1.675, data_time: 0.085, memory: 17624, loss_cls: 0.0827, loss_bbox: 0.2264, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2574, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2367, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2396, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2249, loss: 2.1852, grad_norm: 32.5365
2025-06-11 03:25:22,321 - mmdet - INFO - Epoch [2][1650/7033]	lr: 1.866e-04, eta: 15:26:29, time: 1.635, data_time: 0.065, memory: 17624, loss_cls: 0.0899, loss_bbox: 0.2402, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3599, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0941, d3.loss_bbox: 0.2483, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2389, loss: 2.2992, grad_norm: 73.3165
2025-06-11 03:26:42,962 - mmdet - INFO - Epoch [2][1700/7033]	lr: 1.866e-04, eta: 15:24:57, time: 1.612, data_time: 0.086, memory: 17624, loss_cls: 0.0866, loss_bbox: 0.2397, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3620, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2683, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2506, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2382, loss: 2.2855, grad_norm: 24.3733
2025-06-11 03:28:05,990 - mmdet - INFO - Epoch [2][1750/7033]	lr: 1.866e-04, eta: 15:23:34, time: 1.659, data_time: 0.075, memory: 17624, loss_cls: 0.0880, loss_bbox: 0.2442, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3662, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2555, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2590, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2431, loss: 2.3190, grad_norm: 32.1375
2025-06-11 03:29:28,565 - mmdet - INFO - Epoch [2][1800/7033]	lr: 1.866e-04, eta: 15:22:10, time: 1.653, data_time: 0.089, memory: 17624, loss_cls: 0.0885, loss_bbox: 0.2352, d0.loss_cls: 0.1831, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2658, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2449, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2333, loss: 2.2662, grad_norm: 52.8849
2025-06-11 03:30:52,730 - mmdet - INFO - Epoch [2][1850/7033]	lr: 1.866e-04, eta: 15:20:52, time: 1.683, data_time: 0.150, memory: 17624, loss_cls: 0.0851, loss_bbox: 0.2307, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2600, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2386, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2274, loss: 2.2168, grad_norm: 787.3839
2025-06-11 03:32:19,629 - mmdet - INFO - Epoch [2][1900/7033]	lr: 1.866e-04, eta: 15:19:44, time: 1.738, data_time: 0.170, memory: 17624, loss_cls: 0.0851, loss_bbox: 0.2318, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3516, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2615, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2284, loss: 2.2276, grad_norm: 35.2684
2025-06-11 03:33:41,621 - mmdet - INFO - Epoch [2][1950/7033]	lr: 1.866e-04, eta: 15:18:17, time: 1.640, data_time: 0.131, memory: 17624, loss_cls: 0.0849, loss_bbox: 0.2290, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3451, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2601, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2285, loss: 2.2006, grad_norm: 41.0260
2025-06-11 03:35:04,160 - mmdet - INFO - Epoch [2][2000/7033]	lr: 1.866e-04, eta: 15:16:53, time: 1.651, data_time: 0.146, memory: 17624, loss_cls: 0.0798, loss_bbox: 0.2307, d0.loss_cls: 0.1874, d0.loss_bbox: 0.3484, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2600, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2405, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2288, loss: 2.2049, grad_norm: 52.9403
2025-06-11 03:36:26,499 - mmdet - INFO - Epoch [2][2050/7033]	lr: 1.866e-04, eta: 15:15:28, time: 1.645, data_time: 0.064, memory: 17624, loss_cls: 0.0816, loss_bbox: 0.2389, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3546, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2484, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2361, loss: 2.2515, grad_norm: 23.7952
2025-06-11 03:38:11,324 - mmdet - INFO - Epoch [2][2100/7033]	lr: 1.866e-04, eta: 15:15:24, time: 2.098, data_time: 0.068, memory: 17624, loss_cls: 0.0827, loss_bbox: 0.2305, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2606, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2372, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2264, loss: 2.1928, grad_norm: 28.0447
2025-06-11 03:39:32,083 - mmdet - INFO - Epoch [2][2150/7033]	lr: 1.866e-04, eta: 15:13:53, time: 1.615, data_time: 0.086, memory: 17624, loss_cls: 0.0845, loss_bbox: 0.2420, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3553, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2702, d2.loss_cls: 0.1008, d2.loss_bbox: 0.2512, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2534, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2403, loss: 2.2812, grad_norm: 63.0414
2025-06-11 03:40:58,886 - mmdet - INFO - Epoch [2][2200/7033]	lr: 1.866e-04, eta: 15:12:43, time: 1.736, data_time: 0.064, memory: 17624, loss_cls: 0.0787, loss_bbox: 0.2352, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2644, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2443, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2464, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2344, loss: 2.2151, grad_norm: 29.5236
2025-06-11 03:42:23,124 - mmdet - INFO - Epoch [2][2250/7033]	lr: 1.866e-04, eta: 15:11:24, time: 1.685, data_time: 0.113, memory: 17624, loss_cls: 0.0876, loss_bbox: 0.2424, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3640, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2778, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2545, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2552, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2411, loss: 2.3090, grad_norm: 33.8620
2025-06-11 03:43:46,711 - mmdet - INFO - Epoch [2][2300/7033]	lr: 1.866e-04, eta: 15:10:03, time: 1.672, data_time: 0.121, memory: 17624, loss_cls: 0.0911, loss_bbox: 0.2390, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3607, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2707, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2482, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2492, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2376, loss: 2.3009, grad_norm: 65.0637
2025-06-11 03:45:06,096 - mmdet - INFO - Epoch [2][2350/7033]	lr: 1.866e-04, eta: 15:08:27, time: 1.588, data_time: 0.062, memory: 17624, loss_cls: 0.0856, loss_bbox: 0.2290, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3481, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2569, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2366, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2255, loss: 2.1959, grad_norm: 44.0912
2025-06-11 03:46:28,989 - mmdet - INFO - Epoch [2][2400/7033]	lr: 1.866e-04, eta: 15:07:03, time: 1.658, data_time: 0.070, memory: 17624, loss_cls: 0.0912, loss_bbox: 0.2379, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2722, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2502, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2511, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2369, loss: 2.3053, grad_norm: 33.3040
2025-06-11 03:47:51,463 - mmdet - INFO - Epoch [2][2450/7033]	lr: 1.866e-04, eta: 15:05:38, time: 1.650, data_time: 0.098, memory: 17624, loss_cls: 0.0892, loss_bbox: 0.2302, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3598, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2647, d2.loss_cls: 0.1095, d2.loss_bbox: 0.2402, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2409, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2295, loss: 2.2661, grad_norm: 31.0525
2025-06-11 03:49:12,585 - mmdet - INFO - Epoch [2][2500/7033]	lr: 1.866e-04, eta: 15:04:09, time: 1.622, data_time: 0.089, memory: 17624, loss_cls: 0.0831, loss_bbox: 0.2336, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2644, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0851, d3.loss_bbox: 0.2450, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2328, loss: 2.2231, grad_norm: 37.5297
2025-06-11 03:50:35,662 - mmdet - INFO - Epoch [2][2550/7033]	lr: 1.866e-04, eta: 15:02:46, time: 1.662, data_time: 0.058, memory: 17624, loss_cls: 0.0896, loss_bbox: 0.2415, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3646, d1.loss_cls: 0.1234, d1.loss_bbox: 0.2714, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2512, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2508, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2395, loss: 2.3040, grad_norm: 47.4742
2025-06-11 03:51:57,346 - mmdet - INFO - Epoch [2][2600/7033]	lr: 1.866e-04, eta: 15:01:18, time: 1.633, data_time: 0.064, memory: 17624, loss_cls: 0.0961, loss_bbox: 0.2414, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3704, d1.loss_cls: 0.1283, d1.loss_bbox: 0.2759, d2.loss_cls: 0.1111, d2.loss_bbox: 0.2517, d3.loss_cls: 0.1006, d3.loss_bbox: 0.2530, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2395, loss: 2.3556, grad_norm: 34.5750
2025-06-11 03:53:18,677 - mmdet - INFO - Epoch [2][2650/7033]	lr: 1.866e-04, eta: 14:59:49, time: 1.627, data_time: 0.068, memory: 17624, loss_cls: 0.0851, loss_bbox: 0.2347, d0.loss_cls: 0.1837, d0.loss_bbox: 0.3539, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2656, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2455, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2445, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2337, loss: 2.2455, grad_norm: 33.1149
2025-06-11 03:54:40,654 - mmdet - INFO - Epoch [2][2700/7033]	lr: 1.866e-04, eta: 14:58:23, time: 1.639, data_time: 0.167, memory: 17624, loss_cls: 0.0882, loss_bbox: 0.2396, d0.loss_cls: 0.1876, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2478, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2480, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2378, loss: 2.2747, grad_norm: 54.3549
2025-06-11 03:55:57,269 - mmdet - INFO - Epoch [2][2750/7033]	lr: 1.866e-04, eta: 14:56:38, time: 1.530, data_time: 0.084, memory: 17624, loss_cls: 0.0893, loss_bbox: 0.2410, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3541, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2455, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2466, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2387, loss: 2.2750, grad_norm: 28.3181
2025-06-11 03:57:17,550 - mmdet - INFO - Epoch [2][2800/7033]	lr: 1.866e-04, eta: 14:55:07, time: 1.607, data_time: 0.081, memory: 17624, loss_cls: 0.0790, loss_bbox: 0.2323, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2405, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2293, loss: 2.1857, grad_norm: 42.8035
2025-06-11 03:58:38,267 - mmdet - INFO - Epoch [2][2850/7033]	lr: 1.866e-04, eta: 14:53:36, time: 1.614, data_time: 0.085, memory: 17624, loss_cls: 0.0915, loss_bbox: 0.2382, d0.loss_cls: 0.1886, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1323, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1107, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0974, d3.loss_bbox: 0.2475, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2371, loss: 2.3308, grad_norm: 45.5573
2025-06-11 04:00:01,698 - mmdet - INFO - Epoch [2][2900/7033]	lr: 1.866e-04, eta: 14:52:15, time: 1.669, data_time: 0.141, memory: 17624, loss_cls: 0.0880, loss_bbox: 0.2337, d0.loss_cls: 0.1836, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2407, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2312, loss: 2.2634, grad_norm: 219.6732
2025-06-11 04:01:29,077 - mmdet - INFO - Epoch [2][2950/7033]	lr: 1.866e-04, eta: 14:51:06, time: 1.748, data_time: 0.145, memory: 17624, loss_cls: 0.0837, loss_bbox: 0.2234, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2582, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2355, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2239, loss: 2.1718, grad_norm: 37.5523
2025-06-11 04:02:53,527 - mmdet - INFO - Epoch [2][3000/7033]	lr: 1.866e-04, eta: 14:49:48, time: 1.689, data_time: 0.058, memory: 17624, loss_cls: 0.0748, loss_bbox: 0.2232, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2536, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2307, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2209, loss: 2.1217, grad_norm: 53.2016
2025-06-11 04:04:18,145 - mmdet - INFO - Epoch [2][3050/7033]	lr: 1.866e-04, eta: 14:48:30, time: 1.692, data_time: 0.080, memory: 17624, loss_cls: 0.0915, loss_bbox: 0.2358, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1055, d2.loss_bbox: 0.2489, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2467, d4.loss_cls: 0.0931, d4.loss_bbox: 0.2349, loss: 2.2947, grad_norm: 85.5677
2025-06-11 04:05:39,732 - mmdet - INFO - Epoch [2][3100/7033]	lr: 1.866e-04, eta: 14:47:03, time: 1.632, data_time: 0.086, memory: 17624, loss_cls: 0.0864, loss_bbox: 0.2418, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2749, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2521, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2532, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2400, loss: 2.3154, grad_norm: 26.8659
2025-06-11 04:06:56,418 - mmdet - INFO - Epoch [2][3150/7033]	lr: 1.866e-04, eta: 14:45:20, time: 1.534, data_time: 0.059, memory: 17624, loss_cls: 0.0885, loss_bbox: 0.2365, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3674, d1.loss_cls: 0.1285, d1.loss_bbox: 0.2767, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2486, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2489, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2331, loss: 2.3041, grad_norm: 56.9533
2025-06-11 04:08:20,270 - mmdet - INFO - Epoch [2][3200/7033]	lr: 1.866e-04, eta: 14:44:00, time: 1.677, data_time: 0.066, memory: 17624, loss_cls: 0.0986, loss_bbox: 0.2428, d0.loss_cls: 0.1956, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1351, d1.loss_bbox: 0.2844, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2545, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2531, d4.loss_cls: 0.0996, d4.loss_bbox: 0.2429, loss: 2.3932, grad_norm: 32.9566
2025-06-11 04:09:44,981 - mmdet - INFO - Epoch [2][3250/7033]	lr: 1.866e-04, eta: 14:42:42, time: 1.694, data_time: 0.076, memory: 17624, loss_cls: 0.0855, loss_bbox: 0.2400, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2505, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2392, loss: 2.2874, grad_norm: 84.3960
2025-06-11 04:11:07,889 - mmdet - INFO - Epoch [2][3300/7033]	lr: 1.866e-04, eta: 14:41:19, time: 1.658, data_time: 0.091, memory: 17624, loss_cls: 0.0829, loss_bbox: 0.2318, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1187, d1.loss_bbox: 0.2643, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2431, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2308, loss: 2.2118, grad_norm: 100.9138
2025-06-11 04:12:32,426 - mmdet - INFO - Epoch [2][3350/7033]	lr: 1.866e-04, eta: 14:40:01, time: 1.691, data_time: 0.061, memory: 17624, loss_cls: 0.0764, loss_bbox: 0.2270, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2598, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2363, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2250, loss: 2.1538, grad_norm: 37.0864
2025-06-11 04:13:56,002 - mmdet - INFO - Epoch [2][3400/7033]	lr: 1.866e-04, eta: 14:38:40, time: 1.672, data_time: 0.096, memory: 17624, loss_cls: 0.0897, loss_bbox: 0.2392, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3672, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2791, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2532, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2496, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2403, loss: 2.3250, grad_norm: 158.5438
2025-06-11 04:15:20,248 - mmdet - INFO - Epoch [2][3450/7033]	lr: 1.866e-04, eta: 14:37:20, time: 1.685, data_time: 0.092, memory: 17624, loss_cls: 0.0893, loss_bbox: 0.2280, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2643, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2406, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2293, loss: 2.2223, grad_norm: 25.2758
2025-06-11 04:16:43,040 - mmdet - INFO - Epoch [2][3500/7033]	lr: 1.866e-04, eta: 14:35:57, time: 1.656, data_time: 0.111, memory: 17624, loss_cls: 0.0837, loss_bbox: 0.2338, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3511, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2648, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2436, d3.loss_cls: 0.0887, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2335, loss: 2.2234, grad_norm: 21.6211
2025-06-11 04:18:06,824 - mmdet - INFO - Epoch [2][3550/7033]	lr: 1.866e-04, eta: 14:34:36, time: 1.676, data_time: 0.103, memory: 17624, loss_cls: 0.0868, loss_bbox: 0.2358, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1208, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2486, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2368, loss: 2.2592, grad_norm: 31.6533
2025-06-11 04:19:31,049 - mmdet - INFO - Epoch [2][3600/7033]	lr: 1.866e-04, eta: 14:33:17, time: 1.684, data_time: 0.122, memory: 17624, loss_cls: 0.0838, loss_bbox: 0.2352, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2436, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2427, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2311, loss: 2.2210, grad_norm: 27.6001
2025-06-11 04:21:00,373 - mmdet - INFO - Epoch [2][3650/7033]	lr: 1.866e-04, eta: 14:32:13, time: 1.786, data_time: 0.155, memory: 17624, loss_cls: 0.0806, loss_bbox: 0.2364, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3571, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2698, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2447, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2438, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2332, loss: 2.2213, grad_norm: 36.1310
2025-06-11 04:22:18,640 - mmdet - INFO - Epoch [2][3700/7033]	lr: 1.866e-04, eta: 14:30:36, time: 1.565, data_time: 0.062, memory: 17624, loss_cls: 0.0926, loss_bbox: 0.2316, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2431, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2317, loss: 2.2740, grad_norm: 61.6448
2025-06-11 04:23:35,287 - mmdet - INFO - Epoch [2][3750/7033]	lr: 1.866e-04, eta: 14:28:54, time: 1.533, data_time: 0.068, memory: 17624, loss_cls: 0.0799, loss_bbox: 0.2302, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2381, d4.loss_cls: 0.0828, d4.loss_bbox: 0.2272, loss: 2.1785, grad_norm: 43.7772
2025-06-11 04:24:55,650 - mmdet - INFO - Epoch [2][3800/7033]	lr: 1.866e-04, eta: 14:27:23, time: 1.606, data_time: 0.064, memory: 17624, loss_cls: 0.0774, loss_bbox: 0.2356, d0.loss_cls: 0.1825, d0.loss_bbox: 0.3601, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2749, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2459, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2347, loss: 2.2332, grad_norm: 108.1070
2025-06-11 04:26:15,138 - mmdet - INFO - Epoch [2][3850/7033]	lr: 1.866e-04, eta: 14:25:50, time: 1.591, data_time: 0.066, memory: 17624, loss_cls: 0.0848, loss_bbox: 0.2414, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2744, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2502, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2519, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2400, loss: 2.2631, grad_norm: 37.2599
2025-06-11 04:27:38,889 - mmdet - INFO - Epoch [2][3900/7033]	lr: 1.866e-04, eta: 14:24:30, time: 1.675, data_time: 0.081, memory: 17624, loss_cls: 0.0817, loss_bbox: 0.2436, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3590, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2707, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2494, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2493, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2412, loss: 2.2644, grad_norm: 27.7086
2025-06-11 04:28:59,128 - mmdet - INFO - Epoch [2][3950/7033]	lr: 1.866e-04, eta: 14:22:59, time: 1.605, data_time: 0.064, memory: 17624, loss_cls: 0.0835, loss_bbox: 0.2346, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2691, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2468, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2462, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2335, loss: 2.2370, grad_norm: 27.7691
2025-06-11 04:30:24,688 - mmdet - INFO - Epoch [2][4000/7033]	lr: 1.866e-04, eta: 14:21:44, time: 1.711, data_time: 0.071, memory: 17624, loss_cls: 0.0873, loss_bbox: 0.2416, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1225, d1.loss_bbox: 0.2776, d2.loss_cls: 0.1053, d2.loss_bbox: 0.2525, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2532, d4.loss_cls: 0.0894, d4.loss_bbox: 0.2412, loss: 2.3178, grad_norm: 26.0151
2025-06-11 04:31:48,131 - mmdet - INFO - Epoch [2][4050/7033]	lr: 1.866e-04, eta: 14:20:22, time: 1.667, data_time: 0.063, memory: 17624, loss_cls: 0.0905, loss_bbox: 0.2409, d0.loss_cls: 0.1942, d0.loss_bbox: 0.3626, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2768, d2.loss_cls: 0.1111, d2.loss_bbox: 0.2552, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2546, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2419, loss: 2.3435, grad_norm: 44.7937
2025-06-11 04:33:13,066 - mmdet - INFO - Epoch [2][4100/7033]	lr: 1.866e-04, eta: 14:19:05, time: 1.700, data_time: 0.082, memory: 17624, loss_cls: 0.0908, loss_bbox: 0.2395, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2864, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2530, d4.loss_cls: 0.0950, d4.loss_bbox: 0.2389, loss: 2.3737, grad_norm: 66.9837
2025-06-11 04:34:36,495 - mmdet - INFO - Epoch [2][4150/7033]	lr: 1.866e-04, eta: 14:17:43, time: 1.669, data_time: 0.068, memory: 17624, loss_cls: 0.0868, loss_bbox: 0.2357, d0.loss_cls: 0.1984, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1226, d1.loss_bbox: 0.2775, d2.loss_cls: 0.1113, d2.loss_bbox: 0.2473, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2351, loss: 2.3088, grad_norm: 25.3775
2025-06-11 04:35:56,286 - mmdet - INFO - Epoch [2][4200/7033]	lr: 1.866e-04, eta: 14:16:11, time: 1.596, data_time: 0.081, memory: 17624, loss_cls: 0.0816, loss_bbox: 0.2390, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3613, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2777, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2519, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2527, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2384, loss: 2.2861, grad_norm: 25.0732
2025-06-11 04:37:20,001 - mmdet - INFO - Epoch [2][4250/7033]	lr: 1.866e-04, eta: 14:14:50, time: 1.674, data_time: 0.058, memory: 17624, loss_cls: 0.0867, loss_bbox: 0.2327, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3568, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2458, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2322, loss: 2.2540, grad_norm: 26.6365
2025-06-11 04:38:38,251 - mmdet - INFO - Epoch [2][4300/7033]	lr: 1.866e-04, eta: 14:13:15, time: 1.565, data_time: 0.062, memory: 17624, loss_cls: 0.0861, loss_bbox: 0.2383, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3617, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2762, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2500, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2487, d4.loss_cls: 0.0885, d4.loss_bbox: 0.2368, loss: 2.2859, grad_norm: 67.3038
2025-06-11 04:40:00,575 - mmdet - INFO - Epoch [2][4350/7033]	lr: 1.866e-04, eta: 14:11:50, time: 1.646, data_time: 0.102, memory: 17624, loss_cls: 0.0860, loss_bbox: 0.2329, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2462, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2450, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2329, loss: 2.2534, grad_norm: 34.6572
2025-06-11 04:41:21,831 - mmdet - INFO - Epoch [2][4400/7033]	lr: 1.866e-04, eta: 14:10:23, time: 1.625, data_time: 0.059, memory: 17624, loss_cls: 0.0784, loss_bbox: 0.2272, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2623, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2354, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2271, loss: 2.1712, grad_norm: 31.4898
2025-06-11 04:42:46,577 - mmdet - INFO - Epoch [2][4450/7033]	lr: 1.866e-04, eta: 14:09:04, time: 1.695, data_time: 0.095, memory: 17624, loss_cls: 0.0855, loss_bbox: 0.2319, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2461, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2445, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2338, loss: 2.2744, grad_norm: 35.5988
2025-06-11 04:44:09,530 - mmdet - INFO - Epoch [2][4500/7033]	lr: 1.866e-04, eta: 14:07:42, time: 1.659, data_time: 0.079, memory: 17624, loss_cls: 0.0820, loss_bbox: 0.2330, d0.loss_cls: 0.1931, d0.loss_bbox: 0.3709, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2715, d2.loss_cls: 0.1064, d2.loss_bbox: 0.2419, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2429, d4.loss_cls: 0.0835, d4.loss_bbox: 0.2324, loss: 2.2690, grad_norm: 37.4757
2025-06-11 04:45:27,895 - mmdet - INFO - Epoch [2][4550/7033]	lr: 1.866e-04, eta: 14:06:07, time: 1.567, data_time: 0.061, memory: 17624, loss_cls: 0.0862, loss_bbox: 0.2322, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3575, d1.loss_cls: 0.1237, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1084, d2.loss_bbox: 0.2464, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2307, loss: 2.2733, grad_norm: 48.3509
2025-06-11 04:46:50,475 - mmdet - INFO - Epoch [2][4600/7033]	lr: 1.866e-04, eta: 14:04:43, time: 1.652, data_time: 0.093, memory: 17624, loss_cls: 0.0935, loss_bbox: 0.2298, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3576, d1.loss_cls: 0.1361, d1.loss_bbox: 0.2644, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2354, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2289, loss: 2.2879, grad_norm: 28.2997
2025-06-11 04:48:07,185 - mmdet - INFO - Epoch [2][4650/7033]	lr: 1.866e-04, eta: 14:03:04, time: 1.534, data_time: 0.065, memory: 17624, loss_cls: 0.0861, loss_bbox: 0.2243, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2591, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2221, loss: 2.2002, grad_norm: 23.3234
2025-06-11 04:49:31,343 - mmdet - INFO - Epoch [2][4700/7033]	lr: 1.866e-04, eta: 14:01:44, time: 1.683, data_time: 0.090, memory: 17624, loss_cls: 0.0799, loss_bbox: 0.2313, d0.loss_cls: 0.1938, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2664, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2313, loss: 2.2243, grad_norm: 85.0348
2025-06-11 04:50:53,511 - mmdet - INFO - Epoch [2][4750/7033]	lr: 1.866e-04, eta: 14:00:19, time: 1.644, data_time: 0.067, memory: 17624, loss_cls: 0.0815, loss_bbox: 0.2254, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3541, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2668, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2383, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2260, loss: 2.1777, grad_norm: 37.7449
2025-06-11 04:52:14,198 - mmdet - INFO - Epoch [2][4800/7033]	lr: 1.866e-04, eta: 13:58:51, time: 1.614, data_time: 0.063, memory: 17624, loss_cls: 0.0861, loss_bbox: 0.2245, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2643, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0864, d4.loss_bbox: 0.2253, loss: 2.1906, grad_norm: 32.5983
2025-06-11 04:53:36,084 - mmdet - INFO - Epoch [2][4850/7033]	lr: 1.866e-04, eta: 13:57:25, time: 1.638, data_time: 0.061, memory: 17624, loss_cls: 0.0757, loss_bbox: 0.2185, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2290, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2185, loss: 2.1067, grad_norm: 41.2448
2025-06-11 04:54:59,193 - mmdet - INFO - Epoch [2][4900/7033]	lr: 1.866e-04, eta: 13:56:03, time: 1.662, data_time: 0.064, memory: 17624, loss_cls: 0.0877, loss_bbox: 0.2351, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3554, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2720, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2456, d3.loss_cls: 0.0936, d3.loss_bbox: 0.2443, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2330, loss: 2.2667, grad_norm: 71.9144
2025-06-11 04:56:20,556 - mmdet - INFO - Epoch [2][4950/7033]	lr: 1.866e-04, eta: 13:54:36, time: 1.627, data_time: 0.064, memory: 17624, loss_cls: 0.0797, loss_bbox: 0.2321, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2683, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2300, loss: 2.2042, grad_norm: 30.2233
2025-06-11 04:57:43,319 - mmdet - INFO - Epoch [2][5000/7033]	lr: 1.866e-04, eta: 13:53:13, time: 1.655, data_time: 0.088, memory: 17624, loss_cls: 0.0815, loss_bbox: 0.2249, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0847, d3.loss_bbox: 0.2322, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2222, loss: 2.1520, grad_norm: 20.4836
2025-06-11 04:59:01,827 - mmdet - INFO - Epoch [2][5050/7033]	lr: 1.866e-04, eta: 13:51:40, time: 1.570, data_time: 0.065, memory: 17624, loss_cls: 0.0880, loss_bbox: 0.2283, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2376, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2260, loss: 2.2483, grad_norm: 35.5535
2025-06-11 05:00:25,240 - mmdet - INFO - Epoch [2][5100/7033]	lr: 1.866e-04, eta: 13:50:18, time: 1.668, data_time: 0.059, memory: 17624, loss_cls: 0.0841, loss_bbox: 0.2258, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2642, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2384, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2387, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2253, loss: 2.1925, grad_norm: 30.9300
2025-06-11 05:01:44,093 - mmdet - INFO - Epoch [2][5150/7033]	lr: 1.866e-04, eta: 13:48:45, time: 1.577, data_time: 0.059, memory: 17624, loss_cls: 0.0844, loss_bbox: 0.2360, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3670, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2771, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2469, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2344, loss: 2.2741, grad_norm: 44.7952
2025-06-11 05:03:01,974 - mmdet - INFO - Epoch [2][5200/7033]	lr: 1.866e-04, eta: 13:47:10, time: 1.558, data_time: 0.083, memory: 17624, loss_cls: 0.0881, loss_bbox: 0.2258, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3456, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2606, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2380, d3.loss_cls: 0.0932, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2250, loss: 2.2185, grad_norm: 24.1804
2025-06-11 05:04:25,965 - mmdet - INFO - Epoch [2][5250/7033]	lr: 1.866e-04, eta: 13:45:51, time: 1.680, data_time: 0.111, memory: 17624, loss_cls: 0.0895, loss_bbox: 0.2263, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2369, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2355, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2273, loss: 2.2459, grad_norm: 33.3774
2025-06-11 05:05:45,404 - mmdet - INFO - Epoch [2][5300/7033]	lr: 1.866e-04, eta: 13:44:20, time: 1.589, data_time: 0.072, memory: 17624, loss_cls: 0.0832, loss_bbox: 0.2387, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2509, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2473, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2405, loss: 2.2621, grad_norm: 29.9161
2025-06-11 05:07:07,214 - mmdet - INFO - Epoch [2][5350/7033]	lr: 1.866e-04, eta: 13:42:54, time: 1.636, data_time: 0.058, memory: 17624, loss_cls: 0.0842, loss_bbox: 0.2233, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2593, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2229, loss: 2.1983, grad_norm: 28.7008
2025-06-11 05:08:29,919 - mmdet - INFO - Epoch [2][5400/7033]	lr: 1.866e-04, eta: 13:41:31, time: 1.653, data_time: 0.063, memory: 17624, loss_cls: 0.0778, loss_bbox: 0.2266, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2636, d2.loss_cls: 0.0982, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2350, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2239, loss: 2.1759, grad_norm: 29.3832
2025-06-11 05:10:00,194 - mmdet - INFO - Epoch [2][5450/7033]	lr: 1.866e-04, eta: 13:40:26, time: 1.807, data_time: 0.071, memory: 17624, loss_cls: 0.0759, loss_bbox: 0.2303, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2674, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2418, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2293, loss: 2.1807, grad_norm: 36.0991
2025-06-11 05:11:23,992 - mmdet - INFO - Epoch [2][5500/7033]	lr: 1.866e-04, eta: 13:39:06, time: 1.674, data_time: 0.079, memory: 17624, loss_cls: 0.0884, loss_bbox: 0.2319, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2424, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2423, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2328, loss: 2.2669, grad_norm: 27.0350
2025-06-11 05:12:48,607 - mmdet - INFO - Epoch [2][5550/7033]	lr: 1.866e-04, eta: 13:37:47, time: 1.694, data_time: 0.109, memory: 17624, loss_cls: 0.0860, loss_bbox: 0.2322, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3545, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2681, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2464, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2434, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2320, loss: 2.2441, grad_norm: 20.6755
2025-06-11 05:14:11,635 - mmdet - INFO - Epoch [2][5600/7033]	lr: 1.866e-04, eta: 13:36:25, time: 1.660, data_time: 0.090, memory: 17624, loss_cls: 0.0823, loss_bbox: 0.2249, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3417, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2375, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2355, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2242, loss: 2.1656, grad_norm: 37.2277
2025-06-11 05:15:32,262 - mmdet - INFO - Epoch [2][5650/7033]	lr: 1.866e-04, eta: 13:34:57, time: 1.612, data_time: 0.079, memory: 17624, loss_cls: 0.0842, loss_bbox: 0.2363, d0.loss_cls: 0.1854, d0.loss_bbox: 0.3613, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2722, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2474, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2457, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2377, loss: 2.2665, grad_norm: 32.0694
2025-06-11 05:16:53,455 - mmdet - INFO - Epoch [2][5700/7033]	lr: 1.866e-04, eta: 13:33:30, time: 1.624, data_time: 0.069, memory: 17624, loss_cls: 0.0718, loss_bbox: 0.2192, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2285, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2192, loss: 2.0582, grad_norm: 23.6510
2025-06-11 05:18:22,568 - mmdet - INFO - Epoch [2][5750/7033]	lr: 1.866e-04, eta: 13:32:22, time: 1.782, data_time: 0.069, memory: 17624, loss_cls: 0.0868, loss_bbox: 0.2273, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3499, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2612, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2377, d3.loss_cls: 0.0922, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2274, loss: 2.2274, grad_norm: 39.3850
2025-06-11 05:19:47,541 - mmdet - INFO - Epoch [2][5800/7033]	lr: 1.866e-04, eta: 13:31:04, time: 1.699, data_time: 0.124, memory: 17624, loss_cls: 0.0769, loss_bbox: 0.2255, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2545, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2331, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2215, loss: 2.1123, grad_norm: 29.5113
2025-06-11 05:21:09,351 - mmdet - INFO - Epoch [2][5850/7033]	lr: 1.866e-04, eta: 13:29:38, time: 1.636, data_time: 0.084, memory: 17624, loss_cls: 0.0854, loss_bbox: 0.2300, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2587, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2369, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2270, loss: 2.1823, grad_norm: 384.1364
2025-06-11 05:22:27,448 - mmdet - INFO - Epoch [2][5900/7033]	lr: 1.866e-04, eta: 13:28:05, time: 1.562, data_time: 0.064, memory: 17624, loss_cls: 0.0815, loss_bbox: 0.2251, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3529, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2587, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2226, loss: 2.1869, grad_norm: 24.6646
2025-06-11 05:23:54,555 - mmdet - INFO - Epoch [2][5950/7033]	lr: 1.866e-04, eta: 13:26:52, time: 1.742, data_time: 0.125, memory: 17624, loss_cls: 0.0856, loss_bbox: 0.2312, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3623, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2475, d3.loss_cls: 0.0890, d3.loss_bbox: 0.2429, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2326, loss: 2.2627, grad_norm: 33.4791
2025-06-11 05:25:14,245 - mmdet - INFO - Epoch [2][6000/7033]	lr: 1.866e-04, eta: 13:25:22, time: 1.594, data_time: 0.061, memory: 17624, loss_cls: 0.0742, loss_bbox: 0.2228, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2354, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2237, loss: 2.1074, grad_norm: 108.9875
2025-06-11 05:26:43,326 - mmdet - INFO - Epoch [2][6050/7033]	lr: 1.866e-04, eta: 13:24:13, time: 1.782, data_time: 0.138, memory: 17624, loss_cls: 0.0824, loss_bbox: 0.2349, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3542, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2751, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2473, d3.loss_cls: 0.0890, d3.loss_bbox: 0.2458, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2345, loss: 2.2389, grad_norm: 65.5810
2025-06-11 05:28:06,449 - mmdet - INFO - Epoch [2][6100/7033]	lr: 1.866e-04, eta: 13:22:50, time: 1.661, data_time: 0.106, memory: 17624, loss_cls: 0.0792, loss_bbox: 0.2207, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2585, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2206, loss: 2.1458, grad_norm: 27.9875
2025-06-11 05:29:28,273 - mmdet - INFO - Epoch [2][6150/7033]	lr: 1.866e-04, eta: 13:21:25, time: 1.638, data_time: 0.070, memory: 17624, loss_cls: 0.0815, loss_bbox: 0.2252, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2630, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2389, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2259, loss: 2.1816, grad_norm: 157.2554
2025-06-11 05:30:48,855 - mmdet - INFO - Epoch [2][6200/7033]	lr: 1.866e-04, eta: 13:19:57, time: 1.611, data_time: 0.059, memory: 17624, loss_cls: 0.0778, loss_bbox: 0.2254, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3477, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2673, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2411, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2392, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2277, loss: 2.1716, grad_norm: 33.6264
2025-06-11 05:32:08,306 - mmdet - INFO - Epoch [2][6250/7033]	lr: 1.866e-04, eta: 13:18:27, time: 1.589, data_time: 0.061, memory: 17624, loss_cls: 0.0838, loss_bbox: 0.2265, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2674, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2273, loss: 2.2185, grad_norm: 40.1697
2025-06-11 05:33:27,861 - mmdet - INFO - Epoch [2][6300/7033]	lr: 1.866e-04, eta: 13:16:57, time: 1.591, data_time: 0.072, memory: 17624, loss_cls: 0.0828, loss_bbox: 0.2281, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3517, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2647, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2397, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2378, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2276, loss: 2.1922, grad_norm: 29.2771
2025-06-11 05:34:50,476 - mmdet - INFO - Epoch [2][6350/7033]	lr: 1.866e-04, eta: 13:15:34, time: 1.652, data_time: 0.077, memory: 17624, loss_cls: 0.0879, loss_bbox: 0.2298, d0.loss_cls: 0.1858, d0.loss_bbox: 0.3630, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2730, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2429, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2310, loss: 2.2710, grad_norm: 32.5974
2025-06-11 05:36:13,386 - mmdet - INFO - Epoch [2][6400/7033]	lr: 1.866e-04, eta: 13:14:11, time: 1.658, data_time: 0.080, memory: 17624, loss_cls: 0.0837, loss_bbox: 0.2236, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2579, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2323, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2245, loss: 2.1838, grad_norm: 24.7401
2025-06-11 05:37:34,410 - mmdet - INFO - Epoch [2][6450/7033]	lr: 1.866e-04, eta: 13:12:44, time: 1.621, data_time: 0.080, memory: 17624, loss_cls: 0.0789, loss_bbox: 0.2285, d0.loss_cls: 0.1733, d0.loss_bbox: 0.3557, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0957, d2.loss_bbox: 0.2392, d3.loss_cls: 0.0830, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2266, loss: 2.1774, grad_norm: 134.9571
2025-06-11 05:38:56,651 - mmdet - INFO - Epoch [2][6500/7033]	lr: 1.866e-04, eta: 13:11:20, time: 1.645, data_time: 0.076, memory: 17624, loss_cls: 0.0778, loss_bbox: 0.2245, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1162, d1.loss_bbox: 0.2573, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2331, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2236, loss: 2.1545, grad_norm: 29.3854
2025-06-11 05:40:16,614 - mmdet - INFO - Epoch [2][6550/7033]	lr: 1.866e-04, eta: 13:09:52, time: 1.599, data_time: 0.077, memory: 17624, loss_cls: 0.0724, loss_bbox: 0.2183, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2525, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2192, loss: 2.0907, grad_norm: 186.0663
2025-06-11 05:41:36,102 - mmdet - INFO - Epoch [2][6600/7033]	lr: 1.866e-04, eta: 13:08:22, time: 1.590, data_time: 0.069, memory: 17624, loss_cls: 0.0780, loss_bbox: 0.2211, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2312, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2208, loss: 2.1285, grad_norm: 20.4722
2025-06-11 05:42:58,349 - mmdet - INFO - Epoch [2][6650/7033]	lr: 1.866e-04, eta: 13:06:58, time: 1.643, data_time: 0.060, memory: 17624, loss_cls: 0.0855, loss_bbox: 0.2262, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3548, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2623, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2289, loss: 2.2112, grad_norm: 21.2737
2025-06-11 05:44:20,000 - mmdet - INFO - Epoch [2][6700/7033]	lr: 1.866e-04, eta: 13:05:33, time: 1.635, data_time: 0.063, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2296, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3503, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2645, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2292, loss: 2.1698, grad_norm: 30.7202
2025-06-11 05:45:42,891 - mmdet - INFO - Epoch [2][6750/7033]	lr: 1.866e-04, eta: 13:04:10, time: 1.658, data_time: 0.062, memory: 17624, loss_cls: 0.0782, loss_bbox: 0.2274, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3491, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2656, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2378, d3.loss_cls: 0.0851, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2297, loss: 2.1834, grad_norm: 29.5086
2025-06-11 05:47:04,604 - mmdet - INFO - Epoch [2][6800/7033]	lr: 1.866e-04, eta: 13:02:45, time: 1.634, data_time: 0.065, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2241, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2622, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2344, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2243, loss: 2.1618, grad_norm: 36.4949
2025-06-11 05:48:25,511 - mmdet - INFO - Epoch [2][6850/7033]	lr: 1.866e-04, eta: 13:01:18, time: 1.618, data_time: 0.067, memory: 17624, loss_cls: 0.0857, loss_bbox: 0.2294, d0.loss_cls: 0.1866, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2687, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2392, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2295, loss: 2.2594, grad_norm: 24.6208
2025-06-11 05:49:47,999 - mmdet - INFO - Epoch [2][6900/7033]	lr: 1.866e-04, eta: 12:59:55, time: 1.650, data_time: 0.071, memory: 17624, loss_cls: 0.0833, loss_bbox: 0.2390, d0.loss_cls: 0.1889, d0.loss_bbox: 0.3654, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2480, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2516, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2409, loss: 2.2946, grad_norm: 59.3466
2025-06-11 05:51:07,965 - mmdet - INFO - Epoch [2][6950/7033]	lr: 1.866e-04, eta: 12:58:27, time: 1.599, data_time: 0.058, memory: 17624, loss_cls: 0.0785, loss_bbox: 0.2270, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3499, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2609, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2390, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2318, loss: 2.1988, grad_norm: 35.0250
2025-06-11 05:52:29,591 - mmdet - INFO - Epoch [2][7000/7033]	lr: 1.866e-04, eta: 12:57:02, time: 1.633, data_time: 0.062, memory: 17624, loss_cls: 0.0831, loss_bbox: 0.2279, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2655, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2422, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2420, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2297, loss: 2.2221, grad_norm: 30.8848
2025-06-11 05:53:22,213 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-11 06:31:09,467 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 06:31:09,467 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7813, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8830, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9088, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9225, pts_bbox_NuScenes/car_trans_err: 0.2109, pts_bbox_NuScenes/car_scale_err: 0.1537, pts_bbox_NuScenes/car_orient_err: 0.0487, pts_bbox_NuScenes/car_vel_err: 0.3262, pts_bbox_NuScenes/car_attr_err: 0.1620, pts_bbox_NuScenes/mATE: 0.3099, pts_bbox_NuScenes/mASE: 0.2678, pts_bbox_NuScenes/mAOE: 0.2631, pts_bbox_NuScenes/mAVE: 0.3413, pts_bbox_NuScenes/mAAE: 0.1750, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4257, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6312, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7277, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7649, pts_bbox_NuScenes/truck_trans_err: 0.3439, pts_bbox_NuScenes/truck_scale_err: 0.2068, pts_bbox_NuScenes/truck_orient_err: 0.0541, pts_bbox_NuScenes/truck_vel_err: 0.2992, pts_bbox_NuScenes/truck_attr_err: 0.2047, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0466, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2019, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3953, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4671, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6760, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4387, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7853, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1088, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2896, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4799, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7284, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8896, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9183, pts_bbox_NuScenes/bus_trans_err: 0.3683, pts_bbox_NuScenes/bus_scale_err: 0.1927, pts_bbox_NuScenes/bus_orient_err: 0.0350, pts_bbox_NuScenes/bus_vel_err: 0.5778, pts_bbox_NuScenes/bus_attr_err: 0.2484, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1462, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3991, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5881, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6762, pts_bbox_NuScenes/trailer_trans_err: 0.5236, pts_bbox_NuScenes/trailer_scale_err: 0.2444, pts_bbox_NuScenes/trailer_orient_err: 0.4685, pts_bbox_NuScenes/trailer_vel_err: 0.4570, pts_bbox_NuScenes/trailer_attr_err: 0.1412, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6015, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7021, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7550, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7725, pts_bbox_NuScenes/barrier_trans_err: 0.2335, pts_bbox_NuScenes/barrier_scale_err: 0.2919, pts_bbox_NuScenes/barrier_orient_err: 0.0556, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5891, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7571, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7993, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8082, pts_bbox_NuScenes/motorcycle_trans_err: 0.2520, pts_bbox_NuScenes/motorcycle_scale_err: 0.2486, pts_bbox_NuScenes/motorcycle_orient_err: 0.2296, pts_bbox_NuScenes/motorcycle_vel_err: 0.5027, pts_bbox_NuScenes/motorcycle_attr_err: 0.2334, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5592, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6137, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6256, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6347, pts_bbox_NuScenes/bicycle_trans_err: 0.1790, pts_bbox_NuScenes/bicycle_scale_err: 0.2718, pts_bbox_NuScenes/bicycle_orient_err: 0.3664, pts_bbox_NuScenes/bicycle_vel_err: 0.2303, pts_bbox_NuScenes/bicycle_attr_err: 0.0071, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8110, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8553, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8789, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8924, pts_bbox_NuScenes/pedestrian_trans_err: 0.1566, pts_bbox_NuScenes/pedestrian_scale_err: 0.2929, pts_bbox_NuScenes/pedestrian_orient_err: 0.3249, pts_bbox_NuScenes/pedestrian_vel_err: 0.2282, pts_bbox_NuScenes/pedestrian_attr_err: 0.1138, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6975, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7478, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7766, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8005, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1557, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3361, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6975, pts_bbox_NuScenes/mAP: 0.6665
2025-06-11 06:33:08,549 - mmdet - INFO - Epoch [3][50/7033]	lr: 1.501e-04, eta: 12:54:01, time: 2.310, data_time: 0.314, memory: 17624, loss_cls: 0.0834, loss_bbox: 0.2240, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1168, d1.loss_bbox: 0.2610, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2392, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2370, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2253, loss: 2.1879, grad_norm: 48.3098
2025-06-11 06:34:29,074 - mmdet - INFO - Epoch [3][100/7033]	lr: 1.501e-04, eta: 12:52:34, time: 1.611, data_time: 0.069, memory: 17624, loss_cls: 0.0757, loss_bbox: 0.2231, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3407, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2246, loss: 2.1419, grad_norm: 70.2943
2025-06-11 06:35:46,890 - mmdet - INFO - Epoch [3][150/7033]	lr: 1.501e-04, eta: 12:51:01, time: 1.556, data_time: 0.063, memory: 17624, loss_cls: 0.0721, loss_bbox: 0.2109, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2114, loss: 2.0339, grad_norm: 446.6677
2025-06-11 06:37:10,492 - mmdet - INFO - Epoch [3][200/7033]	lr: 1.501e-04, eta: 12:49:41, time: 1.672, data_time: 0.057, memory: 17624, loss_cls: 0.0782, loss_bbox: 0.2262, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3505, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2389, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2276, loss: 2.1587, grad_norm: 22.0196
2025-06-11 06:38:34,561 - mmdet - INFO - Epoch [3][250/7033]	lr: 1.501e-04, eta: 12:48:21, time: 1.681, data_time: 0.054, memory: 17624, loss_cls: 0.0774, loss_bbox: 0.2139, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2157, loss: 2.0667, grad_norm: 33.4465
2025-06-11 06:39:56,522 - mmdet - INFO - Epoch [3][300/7033]	lr: 1.501e-04, eta: 12:46:56, time: 1.639, data_time: 0.052, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2198, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2192, loss: 2.1100, grad_norm: 31.4321
2025-06-11 06:41:19,314 - mmdet - INFO - Epoch [3][350/7033]	lr: 1.501e-04, eta: 12:45:34, time: 1.655, data_time: 0.058, memory: 17624, loss_cls: 0.0747, loss_bbox: 0.2262, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2270, loss: 2.1538, grad_norm: 42.7921
2025-06-11 06:42:39,858 - mmdet - INFO - Epoch [3][400/7033]	lr: 1.501e-04, eta: 12:44:07, time: 1.611, data_time: 0.064, memory: 17624, loss_cls: 0.0812, loss_bbox: 0.2172, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2169, loss: 2.1231, grad_norm: 28.7037
2025-06-11 06:44:00,650 - mmdet - INFO - Epoch [3][450/7033]	lr: 1.501e-04, eta: 12:42:41, time: 1.616, data_time: 0.055, memory: 17624, loss_cls: 0.0777, loss_bbox: 0.2208, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3333, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2231, loss: 2.1095, grad_norm: 39.7157
2025-06-11 06:45:24,225 - mmdet - INFO - Epoch [3][500/7033]	lr: 1.501e-04, eta: 12:41:20, time: 1.671, data_time: 0.054, memory: 17624, loss_cls: 0.0817, loss_bbox: 0.2181, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2178, loss: 2.1282, grad_norm: 64.7419
2025-06-11 06:46:50,254 - mmdet - INFO - Epoch [3][550/7033]	lr: 1.501e-04, eta: 12:40:04, time: 1.720, data_time: 0.069, memory: 17624, loss_cls: 0.0690, loss_bbox: 0.2181, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2545, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2194, loss: 2.0849, grad_norm: 53.9716
2025-06-11 06:48:13,957 - mmdet - INFO - Epoch [3][600/7033]	lr: 1.501e-04, eta: 12:38:43, time: 1.674, data_time: 0.067, memory: 17624, loss_cls: 0.0842, loss_bbox: 0.2275, d0.loss_cls: 0.1825, d0.loss_bbox: 0.3440, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2285, loss: 2.1980, grad_norm: 34.8844
2025-06-11 06:49:33,120 - mmdet - INFO - Epoch [3][650/7033]	lr: 1.501e-04, eta: 12:37:14, time: 1.584, data_time: 0.061, memory: 17624, loss_cls: 0.0718, loss_bbox: 0.2122, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2512, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2133, loss: 2.0598, grad_norm: 32.1168
2025-06-11 06:50:54,193 - mmdet - INFO - Epoch [3][700/7033]	lr: 1.501e-04, eta: 12:35:48, time: 1.621, data_time: 0.062, memory: 17624, loss_cls: 0.0881, loss_bbox: 0.2269, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3458, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2634, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2346, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2255, loss: 2.2172, grad_norm: 30.1204
2025-06-11 06:52:13,858 - mmdet - INFO - Epoch [3][750/7033]	lr: 1.501e-04, eta: 12:34:20, time: 1.594, data_time: 0.067, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2205, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2230, loss: 2.1477, grad_norm: 351.8831
2025-06-11 06:53:36,345 - mmdet - INFO - Epoch [3][800/7033]	lr: 1.501e-04, eta: 12:32:57, time: 1.650, data_time: 0.088, memory: 17624, loss_cls: 0.0810, loss_bbox: 0.2190, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3433, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2198, loss: 2.1540, grad_norm: 42.8846
2025-06-11 06:54:57,505 - mmdet - INFO - Epoch [3][850/7033]	lr: 1.501e-04, eta: 12:31:32, time: 1.623, data_time: 0.073, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2199, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3397, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2578, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2341, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2224, loss: 2.1106, grad_norm: 77.5230
2025-06-11 06:56:21,273 - mmdet - INFO - Epoch [3][900/7033]	lr: 1.501e-04, eta: 12:30:11, time: 1.675, data_time: 0.064, memory: 17624, loss_cls: 0.0778, loss_bbox: 0.2279, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2628, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2392, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2276, loss: 2.1860, grad_norm: 27.9743
2025-06-11 06:57:43,856 - mmdet - INFO - Epoch [3][950/7033]	lr: 1.501e-04, eta: 12:28:48, time: 1.652, data_time: 0.060, memory: 17624, loss_cls: 0.0747, loss_bbox: 0.2145, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2275, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2167, loss: 2.0838, grad_norm: 22.4992
2025-06-11 06:59:09,247 - mmdet - INFO - Epoch [3][1000/7033]	lr: 1.501e-04, eta: 12:27:31, time: 1.706, data_time: 0.069, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2161, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2166, loss: 2.0723, grad_norm: 33.4233
2025-06-11 07:00:25,946 - mmdet - INFO - Epoch [3][1050/7033]	lr: 1.501e-04, eta: 12:25:57, time: 1.535, data_time: 0.067, memory: 17624, loss_cls: 0.0706, loss_bbox: 0.2146, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3263, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2490, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2167, loss: 2.0372, grad_norm: 79.6724
2025-06-11 07:01:46,393 - mmdet - INFO - Epoch [3][1100/7033]	lr: 1.501e-04, eta: 12:24:31, time: 1.609, data_time: 0.061, memory: 17624, loss_cls: 0.0739, loss_bbox: 0.2110, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3346, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2493, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2234, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2138, loss: 2.0555, grad_norm: 19.5373
2025-06-11 07:03:05,022 - mmdet - INFO - Epoch [3][1150/7033]	lr: 1.501e-04, eta: 12:23:01, time: 1.573, data_time: 0.063, memory: 17624, loss_cls: 0.0795, loss_bbox: 0.2322, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2684, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2444, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2439, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2331, loss: 2.2010, grad_norm: 37.9714
2025-06-11 07:04:26,694 - mmdet - INFO - Epoch [3][1200/7033]	lr: 1.501e-04, eta: 12:21:37, time: 1.633, data_time: 0.068, memory: 17624, loss_cls: 0.0719, loss_bbox: 0.2204, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3397, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2594, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2214, loss: 2.1084, grad_norm: 38.5054
2025-06-11 07:05:47,664 - mmdet - INFO - Epoch [3][1250/7033]	lr: 1.501e-04, eta: 12:20:11, time: 1.619, data_time: 0.074, memory: 17624, loss_cls: 0.0806, loss_bbox: 0.2273, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3499, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2668, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2438, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2282, loss: 2.1896, grad_norm: 25.2772
2025-06-11 07:07:11,107 - mmdet - INFO - Epoch [3][1300/7033]	lr: 1.501e-04, eta: 12:18:50, time: 1.669, data_time: 0.072, memory: 17624, loss_cls: 0.0771, loss_bbox: 0.2192, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2194, loss: 2.1212, grad_norm: 22.5565
2025-06-11 07:08:33,905 - mmdet - INFO - Epoch [3][1350/7033]	lr: 1.501e-04, eta: 12:17:28, time: 1.656, data_time: 0.062, memory: 17624, loss_cls: 0.0810, loss_bbox: 0.2186, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2175, loss: 2.1165, grad_norm: 72.0747
2025-06-11 07:09:52,104 - mmdet - INFO - Epoch [3][1400/7033]	lr: 1.501e-04, eta: 12:15:58, time: 1.564, data_time: 0.063, memory: 17624, loss_cls: 0.0766, loss_bbox: 0.2188, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3413, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2538, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2196, loss: 2.1081, grad_norm: 687.2629
2025-06-11 07:11:09,720 - mmdet - INFO - Epoch [3][1450/7033]	lr: 1.501e-04, eta: 12:14:27, time: 1.552, data_time: 0.066, memory: 17624, loss_cls: 0.0838, loss_bbox: 0.2233, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2630, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2250, loss: 2.1832, grad_norm: 39.7666
2025-06-11 07:12:30,054 - mmdet - INFO - Epoch [3][1500/7033]	lr: 1.501e-04, eta: 12:13:00, time: 1.606, data_time: 0.064, memory: 17624, loss_cls: 0.0822, loss_bbox: 0.2193, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0835, d4.loss_bbox: 0.2230, loss: 2.1641, grad_norm: 89.9261
2025-06-11 07:13:51,055 - mmdet - INFO - Epoch [3][1550/7033]	lr: 1.501e-04, eta: 12:11:35, time: 1.621, data_time: 0.061, memory: 17624, loss_cls: 0.0760, loss_bbox: 0.2201, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3475, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2223, loss: 2.1352, grad_norm: 23.2706
2025-06-11 07:15:12,283 - mmdet - INFO - Epoch [3][1600/7033]	lr: 1.501e-04, eta: 12:10:10, time: 1.625, data_time: 0.053, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2237, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3390, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2593, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2380, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2244, loss: 2.1278, grad_norm: 30.1231
2025-06-11 07:16:34,224 - mmdet - INFO - Epoch [3][1650/7033]	lr: 1.501e-04, eta: 12:08:46, time: 1.639, data_time: 0.077, memory: 17624, loss_cls: 0.0751, loss_bbox: 0.2197, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2609, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2342, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2223, loss: 2.1334, grad_norm: 27.6894
2025-06-11 07:17:58,354 - mmdet - INFO - Epoch [3][1700/7033]	lr: 1.501e-04, eta: 12:07:27, time: 1.683, data_time: 0.068, memory: 17624, loss_cls: 0.0714, loss_bbox: 0.2088, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3290, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2098, loss: 2.0182, grad_norm: 56.5527
2025-06-11 07:19:20,692 - mmdet - INFO - Epoch [3][1750/7033]	lr: 1.501e-04, eta: 12:06:04, time: 1.647, data_time: 0.065, memory: 17624, loss_cls: 0.0724, loss_bbox: 0.2168, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2283, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2178, loss: 2.0713, grad_norm: 27.6942
2025-06-11 07:20:38,847 - mmdet - INFO - Epoch [3][1800/7033]	lr: 1.501e-04, eta: 12:04:34, time: 1.563, data_time: 0.067, memory: 17624, loss_cls: 0.0759, loss_bbox: 0.2191, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3360, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2576, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2194, loss: 2.1048, grad_norm: 30.9500
2025-06-11 07:22:03,900 - mmdet - INFO - Epoch [3][1850/7033]	lr: 1.501e-04, eta: 12:03:15, time: 1.701, data_time: 0.148, memory: 17624, loss_cls: 0.0779, loss_bbox: 0.2152, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2166, loss: 2.1002, grad_norm: 58.3865
2025-06-11 07:23:28,742 - mmdet - INFO - Epoch [3][1900/7033]	lr: 1.501e-04, eta: 12:01:56, time: 1.697, data_time: 0.076, memory: 17624, loss_cls: 0.0816, loss_bbox: 0.2213, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3428, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2607, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2372, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2335, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2241, loss: 2.1616, grad_norm: 43.9266
2025-06-11 07:24:46,613 - mmdet - INFO - Epoch [3][1950/7033]	lr: 1.501e-04, eta: 12:00:26, time: 1.557, data_time: 0.056, memory: 17624, loss_cls: 0.0767, loss_bbox: 0.2212, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2575, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2335, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2231, loss: 2.1356, grad_norm: 47.3388
2025-06-11 07:26:05,943 - mmdet - INFO - Epoch [3][2000/7033]	lr: 1.501e-04, eta: 11:58:58, time: 1.584, data_time: 0.066, memory: 17624, loss_cls: 0.0758, loss_bbox: 0.2153, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3370, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2512, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2172, loss: 2.0938, grad_norm: 222.8331
2025-06-11 07:27:38,679 - mmdet - INFO - Epoch [3][2050/7033]	lr: 1.501e-04, eta: 11:57:52, time: 1.857, data_time: 0.306, memory: 17624, loss_cls: 0.0725, loss_bbox: 0.2167, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3390, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0785, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2163, loss: 2.0883, grad_norm: 62.0627
2025-06-11 07:28:59,405 - mmdet - INFO - Epoch [3][2100/7033]	lr: 1.501e-04, eta: 11:56:27, time: 1.614, data_time: 0.086, memory: 17624, loss_cls: 0.0793, loss_bbox: 0.2274, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1140, d1.loss_bbox: 0.2688, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2431, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2415, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2284, loss: 2.2069, grad_norm: 19.8614
2025-06-11 07:30:17,766 - mmdet - INFO - Epoch [3][2150/7033]	lr: 1.501e-04, eta: 11:54:58, time: 1.567, data_time: 0.087, memory: 17624, loss_cls: 0.0720, loss_bbox: 0.2147, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2169, loss: 2.0557, grad_norm: 109.7884
2025-06-11 07:31:40,532 - mmdet - INFO - Epoch [3][2200/7033]	lr: 1.501e-04, eta: 11:53:35, time: 1.655, data_time: 0.070, memory: 17624, loss_cls: 0.0789, loss_bbox: 0.2142, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3355, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2155, loss: 2.0925, grad_norm: 18.1108
2025-06-11 07:33:01,380 - mmdet - INFO - Epoch [3][2250/7033]	lr: 1.501e-04, eta: 11:52:10, time: 1.617, data_time: 0.083, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2143, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2157, loss: 2.0851, grad_norm: 59.6325
2025-06-11 07:34:25,236 - mmdet - INFO - Epoch [3][2300/7033]	lr: 1.501e-04, eta: 11:50:50, time: 1.677, data_time: 0.087, memory: 17624, loss_cls: 0.0829, loss_bbox: 0.2206, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3423, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2597, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2220, loss: 2.1699, grad_norm: 32.0159
2025-06-11 07:35:48,911 - mmdet - INFO - Epoch [3][2350/7033]	lr: 1.501e-04, eta: 11:49:29, time: 1.673, data_time: 0.114, memory: 17624, loss_cls: 0.0839, loss_bbox: 0.2333, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3528, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2674, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2426, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2331, loss: 2.2293, grad_norm: 43.0913
2025-06-11 07:37:11,190 - mmdet - INFO - Epoch [3][2400/7033]	lr: 1.501e-04, eta: 11:48:06, time: 1.646, data_time: 0.092, memory: 17624, loss_cls: 0.0776, loss_bbox: 0.2257, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2376, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2277, loss: 2.1731, grad_norm: 131.8050
2025-06-11 07:38:34,370 - mmdet - INFO - Epoch [3][2450/7033]	lr: 1.501e-04, eta: 11:46:44, time: 1.664, data_time: 0.083, memory: 17624, loss_cls: 0.0740, loss_bbox: 0.2138, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3372, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2137, loss: 2.0831, grad_norm: 28.1423
2025-06-11 07:40:01,501 - mmdet - INFO - Epoch [3][2500/7033]	lr: 1.501e-04, eta: 11:45:29, time: 1.742, data_time: 0.087, memory: 17624, loss_cls: 0.0716, loss_bbox: 0.2160, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3405, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2527, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2162, loss: 2.0711, grad_norm: 99.1100
2025-06-11 07:41:20,110 - mmdet - INFO - Epoch [3][2550/7033]	lr: 1.501e-04, eta: 11:44:00, time: 1.572, data_time: 0.079, memory: 17624, loss_cls: 0.0753, loss_bbox: 0.2198, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2540, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2220, loss: 2.1078, grad_norm: 39.8744
2025-06-11 07:42:37,842 - mmdet - INFO - Epoch [3][2600/7033]	lr: 1.501e-04, eta: 11:42:30, time: 1.555, data_time: 0.096, memory: 17624, loss_cls: 0.0810, loss_bbox: 0.2203, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2319, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2212, loss: 2.1414, grad_norm: 27.6341
2025-06-11 07:44:01,266 - mmdet - INFO - Epoch [3][2650/7033]	lr: 1.501e-04, eta: 11:41:09, time: 1.667, data_time: 0.058, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2177, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1162, d1.loss_bbox: 0.2543, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2209, loss: 2.1219, grad_norm: 36.2219
2025-06-11 07:45:22,575 - mmdet - INFO - Epoch [3][2700/7033]	lr: 1.501e-04, eta: 11:39:44, time: 1.628, data_time: 0.055, memory: 17624, loss_cls: 0.0827, loss_bbox: 0.2259, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2627, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2377, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2289, loss: 2.2042, grad_norm: 46.5046
2025-06-11 07:46:44,626 - mmdet - INFO - Epoch [3][2750/7033]	lr: 1.501e-04, eta: 11:38:21, time: 1.641, data_time: 0.067, memory: 17624, loss_cls: 0.0763, loss_bbox: 0.2163, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2245, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2166, loss: 2.0914, grad_norm: 31.7116
2025-06-11 07:48:05,805 - mmdet - INFO - Epoch [3][2800/7033]	lr: 1.501e-04, eta: 11:36:57, time: 1.623, data_time: 0.063, memory: 17624, loss_cls: 0.0797, loss_bbox: 0.2228, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3481, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2603, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2242, loss: 2.1673, grad_norm: 34.1303
2025-06-11 07:49:29,175 - mmdet - INFO - Epoch [3][2850/7033]	lr: 1.501e-04, eta: 11:35:35, time: 1.667, data_time: 0.059, memory: 17624, loss_cls: 0.0810, loss_bbox: 0.2262, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2405, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2286, loss: 2.2004, grad_norm: 52.0291
2025-06-11 07:50:49,992 - mmdet - INFO - Epoch [3][2900/7033]	lr: 1.501e-04, eta: 11:34:10, time: 1.617, data_time: 0.070, memory: 17624, loss_cls: 0.0773, loss_bbox: 0.2191, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3272, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2496, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0847, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2190, loss: 2.1022, grad_norm: 22.0803
2025-06-11 07:52:13,566 - mmdet - INFO - Epoch [3][2950/7033]	lr: 1.501e-04, eta: 11:32:49, time: 1.671, data_time: 0.078, memory: 17624, loss_cls: 0.0697, loss_bbox: 0.2124, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3315, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2134, loss: 2.0519, grad_norm: 18.4008
2025-06-11 07:53:37,792 - mmdet - INFO - Epoch [3][3000/7033]	lr: 1.501e-04, eta: 11:31:29, time: 1.685, data_time: 0.083, memory: 17624, loss_cls: 0.0740, loss_bbox: 0.2157, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2171, loss: 2.1044, grad_norm: 35.4126
2025-06-11 07:54:59,706 - mmdet - INFO - Epoch [3][3050/7033]	lr: 1.501e-04, eta: 11:30:06, time: 1.638, data_time: 0.105, memory: 17624, loss_cls: 0.0790, loss_bbox: 0.2192, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2215, loss: 2.1251, grad_norm: 31.1892
2025-06-11 07:56:25,014 - mmdet - INFO - Epoch [3][3100/7033]	lr: 1.501e-04, eta: 11:28:47, time: 1.706, data_time: 0.073, memory: 17624, loss_cls: 0.0757, loss_bbox: 0.2183, d0.loss_cls: 0.1842, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2543, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2200, loss: 2.1431, grad_norm: 38.6531
2025-06-11 07:57:50,637 - mmdet - INFO - Epoch [3][3150/7033]	lr: 1.501e-04, eta: 11:27:29, time: 1.712, data_time: 0.082, memory: 17624, loss_cls: 0.0757, loss_bbox: 0.2112, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3431, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2230, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2122, loss: 2.0798, grad_norm: 39.5876
2025-06-11 07:59:13,796 - mmdet - INFO - Epoch [3][3200/7033]	lr: 1.501e-04, eta: 11:26:07, time: 1.664, data_time: 0.092, memory: 17624, loss_cls: 0.0794, loss_bbox: 0.2193, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2321, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2205, loss: 2.1525, grad_norm: 103.9104
2025-06-11 08:00:34,879 - mmdet - INFO - Epoch [3][3250/7033]	lr: 1.501e-04, eta: 11:24:43, time: 1.622, data_time: 0.083, memory: 17624, loss_cls: 0.0756, loss_bbox: 0.2144, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2281, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2173, loss: 2.0856, grad_norm: 52.0288
2025-06-11 08:01:54,591 - mmdet - INFO - Epoch [3][3300/7033]	lr: 1.501e-04, eta: 11:23:16, time: 1.594, data_time: 0.061, memory: 17624, loss_cls: 0.0788, loss_bbox: 0.2243, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3527, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2561, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2247, loss: 2.1689, grad_norm: 76.6033
2025-06-11 08:03:16,903 - mmdet - INFO - Epoch [3][3350/7033]	lr: 1.501e-04, eta: 11:21:53, time: 1.645, data_time: 0.081, memory: 17624, loss_cls: 0.0730, loss_bbox: 0.2125, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3407, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2528, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2154, loss: 2.0731, grad_norm: 27.7265
2025-06-11 08:04:41,573 - mmdet - INFO - Epoch [3][3400/7033]	lr: 1.501e-04, eta: 11:20:34, time: 1.694, data_time: 0.061, memory: 17624, loss_cls: 0.0739, loss_bbox: 0.2166, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3484, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2176, loss: 2.1114, grad_norm: 46.4996
2025-06-11 08:06:05,450 - mmdet - INFO - Epoch [3][3450/7033]	lr: 1.501e-04, eta: 11:19:13, time: 1.678, data_time: 0.082, memory: 17624, loss_cls: 0.0797, loss_bbox: 0.2281, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3555, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2418, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2300, loss: 2.1950, grad_norm: 40.7602
2025-06-11 08:07:22,517 - mmdet - INFO - Epoch [3][3500/7033]	lr: 1.501e-04, eta: 11:17:43, time: 1.542, data_time: 0.083, memory: 17624, loss_cls: 0.0750, loss_bbox: 0.2168, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2554, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2169, loss: 2.0963, grad_norm: 75.9463
2025-06-11 08:08:45,778 - mmdet - INFO - Epoch [3][3550/7033]	lr: 1.501e-04, eta: 11:16:21, time: 1.665, data_time: 0.079, memory: 17624, loss_cls: 0.0730, loss_bbox: 0.2106, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3284, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2432, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2104, loss: 2.0279, grad_norm: 22.8090
2025-06-11 08:10:08,142 - mmdet - INFO - Epoch [3][3600/7033]	lr: 1.501e-04, eta: 11:14:58, time: 1.648, data_time: 0.091, memory: 17624, loss_cls: 0.0757, loss_bbox: 0.2146, d0.loss_cls: 0.1733, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1133, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2175, loss: 2.0980, grad_norm: 81.9603
2025-06-11 08:11:27,987 - mmdet - INFO - Epoch [3][3650/7033]	lr: 1.501e-04, eta: 11:13:32, time: 1.595, data_time: 0.068, memory: 17624, loss_cls: 0.0746, loss_bbox: 0.2205, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3468, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2553, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2234, loss: 2.1152, grad_norm: 27.5752
2025-06-11 08:12:52,414 - mmdet - INFO - Epoch [3][3700/7033]	lr: 1.501e-04, eta: 11:12:12, time: 1.690, data_time: 0.089, memory: 17624, loss_cls: 0.0917, loss_bbox: 0.2195, d0.loss_cls: 0.1980, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2562, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0937, d4.loss_bbox: 0.2197, loss: 2.2163, grad_norm: 87.3458
2025-06-11 08:14:12,901 - mmdet - INFO - Epoch [3][3750/7033]	lr: 1.501e-04, eta: 11:10:47, time: 1.610, data_time: 0.073, memory: 17624, loss_cls: 0.0794, loss_bbox: 0.2230, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2593, d2.loss_cls: 0.0982, d2.loss_bbox: 0.2381, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0818, d4.loss_bbox: 0.2248, loss: 2.1681, grad_norm: 24.2178
2025-06-11 08:15:35,031 - mmdet - INFO - Epoch [3][3800/7033]	lr: 1.501e-04, eta: 11:09:24, time: 1.642, data_time: 0.091, memory: 17624, loss_cls: 0.0725, loss_bbox: 0.2196, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2323, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2200, loss: 2.1068, grad_norm: 2128.7763
2025-06-11 08:17:00,690 - mmdet - INFO - Epoch [3][3850/7033]	lr: 1.501e-04, eta: 11:08:05, time: 1.713, data_time: 0.141, memory: 17624, loss_cls: 0.0880, loss_bbox: 0.2229, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3417, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2570, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2248, loss: 2.1896, grad_norm: 31.8302
2025-06-11 08:18:22,110 - mmdet - INFO - Epoch [3][3900/7033]	lr: 1.501e-04, eta: 11:06:41, time: 1.629, data_time: 0.071, memory: 17624, loss_cls: 0.0754, loss_bbox: 0.2218, d0.loss_cls: 0.1827, d0.loss_bbox: 0.3416, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2312, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2208, loss: 2.1186, grad_norm: 26.7276
2025-06-11 08:19:46,668 - mmdet - INFO - Epoch [3][3950/7033]	lr: 1.501e-04, eta: 11:05:21, time: 1.691, data_time: 0.093, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.2155, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3420, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2290, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2187, loss: 2.0879, grad_norm: 26.4095
2025-06-11 08:21:09,194 - mmdet - INFO - Epoch [3][4000/7033]	lr: 1.501e-04, eta: 11:03:59, time: 1.651, data_time: 0.082, memory: 17624, loss_cls: 0.0734, loss_bbox: 0.2145, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3280, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2495, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2171, loss: 2.0595, grad_norm: 22.4940
2025-06-11 08:22:30,702 - mmdet - INFO - Epoch [3][4050/7033]	lr: 1.501e-04, eta: 11:02:35, time: 1.629, data_time: 0.062, memory: 17624, loss_cls: 0.0695, loss_bbox: 0.2075, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3176, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2085, loss: 1.9863, grad_norm: 31.0977
2025-06-11 08:23:52,665 - mmdet - INFO - Epoch [3][4100/7033]	lr: 1.501e-04, eta: 11:01:12, time: 1.640, data_time: 0.067, memory: 17624, loss_cls: 0.0744, loss_bbox: 0.2145, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2281, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2176, loss: 2.0894, grad_norm: 35.0453
2025-06-11 08:25:15,581 - mmdet - INFO - Epoch [3][4150/7033]	lr: 1.501e-04, eta: 10:59:50, time: 1.658, data_time: 0.072, memory: 17624, loss_cls: 0.0700, loss_bbox: 0.2145, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3320, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2173, loss: 2.0645, grad_norm: 37.7658
2025-06-11 08:26:39,240 - mmdet - INFO - Epoch [3][4200/7033]	lr: 1.501e-04, eta: 10:58:29, time: 1.673, data_time: 0.113, memory: 17624, loss_cls: 0.0745, loss_bbox: 0.2165, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3338, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2195, loss: 2.0717, grad_norm: 41.0588
2025-06-11 08:28:09,023 - mmdet - INFO - Epoch [3][4250/7033]	lr: 1.501e-04, eta: 10:57:15, time: 1.796, data_time: 0.103, memory: 17624, loss_cls: 0.0814, loss_bbox: 0.2259, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2596, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2264, loss: 2.1920, grad_norm: 26.4466
2025-06-11 08:29:30,504 - mmdet - INFO - Epoch [3][4300/7033]	lr: 1.501e-04, eta: 10:55:51, time: 1.630, data_time: 0.064, memory: 17624, loss_cls: 0.0789, loss_bbox: 0.2268, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3571, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2663, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2416, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2292, loss: 2.1974, grad_norm: 74.2735
2025-06-11 08:30:51,771 - mmdet - INFO - Epoch [3][4350/7033]	lr: 1.501e-04, eta: 10:54:27, time: 1.625, data_time: 0.058, memory: 17624, loss_cls: 0.0805, loss_bbox: 0.2196, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2541, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2201, loss: 2.1394, grad_norm: 39.5961
2025-06-11 08:32:13,516 - mmdet - INFO - Epoch [3][4400/7033]	lr: 1.501e-04, eta: 10:53:04, time: 1.635, data_time: 0.088, memory: 17624, loss_cls: 0.0752, loss_bbox: 0.2158, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1102, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2168, loss: 2.0842, grad_norm: 43.7815
2025-06-11 08:33:38,448 - mmdet - INFO - Epoch [3][4450/7033]	lr: 1.501e-04, eta: 10:51:44, time: 1.698, data_time: 0.078, memory: 17624, loss_cls: 0.0702, loss_bbox: 0.2092, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2111, loss: 1.9947, grad_norm: 31.7352
2025-06-11 08:34:59,274 - mmdet - INFO - Epoch [3][4500/7033]	lr: 1.501e-04, eta: 10:50:19, time: 1.615, data_time: 0.091, memory: 17624, loss_cls: 0.0751, loss_bbox: 0.2135, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2160, loss: 2.0688, grad_norm: 49.6756
2025-06-11 08:36:20,385 - mmdet - INFO - Epoch [3][4550/7033]	lr: 1.501e-04, eta: 10:48:55, time: 1.624, data_time: 0.085, memory: 17624, loss_cls: 0.0743, loss_bbox: 0.2190, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3403, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2320, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2219, loss: 2.1106, grad_norm: 32.7564
2025-06-11 08:37:41,763 - mmdet - INFO - Epoch [3][4600/7033]	lr: 1.501e-04, eta: 10:47:31, time: 1.628, data_time: 0.062, memory: 17624, loss_cls: 0.0747, loss_bbox: 0.2179, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3371, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2311, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2206, loss: 2.0822, grad_norm: 45.9711
2025-06-11 08:39:03,429 - mmdet - INFO - Epoch [3][4650/7033]	lr: 1.501e-04, eta: 10:46:07, time: 1.633, data_time: 0.082, memory: 17624, loss_cls: 0.0816, loss_bbox: 0.2160, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3342, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2196, loss: 2.1115, grad_norm: 26.8736
2025-06-11 08:40:24,391 - mmdet - INFO - Epoch [3][4700/7033]	lr: 1.501e-04, eta: 10:44:43, time: 1.619, data_time: 0.079, memory: 17624, loss_cls: 0.0758, loss_bbox: 0.2213, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3321, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2589, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2341, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2214, loss: 2.1088, grad_norm: 30.8474
2025-06-11 08:41:40,186 - mmdet - INFO - Epoch [3][4750/7033]	lr: 1.501e-04, eta: 10:43:12, time: 1.516, data_time: 0.056, memory: 17624, loss_cls: 0.0759, loss_bbox: 0.2055, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3277, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2071, loss: 2.0499, grad_norm: 42.9366
2025-06-11 08:43:03,640 - mmdet - INFO - Epoch [3][4800/7033]	lr: 1.501e-04, eta: 10:41:50, time: 1.669, data_time: 0.074, memory: 17624, loss_cls: 0.0797, loss_bbox: 0.2114, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2135, loss: 2.0755, grad_norm: 41.0275
2025-06-11 08:44:25,959 - mmdet - INFO - Epoch [3][4850/7033]	lr: 1.501e-04, eta: 10:40:28, time: 1.645, data_time: 0.089, memory: 17624, loss_cls: 0.0680, loss_bbox: 0.2032, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2058, loss: 1.9822, grad_norm: 32.4944
2025-06-11 08:45:47,373 - mmdet - INFO - Epoch [3][4900/7033]	lr: 1.501e-04, eta: 10:39:04, time: 1.630, data_time: 0.061, memory: 17624, loss_cls: 0.0700, loss_bbox: 0.2068, d0.loss_cls: 0.1717, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1061, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2092, loss: 2.0049, grad_norm: 22.1482
2025-06-11 08:47:08,760 - mmdet - INFO - Epoch [3][4950/7033]	lr: 1.501e-04, eta: 10:37:40, time: 1.627, data_time: 0.071, memory: 17624, loss_cls: 0.0697, loss_bbox: 0.2008, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2013, loss: 1.9824, grad_norm: 30.4801
2025-06-11 08:48:28,933 - mmdet - INFO - Epoch [3][5000/7033]	lr: 1.501e-04, eta: 10:36:14, time: 1.604, data_time: 0.128, memory: 17624, loss_cls: 0.0725, loss_bbox: 0.2133, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3269, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2151, loss: 2.0478, grad_norm: 491.2120
2025-06-11 08:49:49,749 - mmdet - INFO - Epoch [3][5050/7033]	lr: 1.501e-04, eta: 10:34:50, time: 1.616, data_time: 0.065, memory: 17624, loss_cls: 0.0782, loss_bbox: 0.2134, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3299, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2167, loss: 2.0882, grad_norm: 27.0575
2025-06-11 08:51:13,078 - mmdet - INFO - Epoch [3][5100/7033]	lr: 1.501e-04, eta: 10:33:28, time: 1.666, data_time: 0.091, memory: 17624, loss_cls: 0.0708, loss_bbox: 0.2042, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3296, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2049, loss: 1.9923, grad_norm: 97.1594
2025-06-11 08:52:35,419 - mmdet - INFO - Epoch [3][5150/7033]	lr: 1.501e-04, eta: 10:32:06, time: 1.647, data_time: 0.061, memory: 17624, loss_cls: 0.0715, loss_bbox: 0.2094, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3260, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2107, loss: 2.0145, grad_norm: 20.2005
2025-06-11 08:53:57,243 - mmdet - INFO - Epoch [3][5200/7033]	lr: 1.501e-04, eta: 10:30:42, time: 1.636, data_time: 0.067, memory: 17624, loss_cls: 0.0729, loss_bbox: 0.2068, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2192, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2092, loss: 2.0164, grad_norm: 22.8928
2025-06-11 08:55:19,376 - mmdet - INFO - Epoch [3][5250/7033]	lr: 1.501e-04, eta: 10:29:19, time: 1.643, data_time: 0.085, memory: 17624, loss_cls: 0.0746, loss_bbox: 0.2134, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3288, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2459, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2142, loss: 2.0476, grad_norm: 28.1808
2025-06-11 08:56:42,673 - mmdet - INFO - Epoch [3][5300/7033]	lr: 1.501e-04, eta: 10:27:58, time: 1.666, data_time: 0.083, memory: 17624, loss_cls: 0.0717, loss_bbox: 0.2224, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2221, loss: 2.1070, grad_norm: 40.5498
2025-06-11 08:58:06,429 - mmdet - INFO - Epoch [3][5350/7033]	lr: 1.501e-04, eta: 10:26:37, time: 1.674, data_time: 0.065, memory: 17624, loss_cls: 0.0702, loss_bbox: 0.2054, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2066, loss: 2.0013, grad_norm: 63.2142
2025-06-11 08:59:29,987 - mmdet - INFO - Epoch [3][5400/7033]	lr: 1.501e-04, eta: 10:25:15, time: 1.671, data_time: 0.063, memory: 17624, loss_cls: 0.0801, loss_bbox: 0.2182, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3352, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0818, d4.loss_bbox: 0.2189, loss: 2.1108, grad_norm: 27.5618
2025-06-11 09:00:53,792 - mmdet - INFO - Epoch [3][5450/7033]	lr: 1.501e-04, eta: 10:23:54, time: 1.677, data_time: 0.070, memory: 17624, loss_cls: 0.0838, loss_bbox: 0.2128, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2478, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2134, loss: 2.1253, grad_norm: 23.2307
2025-06-11 09:02:13,026 - mmdet - INFO - Epoch [3][5500/7033]	lr: 1.501e-04, eta: 10:22:28, time: 1.584, data_time: 0.071, memory: 17624, loss_cls: 0.0707, loss_bbox: 0.2125, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2146, loss: 2.0589, grad_norm: 61.4162
2025-06-11 09:03:32,626 - mmdet - INFO - Epoch [3][5550/7033]	lr: 1.501e-04, eta: 10:21:02, time: 1.592, data_time: 0.084, memory: 17624, loss_cls: 0.0704, loss_bbox: 0.2067, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3251, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2093, loss: 1.9935, grad_norm: 27.7047
2025-06-11 09:04:51,508 - mmdet - INFO - Epoch [3][5600/7033]	lr: 1.501e-04, eta: 10:19:36, time: 1.578, data_time: 0.072, memory: 17624, loss_cls: 0.0705, loss_bbox: 0.2069, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2197, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2088, loss: 2.0172, grad_norm: 53.5614
2025-06-11 09:06:11,697 - mmdet - INFO - Epoch [3][5650/7033]	lr: 1.501e-04, eta: 10:18:10, time: 1.604, data_time: 0.060, memory: 17624, loss_cls: 0.0708, loss_bbox: 0.2022, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2060, loss: 1.9957, grad_norm: 408.0669
2025-06-11 09:07:31,658 - mmdet - INFO - Epoch [3][5700/7033]	lr: 1.501e-04, eta: 10:16:45, time: 1.599, data_time: 0.081, memory: 17624, loss_cls: 0.0735, loss_bbox: 0.2067, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3279, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2106, loss: 2.0301, grad_norm: 22.9546
2025-06-11 09:08:51,398 - mmdet - INFO - Epoch [3][5750/7033]	lr: 1.501e-04, eta: 10:15:19, time: 1.595, data_time: 0.058, memory: 17624, loss_cls: 0.0754, loss_bbox: 0.2041, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2045, loss: 2.0290, grad_norm: 29.1757
2025-06-11 09:10:13,657 - mmdet - INFO - Epoch [3][5800/7033]	lr: 1.501e-04, eta: 10:13:57, time: 1.645, data_time: 0.062, memory: 17624, loss_cls: 0.0758, loss_bbox: 0.2133, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3424, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2155, loss: 2.0953, grad_norm: 18.9473
2025-06-11 09:11:40,889 - mmdet - INFO - Epoch [3][5850/7033]	lr: 1.501e-04, eta: 10:12:40, time: 1.745, data_time: 0.168, memory: 17624, loss_cls: 0.0766, loss_bbox: 0.2162, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2184, loss: 2.1212, grad_norm: 27.5533
2025-06-11 09:13:03,204 - mmdet - INFO - Epoch [3][5900/7033]	lr: 1.501e-04, eta: 10:11:17, time: 1.646, data_time: 0.060, memory: 17624, loss_cls: 0.0746, loss_bbox: 0.2196, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2607, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2234, loss: 2.1491, grad_norm: 29.2346
2025-06-11 09:14:22,601 - mmdet - INFO - Epoch [3][5950/7033]	lr: 1.501e-04, eta: 10:09:51, time: 1.588, data_time: 0.055, memory: 17624, loss_cls: 0.0754, loss_bbox: 0.2115, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2542, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2152, loss: 2.1045, grad_norm: 25.2222
2025-06-11 09:15:41,866 - mmdet - INFO - Epoch [3][6000/7033]	lr: 1.501e-04, eta: 10:08:25, time: 1.585, data_time: 0.056, memory: 17624, loss_cls: 0.0801, loss_bbox: 0.2145, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2566, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0828, d4.loss_bbox: 0.2153, loss: 2.1246, grad_norm: 28.3040
2025-06-11 09:17:07,993 - mmdet - INFO - Epoch [3][6050/7033]	lr: 1.501e-04, eta: 10:07:07, time: 1.722, data_time: 0.075, memory: 17624, loss_cls: 0.0793, loss_bbox: 0.2075, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2446, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2088, loss: 2.0546, grad_norm: 25.9014
2025-06-11 09:18:29,063 - mmdet - INFO - Epoch [3][6100/7033]	lr: 1.501e-04, eta: 10:05:42, time: 1.621, data_time: 0.058, memory: 17624, loss_cls: 0.0795, loss_bbox: 0.2056, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2432, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2086, loss: 2.0565, grad_norm: 28.4178
2025-06-11 09:19:45,436 - mmdet - INFO - Epoch [3][6150/7033]	lr: 1.501e-04, eta: 10:04:13, time: 1.527, data_time: 0.054, memory: 17624, loss_cls: 0.0849, loss_bbox: 0.2177, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2594, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2213, loss: 2.1748, grad_norm: 35.9015
2025-06-11 09:21:08,179 - mmdet - INFO - Epoch [3][6200/7033]	lr: 1.501e-04, eta: 10:02:51, time: 1.655, data_time: 0.070, memory: 17624, loss_cls: 0.0768, loss_bbox: 0.2141, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2156, loss: 2.0863, grad_norm: 20.5418
2025-06-11 09:22:32,750 - mmdet - INFO - Epoch [3][6250/7033]	lr: 1.501e-04, eta: 10:01:31, time: 1.691, data_time: 0.088, memory: 17624, loss_cls: 0.0753, loss_bbox: 0.2185, d0.loss_cls: 0.1711, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0851, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2180, loss: 2.1245, grad_norm: 30.0968
2025-06-11 09:23:54,537 - mmdet - INFO - Epoch [3][6300/7033]	lr: 1.501e-04, eta: 10:00:08, time: 1.636, data_time: 0.082, memory: 17624, loss_cls: 0.0688, loss_bbox: 0.2147, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3365, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2495, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2156, loss: 2.0476, grad_norm: 72.2857
2025-06-11 09:25:13,966 - mmdet - INFO - Epoch [3][6350/7033]	lr: 1.501e-04, eta: 9:58:42, time: 1.589, data_time: 0.068, memory: 17624, loss_cls: 0.0784, loss_bbox: 0.2233, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2608, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2256, loss: 2.1624, grad_norm: 58.9692
2025-06-11 09:26:37,677 - mmdet - INFO - Epoch [3][6400/7033]	lr: 1.501e-04, eta: 9:57:21, time: 1.674, data_time: 0.094, memory: 17624, loss_cls: 0.0728, loss_bbox: 0.2116, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3253, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2490, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2132, loss: 2.0562, grad_norm: 35.0466
2025-06-11 09:28:00,814 - mmdet - INFO - Epoch [3][6450/7033]	lr: 1.501e-04, eta: 9:55:59, time: 1.663, data_time: 0.068, memory: 17624, loss_cls: 0.0822, loss_bbox: 0.2145, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2577, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2169, loss: 2.1450, grad_norm: 44.4648
2025-06-11 09:29:21,907 - mmdet - INFO - Epoch [3][6500/7033]	lr: 1.501e-04, eta: 9:54:35, time: 1.621, data_time: 0.062, memory: 17624, loss_cls: 0.0815, loss_bbox: 0.2225, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3436, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2631, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2229, loss: 2.1558, grad_norm: 658.8842
2025-06-11 09:30:42,211 - mmdet - INFO - Epoch [3][6550/7033]	lr: 1.501e-04, eta: 9:53:11, time: 1.606, data_time: 0.093, memory: 17624, loss_cls: 0.0774, loss_bbox: 0.2168, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2591, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0857, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2198, loss: 2.1418, grad_norm: 30.0540
2025-06-11 09:32:03,649 - mmdet - INFO - Epoch [3][6600/7033]	lr: 1.501e-04, eta: 9:51:47, time: 1.630, data_time: 0.091, memory: 17624, loss_cls: 0.0741, loss_bbox: 0.2205, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2591, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2343, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2321, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2226, loss: 2.1453, grad_norm: 41.4737
2025-06-11 09:33:23,862 - mmdet - INFO - Epoch [3][6650/7033]	lr: 1.501e-04, eta: 9:50:22, time: 1.604, data_time: 0.066, memory: 17624, loss_cls: 0.0714, loss_bbox: 0.2034, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3390, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2460, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2065, loss: 2.0501, grad_norm: 25.5990
2025-06-11 09:34:44,259 - mmdet - INFO - Epoch [3][6700/7033]	lr: 1.501e-04, eta: 9:48:58, time: 1.608, data_time: 0.085, memory: 17624, loss_cls: 0.0736, loss_bbox: 0.2078, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2104, loss: 2.0327, grad_norm: 41.6739
2025-06-11 09:36:02,191 - mmdet - INFO - Epoch [3][6750/7033]	lr: 1.501e-04, eta: 9:47:31, time: 1.558, data_time: 0.059, memory: 17624, loss_cls: 0.0811, loss_bbox: 0.2163, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2588, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0841, d4.loss_bbox: 0.2185, loss: 2.1326, grad_norm: 53.0319
2025-06-11 09:37:20,042 - mmdet - INFO - Epoch [3][6800/7033]	lr: 1.501e-04, eta: 9:46:04, time: 1.557, data_time: 0.060, memory: 17624, loss_cls: 0.0765, loss_bbox: 0.2111, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2212, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2118, loss: 2.0647, grad_norm: 29.2421
2025-06-11 09:38:40,262 - mmdet - INFO - Epoch [3][6850/7033]	lr: 1.501e-04, eta: 9:44:39, time: 1.604, data_time: 0.081, memory: 17624, loss_cls: 0.0763, loss_bbox: 0.2210, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2589, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2234, loss: 2.1323, grad_norm: 43.0323
2025-06-11 09:40:05,743 - mmdet - INFO - Epoch [3][6900/7033]	lr: 1.501e-04, eta: 9:43:20, time: 1.710, data_time: 0.109, memory: 17624, loss_cls: 0.0807, loss_bbox: 0.2164, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3330, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0974, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0857, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2200, loss: 2.1216, grad_norm: 27.4599
2025-06-11 09:41:24,725 - mmdet - INFO - Epoch [3][6950/7033]	lr: 1.501e-04, eta: 9:41:54, time: 1.579, data_time: 0.058, memory: 17624, loss_cls: 0.0734, loss_bbox: 0.2134, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0959, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2167, loss: 2.0728, grad_norm: 24.5536
2025-06-11 09:42:42,667 - mmdet - INFO - Epoch [3][7000/7033]	lr: 1.501e-04, eta: 9:40:27, time: 1.559, data_time: 0.057, memory: 17624, loss_cls: 0.0695, loss_bbox: 0.2124, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2246, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2131, loss: 2.0393, grad_norm: 30.6367
2025-06-11 09:43:41,601 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-11 10:28:05,707 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 10:28:05,708 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7904, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8819, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9075, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9194, pts_bbox_NuScenes/car_trans_err: 0.1831, pts_bbox_NuScenes/car_scale_err: 0.1544, pts_bbox_NuScenes/car_orient_err: 0.0478, pts_bbox_NuScenes/car_vel_err: 0.3067, pts_bbox_NuScenes/car_attr_err: 0.1744, pts_bbox_NuScenes/mATE: 0.2902, pts_bbox_NuScenes/mASE: 0.2634, pts_bbox_NuScenes/mAOE: 0.2667, pts_bbox_NuScenes/mAVE: 0.2906, pts_bbox_NuScenes/mAAE: 0.1809, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4403, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6269, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7280, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7623, pts_bbox_NuScenes/truck_trans_err: 0.3335, pts_bbox_NuScenes/truck_scale_err: 0.1973, pts_bbox_NuScenes/truck_orient_err: 0.0499, pts_bbox_NuScenes/truck_vel_err: 0.2720, pts_bbox_NuScenes/truck_attr_err: 0.1918, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0550, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2093, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4050, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4735, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6615, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4279, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8003, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1139, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2972, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5139, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7431, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8898, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9149, pts_bbox_NuScenes/bus_trans_err: 0.3402, pts_bbox_NuScenes/bus_scale_err: 0.2004, pts_bbox_NuScenes/bus_orient_err: 0.0497, pts_bbox_NuScenes/bus_vel_err: 0.5501, pts_bbox_NuScenes/bus_attr_err: 0.2890, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1905, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4395, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5971, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6821, pts_bbox_NuScenes/trailer_trans_err: 0.4844, pts_bbox_NuScenes/trailer_scale_err: 0.2282, pts_bbox_NuScenes/trailer_orient_err: 0.5352, pts_bbox_NuScenes/trailer_vel_err: 0.2213, pts_bbox_NuScenes/trailer_attr_err: 0.1347, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6131, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7101, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7623, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7783, pts_bbox_NuScenes/barrier_trans_err: 0.2171, pts_bbox_NuScenes/barrier_scale_err: 0.2831, pts_bbox_NuScenes/barrier_orient_err: 0.0483, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6349, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7700, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8042, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8140, pts_bbox_NuScenes/motorcycle_trans_err: 0.2156, pts_bbox_NuScenes/motorcycle_scale_err: 0.2568, pts_bbox_NuScenes/motorcycle_orient_err: 0.2246, pts_bbox_NuScenes/motorcycle_vel_err: 0.3995, pts_bbox_NuScenes/motorcycle_attr_err: 0.2399, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5534, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6074, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6168, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6258, pts_bbox_NuScenes/bicycle_trans_err: 0.1755, pts_bbox_NuScenes/bicycle_scale_err: 0.2637, pts_bbox_NuScenes/bicycle_orient_err: 0.3132, pts_bbox_NuScenes/bicycle_vel_err: 0.2260, pts_bbox_NuScenes/bicycle_attr_err: 0.0061, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8046, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8466, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8681, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8804, pts_bbox_NuScenes/pedestrian_trans_err: 0.1506, pts_bbox_NuScenes/pedestrian_scale_err: 0.2995, pts_bbox_NuScenes/pedestrian_orient_err: 0.3312, pts_bbox_NuScenes/pedestrian_vel_err: 0.2351, pts_bbox_NuScenes/pedestrian_attr_err: 0.1140, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7151, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7537, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7800, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8017, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1410, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3228, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7072, pts_bbox_NuScenes/mAP: 0.6728
2025-06-11 10:29:43,020 - mmdet - INFO - Epoch [4][50/7033]	lr: 1.001e-04, eta: 9:37:26, time: 1.857, data_time: 0.361, memory: 17624, loss_cls: 0.0750, loss_bbox: 0.2021, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2042, loss: 2.0199, grad_norm: 32.8719
2025-06-11 10:31:06,382 - mmdet - INFO - Epoch [4][100/7033]	lr: 1.001e-04, eta: 9:36:05, time: 1.667, data_time: 0.062, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.2005, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3142, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2368, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2020, loss: 1.9504, grad_norm: 50.5370
2025-06-11 10:32:31,204 - mmdet - INFO - Epoch [4][150/7033]	lr: 1.001e-04, eta: 9:34:45, time: 1.696, data_time: 0.077, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.2010, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3202, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2359, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2025, loss: 1.9375, grad_norm: 26.5358
2025-06-11 10:33:54,158 - mmdet - INFO - Epoch [4][200/7033]	lr: 1.001e-04, eta: 9:33:24, time: 1.659, data_time: 0.068, memory: 17624, loss_cls: 0.0668, loss_bbox: 0.2073, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2103, loss: 1.9682, grad_norm: 19.2380
2025-06-11 10:35:17,056 - mmdet - INFO - Epoch [4][250/7033]	lr: 1.001e-04, eta: 9:32:02, time: 1.658, data_time: 0.112, memory: 17624, loss_cls: 0.0706, loss_bbox: 0.2020, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3157, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2130, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2036, loss: 1.9724, grad_norm: 225.7583
2025-06-11 10:36:42,224 - mmdet - INFO - Epoch [4][300/7033]	lr: 1.001e-04, eta: 9:30:42, time: 1.703, data_time: 0.092, memory: 17624, loss_cls: 0.0677, loss_bbox: 0.2050, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3314, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2196, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2078, loss: 1.9939, grad_norm: 36.4383
2025-06-11 10:38:02,503 - mmdet - INFO - Epoch [4][350/7033]	lr: 1.001e-04, eta: 9:29:18, time: 1.605, data_time: 0.068, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.2091, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3189, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2108, loss: 2.0171, grad_norm: 26.6270
2025-06-11 10:39:21,319 - mmdet - INFO - Epoch [4][400/7033]	lr: 1.001e-04, eta: 9:27:52, time: 1.576, data_time: 0.087, memory: 17624, loss_cls: 0.0779, loss_bbox: 0.2041, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3352, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2182, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2070, loss: 2.0476, grad_norm: 56.4210
2025-06-11 10:40:43,900 - mmdet - INFO - Epoch [4][450/7033]	lr: 1.001e-04, eta: 9:26:30, time: 1.651, data_time: 0.064, memory: 17624, loss_cls: 0.0667, loss_bbox: 0.2005, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3174, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2021, loss: 1.9451, grad_norm: 34.5723
2025-06-11 10:42:04,344 - mmdet - INFO - Epoch [4][500/7033]	lr: 1.001e-04, eta: 9:25:06, time: 1.609, data_time: 0.064, memory: 17624, loss_cls: 0.0717, loss_bbox: 0.2069, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3253, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2224, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2090, loss: 2.0332, grad_norm: 43.9110
2025-06-11 10:43:25,713 - mmdet - INFO - Epoch [4][550/7033]	lr: 1.001e-04, eta: 9:23:43, time: 1.627, data_time: 0.107, memory: 17624, loss_cls: 0.0722, loss_bbox: 0.2067, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2097, loss: 2.0255, grad_norm: 32.9470
2025-06-11 10:44:45,567 - mmdet - INFO - Epoch [4][600/7033]	lr: 1.001e-04, eta: 9:22:18, time: 1.597, data_time: 0.065, memory: 17624, loss_cls: 0.0737, loss_bbox: 0.1961, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3090, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0773, d4.loss_bbox: 0.1973, loss: 1.9526, grad_norm: 74.7127
2025-06-11 10:46:07,558 - mmdet - INFO - Epoch [4][650/7033]	lr: 1.001e-04, eta: 9:20:56, time: 1.640, data_time: 0.067, memory: 17624, loss_cls: 0.0708, loss_bbox: 0.2088, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2243, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2108, loss: 2.0277, grad_norm: 30.7290
2025-06-11 10:47:30,486 - mmdet - INFO - Epoch [4][700/7033]	lr: 1.001e-04, eta: 9:19:34, time: 1.658, data_time: 0.084, memory: 17624, loss_cls: 0.0746, loss_bbox: 0.2066, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3316, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2178, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2096, loss: 2.0419, grad_norm: 1925.5962
2025-06-11 10:49:11,771 - mmdet - INFO - Epoch [4][750/7033]	lr: 1.001e-04, eta: 9:18:29, time: 2.025, data_time: 0.058, memory: 17624, loss_cls: 0.0729, loss_bbox: 0.2014, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2126, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2036, loss: 1.9974, grad_norm: 25.6093
2025-06-11 10:50:36,755 - mmdet - INFO - Epoch [4][800/7033]	lr: 1.001e-04, eta: 9:17:10, time: 1.700, data_time: 0.121, memory: 17624, loss_cls: 0.0718, loss_bbox: 0.2030, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2060, loss: 2.0073, grad_norm: 28.0906
2025-06-11 10:51:58,302 - mmdet - INFO - Epoch [4][850/7033]	lr: 1.001e-04, eta: 9:15:46, time: 1.630, data_time: 0.103, memory: 17624, loss_cls: 0.0716, loss_bbox: 0.2089, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2481, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2120, loss: 2.0363, grad_norm: 26.0995
2025-06-11 10:53:19,307 - mmdet - INFO - Epoch [4][900/7033]	lr: 1.001e-04, eta: 9:14:23, time: 1.620, data_time: 0.089, memory: 17624, loss_cls: 0.0695, loss_bbox: 0.2021, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3216, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2042, loss: 1.9540, grad_norm: 29.1350
2025-06-11 10:54:42,525 - mmdet - INFO - Epoch [4][950/7033]	lr: 1.001e-04, eta: 9:13:01, time: 1.665, data_time: 0.101, memory: 17624, loss_cls: 0.0737, loss_bbox: 0.2103, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1089, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2221, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2132, loss: 2.0569, grad_norm: 25.6776
2025-06-11 10:56:04,799 - mmdet - INFO - Epoch [4][1000/7033]	lr: 1.001e-04, eta: 9:11:39, time: 1.645, data_time: 0.055, memory: 17624, loss_cls: 0.0726, loss_bbox: 0.2132, d0.loss_cls: 0.1619, d0.loss_bbox: 0.3264, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2154, loss: 2.0364, grad_norm: 98.5848
2025-06-11 10:57:25,398 - mmdet - INFO - Epoch [4][1050/7033]	lr: 1.001e-04, eta: 9:10:15, time: 1.612, data_time: 0.086, memory: 17624, loss_cls: 0.0607, loss_bbox: 0.1952, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3166, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0645, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0624, d4.loss_bbox: 0.1978, loss: 1.8740, grad_norm: 60.4881
2025-06-11 10:58:46,054 - mmdet - INFO - Epoch [4][1100/7033]	lr: 1.001e-04, eta: 9:08:51, time: 1.613, data_time: 0.078, memory: 17624, loss_cls: 0.0666, loss_bbox: 0.2075, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2084, loss: 1.9836, grad_norm: 24.0216
2025-06-11 11:00:09,346 - mmdet - INFO - Epoch [4][1150/7033]	lr: 1.001e-04, eta: 9:07:30, time: 1.666, data_time: 0.085, memory: 17624, loss_cls: 0.0718, loss_bbox: 0.2044, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3247, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2160, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2078, loss: 2.0077, grad_norm: 35.5947
2025-06-11 11:01:29,228 - mmdet - INFO - Epoch [4][1200/7033]	lr: 1.001e-04, eta: 9:06:05, time: 1.598, data_time: 0.072, memory: 17624, loss_cls: 0.0688, loss_bbox: 0.2012, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3233, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2032, loss: 1.9581, grad_norm: 89.2407
2025-06-11 11:02:50,506 - mmdet - INFO - Epoch [4][1250/7033]	lr: 1.001e-04, eta: 9:04:42, time: 1.626, data_time: 0.067, memory: 17624, loss_cls: 0.0747, loss_bbox: 0.2082, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2094, loss: 2.0476, grad_norm: 29.7700
2025-06-11 11:04:11,554 - mmdet - INFO - Epoch [4][1300/7033]	lr: 1.001e-04, eta: 9:03:18, time: 1.621, data_time: 0.066, memory: 17624, loss_cls: 0.0728, loss_bbox: 0.2052, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2080, loss: 2.0142, grad_norm: 21.6252
2025-06-11 11:05:33,709 - mmdet - INFO - Epoch [4][1350/7033]	lr: 1.001e-04, eta: 9:01:56, time: 1.643, data_time: 0.079, memory: 17624, loss_cls: 0.0672, loss_bbox: 0.2031, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2059, loss: 1.9631, grad_norm: 42.6537
2025-06-11 11:06:52,659 - mmdet - INFO - Epoch [4][1400/7033]	lr: 1.001e-04, eta: 9:00:31, time: 1.579, data_time: 0.059, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2050, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3161, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2070, loss: 1.9894, grad_norm: 20.9753
2025-06-11 11:08:19,442 - mmdet - INFO - Epoch [4][1450/7033]	lr: 1.001e-04, eta: 8:59:12, time: 1.736, data_time: 0.056, memory: 17624, loss_cls: 0.0706, loss_bbox: 0.1974, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3235, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2359, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2010, loss: 1.9635, grad_norm: 32.2237
2025-06-11 11:09:43,443 - mmdet - INFO - Epoch [4][1500/7033]	lr: 1.001e-04, eta: 8:57:51, time: 1.680, data_time: 0.075, memory: 17624, loss_cls: 0.0687, loss_bbox: 0.2033, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3284, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2063, loss: 1.9787, grad_norm: 28.2176
2025-06-11 11:11:06,825 - mmdet - INFO - Epoch [4][1550/7033]	lr: 1.001e-04, eta: 8:56:30, time: 1.667, data_time: 0.073, memory: 17624, loss_cls: 0.0662, loss_bbox: 0.2072, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2088, loss: 1.9595, grad_norm: 67.7896
2025-06-11 11:12:30,825 - mmdet - INFO - Epoch [4][1600/7033]	lr: 1.001e-04, eta: 8:55:09, time: 1.680, data_time: 0.085, memory: 17624, loss_cls: 0.0712, loss_bbox: 0.2067, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3269, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2103, loss: 1.9961, grad_norm: 45.8955
2025-06-11 11:13:53,536 - mmdet - INFO - Epoch [4][1650/7033]	lr: 1.001e-04, eta: 8:53:47, time: 1.653, data_time: 0.095, memory: 17624, loss_cls: 0.0670, loss_bbox: 0.1998, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2021, loss: 1.9660, grad_norm: 23.8018
2025-06-11 11:15:13,932 - mmdet - INFO - Epoch [4][1700/7033]	lr: 1.001e-04, eta: 8:52:23, time: 1.609, data_time: 0.062, memory: 17624, loss_cls: 0.0700, loss_bbox: 0.2117, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3301, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2135, loss: 2.0450, grad_norm: 28.2199
2025-06-11 11:16:36,624 - mmdet - INFO - Epoch [4][1750/7033]	lr: 1.001e-04, eta: 8:51:01, time: 1.652, data_time: 0.079, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2022, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2043, loss: 1.9761, grad_norm: 32.1415
2025-06-11 11:18:00,046 - mmdet - INFO - Epoch [4][1800/7033]	lr: 1.001e-04, eta: 8:49:40, time: 1.670, data_time: 0.082, memory: 17624, loss_cls: 0.0679, loss_bbox: 0.2014, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3226, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2022, loss: 1.9562, grad_norm: 16.5374
2025-06-11 11:19:24,358 - mmdet - INFO - Epoch [4][1850/7033]	lr: 1.001e-04, eta: 8:48:19, time: 1.686, data_time: 0.082, memory: 17624, loss_cls: 0.0751, loss_bbox: 0.2007, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3194, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2031, loss: 1.9966, grad_norm: 44.2223
2025-06-11 11:20:48,865 - mmdet - INFO - Epoch [4][1900/7033]	lr: 1.001e-04, eta: 8:46:58, time: 1.690, data_time: 0.066, memory: 17624, loss_cls: 0.0752, loss_bbox: 0.2077, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2085, loss: 2.0330, grad_norm: 22.3536
2025-06-11 11:22:13,721 - mmdet - INFO - Epoch [4][1950/7033]	lr: 1.001e-04, eta: 8:45:38, time: 1.697, data_time: 0.057, memory: 17624, loss_cls: 0.0737, loss_bbox: 0.2090, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2111, loss: 2.0348, grad_norm: 31.8822
2025-06-11 11:23:38,948 - mmdet - INFO - Epoch [4][2000/7033]	lr: 1.001e-04, eta: 8:44:18, time: 1.705, data_time: 0.068, memory: 17624, loss_cls: 0.0711, loss_bbox: 0.2068, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2092, loss: 2.0081, grad_norm: 20.7966
2025-06-11 11:25:04,700 - mmdet - INFO - Epoch [4][2050/7033]	lr: 1.001e-04, eta: 8:42:59, time: 1.715, data_time: 0.073, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2049, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3310, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2079, loss: 1.9910, grad_norm: 43.0690
2025-06-11 11:26:28,810 - mmdet - INFO - Epoch [4][2100/7033]	lr: 1.001e-04, eta: 8:41:38, time: 1.682, data_time: 0.059, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2004, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3182, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2031, loss: 1.9577, grad_norm: 38.6121
2025-06-11 11:27:49,605 - mmdet - INFO - Epoch [4][2150/7033]	lr: 1.001e-04, eta: 8:40:14, time: 1.616, data_time: 0.064, memory: 17624, loss_cls: 0.0772, loss_bbox: 0.2111, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2135, loss: 2.0669, grad_norm: 21.3038
2025-06-11 11:29:12,684 - mmdet - INFO - Epoch [4][2200/7033]	lr: 1.001e-04, eta: 8:38:52, time: 1.662, data_time: 0.101, memory: 17624, loss_cls: 0.0704, loss_bbox: 0.2045, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3319, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2086, loss: 1.9975, grad_norm: 20.7579
2025-06-11 11:30:39,182 - mmdet - INFO - Epoch [4][2250/7033]	lr: 1.001e-04, eta: 8:37:33, time: 1.730, data_time: 0.137, memory: 17624, loss_cls: 0.0638, loss_bbox: 0.1966, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3090, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2305, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1983, loss: 1.8896, grad_norm: 34.8491
2025-06-11 11:32:01,356 - mmdet - INFO - Epoch [4][2300/7033]	lr: 1.001e-04, eta: 8:36:11, time: 1.643, data_time: 0.064, memory: 17624, loss_cls: 0.0736, loss_bbox: 0.2133, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3242, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2141, loss: 2.0462, grad_norm: 79.0971
2025-06-11 11:33:22,875 - mmdet - INFO - Epoch [4][2350/7033]	lr: 1.001e-04, eta: 8:34:48, time: 1.630, data_time: 0.079, memory: 17624, loss_cls: 0.0627, loss_bbox: 0.1979, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3194, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2008, loss: 1.9203, grad_norm: 30.4232
2025-06-11 11:34:41,161 - mmdet - INFO - Epoch [4][2400/7033]	lr: 1.001e-04, eta: 8:33:22, time: 1.566, data_time: 0.102, memory: 17624, loss_cls: 0.0712, loss_bbox: 0.2066, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3238, d1.loss_cls: 0.1083, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2093, loss: 2.0029, grad_norm: 27.6607
2025-06-11 11:36:07,608 - mmdet - INFO - Epoch [4][2450/7033]	lr: 1.001e-04, eta: 8:32:03, time: 1.729, data_time: 0.079, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.2089, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2102, loss: 1.9950, grad_norm: 24.2121
2025-06-11 11:37:37,138 - mmdet - INFO - Epoch [4][2500/7033]	lr: 1.001e-04, eta: 8:30:46, time: 1.789, data_time: 0.071, memory: 17624, loss_cls: 0.0698, loss_bbox: 0.2083, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2462, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2106, loss: 2.0203, grad_norm: 28.6352
2025-06-11 11:39:02,612 - mmdet - INFO - Epoch [4][2550/7033]	lr: 1.001e-04, eta: 8:29:26, time: 1.711, data_time: 0.078, memory: 17624, loss_cls: 0.0719, loss_bbox: 0.2103, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3306, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2234, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2136, loss: 2.0493, grad_norm: 19.6485
2025-06-11 11:40:27,567 - mmdet - INFO - Epoch [4][2600/7033]	lr: 1.001e-04, eta: 8:28:06, time: 1.699, data_time: 0.097, memory: 17624, loss_cls: 0.0703, loss_bbox: 0.2095, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3299, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2119, loss: 2.0379, grad_norm: 63.4983
2025-06-11 11:41:54,199 - mmdet - INFO - Epoch [4][2650/7033]	lr: 1.001e-04, eta: 8:26:47, time: 1.733, data_time: 0.075, memory: 17624, loss_cls: 0.0720, loss_bbox: 0.2087, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3248, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2521, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2205, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2125, loss: 2.0486, grad_norm: 190.1475
2025-06-11 11:43:21,730 - mmdet - INFO - Epoch [4][2700/7033]	lr: 1.001e-04, eta: 8:25:28, time: 1.750, data_time: 0.071, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2007, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3152, d1.loss_cls: 0.1063, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2033, loss: 1.9746, grad_norm: 27.3972
2025-06-11 11:44:49,277 - mmdet - INFO - Epoch [4][2750/7033]	lr: 1.001e-04, eta: 8:24:10, time: 1.751, data_time: 0.081, memory: 17624, loss_cls: 0.0682, loss_bbox: 0.2032, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3227, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2431, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2064, loss: 1.9766, grad_norm: 32.9869
2025-06-11 11:46:18,178 - mmdet - INFO - Epoch [4][2800/7033]	lr: 1.001e-04, eta: 8:22:52, time: 1.777, data_time: 0.060, memory: 17624, loss_cls: 0.0708, loss_bbox: 0.1985, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3240, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2023, loss: 1.9657, grad_norm: 38.8517
2025-06-11 11:47:46,068 - mmdet - INFO - Epoch [4][2850/7033]	lr: 1.001e-04, eta: 8:21:34, time: 1.759, data_time: 0.077, memory: 17624, loss_cls: 0.0687, loss_bbox: 0.2115, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2145, loss: 2.0415, grad_norm: 28.0350
2025-06-11 11:49:11,745 - mmdet - INFO - Epoch [4][2900/7033]	lr: 1.001e-04, eta: 8:20:14, time: 1.714, data_time: 0.066, memory: 17624, loss_cls: 0.0666, loss_bbox: 0.2035, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2462, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2063, loss: 1.9904, grad_norm: 36.4251
2025-06-11 11:50:37,301 - mmdet - INFO - Epoch [4][2950/7033]	lr: 1.001e-04, eta: 8:18:54, time: 1.710, data_time: 0.086, memory: 17624, loss_cls: 0.0667, loss_bbox: 0.2003, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3243, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2028, loss: 1.9633, grad_norm: 36.6120
2025-06-11 11:52:02,184 - mmdet - INFO - Epoch [4][3000/7033]	lr: 1.001e-04, eta: 8:17:33, time: 1.698, data_time: 0.086, memory: 17624, loss_cls: 0.0736, loss_bbox: 0.2113, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1140, d1.loss_bbox: 0.2543, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2135, loss: 2.0883, grad_norm: 48.3109
2025-06-11 11:53:30,401 - mmdet - INFO - Epoch [4][3050/7033]	lr: 1.001e-04, eta: 8:16:15, time: 1.765, data_time: 0.104, memory: 17624, loss_cls: 0.0791, loss_bbox: 0.2111, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2532, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2140, loss: 2.0937, grad_norm: 20.7430
2025-06-11 11:54:54,362 - mmdet - INFO - Epoch [4][3100/7033]	lr: 1.001e-04, eta: 8:14:54, time: 1.679, data_time: 0.060, memory: 17624, loss_cls: 0.0701, loss_bbox: 0.2100, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2119, loss: 2.0244, grad_norm: 27.9221
2025-06-11 11:56:25,789 - mmdet - INFO - Epoch [4][3150/7033]	lr: 1.001e-04, eta: 8:13:38, time: 1.828, data_time: 0.166, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.2061, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3263, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2092, loss: 2.0023, grad_norm: 33.2028
2025-06-11 11:57:48,471 - mmdet - INFO - Epoch [4][3200/7033]	lr: 1.001e-04, eta: 8:12:15, time: 1.654, data_time: 0.071, memory: 17624, loss_cls: 0.0717, loss_bbox: 0.2038, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2224, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2076, loss: 2.0071, grad_norm: 76.6276
2025-06-11 11:59:13,599 - mmdet - INFO - Epoch [4][3250/7033]	lr: 1.001e-04, eta: 8:10:55, time: 1.702, data_time: 0.081, memory: 17624, loss_cls: 0.0678, loss_bbox: 0.2139, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3370, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2566, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2170, loss: 2.0581, grad_norm: 113.5991
2025-06-11 12:00:39,799 - mmdet - INFO - Epoch [4][3300/7033]	lr: 1.001e-04, eta: 8:09:35, time: 1.724, data_time: 0.064, memory: 17624, loss_cls: 0.0680, loss_bbox: 0.1926, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3100, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2290, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2072, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2033, d4.loss_cls: 0.0688, d4.loss_bbox: 0.1955, loss: 1.8943, grad_norm: 20.7358
2025-06-11 12:02:17,375 - mmdet - INFO - Epoch [4][3350/7033]	lr: 1.001e-04, eta: 8:08:23, time: 1.951, data_time: 0.130, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2074, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3223, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0785, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2096, loss: 2.0103, grad_norm: 68.3588
2025-06-11 12:03:49,857 - mmdet - INFO - Epoch [4][3400/7033]	lr: 1.001e-04, eta: 8:07:08, time: 1.849, data_time: 0.105, memory: 17624, loss_cls: 0.0635, loss_bbox: 0.2081, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3277, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2221, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2191, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2108, loss: 1.9746, grad_norm: 24.7893
2025-06-11 12:05:16,905 - mmdet - INFO - Epoch [4][3450/7033]	lr: 1.001e-04, eta: 8:05:49, time: 1.743, data_time: 0.069, memory: 17624, loss_cls: 0.0723, loss_bbox: 0.2115, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2145, loss: 2.0366, grad_norm: 23.2306
2025-06-11 12:06:42,168 - mmdet - INFO - Epoch [4][3500/7033]	lr: 1.001e-04, eta: 8:04:28, time: 1.705, data_time: 0.077, memory: 17624, loss_cls: 0.0706, loss_bbox: 0.2022, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3228, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2168, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2050, loss: 1.9818, grad_norm: 17.2369
2025-06-11 12:08:06,063 - mmdet - INFO - Epoch [4][3550/7033]	lr: 1.001e-04, eta: 8:03:06, time: 1.678, data_time: 0.084, memory: 17624, loss_cls: 0.0752, loss_bbox: 0.2078, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3343, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2503, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2106, loss: 2.0521, grad_norm: 27.7560
2025-06-11 12:09:30,235 - mmdet - INFO - Epoch [4][3600/7033]	lr: 1.001e-04, eta: 8:01:45, time: 1.683, data_time: 0.108, memory: 17624, loss_cls: 0.0794, loss_bbox: 0.2092, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3260, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2230, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2108, loss: 2.0689, grad_norm: 38.3947
2025-06-11 12:10:56,069 - mmdet - INFO - Epoch [4][3650/7033]	lr: 1.001e-04, eta: 8:00:24, time: 1.717, data_time: 0.062, memory: 17624, loss_cls: 0.0717, loss_bbox: 0.2026, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3290, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2182, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2052, loss: 1.9990, grad_norm: 24.8156
2025-06-11 12:12:23,149 - mmdet - INFO - Epoch [4][3700/7033]	lr: 1.001e-04, eta: 7:59:05, time: 1.740, data_time: 0.084, memory: 17624, loss_cls: 0.0718, loss_bbox: 0.2020, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3219, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2057, loss: 1.9726, grad_norm: 35.3045
2025-06-11 12:13:54,340 - mmdet - INFO - Epoch [4][3750/7033]	lr: 1.001e-04, eta: 7:57:48, time: 1.826, data_time: 0.162, memory: 17624, loss_cls: 0.0684, loss_bbox: 0.2028, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3181, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2351, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2045, loss: 1.9555, grad_norm: 256.5710
2025-06-11 12:15:20,020 - mmdet - INFO - Epoch [4][3800/7033]	lr: 1.001e-04, eta: 7:56:28, time: 1.713, data_time: 0.065, memory: 17624, loss_cls: 0.0713, loss_bbox: 0.2072, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2078, loss: 1.9942, grad_norm: 36.3675
2025-06-11 12:16:43,389 - mmdet - INFO - Epoch [4][3850/7033]	lr: 1.001e-04, eta: 7:55:06, time: 1.668, data_time: 0.109, memory: 17624, loss_cls: 0.0648, loss_bbox: 0.1966, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3029, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2072, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1978, loss: 1.8792, grad_norm: 43.3476
2025-06-11 12:18:04,910 - mmdet - INFO - Epoch [4][3900/7033]	lr: 1.001e-04, eta: 7:53:42, time: 1.630, data_time: 0.092, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.1948, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3171, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0706, d4.loss_bbox: 0.1974, loss: 1.9327, grad_norm: 27.5861
2025-06-11 12:19:30,516 - mmdet - INFO - Epoch [4][3950/7033]	lr: 1.001e-04, eta: 7:52:22, time: 1.712, data_time: 0.113, memory: 17624, loss_cls: 0.0719, loss_bbox: 0.2043, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3205, d1.loss_cls: 0.1057, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2053, loss: 1.9905, grad_norm: 21.3787
2025-06-11 12:20:52,575 - mmdet - INFO - Epoch [4][4000/7033]	lr: 1.001e-04, eta: 7:50:59, time: 1.641, data_time: 0.078, memory: 17624, loss_cls: 0.0736, loss_bbox: 0.2034, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3333, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2066, loss: 2.0228, grad_norm: 18.4602
2025-06-11 12:22:14,921 - mmdet - INFO - Epoch [4][4050/7033]	lr: 1.001e-04, eta: 7:49:36, time: 1.647, data_time: 0.054, memory: 17624, loss_cls: 0.0678, loss_bbox: 0.2028, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3253, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2059, loss: 1.9573, grad_norm: 39.5771
2025-06-11 12:23:34,908 - mmdet - INFO - Epoch [4][4100/7033]	lr: 1.001e-04, eta: 7:48:12, time: 1.600, data_time: 0.070, memory: 17624, loss_cls: 0.0715, loss_bbox: 0.2088, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3234, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2126, loss: 2.0203, grad_norm: 67.0188
2025-06-11 12:24:56,070 - mmdet - INFO - Epoch [4][4150/7033]	lr: 1.001e-04, eta: 7:46:48, time: 1.621, data_time: 0.066, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.2061, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3143, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2062, loss: 1.9857, grad_norm: 25.2023
2025-06-11 12:26:14,765 - mmdet - INFO - Epoch [4][4200/7033]	lr: 1.001e-04, eta: 7:45:23, time: 1.576, data_time: 0.067, memory: 17624, loss_cls: 0.0674, loss_bbox: 0.2114, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3314, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2488, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2129, loss: 2.0017, grad_norm: 27.3923
2025-06-11 12:27:34,515 - mmdet - INFO - Epoch [4][4250/7033]	lr: 1.001e-04, eta: 7:43:58, time: 1.595, data_time: 0.062, memory: 17624, loss_cls: 0.0722, loss_bbox: 0.2021, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3247, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2029, loss: 2.0032, grad_norm: 230.2063
2025-06-11 12:28:58,593 - mmdet - INFO - Epoch [4][4300/7033]	lr: 1.001e-04, eta: 7:42:37, time: 1.682, data_time: 0.062, memory: 17624, loss_cls: 0.0774, loss_bbox: 0.2104, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3366, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2223, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2136, loss: 2.0795, grad_norm: 25.6315
2025-06-11 12:30:23,247 - mmdet - INFO - Epoch [4][4350/7033]	lr: 1.001e-04, eta: 7:41:15, time: 1.693, data_time: 0.077, memory: 17624, loss_cls: 0.0711, loss_bbox: 0.2077, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2083, loss: 2.0112, grad_norm: 21.0503
2025-06-11 12:31:57,146 - mmdet - INFO - Epoch [4][4400/7033]	lr: 1.001e-04, eta: 7:40:00, time: 1.878, data_time: 0.167, memory: 17624, loss_cls: 0.0783, loss_bbox: 0.2134, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1102, d1.loss_bbox: 0.2542, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2162, loss: 2.0945, grad_norm: 17.9033
2025-06-11 12:33:18,396 - mmdet - INFO - Epoch [4][4450/7033]	lr: 1.001e-04, eta: 7:38:36, time: 1.625, data_time: 0.051, memory: 17624, loss_cls: 0.0644, loss_bbox: 0.1961, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3108, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2332, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0668, d4.loss_bbox: 0.1992, loss: 1.9060, grad_norm: 33.8868
2025-06-11 12:34:40,561 - mmdet - INFO - Epoch [4][4500/7033]	lr: 1.001e-04, eta: 7:37:14, time: 1.644, data_time: 0.057, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.2042, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3185, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2056, loss: 1.9483, grad_norm: 20.1261
2025-06-11 12:36:00,980 - mmdet - INFO - Epoch [4][4550/7033]	lr: 1.001e-04, eta: 7:35:49, time: 1.608, data_time: 0.066, memory: 17624, loss_cls: 0.0691, loss_bbox: 0.2060, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3298, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2202, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2071, loss: 1.9970, grad_norm: 36.1680
2025-06-11 12:38:47,413 - mmdet - INFO - Epoch [4][4600/7033]	lr: 1.001e-04, eta: 7:35:21, time: 3.327, data_time: 1.578, memory: 17624, loss_cls: 0.0755, loss_bbox: 0.2123, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2147, loss: 2.0778, grad_norm: 34.3799
2025-06-11 12:40:09,512 - mmdet - INFO - Epoch [4][4650/7033]	lr: 1.001e-04, eta: 7:33:57, time: 1.644, data_time: 0.066, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.2021, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3179, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2045, loss: 1.9613, grad_norm: 26.2819
2025-06-11 12:41:31,039 - mmdet - INFO - Epoch [4][4700/7033]	lr: 1.001e-04, eta: 7:32:34, time: 1.631, data_time: 0.080, memory: 17624, loss_cls: 0.0703, loss_bbox: 0.2117, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3296, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2132, loss: 2.0480, grad_norm: 56.3658
2025-06-11 12:42:49,520 - mmdet - INFO - Epoch [4][4750/7033]	lr: 1.001e-04, eta: 7:31:08, time: 1.570, data_time: 0.066, memory: 17624, loss_cls: 0.0750, loss_bbox: 0.2082, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2477, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2124, loss: 2.0578, grad_norm: 36.9005
2025-06-11 12:44:07,536 - mmdet - INFO - Epoch [4][4800/7033]	lr: 1.001e-04, eta: 7:29:42, time: 1.560, data_time: 0.063, memory: 17624, loss_cls: 0.0741, loss_bbox: 0.2108, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3191, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2131, loss: 2.0258, grad_norm: 38.8318
2025-06-11 12:45:40,759 - mmdet - INFO - Epoch [4][4850/7033]	lr: 1.001e-04, eta: 7:28:26, time: 1.864, data_time: 0.272, memory: 17624, loss_cls: 0.0674, loss_bbox: 0.2009, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2029, loss: 1.9359, grad_norm: 30.6194
2025-06-11 12:47:02,653 - mmdet - INFO - Epoch [4][4900/7033]	lr: 1.001e-04, eta: 7:27:03, time: 1.638, data_time: 0.059, memory: 17624, loss_cls: 0.0777, loss_bbox: 0.2116, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2232, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2152, loss: 2.0769, grad_norm: 36.3989
2025-06-11 12:48:21,509 - mmdet - INFO - Epoch [4][4950/7033]	lr: 1.001e-04, eta: 7:25:38, time: 1.577, data_time: 0.066, memory: 17624, loss_cls: 0.0661, loss_bbox: 0.2064, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2090, loss: 1.9886, grad_norm: 23.5539
2025-06-11 12:50:34,468 - mmdet - INFO - Epoch [4][5000/7033]	lr: 1.001e-04, eta: 7:24:46, time: 2.659, data_time: 1.144, memory: 17624, loss_cls: 0.0765, loss_bbox: 0.2091, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3251, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2471, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2113, loss: 2.0576, grad_norm: 26.9593
2025-06-11 12:52:08,111 - mmdet - INFO - Epoch [4][5050/7033]	lr: 1.001e-04, eta: 7:23:30, time: 1.873, data_time: 0.329, memory: 17624, loss_cls: 0.0738, loss_bbox: 0.2019, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2049, loss: 1.9820, grad_norm: 26.7773
2025-06-11 12:53:29,945 - mmdet - INFO - Epoch [4][5100/7033]	lr: 1.001e-04, eta: 7:22:06, time: 1.636, data_time: 0.085, memory: 17624, loss_cls: 0.0814, loss_bbox: 0.2195, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2609, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2223, loss: 2.1534, grad_norm: 18.8638
2025-06-11 12:54:51,229 - mmdet - INFO - Epoch [4][5150/7033]	lr: 1.001e-04, eta: 7:20:42, time: 1.626, data_time: 0.065, memory: 17624, loss_cls: 0.0682, loss_bbox: 0.2001, d0.loss_cls: 0.1644, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2031, loss: 1.9626, grad_norm: 41.7912
2025-06-11 12:56:10,659 - mmdet - INFO - Epoch [4][5200/7033]	lr: 1.001e-04, eta: 7:19:17, time: 1.589, data_time: 0.065, memory: 17624, loss_cls: 0.0641, loss_bbox: 0.1994, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3175, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2120, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2023, loss: 1.9377, grad_norm: 40.4648
2025-06-11 12:57:31,953 - mmdet - INFO - Epoch [4][5250/7033]	lr: 1.001e-04, eta: 7:17:53, time: 1.626, data_time: 0.068, memory: 17624, loss_cls: 0.0688, loss_bbox: 0.2007, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3019, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2100, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2021, loss: 1.9229, grad_norm: 35.9917
2025-06-11 12:58:51,681 - mmdet - INFO - Epoch [4][5300/7033]	lr: 1.001e-04, eta: 7:16:28, time: 1.594, data_time: 0.057, memory: 17624, loss_cls: 0.0740, loss_bbox: 0.2031, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3157, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2059, loss: 1.9834, grad_norm: 24.8776
2025-06-11 13:00:11,227 - mmdet - INFO - Epoch [4][5350/7033]	lr: 1.001e-04, eta: 7:15:04, time: 1.590, data_time: 0.062, memory: 17624, loss_cls: 0.0639, loss_bbox: 0.2000, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3166, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2385, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0638, d4.loss_bbox: 0.2039, loss: 1.9232, grad_norm: 19.5110
2025-06-11 13:01:36,303 - mmdet - INFO - Epoch [4][5400/7033]	lr: 1.001e-04, eta: 7:13:42, time: 1.701, data_time: 0.054, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.2048, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2138, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2078, loss: 1.9784, grad_norm: 25.1509
2025-06-11 13:02:55,396 - mmdet - INFO - Epoch [4][5450/7033]	lr: 1.001e-04, eta: 7:12:17, time: 1.583, data_time: 0.065, memory: 17624, loss_cls: 0.0641, loss_bbox: 0.1894, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3047, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2036, d3.loss_cls: 0.0687, d3.loss_bbox: 0.1997, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1920, loss: 1.8536, grad_norm: 40.7590
2025-06-11 13:04:18,134 - mmdet - INFO - Epoch [4][5500/7033]	lr: 1.001e-04, eta: 7:10:54, time: 1.654, data_time: 0.107, memory: 17624, loss_cls: 0.0657, loss_bbox: 0.1996, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3189, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2046, loss: 1.9449, grad_norm: 62.8100
2025-06-11 13:05:37,723 - mmdet - INFO - Epoch [4][5550/7033]	lr: 1.001e-04, eta: 7:09:29, time: 1.593, data_time: 0.070, memory: 17624, loss_cls: 0.0702, loss_bbox: 0.2034, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3181, d1.loss_cls: 0.1064, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2052, loss: 1.9711, grad_norm: 52.1166
2025-06-11 13:06:59,688 - mmdet - INFO - Epoch [4][5600/7033]	lr: 1.001e-04, eta: 7:08:06, time: 1.639, data_time: 0.072, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1943, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3092, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1962, loss: 1.8672, grad_norm: 37.8217
2025-06-11 13:08:21,657 - mmdet - INFO - Epoch [4][5650/7033]	lr: 1.001e-04, eta: 7:06:42, time: 1.639, data_time: 0.063, memory: 17624, loss_cls: 0.0726, loss_bbox: 0.2106, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3305, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2138, loss: 2.0436, grad_norm: 19.7729
2025-06-11 13:09:41,839 - mmdet - INFO - Epoch [4][5700/7033]	lr: 1.001e-04, eta: 7:05:18, time: 1.602, data_time: 0.061, memory: 17624, loss_cls: 0.0776, loss_bbox: 0.2038, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3263, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2060, loss: 2.0316, grad_norm: 38.6375
2025-06-11 13:11:05,203 - mmdet - INFO - Epoch [4][5750/7033]	lr: 1.001e-04, eta: 7:03:55, time: 1.669, data_time: 0.062, memory: 17624, loss_cls: 0.0724, loss_bbox: 0.2119, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3291, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2527, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2146, loss: 2.0442, grad_norm: 35.6136
2025-06-11 13:12:25,152 - mmdet - INFO - Epoch [4][5800/7033]	lr: 1.001e-04, eta: 7:02:31, time: 1.599, data_time: 0.065, memory: 17624, loss_cls: 0.0736, loss_bbox: 0.2027, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3150, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2057, loss: 1.9880, grad_norm: 92.8192
2025-06-11 13:13:46,313 - mmdet - INFO - Epoch [4][5850/7033]	lr: 1.001e-04, eta: 7:01:07, time: 1.623, data_time: 0.092, memory: 17624, loss_cls: 0.0733, loss_bbox: 0.2115, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3314, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2245, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2143, loss: 2.0552, grad_norm: 20.6435
2025-06-11 13:15:09,669 - mmdet - INFO - Epoch [4][5900/7033]	lr: 1.001e-04, eta: 6:59:45, time: 1.667, data_time: 0.078, memory: 17624, loss_cls: 0.0703, loss_bbox: 0.2042, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3185, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2054, loss: 1.9781, grad_norm: 26.3796
2025-06-11 13:16:33,437 - mmdet - INFO - Epoch [4][5950/7033]	lr: 1.001e-04, eta: 6:58:22, time: 1.672, data_time: 0.104, memory: 17624, loss_cls: 0.0697, loss_bbox: 0.1961, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3214, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0713, d4.loss_bbox: 0.1992, loss: 1.9397, grad_norm: 18.9447
2025-06-11 13:17:56,787 - mmdet - INFO - Epoch [4][6000/7033]	lr: 1.001e-04, eta: 6:57:00, time: 1.670, data_time: 0.074, memory: 17624, loss_cls: 0.0708, loss_bbox: 0.2035, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2152, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2050, loss: 1.9822, grad_norm: 40.9867
2025-06-11 13:19:20,579 - mmdet - INFO - Epoch [4][6050/7033]	lr: 1.001e-04, eta: 6:55:37, time: 1.676, data_time: 0.109, memory: 17624, loss_cls: 0.0670, loss_bbox: 0.1983, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2014, loss: 1.9494, grad_norm: 30.3077
2025-06-11 13:21:23,056 - mmdet - INFO - Epoch [4][6100/7033]	lr: 1.001e-04, eta: 6:54:36, time: 2.449, data_time: 0.869, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.1985, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3197, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2016, loss: 1.9202, grad_norm: 23.3291
2025-06-11 13:22:45,223 - mmdet - INFO - Epoch [4][6150/7033]	lr: 1.001e-04, eta: 6:53:13, time: 1.643, data_time: 0.085, memory: 17624, loss_cls: 0.0743, loss_bbox: 0.2095, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3291, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2463, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2132, loss: 2.0439, grad_norm: 27.2040
2025-06-11 13:24:03,277 - mmdet - INFO - Epoch [4][6200/7033]	lr: 1.001e-04, eta: 6:51:47, time: 1.562, data_time: 0.067, memory: 17624, loss_cls: 0.0699, loss_bbox: 0.2016, d0.loss_cls: 0.1644, d0.loss_bbox: 0.3180, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2050, loss: 1.9648, grad_norm: 28.7933
2025-06-11 13:25:23,269 - mmdet - INFO - Epoch [4][6250/7033]	lr: 1.001e-04, eta: 6:50:23, time: 1.600, data_time: 0.087, memory: 17624, loss_cls: 0.0684, loss_bbox: 0.2011, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2034, loss: 1.9750, grad_norm: 34.8505
2025-06-11 13:26:46,980 - mmdet - INFO - Epoch [4][6300/7033]	lr: 1.001e-04, eta: 6:49:00, time: 1.674, data_time: 0.126, memory: 17624, loss_cls: 0.0730, loss_bbox: 0.2067, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3288, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2093, loss: 2.0190, grad_norm: 45.6600
2025-06-11 13:28:09,436 - mmdet - INFO - Epoch [4][6350/7033]	lr: 1.001e-04, eta: 6:47:37, time: 1.649, data_time: 0.086, memory: 17624, loss_cls: 0.0713, loss_bbox: 0.2047, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3290, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2081, loss: 2.0005, grad_norm: 22.3768
2025-06-11 13:29:30,707 - mmdet - INFO - Epoch [4][6400/7033]	lr: 1.001e-04, eta: 6:46:13, time: 1.625, data_time: 0.070, memory: 17624, loss_cls: 0.0747, loss_bbox: 0.1984, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3175, d1.loss_cls: 0.1064, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2013, loss: 1.9723, grad_norm: 26.2303
2025-06-11 13:30:52,036 - mmdet - INFO - Epoch [4][6450/7033]	lr: 1.001e-04, eta: 6:44:50, time: 1.627, data_time: 0.081, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.2005, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3155, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2026, loss: 1.9409, grad_norm: 26.8704
2025-06-11 13:32:11,304 - mmdet - INFO - Epoch [4][6500/7033]	lr: 1.001e-04, eta: 6:43:25, time: 1.585, data_time: 0.061, memory: 17624, loss_cls: 0.0692, loss_bbox: 0.2020, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3231, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2050, loss: 1.9683, grad_norm: 32.5026
2025-06-11 13:33:29,275 - mmdet - INFO - Epoch [4][6550/7033]	lr: 1.001e-04, eta: 6:41:59, time: 1.559, data_time: 0.083, memory: 17624, loss_cls: 0.0697, loss_bbox: 0.2016, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3152, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2130, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2042, loss: 1.9502, grad_norm: 49.4880
2025-06-11 13:34:52,991 - mmdet - INFO - Epoch [4][6600/7033]	lr: 1.001e-04, eta: 6:40:37, time: 1.674, data_time: 0.068, memory: 17624, loss_cls: 0.0772, loss_bbox: 0.2066, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3109, d1.loss_cls: 0.1063, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2080, loss: 2.0032, grad_norm: 20.1306
2025-06-11 13:36:13,275 - mmdet - INFO - Epoch [4][6650/7033]	lr: 1.001e-04, eta: 6:39:13, time: 1.606, data_time: 0.069, memory: 17624, loss_cls: 0.0675, loss_bbox: 0.2023, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3162, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2044, loss: 1.9483, grad_norm: 24.0962
2025-06-11 13:37:35,413 - mmdet - INFO - Epoch [4][6700/7033]	lr: 1.001e-04, eta: 6:37:49, time: 1.643, data_time: 0.081, memory: 17624, loss_cls: 0.0789, loss_bbox: 0.2061, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2186, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2160, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2077, loss: 2.0403, grad_norm: 88.4672
2025-06-11 13:38:57,968 - mmdet - INFO - Epoch [4][6750/7033]	lr: 1.001e-04, eta: 6:36:26, time: 1.651, data_time: 0.070, memory: 17624, loss_cls: 0.0763, loss_bbox: 0.2094, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3342, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2231, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2123, loss: 2.0713, grad_norm: 75.7850
2025-06-11 13:40:26,898 - mmdet - INFO - Epoch [4][6800/7033]	lr: 1.001e-04, eta: 6:35:06, time: 1.779, data_time: 0.102, memory: 17624, loss_cls: 0.0629, loss_bbox: 0.1985, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3163, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0662, d4.loss_bbox: 0.2016, loss: 1.9227, grad_norm: 46.6481
2025-06-11 13:41:47,419 - mmdet - INFO - Epoch [4][6850/7033]	lr: 1.001e-04, eta: 6:33:42, time: 1.610, data_time: 0.059, memory: 17624, loss_cls: 0.0629, loss_bbox: 0.1981, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3125, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0645, d4.loss_bbox: 0.2013, loss: 1.9037, grad_norm: 28.3633
2025-06-11 13:43:06,475 - mmdet - INFO - Epoch [4][6900/7033]	lr: 1.001e-04, eta: 6:32:18, time: 1.580, data_time: 0.081, memory: 17624, loss_cls: 0.0711, loss_bbox: 0.2078, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2101, loss: 2.0320, grad_norm: 38.9371
2025-06-11 13:44:29,589 - mmdet - INFO - Epoch [4][6950/7033]	lr: 1.001e-04, eta: 6:30:55, time: 1.663, data_time: 0.131, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.2099, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3230, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2126, loss: 1.9911, grad_norm: 94.6947
2025-06-11 13:45:48,406 - mmdet - INFO - Epoch [4][7000/7033]	lr: 1.001e-04, eta: 6:29:30, time: 1.577, data_time: 0.057, memory: 17624, loss_cls: 0.0676, loss_bbox: 0.2014, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3174, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2408, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2046, loss: 1.9426, grad_norm: 24.8549
2025-06-11 13:46:44,811 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-06-11 14:25:43,147 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 14:25:43,147 - mmdet - INFO - Epoch(val) [4][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7951, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8838, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9096, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9225, pts_bbox_NuScenes/car_trans_err: 0.1743, pts_bbox_NuScenes/car_scale_err: 0.1523, pts_bbox_NuScenes/car_orient_err: 0.0448, pts_bbox_NuScenes/car_vel_err: 0.3013, pts_bbox_NuScenes/car_attr_err: 0.1881, pts_bbox_NuScenes/mATE: 0.2854, pts_bbox_NuScenes/mASE: 0.2620, pts_bbox_NuScenes/mAOE: 0.2632, pts_bbox_NuScenes/mAVE: 0.2819, pts_bbox_NuScenes/mAAE: 0.1821, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4266, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6141, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7223, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7576, pts_bbox_NuScenes/truck_trans_err: 0.3444, pts_bbox_NuScenes/truck_scale_err: 0.1985, pts_bbox_NuScenes/truck_orient_err: 0.0494, pts_bbox_NuScenes/truck_vel_err: 0.2781, pts_bbox_NuScenes/truck_attr_err: 0.1982, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0622, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2181, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4099, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4748, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6416, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4358, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8305, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1111, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2936, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5318, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7523, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9012, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9250, pts_bbox_NuScenes/bus_trans_err: 0.3240, pts_bbox_NuScenes/bus_scale_err: 0.1802, pts_bbox_NuScenes/bus_orient_err: 0.0607, pts_bbox_NuScenes/bus_vel_err: 0.4546, pts_bbox_NuScenes/bus_attr_err: 0.2700, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1760, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4364, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5917, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6711, pts_bbox_NuScenes/trailer_trans_err: 0.4726, pts_bbox_NuScenes/trailer_scale_err: 0.2275, pts_bbox_NuScenes/trailer_orient_err: 0.5124, pts_bbox_NuScenes/trailer_vel_err: 0.2711, pts_bbox_NuScenes/trailer_attr_err: 0.1496, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6100, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7143, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7620, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7784, pts_bbox_NuScenes/barrier_trans_err: 0.2168, pts_bbox_NuScenes/barrier_scale_err: 0.2877, pts_bbox_NuScenes/barrier_orient_err: 0.0471, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6323, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7576, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7935, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8047, pts_bbox_NuScenes/motorcycle_trans_err: 0.2196, pts_bbox_NuScenes/motorcycle_scale_err: 0.2481, pts_bbox_NuScenes/motorcycle_orient_err: 0.1948, pts_bbox_NuScenes/motorcycle_vel_err: 0.4063, pts_bbox_NuScenes/motorcycle_attr_err: 0.2353, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5449, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5945, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6067, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6163, pts_bbox_NuScenes/bicycle_trans_err: 0.1777, pts_bbox_NuScenes/bicycle_scale_err: 0.2652, pts_bbox_NuScenes/bicycle_orient_err: 0.2865, pts_bbox_NuScenes/bicycle_vel_err: 0.2080, pts_bbox_NuScenes/bicycle_attr_err: 0.0054, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8119, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8549, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8774, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8904, pts_bbox_NuScenes/pedestrian_trans_err: 0.1449, pts_bbox_NuScenes/pedestrian_scale_err: 0.2967, pts_bbox_NuScenes/pedestrian_orient_err: 0.3430, pts_bbox_NuScenes/pedestrian_vel_err: 0.2250, pts_bbox_NuScenes/pedestrian_attr_err: 0.1165, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7278, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7687, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7944, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8165, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1379, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3276, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7093, pts_bbox_NuScenes/mAP: 0.6735
2025-06-11 14:27:23,710 - mmdet - INFO - Epoch [5][50/7033]	lr: 5.015e-05, eta: 6:26:52, time: 1.940, data_time: 0.376, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.2061, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2082, loss: 2.0146, grad_norm: 26.5602
2025-06-11 14:29:05,348 - mmdet - INFO - Epoch [5][100/7033]	lr: 5.015e-05, eta: 6:25:39, time: 2.033, data_time: 0.066, memory: 17624, loss_cls: 0.0648, loss_bbox: 0.1961, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3094, d1.loss_cls: 0.1000, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2117, d3.loss_cls: 0.0703, d3.loss_bbox: 0.2073, d4.loss_cls: 0.0652, d4.loss_bbox: 0.2001, loss: 1.9041, grad_norm: 117.0106
2025-06-11 14:30:22,635 - mmdet - INFO - Epoch [5][150/7033]	lr: 5.015e-05, eta: 6:24:13, time: 1.546, data_time: 0.069, memory: 17624, loss_cls: 0.0764, loss_bbox: 0.2027, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2150, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2066, loss: 2.0167, grad_norm: 24.6389
2025-06-11 14:31:43,615 - mmdet - INFO - Epoch [5][200/7033]	lr: 5.015e-05, eta: 6:22:49, time: 1.619, data_time: 0.081, memory: 17624, loss_cls: 0.0684, loss_bbox: 0.1984, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3139, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0711, d4.loss_bbox: 0.1997, loss: 1.9347, grad_norm: 33.4114
2025-06-11 14:33:05,570 - mmdet - INFO - Epoch [5][250/7033]	lr: 5.015e-05, eta: 6:21:26, time: 1.639, data_time: 0.068, memory: 17624, loss_cls: 0.0656, loss_bbox: 0.2016, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3151, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2044, loss: 1.9387, grad_norm: 20.4560
2025-06-11 14:34:25,646 - mmdet - INFO - Epoch [5][300/7033]	lr: 5.015e-05, eta: 6:20:02, time: 1.602, data_time: 0.060, memory: 17624, loss_cls: 0.0692, loss_bbox: 0.2046, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3167, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2068, loss: 1.9747, grad_norm: 21.0137
2025-06-11 14:35:45,373 - mmdet - INFO - Epoch [5][350/7033]	lr: 5.015e-05, eta: 6:18:38, time: 1.594, data_time: 0.069, memory: 17624, loss_cls: 0.0653, loss_bbox: 0.1997, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1025, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2017, loss: 1.9424, grad_norm: 23.7927
2025-06-11 14:37:04,364 - mmdet - INFO - Epoch [5][400/7033]	lr: 5.015e-05, eta: 6:17:13, time: 1.580, data_time: 0.063, memory: 17624, loss_cls: 0.0573, loss_bbox: 0.1921, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2291, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2061, d3.loss_cls: 0.0634, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0600, d4.loss_bbox: 0.1931, loss: 1.8397, grad_norm: 21.1816
2025-06-11 14:38:24,044 - mmdet - INFO - Epoch [5][450/7033]	lr: 5.015e-05, eta: 6:15:49, time: 1.594, data_time: 0.062, memory: 17624, loss_cls: 0.0705, loss_bbox: 0.2005, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3102, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0872, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2034, loss: 1.9529, grad_norm: 35.6977
2025-06-11 14:39:46,900 - mmdet - INFO - Epoch [5][500/7033]	lr: 5.015e-05, eta: 6:14:26, time: 1.657, data_time: 0.064, memory: 17624, loss_cls: 0.0720, loss_bbox: 0.2051, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2072, loss: 2.0263, grad_norm: 25.9628
2025-06-11 14:41:08,381 - mmdet - INFO - Epoch [5][550/7033]	lr: 5.015e-05, eta: 6:13:02, time: 1.628, data_time: 0.069, memory: 17624, loss_cls: 0.0675, loss_bbox: 0.1932, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1953, loss: 1.8968, grad_norm: 25.5523
2025-06-11 14:42:31,229 - mmdet - INFO - Epoch [5][600/7033]	lr: 5.015e-05, eta: 6:11:40, time: 1.659, data_time: 0.079, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.1976, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3187, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2100, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2003, loss: 1.9441, grad_norm: 48.2891
2025-06-11 14:45:14,694 - mmdet - INFO - Epoch [5][650/7033]	lr: 5.015e-05, eta: 6:10:54, time: 3.269, data_time: 1.710, memory: 17624, loss_cls: 0.0668, loss_bbox: 0.2077, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3203, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2112, loss: 1.9842, grad_norm: 82.9278
2025-06-11 14:46:37,630 - mmdet - INFO - Epoch [5][700/7033]	lr: 5.015e-05, eta: 6:09:31, time: 1.659, data_time: 0.083, memory: 17624, loss_cls: 0.0607, loss_bbox: 0.1957, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3146, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1987, loss: 1.8880, grad_norm: 33.9795
2025-06-11 14:47:58,042 - mmdet - INFO - Epoch [5][750/7033]	lr: 5.015e-05, eta: 6:08:07, time: 1.608, data_time: 0.085, memory: 17624, loss_cls: 0.0623, loss_bbox: 0.1924, d0.loss_cls: 0.1573, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2299, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2079, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1961, loss: 1.8696, grad_norm: 14.3623
2025-06-11 14:49:17,933 - mmdet - INFO - Epoch [5][800/7033]	lr: 5.015e-05, eta: 6:06:43, time: 1.598, data_time: 0.065, memory: 17624, loss_cls: 0.0639, loss_bbox: 0.1978, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3207, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0656, d4.loss_bbox: 0.2007, loss: 1.9162, grad_norm: 21.4003
2025-06-11 14:50:39,355 - mmdet - INFO - Epoch [5][850/7033]	lr: 5.015e-05, eta: 6:05:19, time: 1.629, data_time: 0.076, memory: 17624, loss_cls: 0.0667, loss_bbox: 0.2021, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3226, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2048, loss: 1.9520, grad_norm: 23.2056
2025-06-11 14:52:00,227 - mmdet - INFO - Epoch [5][900/7033]	lr: 5.015e-05, eta: 6:03:56, time: 1.617, data_time: 0.065, memory: 17624, loss_cls: 0.0649, loss_bbox: 0.1990, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3084, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2104, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2018, loss: 1.9204, grad_norm: 29.7170
2025-06-11 14:53:21,952 - mmdet - INFO - Epoch [5][950/7033]	lr: 5.015e-05, eta: 6:02:32, time: 1.635, data_time: 0.065, memory: 17624, loss_cls: 0.0555, loss_bbox: 0.1851, d0.loss_cls: 0.1506, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0862, d1.loss_bbox: 0.2288, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2039, d3.loss_cls: 0.0605, d3.loss_bbox: 0.1975, d4.loss_cls: 0.0565, d4.loss_bbox: 0.1888, loss: 1.7958, grad_norm: 27.9123
2025-06-11 14:54:43,722 - mmdet - INFO - Epoch [5][1000/7033]	lr: 5.015e-05, eta: 6:01:09, time: 1.635, data_time: 0.070, memory: 17624, loss_cls: 0.0693, loss_bbox: 0.1986, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3150, d1.loss_cls: 0.1011, d1.loss_bbox: 0.2385, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2015, loss: 1.9377, grad_norm: 24.3280
2025-06-11 14:56:02,873 - mmdet - INFO - Epoch [5][1050/7033]	lr: 5.015e-05, eta: 5:59:44, time: 1.583, data_time: 0.062, memory: 17624, loss_cls: 0.0629, loss_bbox: 0.1961, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3173, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0627, d4.loss_bbox: 0.2009, loss: 1.9127, grad_norm: 34.9323
2025-06-11 14:57:24,246 - mmdet - INFO - Epoch [5][1100/7033]	lr: 5.015e-05, eta: 5:58:20, time: 1.628, data_time: 0.067, memory: 17624, loss_cls: 0.0630, loss_bbox: 0.1939, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3129, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0651, d4.loss_bbox: 0.1977, loss: 1.8774, grad_norm: 20.7550
2025-06-11 14:58:44,589 - mmdet - INFO - Epoch [5][1150/7033]	lr: 5.015e-05, eta: 5:56:56, time: 1.607, data_time: 0.061, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1925, d0.loss_cls: 0.1449, d0.loss_bbox: 0.3064, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2308, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1953, loss: 1.8484, grad_norm: 16.3888
2025-06-11 15:00:04,064 - mmdet - INFO - Epoch [5][1200/7033]	lr: 5.015e-05, eta: 5:55:32, time: 1.590, data_time: 0.065, memory: 17624, loss_cls: 0.0641, loss_bbox: 0.1919, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3061, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2271, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1949, loss: 1.8623, grad_norm: 24.8411
2025-06-11 15:01:23,720 - mmdet - INFO - Epoch [5][1250/7033]	lr: 5.015e-05, eta: 5:54:08, time: 1.593, data_time: 0.065, memory: 17624, loss_cls: 0.0639, loss_bbox: 0.1987, d0.loss_cls: 0.1581, d0.loss_bbox: 0.3154, d1.loss_cls: 0.0955, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0640, d4.loss_bbox: 0.2031, loss: 1.9094, grad_norm: 17.7591
2025-06-11 15:02:45,628 - mmdet - INFO - Epoch [5][1300/7033]	lr: 5.015e-05, eta: 5:52:44, time: 1.638, data_time: 0.062, memory: 17624, loss_cls: 0.0649, loss_bbox: 0.1972, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3104, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0670, d4.loss_bbox: 0.2007, loss: 1.8980, grad_norm: 21.2474
2025-06-11 15:04:04,049 - mmdet - INFO - Epoch [5][1350/7033]	lr: 5.015e-05, eta: 5:51:19, time: 1.568, data_time: 0.065, memory: 17624, loss_cls: 0.0709, loss_bbox: 0.1997, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3167, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2035, loss: 1.9567, grad_norm: 19.1872
2025-06-11 15:05:25,128 - mmdet - INFO - Epoch [5][1400/7033]	lr: 5.015e-05, eta: 5:49:56, time: 1.622, data_time: 0.062, memory: 17624, loss_cls: 0.0592, loss_bbox: 0.1878, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2267, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2037, d3.loss_cls: 0.0668, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1903, loss: 1.8274, grad_norm: 29.9454
2025-06-11 15:06:42,327 - mmdet - INFO - Epoch [5][1450/7033]	lr: 5.015e-05, eta: 5:48:30, time: 1.544, data_time: 0.066, memory: 17624, loss_cls: 0.0691, loss_bbox: 0.2075, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3235, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2503, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2101, loss: 2.0016, grad_norm: 32.4615
2025-06-11 15:08:03,473 - mmdet - INFO - Epoch [5][1500/7033]	lr: 5.015e-05, eta: 5:47:07, time: 1.623, data_time: 0.063, memory: 17624, loss_cls: 0.0611, loss_bbox: 0.1982, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0634, d4.loss_bbox: 0.2002, loss: 1.8855, grad_norm: 22.8608
2025-06-11 15:09:24,418 - mmdet - INFO - Epoch [5][1550/7033]	lr: 5.015e-05, eta: 5:45:43, time: 1.619, data_time: 0.065, memory: 17624, loss_cls: 0.0661, loss_bbox: 0.2032, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2062, loss: 1.9576, grad_norm: 21.0266
2025-06-11 15:10:45,853 - mmdet - INFO - Epoch [5][1600/7033]	lr: 5.015e-05, eta: 5:44:20, time: 1.629, data_time: 0.107, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.1996, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3117, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2339, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2020, loss: 1.9185, grad_norm: 20.8162
2025-06-11 15:12:05,953 - mmdet - INFO - Epoch [5][1650/7033]	lr: 5.015e-05, eta: 5:42:56, time: 1.602, data_time: 0.063, memory: 17624, loss_cls: 0.0654, loss_bbox: 0.2041, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3204, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2090, loss: 1.9720, grad_norm: 19.5833
2025-06-11 15:13:24,734 - mmdet - INFO - Epoch [5][1700/7033]	lr: 5.015e-05, eta: 5:41:31, time: 1.575, data_time: 0.054, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1967, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0626, d4.loss_bbox: 0.1987, loss: 1.8739, grad_norm: 21.3741
2025-06-11 15:14:43,699 - mmdet - INFO - Epoch [5][1750/7033]	lr: 5.015e-05, eta: 5:40:07, time: 1.580, data_time: 0.054, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.1961, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3069, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0707, d4.loss_bbox: 0.1984, loss: 1.9166, grad_norm: 26.6484
2025-06-11 15:16:06,951 - mmdet - INFO - Epoch [5][1800/7033]	lr: 5.015e-05, eta: 5:38:44, time: 1.665, data_time: 0.064, memory: 17624, loss_cls: 0.0647, loss_bbox: 0.2018, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3169, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2046, loss: 1.9336, grad_norm: 651.0016
2025-06-11 15:17:27,230 - mmdet - INFO - Epoch [5][1850/7033]	lr: 5.015e-05, eta: 5:37:20, time: 1.605, data_time: 0.068, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1939, d0.loss_cls: 0.1476, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0901, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0648, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1974, loss: 1.8442, grad_norm: 18.6881
2025-06-11 15:18:49,992 - mmdet - INFO - Epoch [5][1900/7033]	lr: 5.015e-05, eta: 5:35:57, time: 1.655, data_time: 0.062, memory: 17624, loss_cls: 0.0669, loss_bbox: 0.2085, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3263, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2488, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2197, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2108, loss: 1.9852, grad_norm: 29.9819
2025-06-11 15:20:08,703 - mmdet - INFO - Epoch [5][1950/7033]	lr: 5.015e-05, eta: 5:34:33, time: 1.572, data_time: 0.062, memory: 17624, loss_cls: 0.0653, loss_bbox: 0.2025, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3164, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2135, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2058, loss: 1.9527, grad_norm: 24.5742
2025-06-11 15:21:29,526 - mmdet - INFO - Epoch [5][2000/7033]	lr: 5.015e-05, eta: 5:33:09, time: 1.618, data_time: 0.058, memory: 17624, loss_cls: 0.0653, loss_bbox: 0.2005, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3180, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2041, loss: 1.9244, grad_norm: 46.2388
2025-06-11 15:22:52,177 - mmdet - INFO - Epoch [5][2050/7033]	lr: 5.015e-05, eta: 5:31:46, time: 1.653, data_time: 0.059, memory: 17624, loss_cls: 0.0669, loss_bbox: 0.1999, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3086, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0690, d4.loss_bbox: 0.2017, loss: 1.9221, grad_norm: 35.5354
2025-06-11 15:24:13,201 - mmdet - INFO - Epoch [5][2100/7033]	lr: 5.015e-05, eta: 5:30:23, time: 1.621, data_time: 0.071, memory: 17624, loss_cls: 0.0667, loss_bbox: 0.1964, d0.loss_cls: 0.1568, d0.loss_bbox: 0.3265, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0684, d4.loss_bbox: 0.1992, loss: 1.9324, grad_norm: 25.1728
2025-06-11 15:25:36,171 - mmdet - INFO - Epoch [5][2150/7033]	lr: 5.015e-05, eta: 5:29:00, time: 1.659, data_time: 0.062, memory: 17624, loss_cls: 0.0657, loss_bbox: 0.1986, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2035, loss: 1.9228, grad_norm: 19.2999
2025-06-11 15:27:01,128 - mmdet - INFO - Epoch [5][2200/7033]	lr: 5.015e-05, eta: 5:27:38, time: 1.699, data_time: 0.074, memory: 17624, loss_cls: 0.0661, loss_bbox: 0.1965, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0680, d4.loss_bbox: 0.1994, loss: 1.9006, grad_norm: 54.7137
2025-06-11 15:28:22,233 - mmdet - INFO - Epoch [5][2250/7033]	lr: 5.015e-05, eta: 5:26:14, time: 1.622, data_time: 0.074, memory: 17624, loss_cls: 0.0609, loss_bbox: 0.1967, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1993, loss: 1.8713, grad_norm: 17.9241
2025-06-11 15:29:43,347 - mmdet - INFO - Epoch [5][2300/7033]	lr: 5.015e-05, eta: 5:24:51, time: 1.622, data_time: 0.066, memory: 17624, loss_cls: 0.0687, loss_bbox: 0.1957, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3218, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0708, d4.loss_bbox: 0.1991, loss: 1.9303, grad_norm: 21.8728
2025-06-11 15:31:03,436 - mmdet - INFO - Epoch [5][2350/7033]	lr: 5.015e-05, eta: 5:23:27, time: 1.602, data_time: 0.073, memory: 17624, loss_cls: 0.0648, loss_bbox: 0.1974, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1990, loss: 1.8822, grad_norm: 19.3526
2025-06-11 15:32:25,997 - mmdet - INFO - Epoch [5][2400/7033]	lr: 5.015e-05, eta: 5:22:04, time: 1.651, data_time: 0.070, memory: 17624, loss_cls: 0.0637, loss_bbox: 0.1945, d0.loss_cls: 0.1522, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0651, d4.loss_bbox: 0.1966, loss: 1.8813, grad_norm: 35.5261
2025-06-11 15:33:47,422 - mmdet - INFO - Epoch [5][2450/7033]	lr: 5.015e-05, eta: 5:20:41, time: 1.629, data_time: 0.061, memory: 17624, loss_cls: 0.0662, loss_bbox: 0.2036, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3246, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0676, d4.loss_bbox: 0.2053, loss: 1.9581, grad_norm: 27.5145
2025-06-11 15:35:13,525 - mmdet - INFO - Epoch [5][2500/7033]	lr: 5.015e-05, eta: 5:19:19, time: 1.722, data_time: 0.073, memory: 17624, loss_cls: 0.0585, loss_bbox: 0.1888, d0.loss_cls: 0.1460, d0.loss_bbox: 0.2990, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2248, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2033, d3.loss_cls: 0.0634, d3.loss_bbox: 0.1997, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1918, loss: 1.7982, grad_norm: 23.9243
2025-06-11 15:36:34,677 - mmdet - INFO - Epoch [5][2550/7033]	lr: 5.015e-05, eta: 5:17:56, time: 1.623, data_time: 0.069, memory: 17624, loss_cls: 0.0633, loss_bbox: 0.1919, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3086, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2071, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2026, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1943, loss: 1.8788, grad_norm: 22.9227
2025-06-11 15:37:54,849 - mmdet - INFO - Epoch [5][2600/7033]	lr: 5.015e-05, eta: 5:16:32, time: 1.603, data_time: 0.062, memory: 17624, loss_cls: 0.0731, loss_bbox: 0.1986, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3181, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2104, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2012, loss: 1.9708, grad_norm: 25.5791
2025-06-11 15:39:16,189 - mmdet - INFO - Epoch [5][2650/7033]	lr: 5.015e-05, eta: 5:15:08, time: 1.627, data_time: 0.081, memory: 17624, loss_cls: 0.0657, loss_bbox: 0.1954, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3114, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2339, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2090, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0683, d4.loss_bbox: 0.1988, loss: 1.9019, grad_norm: 55.2909
2025-06-11 15:40:35,829 - mmdet - INFO - Epoch [5][2700/7033]	lr: 5.015e-05, eta: 5:13:44, time: 1.593, data_time: 0.066, memory: 17624, loss_cls: 0.0638, loss_bbox: 0.1939, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2040, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1960, loss: 1.8763, grad_norm: 46.7845
2025-06-11 15:41:57,695 - mmdet - INFO - Epoch [5][2750/7033]	lr: 5.015e-05, eta: 5:12:21, time: 1.637, data_time: 0.073, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.1994, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3117, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2101, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2016, loss: 1.9462, grad_norm: 50.4011
2025-06-11 15:43:19,238 - mmdet - INFO - Epoch [5][2800/7033]	lr: 5.015e-05, eta: 5:10:58, time: 1.631, data_time: 0.072, memory: 17624, loss_cls: 0.0679, loss_bbox: 0.2022, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3134, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2063, loss: 1.9424, grad_norm: 35.9379
2025-06-11 15:44:41,190 - mmdet - INFO - Epoch [5][2850/7033]	lr: 5.015e-05, eta: 5:09:35, time: 1.638, data_time: 0.066, memory: 17624, loss_cls: 0.0740, loss_bbox: 0.2024, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2064, loss: 1.9894, grad_norm: 27.7773
2025-06-11 15:46:01,290 - mmdet - INFO - Epoch [5][2900/7033]	lr: 5.015e-05, eta: 5:08:11, time: 1.603, data_time: 0.072, memory: 17624, loss_cls: 0.0604, loss_bbox: 0.1893, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3055, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2250, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2053, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2004, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1920, loss: 1.8286, grad_norm: 48.3164
2025-06-11 15:47:21,395 - mmdet - INFO - Epoch [5][2950/7033]	lr: 5.015e-05, eta: 5:06:47, time: 1.602, data_time: 0.077, memory: 17624, loss_cls: 0.0697, loss_bbox: 0.2071, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3244, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2096, loss: 2.0013, grad_norm: 20.2860
2025-06-11 15:48:42,439 - mmdet - INFO - Epoch [5][3000/7033]	lr: 5.015e-05, eta: 5:05:24, time: 1.621, data_time: 0.066, memory: 17624, loss_cls: 0.0638, loss_bbox: 0.1916, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3068, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2299, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2066, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1937, loss: 1.8621, grad_norm: 31.4323
2025-06-11 15:50:02,227 - mmdet - INFO - Epoch [5][3050/7033]	lr: 5.015e-05, eta: 5:04:00, time: 1.596, data_time: 0.058, memory: 17624, loss_cls: 0.0600, loss_bbox: 0.1994, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3143, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0632, d4.loss_bbox: 0.2020, loss: 1.8905, grad_norm: 33.2528
2025-06-11 15:51:24,257 - mmdet - INFO - Epoch [5][3100/7033]	lr: 5.015e-05, eta: 5:02:37, time: 1.641, data_time: 0.064, memory: 17624, loss_cls: 0.0702, loss_bbox: 0.2048, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2070, loss: 1.9931, grad_norm: 22.7181
2025-06-11 15:53:00,720 - mmdet - INFO - Epoch [5][3150/7033]	lr: 5.015e-05, eta: 5:01:19, time: 1.929, data_time: 0.237, memory: 17624, loss_cls: 0.0614, loss_bbox: 0.1967, d0.loss_cls: 0.1501, d0.loss_bbox: 0.3109, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2093, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1997, loss: 1.8802, grad_norm: 22.2627
2025-06-11 15:54:21,695 - mmdet - INFO - Epoch [5][3200/7033]	lr: 5.015e-05, eta: 4:59:56, time: 1.620, data_time: 0.062, memory: 17624, loss_cls: 0.0620, loss_bbox: 0.1920, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2253, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2059, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1951, loss: 1.8444, grad_norm: 28.0563
2025-06-11 15:55:43,982 - mmdet - INFO - Epoch [5][3250/7033]	lr: 5.015e-05, eta: 4:58:33, time: 1.645, data_time: 0.061, memory: 17624, loss_cls: 0.0633, loss_bbox: 0.1993, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3198, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2373, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2025, loss: 1.9270, grad_norm: 48.7346
2025-06-11 15:57:04,580 - mmdet - INFO - Epoch [5][3300/7033]	lr: 5.015e-05, eta: 4:57:09, time: 1.612, data_time: 0.059, memory: 17624, loss_cls: 0.0676, loss_bbox: 0.2058, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3206, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2086, loss: 1.9715, grad_norm: 29.2337
2025-06-11 15:58:26,506 - mmdet - INFO - Epoch [5][3350/7033]	lr: 5.015e-05, eta: 4:55:46, time: 1.637, data_time: 0.062, memory: 17624, loss_cls: 0.0626, loss_bbox: 0.2000, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3203, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0693, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0651, d4.loss_bbox: 0.2025, loss: 1.9208, grad_norm: 53.5105
2025-06-11 15:59:43,154 - mmdet - INFO - Epoch [5][3400/7033]	lr: 5.015e-05, eta: 4:54:21, time: 1.534, data_time: 0.057, memory: 17624, loss_cls: 0.0636, loss_bbox: 0.1947, d0.loss_cls: 0.1511, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0955, d1.loss_bbox: 0.2317, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0660, d4.loss_bbox: 0.1968, loss: 1.8714, grad_norm: 16.4270
2025-06-11 16:01:01,669 - mmdet - INFO - Epoch [5][3450/7033]	lr: 5.015e-05, eta: 4:52:57, time: 1.570, data_time: 0.060, memory: 17624, loss_cls: 0.0619, loss_bbox: 0.1970, d0.loss_cls: 0.1487, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0927, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0763, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1995, loss: 1.8648, grad_norm: 15.7694
2025-06-11 16:02:20,163 - mmdet - INFO - Epoch [5][3500/7033]	lr: 5.015e-05, eta: 4:51:33, time: 1.570, data_time: 0.057, memory: 17624, loss_cls: 0.0640, loss_bbox: 0.1951, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3027, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1980, loss: 1.8710, grad_norm: 23.1462
2025-06-11 16:03:36,361 - mmdet - INFO - Epoch [5][3550/7033]	lr: 5.015e-05, eta: 4:50:08, time: 1.524, data_time: 0.059, memory: 17624, loss_cls: 0.0628, loss_bbox: 0.1969, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3075, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0647, d4.loss_bbox: 0.1987, loss: 1.8862, grad_norm: 1056.1524
2025-06-11 16:06:44,428 - mmdet - INFO - Epoch [5][3600/7033]	lr: 5.015e-05, eta: 4:49:20, time: 3.761, data_time: 2.184, memory: 17624, loss_cls: 0.0637, loss_bbox: 0.1935, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3028, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1962, loss: 1.8790, grad_norm: 28.1185
2025-06-11 16:11:25,931 - mmdet - INFO - Epoch [5][3650/7033]	lr: 5.015e-05, eta: 4:49:02, time: 5.628, data_time: 4.035, memory: 17624, loss_cls: 0.0674, loss_bbox: 0.1959, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3149, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2335, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1998, loss: 1.9123, grad_norm: 35.7386
2025-06-11 16:12:44,071 - mmdet - INFO - Epoch [5][3700/7033]	lr: 5.015e-05, eta: 4:47:37, time: 1.564, data_time: 0.080, memory: 17624, loss_cls: 0.0618, loss_bbox: 0.1943, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2094, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1968, loss: 1.8750, grad_norm: 23.0778
2025-06-11 16:14:06,040 - mmdet - INFO - Epoch [5][3750/7033]	lr: 5.015e-05, eta: 4:46:13, time: 1.639, data_time: 0.080, memory: 17624, loss_cls: 0.0645, loss_bbox: 0.2068, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3199, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2169, d4.loss_cls: 0.0667, d4.loss_bbox: 0.2096, loss: 1.9638, grad_norm: 21.9429
2025-06-11 16:15:29,042 - mmdet - INFO - Epoch [5][3800/7033]	lr: 5.015e-05, eta: 4:44:50, time: 1.658, data_time: 0.080, memory: 17624, loss_cls: 0.0696, loss_bbox: 0.1920, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3049, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2297, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0708, d4.loss_bbox: 0.1952, loss: 1.9004, grad_norm: 21.6148
2025-06-11 16:16:59,264 - mmdet - INFO - Epoch [5][3850/7033]	lr: 5.015e-05, eta: 4:43:29, time: 1.804, data_time: 0.213, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1874, d0.loss_cls: 0.1506, d0.loss_bbox: 0.2996, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2241, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2033, d3.loss_cls: 0.0632, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1906, loss: 1.8075, grad_norm: 34.2412
2025-06-11 16:18:20,039 - mmdet - INFO - Epoch [5][3900/7033]	lr: 5.015e-05, eta: 4:42:05, time: 1.617, data_time: 0.093, memory: 17624, loss_cls: 0.0673, loss_bbox: 0.1989, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3163, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2022, loss: 1.9319, grad_norm: 34.8687
2025-06-11 16:19:41,163 - mmdet - INFO - Epoch [5][3950/7033]	lr: 5.015e-05, eta: 4:40:41, time: 1.622, data_time: 0.071, memory: 17624, loss_cls: 0.0625, loss_bbox: 0.1943, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2356, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0652, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1970, loss: 1.8765, grad_norm: 43.5198
2025-06-11 16:21:04,628 - mmdet - INFO - Epoch [5][4000/7033]	lr: 5.015e-05, eta: 4:39:18, time: 1.669, data_time: 0.074, memory: 17624, loss_cls: 0.0663, loss_bbox: 0.1944, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3094, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1959, loss: 1.8941, grad_norm: 54.8606
2025-06-11 16:22:26,501 - mmdet - INFO - Epoch [5][4050/7033]	lr: 5.015e-05, eta: 4:37:54, time: 1.637, data_time: 0.091, memory: 17624, loss_cls: 0.0677, loss_bbox: 0.1965, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2067, d4.loss_cls: 0.0696, d4.loss_bbox: 0.1984, loss: 1.9114, grad_norm: 25.9715
2025-06-11 16:23:47,966 - mmdet - INFO - Epoch [5][4100/7033]	lr: 5.015e-05, eta: 4:36:30, time: 1.629, data_time: 0.083, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.1976, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2104, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2011, loss: 1.9367, grad_norm: 26.2678
2025-06-11 16:25:12,806 - mmdet - INFO - Epoch [5][4150/7033]	lr: 5.015e-05, eta: 4:35:07, time: 1.697, data_time: 0.100, memory: 17624, loss_cls: 0.0619, loss_bbox: 0.1971, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2065, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1980, loss: 1.9025, grad_norm: 36.0128
2025-06-11 16:26:35,987 - mmdet - INFO - Epoch [5][4200/7033]	lr: 5.015e-05, eta: 4:33:44, time: 1.664, data_time: 0.057, memory: 17624, loss_cls: 0.0720, loss_bbox: 0.2024, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3205, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2463, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2057, loss: 1.9827, grad_norm: 24.2898
2025-06-11 16:27:58,257 - mmdet - INFO - Epoch [5][4250/7033]	lr: 5.015e-05, eta: 4:32:21, time: 1.645, data_time: 0.098, memory: 17624, loss_cls: 0.0673, loss_bbox: 0.2007, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3110, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2033, loss: 1.9399, grad_norm: 25.1475
2025-06-11 16:29:22,950 - mmdet - INFO - Epoch [5][4300/7033]	lr: 5.015e-05, eta: 4:30:58, time: 1.694, data_time: 0.097, memory: 17624, loss_cls: 0.0641, loss_bbox: 0.1927, d0.loss_cls: 0.1530, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2047, d4.loss_cls: 0.0660, d4.loss_bbox: 0.1956, loss: 1.8696, grad_norm: 20.1288
2025-06-11 16:30:38,749 - mmdet - INFO - Epoch [5][4350/7033]	lr: 5.015e-05, eta: 4:29:32, time: 1.516, data_time: 0.055, memory: 17624, loss_cls: 0.0567, loss_bbox: 0.1873, d0.loss_cls: 0.1480, d0.loss_bbox: 0.2945, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2220, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2027, d3.loss_cls: 0.0622, d3.loss_bbox: 0.1984, d4.loss_cls: 0.0577, d4.loss_bbox: 0.1909, loss: 1.7807, grad_norm: 28.4049
2025-06-11 16:31:58,749 - mmdet - INFO - Epoch [5][4400/7033]	lr: 5.015e-05, eta: 4:28:08, time: 1.600, data_time: 0.096, memory: 17624, loss_cls: 0.0695, loss_bbox: 0.1963, d0.loss_cls: 0.1581, d0.loss_bbox: 0.3060, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2079, d4.loss_cls: 0.0710, d4.loss_bbox: 0.1996, loss: 1.9233, grad_norm: 98.6684
2025-06-11 16:33:23,954 - mmdet - INFO - Epoch [5][4450/7033]	lr: 5.015e-05, eta: 4:26:46, time: 1.702, data_time: 0.119, memory: 17624, loss_cls: 0.0687, loss_bbox: 0.2023, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3144, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2036, loss: 1.9424, grad_norm: 31.4469
2025-06-11 16:34:42,695 - mmdet - INFO - Epoch [5][4500/7033]	lr: 5.015e-05, eta: 4:25:21, time: 1.577, data_time: 0.063, memory: 17624, loss_cls: 0.0624, loss_bbox: 0.1905, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3038, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2260, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1934, loss: 1.8397, grad_norm: 25.7090
2025-06-11 16:36:01,551 - mmdet - INFO - Epoch [5][4550/7033]	lr: 5.015e-05, eta: 4:23:57, time: 1.577, data_time: 0.063, memory: 17624, loss_cls: 0.0609, loss_bbox: 0.1970, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2067, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1994, loss: 1.8703, grad_norm: 21.7208
2025-06-11 16:37:24,219 - mmdet - INFO - Epoch [5][4600/7033]	lr: 5.015e-05, eta: 4:22:33, time: 1.653, data_time: 0.092, memory: 17624, loss_cls: 0.0673, loss_bbox: 0.1987, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3151, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2004, loss: 1.9228, grad_norm: 18.3765
2025-06-11 16:38:52,146 - mmdet - INFO - Epoch [5][4650/7033]	lr: 5.015e-05, eta: 4:21:11, time: 1.759, data_time: 0.079, memory: 17624, loss_cls: 0.0602, loss_bbox: 0.1908, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3116, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2297, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2059, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1941, loss: 1.8641, grad_norm: 25.3923
2025-06-11 16:40:10,520 - mmdet - INFO - Epoch [5][4700/7033]	lr: 5.015e-05, eta: 4:19:47, time: 1.567, data_time: 0.085, memory: 17624, loss_cls: 0.0636, loss_bbox: 0.1941, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3112, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1965, loss: 1.8846, grad_norm: 21.5915
2025-06-11 16:41:31,934 - mmdet - INFO - Epoch [5][4750/7033]	lr: 5.015e-05, eta: 4:18:23, time: 1.628, data_time: 0.072, memory: 17624, loss_cls: 0.0655, loss_bbox: 0.1935, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3009, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0672, d4.loss_bbox: 0.1966, loss: 1.8695, grad_norm: 26.4322
2025-06-11 16:42:56,555 - mmdet - INFO - Epoch [5][4800/7033]	lr: 5.015e-05, eta: 4:17:00, time: 1.692, data_time: 0.144, memory: 17624, loss_cls: 0.0661, loss_bbox: 0.1923, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3082, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1953, loss: 1.8744, grad_norm: 706.9690
2025-06-11 16:44:16,474 - mmdet - INFO - Epoch [5][4850/7033]	lr: 5.015e-05, eta: 4:15:36, time: 1.598, data_time: 0.083, memory: 17624, loss_cls: 0.0643, loss_bbox: 0.1983, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3165, d1.loss_cls: 0.0971, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2002, loss: 1.9139, grad_norm: 24.7272
2025-06-11 16:45:39,910 - mmdet - INFO - Epoch [5][4900/7033]	lr: 5.015e-05, eta: 4:14:13, time: 1.668, data_time: 0.117, memory: 17624, loss_cls: 0.0621, loss_bbox: 0.1940, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3132, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0693, d3.loss_bbox: 0.2052, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1963, loss: 1.8853, grad_norm: 29.2765
2025-06-11 16:46:59,546 - mmdet - INFO - Epoch [5][4950/7033]	lr: 5.015e-05, eta: 4:12:49, time: 1.594, data_time: 0.106, memory: 17624, loss_cls: 0.0600, loss_bbox: 0.1857, d0.loss_cls: 0.1519, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2245, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2006, d3.loss_cls: 0.0671, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1895, loss: 1.8194, grad_norm: 31.5564
2025-06-11 16:48:21,470 - mmdet - INFO - Epoch [5][5000/7033]	lr: 5.015e-05, eta: 4:11:25, time: 1.639, data_time: 0.062, memory: 17624, loss_cls: 0.0545, loss_bbox: 0.1869, d0.loss_cls: 0.1452, d0.loss_bbox: 0.2997, d1.loss_cls: 0.0874, d1.loss_bbox: 0.2240, d2.loss_cls: 0.0712, d2.loss_bbox: 0.2018, d3.loss_cls: 0.0605, d3.loss_bbox: 0.1973, d4.loss_cls: 0.0567, d4.loss_bbox: 0.1901, loss: 1.7753, grad_norm: 18.7344
2025-06-11 16:49:44,947 - mmdet - INFO - Epoch [5][5050/7033]	lr: 5.015e-05, eta: 4:10:02, time: 1.670, data_time: 0.092, memory: 17624, loss_cls: 0.0576, loss_bbox: 0.1901, d0.loss_cls: 0.1523, d0.loss_bbox: 0.2966, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2241, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2031, d3.loss_cls: 0.0621, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0593, d4.loss_bbox: 0.1931, loss: 1.8024, grad_norm: 34.6805
2025-06-11 16:51:07,606 - mmdet - INFO - Epoch [5][5100/7033]	lr: 5.015e-05, eta: 4:08:39, time: 1.653, data_time: 0.075, memory: 17624, loss_cls: 0.0675, loss_bbox: 0.1977, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0685, d4.loss_bbox: 0.2015, loss: 1.9349, grad_norm: 23.4523
2025-06-11 16:52:27,169 - mmdet - INFO - Epoch [5][5150/7033]	lr: 5.015e-05, eta: 4:07:15, time: 1.591, data_time: 0.121, memory: 17624, loss_cls: 0.0636, loss_bbox: 0.1976, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2330, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2121, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0657, d4.loss_bbox: 0.1990, loss: 1.8923, grad_norm: 37.4872
2025-06-11 16:53:49,938 - mmdet - INFO - Epoch [5][5200/7033]	lr: 5.015e-05, eta: 4:05:51, time: 1.655, data_time: 0.082, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1939, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3079, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2092, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1950, loss: 1.8633, grad_norm: 29.9338
2025-06-11 16:55:26,873 - mmdet - INFO - Epoch [5][5250/7033]	lr: 5.015e-05, eta: 4:04:32, time: 1.939, data_time: 0.098, memory: 17624, loss_cls: 0.0609, loss_bbox: 0.1970, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3192, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0620, d4.loss_bbox: 0.2010, loss: 1.9011, grad_norm: 28.2511
2025-06-11 16:56:50,458 - mmdet - INFO - Epoch [5][5300/7033]	lr: 5.015e-05, eta: 4:03:09, time: 1.672, data_time: 0.102, memory: 17624, loss_cls: 0.0595, loss_bbox: 0.1912, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3063, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2293, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2024, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1939, loss: 1.8412, grad_norm: 20.7319
2025-06-11 16:58:12,555 - mmdet - INFO - Epoch [5][5350/7033]	lr: 5.015e-05, eta: 4:01:45, time: 1.641, data_time: 0.100, memory: 17624, loss_cls: 0.0717, loss_bbox: 0.2113, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2142, loss: 2.0286, grad_norm: 24.5702
2025-06-11 16:59:33,482 - mmdet - INFO - Epoch [5][5400/7033]	lr: 5.015e-05, eta: 4:00:21, time: 1.619, data_time: 0.067, memory: 17624, loss_cls: 0.0600, loss_bbox: 0.1938, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3077, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2074, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2036, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1963, loss: 1.8491, grad_norm: 21.8294
2025-06-11 17:00:56,684 - mmdet - INFO - Epoch [5][5450/7033]	lr: 5.015e-05, eta: 3:58:58, time: 1.664, data_time: 0.057, memory: 17624, loss_cls: 0.0680, loss_bbox: 0.1977, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3101, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2001, loss: 1.9214, grad_norm: 34.5969
2025-06-11 17:02:19,742 - mmdet - INFO - Epoch [5][5500/7033]	lr: 5.015e-05, eta: 3:57:35, time: 1.661, data_time: 0.076, memory: 17624, loss_cls: 0.0674, loss_bbox: 0.2026, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3156, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2059, loss: 1.9573, grad_norm: 29.3983
2025-06-11 17:03:42,488 - mmdet - INFO - Epoch [5][5550/7033]	lr: 5.015e-05, eta: 3:56:11, time: 1.655, data_time: 0.061, memory: 17624, loss_cls: 0.0633, loss_bbox: 0.1923, d0.loss_cls: 0.1517, d0.loss_bbox: 0.3083, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2289, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2034, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1948, loss: 1.8596, grad_norm: 26.9452
2025-06-11 17:05:11,799 - mmdet - INFO - Epoch [5][5600/7033]	lr: 5.015e-05, eta: 3:54:50, time: 1.786, data_time: 0.104, memory: 17624, loss_cls: 0.0657, loss_bbox: 0.1921, d0.loss_cls: 0.1549, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0686, d4.loss_bbox: 0.1943, loss: 1.8825, grad_norm: 61.7880
2025-06-11 17:06:40,442 - mmdet - INFO - Epoch [5][5650/7033]	lr: 5.015e-05, eta: 3:53:28, time: 1.773, data_time: 0.063, memory: 17624, loss_cls: 0.0645, loss_bbox: 0.1913, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2303, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1937, loss: 1.8790, grad_norm: 71.7073
2025-06-11 17:08:03,927 - mmdet - INFO - Epoch [5][5700/7033]	lr: 5.015e-05, eta: 3:52:05, time: 1.670, data_time: 0.103, memory: 17624, loss_cls: 0.0671, loss_bbox: 0.2015, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3090, d1.loss_cls: 0.1000, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0685, d4.loss_bbox: 0.2043, loss: 1.9311, grad_norm: 28.7336
2025-06-11 17:09:26,594 - mmdet - INFO - Epoch [5][5750/7033]	lr: 5.015e-05, eta: 3:50:41, time: 1.653, data_time: 0.069, memory: 17624, loss_cls: 0.0710, loss_bbox: 0.1981, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3110, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2084, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2011, loss: 1.9398, grad_norm: 33.0618
2025-06-11 17:10:47,428 - mmdet - INFO - Epoch [5][5800/7033]	lr: 5.015e-05, eta: 3:49:18, time: 1.616, data_time: 0.065, memory: 17624, loss_cls: 0.0580, loss_bbox: 0.1854, d0.loss_cls: 0.1430, d0.loss_bbox: 0.2987, d1.loss_cls: 0.0920, d1.loss_bbox: 0.2237, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2010, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1964, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1894, loss: 1.7913, grad_norm: 144.1326
2025-06-11 17:12:09,557 - mmdet - INFO - Epoch [5][5850/7033]	lr: 5.015e-05, eta: 3:47:54, time: 1.643, data_time: 0.101, memory: 17624, loss_cls: 0.0669, loss_bbox: 0.1989, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2125, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2024, loss: 1.9664, grad_norm: 20.4658
2025-06-11 17:13:34,522 - mmdet - INFO - Epoch [5][5900/7033]	lr: 5.015e-05, eta: 3:46:31, time: 1.700, data_time: 0.119, memory: 17624, loss_cls: 0.0665, loss_bbox: 0.1974, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3128, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1995, loss: 1.9153, grad_norm: 21.2036
2025-06-11 17:14:57,954 - mmdet - INFO - Epoch [5][5950/7033]	lr: 5.015e-05, eta: 3:45:08, time: 1.668, data_time: 0.064, memory: 17624, loss_cls: 0.0630, loss_bbox: 0.1963, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1995, loss: 1.8991, grad_norm: 107.8808
2025-06-11 17:16:20,358 - mmdet - INFO - Epoch [5][6000/7033]	lr: 5.015e-05, eta: 3:43:45, time: 1.648, data_time: 0.103, memory: 17624, loss_cls: 0.0669, loss_bbox: 0.1889, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1916, loss: 1.8579, grad_norm: 19.5354
2025-06-11 17:17:44,745 - mmdet - INFO - Epoch [5][6050/7033]	lr: 5.015e-05, eta: 3:42:22, time: 1.686, data_time: 0.074, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1946, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3066, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0652, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1989, loss: 1.8532, grad_norm: 27.8435
2025-06-11 17:19:02,855 - mmdet - INFO - Epoch [5][6100/7033]	lr: 5.015e-05, eta: 3:40:57, time: 1.564, data_time: 0.103, memory: 17624, loss_cls: 0.0647, loss_bbox: 0.1969, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0670, d4.loss_bbox: 0.1995, loss: 1.8824, grad_norm: 14.8686
2025-06-11 17:20:21,227 - mmdet - INFO - Epoch [5][6150/7033]	lr: 5.015e-05, eta: 3:39:33, time: 1.567, data_time: 0.079, memory: 17624, loss_cls: 0.0724, loss_bbox: 0.2027, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2169, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2139, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2060, loss: 1.9757, grad_norm: 21.2716
2025-06-11 17:21:43,057 - mmdet - INFO - Epoch [5][6200/7033]	lr: 5.015e-05, eta: 3:38:10, time: 1.636, data_time: 0.056, memory: 17624, loss_cls: 0.0630, loss_bbox: 0.1991, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2359, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0634, d4.loss_bbox: 0.2026, loss: 1.9006, grad_norm: 41.9378
2025-06-11 17:23:01,773 - mmdet - INFO - Epoch [5][6250/7033]	lr: 5.015e-05, eta: 3:36:45, time: 1.575, data_time: 0.052, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.2015, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2045, loss: 1.9211, grad_norm: 54.4168
2025-06-11 17:24:22,364 - mmdet - INFO - Epoch [5][6300/7033]	lr: 5.015e-05, eta: 3:35:21, time: 1.610, data_time: 0.059, memory: 17624, loss_cls: 0.0667, loss_bbox: 0.1941, d0.loss_cls: 0.1556, d0.loss_bbox: 0.2944, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2253, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1958, loss: 1.8638, grad_norm: 92.2451
2025-06-11 17:25:44,126 - mmdet - INFO - Epoch [5][6350/7033]	lr: 5.015e-05, eta: 3:33:58, time: 1.637, data_time: 0.060, memory: 17624, loss_cls: 0.0635, loss_bbox: 0.1927, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1959, loss: 1.8687, grad_norm: 19.1730
2025-06-11 17:27:04,969 - mmdet - INFO - Epoch [5][6400/7033]	lr: 5.015e-05, eta: 3:32:34, time: 1.616, data_time: 0.051, memory: 17624, loss_cls: 0.0654, loss_bbox: 0.1912, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3050, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2057, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1930, loss: 1.8709, grad_norm: 28.4333
2025-06-11 17:28:29,596 - mmdet - INFO - Epoch [5][6450/7033]	lr: 5.015e-05, eta: 3:31:11, time: 1.693, data_time: 0.061, memory: 17624, loss_cls: 0.0640, loss_bbox: 0.1874, d0.loss_cls: 0.1499, d0.loss_bbox: 0.3108, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2258, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2039, d3.loss_cls: 0.0682, d3.loss_bbox: 0.1985, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1912, loss: 1.8368, grad_norm: 32.8153
2025-06-11 17:29:52,535 - mmdet - INFO - Epoch [5][6500/7033]	lr: 5.015e-05, eta: 3:29:48, time: 1.659, data_time: 0.075, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.1931, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3187, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2055, d4.loss_cls: 0.0669, d4.loss_bbox: 0.1970, loss: 1.9009, grad_norm: 16.9801
2025-06-11 17:31:16,325 - mmdet - INFO - Epoch [5][6550/7033]	lr: 5.015e-05, eta: 3:28:25, time: 1.676, data_time: 0.092, memory: 17624, loss_cls: 0.0608, loss_bbox: 0.1901, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3113, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2059, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1921, loss: 1.8495, grad_norm: 186.0780
2025-06-11 17:32:40,010 - mmdet - INFO - Epoch [5][6600/7033]	lr: 5.015e-05, eta: 3:27:02, time: 1.674, data_time: 0.064, memory: 17624, loss_cls: 0.0614, loss_bbox: 0.1986, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3194, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0773, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0614, d4.loss_bbox: 0.2038, loss: 1.9029, grad_norm: 23.9839
2025-06-11 17:34:00,121 - mmdet - INFO - Epoch [5][6650/7033]	lr: 5.015e-05, eta: 3:25:38, time: 1.602, data_time: 0.072, memory: 17624, loss_cls: 0.0616, loss_bbox: 0.1915, d0.loss_cls: 0.1573, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0628, d4.loss_bbox: 0.1951, loss: 1.8639, grad_norm: 22.1760
2025-06-11 17:35:24,529 - mmdet - INFO - Epoch [5][6700/7033]	lr: 5.015e-05, eta: 3:24:15, time: 1.688, data_time: 0.057, memory: 17624, loss_cls: 0.0621, loss_bbox: 0.1979, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2005, loss: 1.8975, grad_norm: 22.1862
2025-06-11 17:36:44,676 - mmdet - INFO - Epoch [5][6750/7033]	lr: 5.015e-05, eta: 3:22:51, time: 1.603, data_time: 0.059, memory: 17624, loss_cls: 0.0529, loss_bbox: 0.1850, d0.loss_cls: 0.1475, d0.loss_bbox: 0.2976, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2177, d2.loss_cls: 0.0704, d2.loss_bbox: 0.2010, d3.loss_cls: 0.0593, d3.loss_bbox: 0.1955, d4.loss_cls: 0.0540, d4.loss_bbox: 0.1883, loss: 1.7582, grad_norm: 27.3077
2025-06-11 17:38:07,255 - mmdet - INFO - Epoch [5][6800/7033]	lr: 5.015e-05, eta: 3:21:28, time: 1.652, data_time: 0.066, memory: 17624, loss_cls: 0.0662, loss_bbox: 0.1944, d0.loss_cls: 0.1568, d0.loss_bbox: 0.3118, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1989, loss: 1.8965, grad_norm: 21.6173
2025-06-11 17:39:30,656 - mmdet - INFO - Epoch [5][6850/7033]	lr: 5.015e-05, eta: 3:20:05, time: 1.668, data_time: 0.055, memory: 17624, loss_cls: 0.0626, loss_bbox: 0.1998, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3086, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0643, d4.loss_bbox: 0.2029, loss: 1.8993, grad_norm: 22.4843
2025-06-11 17:40:51,896 - mmdet - INFO - Epoch [5][6900/7033]	lr: 5.015e-05, eta: 3:18:41, time: 1.625, data_time: 0.067, memory: 17624, loss_cls: 0.0643, loss_bbox: 0.2011, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2032, loss: 1.9140, grad_norm: 58.3391
2025-06-11 17:42:11,901 - mmdet - INFO - Epoch [5][6950/7033]	lr: 5.015e-05, eta: 3:17:18, time: 1.600, data_time: 0.067, memory: 17624, loss_cls: 0.0633, loss_bbox: 0.1982, d0.loss_cls: 0.1501, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0661, d4.loss_bbox: 0.2014, loss: 1.9001, grad_norm: 33.6544
2025-06-11 17:43:34,657 - mmdet - INFO - Epoch [5][7000/7033]	lr: 5.015e-05, eta: 3:15:54, time: 1.655, data_time: 0.055, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.1939, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3099, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0712, d4.loss_bbox: 0.1963, loss: 1.9029, grad_norm: 54.0038
2025-06-11 17:44:31,604 - mmdet - INFO - Saving checkpoint at 5 epochs
2025-06-11 18:23:05,053 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 18:23:05,054 - mmdet - INFO - Epoch(val) [5][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7978, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8856, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9111, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9232, pts_bbox_NuScenes/car_trans_err: 0.1729, pts_bbox_NuScenes/car_scale_err: 0.1485, pts_bbox_NuScenes/car_orient_err: 0.0404, pts_bbox_NuScenes/car_vel_err: 0.3270, pts_bbox_NuScenes/car_attr_err: 0.1885, pts_bbox_NuScenes/mATE: 0.2836, pts_bbox_NuScenes/mASE: 0.2602, pts_bbox_NuScenes/mAOE: 0.2587, pts_bbox_NuScenes/mAVE: 0.2969, pts_bbox_NuScenes/mAAE: 0.1865, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4289, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6126, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7210, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7549, pts_bbox_NuScenes/truck_trans_err: 0.3417, pts_bbox_NuScenes/truck_scale_err: 0.1952, pts_bbox_NuScenes/truck_orient_err: 0.0488, pts_bbox_NuScenes/truck_vel_err: 0.2760, pts_bbox_NuScenes/truck_attr_err: 0.1985, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0541, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2076, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4013, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4688, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6598, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4295, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8183, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1169, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3024, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5478, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7589, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8995, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9219, pts_bbox_NuScenes/bus_trans_err: 0.3235, pts_bbox_NuScenes/bus_scale_err: 0.1835, pts_bbox_NuScenes/bus_orient_err: 0.0425, pts_bbox_NuScenes/bus_vel_err: 0.5116, pts_bbox_NuScenes/bus_attr_err: 0.2722, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1860, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4347, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.6006, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6863, pts_bbox_NuScenes/trailer_trans_err: 0.4749, pts_bbox_NuScenes/trailer_scale_err: 0.2223, pts_bbox_NuScenes/trailer_orient_err: 0.4823, pts_bbox_NuScenes/trailer_vel_err: 0.2655, pts_bbox_NuScenes/trailer_attr_err: 0.1665, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6268, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7252, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7707, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7845, pts_bbox_NuScenes/barrier_trans_err: 0.2040, pts_bbox_NuScenes/barrier_scale_err: 0.2878, pts_bbox_NuScenes/barrier_orient_err: 0.0465, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6555, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7828, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8089, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8149, pts_bbox_NuScenes/motorcycle_trans_err: 0.2123, pts_bbox_NuScenes/motorcycle_scale_err: 0.2471, pts_bbox_NuScenes/motorcycle_orient_err: 0.2223, pts_bbox_NuScenes/motorcycle_vel_err: 0.4173, pts_bbox_NuScenes/motorcycle_attr_err: 0.2470, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5443, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5911, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5999, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6092, pts_bbox_NuScenes/bicycle_trans_err: 0.1740, pts_bbox_NuScenes/bicycle_scale_err: 0.2682, pts_bbox_NuScenes/bicycle_orient_err: 0.3057, pts_bbox_NuScenes/bicycle_vel_err: 0.2326, pts_bbox_NuScenes/bicycle_attr_err: 0.0057, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8173, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8567, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8763, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8888, pts_bbox_NuScenes/pedestrian_trans_err: 0.1417, pts_bbox_NuScenes/pedestrian_scale_err: 0.2943, pts_bbox_NuScenes/pedestrian_orient_err: 0.3218, pts_bbox_NuScenes/pedestrian_vel_err: 0.2284, pts_bbox_NuScenes/pedestrian_attr_err: 0.1115, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7429, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7780, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.8007, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8216, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1312, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3256, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7101, pts_bbox_NuScenes/mAP: 0.6775
2025-06-11 18:24:45,379 - mmdet - INFO - Epoch [6][50/7033]	lr: 1.358e-05, eta: 3:13:28, time: 1.935, data_time: 0.308, memory: 17624, loss_cls: 0.0657, loss_bbox: 0.1942, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1974, loss: 1.9136, grad_norm: 28.8761
2025-06-11 18:26:05,418 - mmdet - INFO - Epoch [6][100/7033]	lr: 1.358e-05, eta: 3:12:04, time: 1.601, data_time: 0.072, memory: 17624, loss_cls: 0.0610, loss_bbox: 0.1944, d0.loss_cls: 0.1498, d0.loss_bbox: 0.3110, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2313, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1960, loss: 1.8674, grad_norm: 37.6024
2025-06-11 18:27:24,475 - mmdet - INFO - Epoch [6][150/7033]	lr: 1.358e-05, eta: 3:10:40, time: 1.581, data_time: 0.067, memory: 17624, loss_cls: 0.0637, loss_bbox: 0.1925, d0.loss_cls: 0.1517, d0.loss_bbox: 0.3056, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2281, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1954, loss: 1.8588, grad_norm: 17.7287
2025-06-11 18:28:45,153 - mmdet - INFO - Epoch [6][200/7033]	lr: 1.358e-05, eta: 3:09:17, time: 1.613, data_time: 0.073, memory: 17624, loss_cls: 0.0688, loss_bbox: 0.1993, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3129, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2011, loss: 1.9356, grad_norm: 16.7200
2025-06-11 18:30:08,122 - mmdet - INFO - Epoch [6][250/7033]	lr: 1.358e-05, eta: 3:07:54, time: 1.659, data_time: 0.085, memory: 17624, loss_cls: 0.0571, loss_bbox: 0.1857, d0.loss_cls: 0.1433, d0.loss_bbox: 0.2965, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2217, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2009, d3.loss_cls: 0.0627, d3.loss_bbox: 0.1957, d4.loss_cls: 0.0580, d4.loss_bbox: 0.1888, loss: 1.7722, grad_norm: 22.5904
2025-06-11 18:31:30,520 - mmdet - INFO - Epoch [6][300/7033]	lr: 1.358e-05, eta: 3:06:30, time: 1.648, data_time: 0.063, memory: 17624, loss_cls: 0.0589, loss_bbox: 0.1905, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3014, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2252, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2044, d3.loss_cls: 0.0660, d3.loss_bbox: 0.1989, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1929, loss: 1.8296, grad_norm: 25.3241
2025-06-11 18:32:50,486 - mmdet - INFO - Epoch [6][350/7033]	lr: 1.358e-05, eta: 3:05:07, time: 1.599, data_time: 0.066, memory: 17624, loss_cls: 0.0575, loss_bbox: 0.1895, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0763, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0620, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0597, d4.loss_bbox: 0.1927, loss: 1.8251, grad_norm: 19.8885
2025-06-11 18:34:09,748 - mmdet - INFO - Epoch [6][400/7033]	lr: 1.358e-05, eta: 3:03:43, time: 1.585, data_time: 0.063, memory: 17624, loss_cls: 0.0698, loss_bbox: 0.1981, d0.loss_cls: 0.1569, d0.loss_bbox: 0.3139, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2017, loss: 1.9371, grad_norm: 32.4738
2025-06-11 18:35:31,026 - mmdet - INFO - Epoch [6][450/7033]	lr: 1.358e-05, eta: 3:02:19, time: 1.624, data_time: 0.066, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.1986, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3171, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2093, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2012, loss: 1.9447, grad_norm: 128.6677
2025-06-11 18:36:47,764 - mmdet - INFO - Epoch [6][500/7033]	lr: 1.358e-05, eta: 3:00:55, time: 1.536, data_time: 0.067, memory: 17624, loss_cls: 0.0578, loss_bbox: 0.1918, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2030, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1953, loss: 1.8274, grad_norm: 37.0983
2025-06-11 18:38:07,126 - mmdet - INFO - Epoch [6][550/7033]	lr: 1.358e-05, eta: 2:59:31, time: 1.587, data_time: 0.066, memory: 17624, loss_cls: 0.0689, loss_bbox: 0.1982, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3211, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2013, loss: 1.9548, grad_norm: 29.7464
2025-06-11 18:39:30,185 - mmdet - INFO - Epoch [6][600/7033]	lr: 1.358e-05, eta: 2:58:08, time: 1.661, data_time: 0.067, memory: 17624, loss_cls: 0.0616, loss_bbox: 0.1918, d0.loss_cls: 0.1496, d0.loss_bbox: 0.2969, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1948, loss: 1.8367, grad_norm: 18.7352
2025-06-11 18:40:55,180 - mmdet - INFO - Epoch [6][650/7033]	lr: 1.358e-05, eta: 2:56:46, time: 1.700, data_time: 0.062, memory: 17624, loss_cls: 0.0572, loss_bbox: 0.1909, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3029, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2063, d3.loss_cls: 0.0642, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1938, loss: 1.8204, grad_norm: 37.1641
2025-06-11 18:42:17,957 - mmdet - INFO - Epoch [6][700/7033]	lr: 1.358e-05, eta: 2:55:22, time: 1.654, data_time: 0.076, memory: 17624, loss_cls: 0.0565, loss_bbox: 0.1859, d0.loss_cls: 0.1448, d0.loss_bbox: 0.2968, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2223, d2.loss_cls: 0.0736, d2.loss_bbox: 0.2030, d3.loss_cls: 0.0635, d3.loss_bbox: 0.1963, d4.loss_cls: 0.0581, d4.loss_bbox: 0.1894, loss: 1.7801, grad_norm: 36.6323
2025-06-11 18:43:40,668 - mmdet - INFO - Epoch [6][750/7033]	lr: 1.358e-05, eta: 2:53:59, time: 1.654, data_time: 0.065, memory: 17624, loss_cls: 0.0574, loss_bbox: 0.1896, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3035, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2274, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0619, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1923, loss: 1.8213, grad_norm: 22.8704
2025-06-11 18:45:04,253 - mmdet - INFO - Epoch [6][800/7033]	lr: 1.358e-05, eta: 2:52:36, time: 1.672, data_time: 0.077, memory: 17624, loss_cls: 0.0584, loss_bbox: 0.1937, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3061, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2090, d3.loss_cls: 0.0665, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1960, loss: 1.8497, grad_norm: 18.2400
2025-06-11 18:46:21,929 - mmdet - INFO - Epoch [6][850/7033]	lr: 1.358e-05, eta: 2:51:12, time: 1.555, data_time: 0.068, memory: 17624, loss_cls: 0.0630, loss_bbox: 0.1913, d0.loss_cls: 0.1516, d0.loss_bbox: 0.2992, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1930, loss: 1.8472, grad_norm: 78.5975
2025-06-11 18:47:41,114 - mmdet - INFO - Epoch [6][900/7033]	lr: 1.358e-05, eta: 2:49:49, time: 1.584, data_time: 0.069, memory: 17624, loss_cls: 0.0584, loss_bbox: 0.1813, d0.loss_cls: 0.1470, d0.loss_bbox: 0.2959, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2188, d2.loss_cls: 0.0754, d2.loss_bbox: 0.1973, d3.loss_cls: 0.0647, d3.loss_bbox: 0.1915, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1851, loss: 1.7657, grad_norm: 25.7165
2025-06-11 18:49:04,643 - mmdet - INFO - Epoch [6][950/7033]	lr: 1.358e-05, eta: 2:48:26, time: 1.670, data_time: 0.061, memory: 17624, loss_cls: 0.0584, loss_bbox: 0.1865, d0.loss_cls: 0.1508, d0.loss_bbox: 0.2998, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2225, d2.loss_cls: 0.0772, d2.loss_bbox: 0.1995, d3.loss_cls: 0.0641, d3.loss_bbox: 0.1950, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1889, loss: 1.7987, grad_norm: 20.6889
2025-06-11 18:50:26,211 - mmdet - INFO - Epoch [6][1000/7033]	lr: 1.358e-05, eta: 2:47:02, time: 1.631, data_time: 0.064, memory: 17624, loss_cls: 0.0536, loss_bbox: 0.1809, d0.loss_cls: 0.1481, d0.loss_bbox: 0.2895, d1.loss_cls: 0.0870, d1.loss_bbox: 0.2181, d2.loss_cls: 0.0715, d2.loss_bbox: 0.1961, d3.loss_cls: 0.0595, d3.loss_bbox: 0.1925, d4.loss_cls: 0.0561, d4.loss_bbox: 0.1845, loss: 1.7374, grad_norm: 36.4565
2025-06-11 18:51:48,419 - mmdet - INFO - Epoch [6][1050/7033]	lr: 1.358e-05, eta: 2:45:39, time: 1.644, data_time: 0.057, memory: 17624, loss_cls: 0.0659, loss_bbox: 0.1951, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3111, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0666, d4.loss_bbox: 0.1991, loss: 1.9035, grad_norm: 74.1837
2025-06-11 18:53:09,397 - mmdet - INFO - Epoch [6][1100/7033]	lr: 1.358e-05, eta: 2:44:16, time: 1.619, data_time: 0.064, memory: 17624, loss_cls: 0.0595, loss_bbox: 0.1921, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0652, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1950, loss: 1.8483, grad_norm: 24.6706
2025-06-11 18:54:30,770 - mmdet - INFO - Epoch [6][1150/7033]	lr: 1.358e-05, eta: 2:42:52, time: 1.627, data_time: 0.064, memory: 17624, loss_cls: 0.0623, loss_bbox: 0.1964, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3039, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2055, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1967, loss: 1.8688, grad_norm: 19.8461
2025-06-11 18:55:54,102 - mmdet - INFO - Epoch [6][1200/7033]	lr: 1.358e-05, eta: 2:41:29, time: 1.665, data_time: 0.068, memory: 17624, loss_cls: 0.0663, loss_bbox: 0.1867, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3006, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2241, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2045, d3.loss_cls: 0.0718, d3.loss_bbox: 0.1991, d4.loss_cls: 0.0669, d4.loss_bbox: 0.1916, loss: 1.8435, grad_norm: 19.2190
2025-06-11 18:57:17,053 - mmdet - INFO - Epoch [6][1250/7033]	lr: 1.358e-05, eta: 2:40:06, time: 1.661, data_time: 0.066, memory: 17624, loss_cls: 0.0622, loss_bbox: 0.1903, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3019, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2264, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2064, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0639, d4.loss_bbox: 0.1937, loss: 1.8388, grad_norm: 22.5155
2025-06-11 18:59:03,629 - mmdet - INFO - Epoch [6][1300/7033]	lr: 1.358e-05, eta: 2:38:47, time: 2.132, data_time: 0.065, memory: 17624, loss_cls: 0.0628, loss_bbox: 0.1961, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1997, loss: 1.8889, grad_norm: 16.1879
2025-06-11 19:00:25,834 - mmdet - INFO - Epoch [6][1350/7033]	lr: 1.358e-05, eta: 2:37:24, time: 1.644, data_time: 0.077, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1880, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3018, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2059, d3.loss_cls: 0.0669, d3.loss_bbox: 0.1997, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1924, loss: 1.8198, grad_norm: 26.9052
2025-06-11 19:01:44,794 - mmdet - INFO - Epoch [6][1400/7033]	lr: 1.358e-05, eta: 2:36:00, time: 1.578, data_time: 0.058, memory: 17624, loss_cls: 0.0564, loss_bbox: 0.1942, d0.loss_cls: 0.1479, d0.loss_bbox: 0.3014, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0741, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0622, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0578, d4.loss_bbox: 0.1973, loss: 1.8254, grad_norm: 73.3990
2025-06-11 19:03:07,382 - mmdet - INFO - Epoch [6][1450/7033]	lr: 1.358e-05, eta: 2:34:37, time: 1.652, data_time: 0.065, memory: 17624, loss_cls: 0.0531, loss_bbox: 0.1877, d0.loss_cls: 0.1424, d0.loss_bbox: 0.2963, d1.loss_cls: 0.0840, d1.loss_bbox: 0.2228, d2.loss_cls: 0.0704, d2.loss_bbox: 0.2024, d3.loss_cls: 0.0579, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0536, d4.loss_bbox: 0.1912, loss: 1.7608, grad_norm: 14.8918
2025-06-11 19:04:27,237 - mmdet - INFO - Epoch [6][1500/7033]	lr: 1.358e-05, eta: 2:33:13, time: 1.597, data_time: 0.060, memory: 17624, loss_cls: 0.0593, loss_bbox: 0.1927, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3070, d1.loss_cls: 0.0899, d1.loss_bbox: 0.2293, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2062, d3.loss_cls: 0.0649, d3.loss_bbox: 0.2016, d4.loss_cls: 0.0617, d4.loss_bbox: 0.1943, loss: 1.8381, grad_norm: 21.8450
2025-06-11 19:05:50,318 - mmdet - INFO - Epoch [6][1550/7033]	lr: 1.358e-05, eta: 2:31:50, time: 1.660, data_time: 0.064, memory: 17624, loss_cls: 0.0661, loss_bbox: 0.1906, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2081, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1941, loss: 1.8787, grad_norm: 22.8486
2025-06-11 19:07:14,836 - mmdet - INFO - Epoch [6][1600/7033]	lr: 1.358e-05, eta: 2:30:27, time: 1.692, data_time: 0.066, memory: 17624, loss_cls: 0.0601, loss_bbox: 0.1905, d0.loss_cls: 0.1500, d0.loss_bbox: 0.3086, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2091, d3.loss_cls: 0.0666, d3.loss_bbox: 0.2034, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1942, loss: 1.8470, grad_norm: 25.4157
2025-06-11 19:08:34,581 - mmdet - INFO - Epoch [6][1650/7033]	lr: 1.358e-05, eta: 2:29:04, time: 1.595, data_time: 0.069, memory: 17624, loss_cls: 0.0670, loss_bbox: 0.1990, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3121, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0674, d4.loss_bbox: 0.2017, loss: 1.9205, grad_norm: 23.3646
2025-06-11 19:09:56,649 - mmdet - INFO - Epoch [6][1700/7033]	lr: 1.358e-05, eta: 2:27:40, time: 1.641, data_time: 0.064, memory: 17624, loss_cls: 0.0588, loss_bbox: 0.1925, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2094, d3.loss_cls: 0.0645, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0593, d4.loss_bbox: 0.1963, loss: 1.8540, grad_norm: 29.4424
2025-06-11 19:11:20,697 - mmdet - INFO - Epoch [6][1750/7033]	lr: 1.358e-05, eta: 2:26:17, time: 1.681, data_time: 0.059, memory: 17624, loss_cls: 0.0604, loss_bbox: 0.1883, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3034, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2270, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2055, d3.loss_cls: 0.0675, d3.loss_bbox: 0.1999, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1910, loss: 1.8273, grad_norm: 39.7893
2025-06-11 19:12:42,509 - mmdet - INFO - Epoch [6][1800/7033]	lr: 1.358e-05, eta: 2:24:54, time: 1.636, data_time: 0.069, memory: 17624, loss_cls: 0.0624, loss_bbox: 0.1925, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2074, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1936, loss: 1.8737, grad_norm: 26.2356
2025-06-11 19:14:03,013 - mmdet - INFO - Epoch [6][1850/7033]	lr: 1.358e-05, eta: 2:23:31, time: 1.610, data_time: 0.056, memory: 17624, loss_cls: 0.0675, loss_bbox: 0.1963, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3112, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0695, d4.loss_bbox: 0.1991, loss: 1.9185, grad_norm: 19.5291
2025-06-11 19:15:25,530 - mmdet - INFO - Epoch [6][1900/7033]	lr: 1.358e-05, eta: 2:22:08, time: 1.650, data_time: 0.075, memory: 17624, loss_cls: 0.0607, loss_bbox: 0.1905, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3025, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0645, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1936, loss: 1.8306, grad_norm: 19.3209
2025-06-11 19:16:45,787 - mmdet - INFO - Epoch [6][1950/7033]	lr: 1.358e-05, eta: 2:20:44, time: 1.605, data_time: 0.069, memory: 17624, loss_cls: 0.0581, loss_bbox: 0.1883, d0.loss_cls: 0.1496, d0.loss_bbox: 0.2998, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2248, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2031, d3.loss_cls: 0.0638, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1903, loss: 1.8073, grad_norm: 23.7619
2025-06-11 19:18:07,774 - mmdet - INFO - Epoch [6][2000/7033]	lr: 1.358e-05, eta: 2:19:21, time: 1.640, data_time: 0.058, memory: 17624, loss_cls: 0.0601, loss_bbox: 0.1861, d0.loss_cls: 0.1517, d0.loss_bbox: 0.3013, d1.loss_cls: 0.0891, d1.loss_bbox: 0.2266, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2040, d3.loss_cls: 0.0647, d3.loss_bbox: 0.1991, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1900, loss: 1.8086, grad_norm: 20.1540
2025-06-11 19:19:29,072 - mmdet - INFO - Epoch [6][2050/7033]	lr: 1.358e-05, eta: 2:17:58, time: 1.626, data_time: 0.069, memory: 17624, loss_cls: 0.0576, loss_bbox: 0.1927, d0.loss_cls: 0.1447, d0.loss_bbox: 0.3042, d1.loss_cls: 0.0856, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0739, d2.loss_bbox: 0.2064, d3.loss_cls: 0.0628, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0597, d4.loss_bbox: 0.1952, loss: 1.8135, grad_norm: 15.6462
2025-06-11 19:20:54,830 - mmdet - INFO - Epoch [6][2100/7033]	lr: 1.358e-05, eta: 2:16:35, time: 1.715, data_time: 0.068, memory: 17624, loss_cls: 0.0625, loss_bbox: 0.1951, d0.loss_cls: 0.1522, d0.loss_bbox: 0.3071, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2339, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1993, loss: 1.8726, grad_norm: 37.3498
2025-06-11 19:22:11,352 - mmdet - INFO - Epoch [6][2150/7033]	lr: 1.358e-05, eta: 2:15:11, time: 1.530, data_time: 0.064, memory: 17624, loss_cls: 0.0607, loss_bbox: 0.1990, d0.loss_cls: 0.1581, d0.loss_bbox: 0.3154, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0626, d4.loss_bbox: 0.2009, loss: 1.9045, grad_norm: 60.9357
2025-06-11 19:23:31,903 - mmdet - INFO - Epoch [6][2200/7033]	lr: 1.358e-05, eta: 2:13:48, time: 1.611, data_time: 0.066, memory: 17624, loss_cls: 0.0681, loss_bbox: 0.1920, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3097, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1953, loss: 1.8879, grad_norm: 58.2400
2025-06-11 19:24:51,705 - mmdet - INFO - Epoch [6][2250/7033]	lr: 1.358e-05, eta: 2:12:24, time: 1.596, data_time: 0.073, memory: 17624, loss_cls: 0.0579, loss_bbox: 0.1903, d0.loss_cls: 0.1536, d0.loss_bbox: 0.3026, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2284, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2075, d3.loss_cls: 0.0632, d3.loss_bbox: 0.2024, d4.loss_cls: 0.0584, d4.loss_bbox: 0.1943, loss: 1.8249, grad_norm: 19.8651
2025-06-11 19:26:10,710 - mmdet - INFO - Epoch [6][2300/7033]	lr: 1.358e-05, eta: 2:11:01, time: 1.580, data_time: 0.059, memory: 17624, loss_cls: 0.0624, loss_bbox: 0.1902, d0.loss_cls: 0.1509, d0.loss_bbox: 0.3004, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1932, loss: 1.8381, grad_norm: 37.0317
2025-06-11 19:27:32,396 - mmdet - INFO - Epoch [6][2350/7033]	lr: 1.358e-05, eta: 2:09:37, time: 1.634, data_time: 0.057, memory: 17624, loss_cls: 0.0617, loss_bbox: 0.1931, d0.loss_cls: 0.1405, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1957, loss: 1.8467, grad_norm: 17.1025
2025-06-11 19:28:51,450 - mmdet - INFO - Epoch [6][2400/7033]	lr: 1.358e-05, eta: 2:08:14, time: 1.579, data_time: 0.056, memory: 17624, loss_cls: 0.0596, loss_bbox: 0.1880, d0.loss_cls: 0.1471, d0.loss_bbox: 0.2975, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2242, d2.loss_cls: 0.0733, d2.loss_bbox: 0.2043, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1908, loss: 1.8000, grad_norm: 20.1697
2025-06-11 19:30:11,964 - mmdet - INFO - Epoch [6][2450/7033]	lr: 1.358e-05, eta: 2:06:51, time: 1.612, data_time: 0.058, memory: 17624, loss_cls: 0.0581, loss_bbox: 0.1892, d0.loss_cls: 0.1513, d0.loss_bbox: 0.3032, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0641, d3.loss_bbox: 0.2006, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1913, loss: 1.8188, grad_norm: 42.1061
2025-06-11 19:31:32,850 - mmdet - INFO - Epoch [6][2500/7033]	lr: 1.358e-05, eta: 2:05:27, time: 1.617, data_time: 0.071, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.2024, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0667, d4.loss_bbox: 0.2057, loss: 1.9328, grad_norm: 22.3350
2025-06-11 19:32:55,152 - mmdet - INFO - Epoch [6][2550/7033]	lr: 1.358e-05, eta: 2:04:04, time: 1.647, data_time: 0.064, memory: 17624, loss_cls: 0.0643, loss_bbox: 0.1869, d0.loss_cls: 0.1517, d0.loss_bbox: 0.2924, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2265, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2042, d3.loss_cls: 0.0692, d3.loss_bbox: 0.1993, d4.loss_cls: 0.0661, d4.loss_bbox: 0.1904, loss: 1.8258, grad_norm: 33.8836
2025-06-11 19:34:19,120 - mmdet - INFO - Epoch [6][2600/7033]	lr: 1.358e-05, eta: 2:02:41, time: 1.678, data_time: 0.061, memory: 17624, loss_cls: 0.0623, loss_bbox: 0.1882, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2050, d3.loss_cls: 0.0680, d3.loss_bbox: 0.1998, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1929, loss: 1.8375, grad_norm: 39.8661
2025-06-11 19:35:38,173 - mmdet - INFO - Epoch [6][2650/7033]	lr: 1.358e-05, eta: 2:01:18, time: 1.582, data_time: 0.066, memory: 17624, loss_cls: 0.0662, loss_bbox: 0.1922, d0.loss_cls: 0.1492, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0683, d4.loss_bbox: 0.1949, loss: 1.8834, grad_norm: 16.0008
2025-06-11 19:36:59,133 - mmdet - INFO - Epoch [6][2700/7033]	lr: 1.358e-05, eta: 1:59:54, time: 1.619, data_time: 0.066, memory: 17624, loss_cls: 0.0562, loss_bbox: 0.1896, d0.loss_cls: 0.1448, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2051, d3.loss_cls: 0.0607, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0571, d4.loss_bbox: 0.1939, loss: 1.8129, grad_norm: 29.2559
2025-06-11 19:38:20,819 - mmdet - INFO - Epoch [6][2750/7033]	lr: 1.358e-05, eta: 1:58:31, time: 1.634, data_time: 0.059, memory: 17624, loss_cls: 0.0638, loss_bbox: 0.1922, d0.loss_cls: 0.1483, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2094, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1953, loss: 1.8586, grad_norm: 17.2984
2025-06-11 19:39:38,054 - mmdet - INFO - Epoch [6][2800/7033]	lr: 1.358e-05, eta: 1:57:08, time: 1.545, data_time: 0.062, memory: 17624, loss_cls: 0.0601, loss_bbox: 0.1812, d0.loss_cls: 0.1446, d0.loss_bbox: 0.2989, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2231, d2.loss_cls: 0.0773, d2.loss_bbox: 0.1991, d3.loss_cls: 0.0667, d3.loss_bbox: 0.1928, d4.loss_cls: 0.0613, d4.loss_bbox: 0.1852, loss: 1.7813, grad_norm: 16.6828
2025-06-11 19:40:57,884 - mmdet - INFO - Epoch [6][2850/7033]	lr: 1.358e-05, eta: 1:55:44, time: 1.596, data_time: 0.056, memory: 17624, loss_cls: 0.0568, loss_bbox: 0.1842, d0.loss_cls: 0.1468, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2215, d2.loss_cls: 0.0748, d2.loss_bbox: 0.1998, d3.loss_cls: 0.0628, d3.loss_bbox: 0.1958, d4.loss_cls: 0.0583, d4.loss_bbox: 0.1875, loss: 1.7772, grad_norm: 15.3962
2025-06-11 19:42:18,812 - mmdet - INFO - Epoch [6][2900/7033]	lr: 1.358e-05, eta: 1:54:21, time: 1.619, data_time: 0.060, memory: 17624, loss_cls: 0.0679, loss_bbox: 0.2002, d0.loss_cls: 0.1585, d0.loss_bbox: 0.3173, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2029, loss: 1.9474, grad_norm: 56.6556
2025-06-11 19:43:38,763 - mmdet - INFO - Epoch [6][2950/7033]	lr: 1.358e-05, eta: 1:52:58, time: 1.599, data_time: 0.060, memory: 17624, loss_cls: 0.0543, loss_bbox: 0.1899, d0.loss_cls: 0.1480, d0.loss_bbox: 0.3069, d1.loss_cls: 0.0882, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2056, d3.loss_cls: 0.0608, d3.loss_bbox: 0.2007, d4.loss_cls: 0.0557, d4.loss_bbox: 0.1932, loss: 1.8042, grad_norm: 20.0431
2025-06-11 19:44:56,988 - mmdet - INFO - Epoch [6][3000/7033]	lr: 1.358e-05, eta: 1:51:34, time: 1.564, data_time: 0.057, memory: 17624, loss_cls: 0.0612, loss_bbox: 0.1919, d0.loss_cls: 0.1426, d0.loss_bbox: 0.3001, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1948, loss: 1.8272, grad_norm: 20.8780
2025-06-11 19:46:19,081 - mmdet - INFO - Epoch [6][3050/7033]	lr: 1.358e-05, eta: 1:50:11, time: 1.642, data_time: 0.073, memory: 17624, loss_cls: 0.0558, loss_bbox: 0.1834, d0.loss_cls: 0.1460, d0.loss_bbox: 0.2889, d1.loss_cls: 0.0868, d1.loss_bbox: 0.2198, d2.loss_cls: 0.0725, d2.loss_bbox: 0.1987, d3.loss_cls: 0.0622, d3.loss_bbox: 0.1933, d4.loss_cls: 0.0575, d4.loss_bbox: 0.1864, loss: 1.7514, grad_norm: 17.4146
2025-06-11 19:47:39,593 - mmdet - INFO - Epoch [6][3100/7033]	lr: 1.358e-05, eta: 1:48:48, time: 1.610, data_time: 0.062, memory: 17624, loss_cls: 0.0577, loss_bbox: 0.1883, d0.loss_cls: 0.1498, d0.loss_bbox: 0.2939, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2244, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2033, d3.loss_cls: 0.0627, d3.loss_bbox: 0.1997, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1917, loss: 1.7967, grad_norm: 43.4867
2025-06-11 19:48:58,001 - mmdet - INFO - Epoch [6][3150/7033]	lr: 1.358e-05, eta: 1:47:24, time: 1.568, data_time: 0.068, memory: 17624, loss_cls: 0.0608, loss_bbox: 0.1898, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3024, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0659, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0621, d4.loss_bbox: 0.1938, loss: 1.8436, grad_norm: 78.7853
2025-06-11 19:50:22,148 - mmdet - INFO - Epoch [6][3200/7033]	lr: 1.358e-05, eta: 1:46:01, time: 1.683, data_time: 0.063, memory: 17624, loss_cls: 0.0577, loss_bbox: 0.1881, d0.loss_cls: 0.1526, d0.loss_bbox: 0.2993, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2251, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2017, d3.loss_cls: 0.0640, d3.loss_bbox: 0.1988, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1902, loss: 1.8045, grad_norm: 15.4978
2025-06-11 19:51:40,448 - mmdet - INFO - Epoch [6][3250/7033]	lr: 1.358e-05, eta: 1:44:38, time: 1.566, data_time: 0.062, memory: 17624, loss_cls: 0.0608, loss_bbox: 0.1895, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2071, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1921, loss: 1.8519, grad_norm: 20.0408
2025-06-11 19:52:55,667 - mmdet - INFO - Epoch [6][3300/7033]	lr: 1.358e-05, eta: 1:43:14, time: 1.504, data_time: 0.058, memory: 17624, loss_cls: 0.0616, loss_bbox: 0.1910, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3026, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2040, d3.loss_cls: 0.0680, d3.loss_bbox: 0.1992, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1930, loss: 1.8365, grad_norm: 39.3243
2025-06-11 19:54:16,099 - mmdet - INFO - Epoch [6][3350/7033]	lr: 1.358e-05, eta: 1:41:51, time: 1.609, data_time: 0.061, memory: 17624, loss_cls: 0.0633, loss_bbox: 0.1941, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3012, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2098, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1968, loss: 1.8632, grad_norm: 23.9090
2025-06-11 19:55:37,431 - mmdet - INFO - Epoch [6][3400/7033]	lr: 1.358e-05, eta: 1:40:28, time: 1.625, data_time: 0.057, memory: 17624, loss_cls: 0.0632, loss_bbox: 0.1894, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3098, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2298, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2016, d4.loss_cls: 0.0651, d4.loss_bbox: 0.1933, loss: 1.8521, grad_norm: 55.1407
2025-06-11 19:56:57,932 - mmdet - INFO - Epoch [6][3450/7033]	lr: 1.358e-05, eta: 1:39:05, time: 1.610, data_time: 0.066, memory: 17624, loss_cls: 0.0683, loss_bbox: 0.1954, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3115, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1999, loss: 1.9217, grad_norm: 29.3963
2025-06-11 19:58:22,526 - mmdet - INFO - Epoch [6][3500/7033]	lr: 1.358e-05, eta: 1:37:42, time: 1.693, data_time: 0.090, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1867, d0.loss_cls: 0.1523, d0.loss_bbox: 0.2965, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2035, d3.loss_cls: 0.0641, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1887, loss: 1.8037, grad_norm: 14.1931
2025-06-11 19:59:52,018 - mmdet - INFO - Epoch [6][3550/7033]	lr: 1.358e-05, eta: 1:36:20, time: 1.790, data_time: 0.140, memory: 17624, loss_cls: 0.0548, loss_bbox: 0.1863, d0.loss_cls: 0.1455, d0.loss_bbox: 0.2967, d1.loss_cls: 0.0865, d1.loss_bbox: 0.2242, d2.loss_cls: 0.0719, d2.loss_bbox: 0.2022, d3.loss_cls: 0.0595, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0565, d4.loss_bbox: 0.1885, loss: 1.7693, grad_norm: 434.5477
2025-06-11 20:01:49,420 - mmdet - INFO - Epoch [6][3600/7033]	lr: 1.358e-05, eta: 1:35:00, time: 2.346, data_time: 0.828, memory: 17624, loss_cls: 0.0570, loss_bbox: 0.1934, d0.loss_cls: 0.1460, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2092, d3.loss_cls: 0.0647, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1959, loss: 1.8332, grad_norm: 122.5225
2025-06-11 20:03:11,437 - mmdet - INFO - Epoch [6][3650/7033]	lr: 1.358e-05, eta: 1:33:37, time: 1.643, data_time: 0.099, memory: 17624, loss_cls: 0.0675, loss_bbox: 0.1954, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3037, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0689, d4.loss_bbox: 0.1963, loss: 1.8876, grad_norm: 27.5143
2025-06-11 20:04:42,968 - mmdet - INFO - Epoch [6][3700/7033]	lr: 1.358e-05, eta: 1:32:14, time: 1.831, data_time: 0.346, memory: 17624, loss_cls: 0.0595, loss_bbox: 0.1923, d0.loss_cls: 0.1498, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2028, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1952, loss: 1.8353, grad_norm: 15.7077
2025-06-11 20:05:59,856 - mmdet - INFO - Epoch [6][3750/7033]	lr: 1.358e-05, eta: 1:30:51, time: 1.536, data_time: 0.059, memory: 17624, loss_cls: 0.0593, loss_bbox: 0.1887, d0.loss_cls: 0.1476, d0.loss_bbox: 0.2961, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2256, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1910, loss: 1.8212, grad_norm: 27.5596
2025-06-11 20:07:29,339 - mmdet - INFO - Epoch [6][3800/7033]	lr: 1.358e-05, eta: 1:29:28, time: 1.791, data_time: 0.099, memory: 17624, loss_cls: 0.0634, loss_bbox: 0.1923, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3063, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2076, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1952, loss: 1.8651, grad_norm: 20.1758
2025-06-11 20:09:22,249 - mmdet - INFO - Epoch [6][3850/7033]	lr: 1.358e-05, eta: 1:28:08, time: 2.258, data_time: 0.655, memory: 17624, loss_cls: 0.0556, loss_bbox: 0.1809, d0.loss_cls: 0.1403, d0.loss_bbox: 0.2903, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2198, d2.loss_cls: 0.0733, d2.loss_bbox: 0.1975, d3.loss_cls: 0.0613, d3.loss_bbox: 0.1928, d4.loss_cls: 0.0569, d4.loss_bbox: 0.1836, loss: 1.7411, grad_norm: 26.3688
2025-06-11 20:12:02,830 - mmdet - INFO - Epoch [6][3900/7033]	lr: 1.358e-05, eta: 1:26:51, time: 3.212, data_time: 1.608, memory: 17624, loss_cls: 0.0547, loss_bbox: 0.1901, d0.loss_cls: 0.1484, d0.loss_bbox: 0.3085, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2269, d2.loss_cls: 0.0729, d2.loss_bbox: 0.2051, d3.loss_cls: 0.0601, d3.loss_bbox: 0.2008, d4.loss_cls: 0.0559, d4.loss_bbox: 0.1930, loss: 1.8055, grad_norm: 25.6691
2025-06-11 20:13:28,659 - mmdet - INFO - Epoch [6][3950/7033]	lr: 1.358e-05, eta: 1:25:28, time: 1.716, data_time: 0.063, memory: 17624, loss_cls: 0.0602, loss_bbox: 0.1971, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3049, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0607, d4.loss_bbox: 0.2003, loss: 1.8543, grad_norm: 36.0819
2025-06-11 20:15:06,211 - mmdet - INFO - Epoch [6][4000/7033]	lr: 1.358e-05, eta: 1:24:06, time: 1.951, data_time: 0.355, memory: 17624, loss_cls: 0.0562, loss_bbox: 0.1859, d0.loss_cls: 0.1448, d0.loss_bbox: 0.2955, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2239, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2011, d3.loss_cls: 0.0620, d3.loss_bbox: 0.1958, d4.loss_cls: 0.0579, d4.loss_bbox: 0.1892, loss: 1.7769, grad_norm: 21.4073
2025-06-11 20:16:27,719 - mmdet - INFO - Epoch [6][4050/7033]	lr: 1.358e-05, eta: 1:22:42, time: 1.629, data_time: 0.080, memory: 17624, loss_cls: 0.0548, loss_bbox: 0.1818, d0.loss_cls: 0.1504, d0.loss_bbox: 0.2921, d1.loss_cls: 0.0884, d1.loss_bbox: 0.2193, d2.loss_cls: 0.0753, d2.loss_bbox: 0.1986, d3.loss_cls: 0.0626, d3.loss_bbox: 0.1933, d4.loss_cls: 0.0569, d4.loss_bbox: 0.1865, loss: 1.7601, grad_norm: 15.8700
2025-06-11 20:17:50,536 - mmdet - INFO - Epoch [6][4100/7033]	lr: 1.358e-05, eta: 1:21:19, time: 1.657, data_time: 0.075, memory: 17624, loss_cls: 0.0560, loss_bbox: 0.1866, d0.loss_cls: 0.1418, d0.loss_bbox: 0.2969, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2227, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2012, d3.loss_cls: 0.0641, d3.loss_bbox: 0.1969, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1884, loss: 1.7799, grad_norm: 27.4948
2025-06-11 20:19:12,296 - mmdet - INFO - Epoch [6][4150/7033]	lr: 1.358e-05, eta: 1:19:56, time: 1.635, data_time: 0.092, memory: 17624, loss_cls: 0.0580, loss_bbox: 0.1826, d0.loss_cls: 0.1426, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2215, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2002, d3.loss_cls: 0.0631, d3.loss_bbox: 0.1954, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1866, loss: 1.7728, grad_norm: 41.1061
2025-06-11 20:20:35,905 - mmdet - INFO - Epoch [6][4200/7033]	lr: 1.358e-05, eta: 1:18:33, time: 1.672, data_time: 0.070, memory: 17624, loss_cls: 0.0559, loss_bbox: 0.1825, d0.loss_cls: 0.1413, d0.loss_bbox: 0.2920, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2173, d2.loss_cls: 0.0719, d2.loss_bbox: 0.1964, d3.loss_cls: 0.0612, d3.loss_bbox: 0.1913, d4.loss_cls: 0.0576, d4.loss_bbox: 0.1841, loss: 1.7406, grad_norm: 41.4556
2025-06-11 20:21:56,660 - mmdet - INFO - Epoch [6][4250/7033]	lr: 1.358e-05, eta: 1:17:09, time: 1.615, data_time: 0.058, memory: 17624, loss_cls: 0.0560, loss_bbox: 0.1840, d0.loss_cls: 0.1437, d0.loss_bbox: 0.2962, d1.loss_cls: 0.0880, d1.loss_bbox: 0.2214, d2.loss_cls: 0.0725, d2.loss_bbox: 0.2000, d3.loss_cls: 0.0613, d3.loss_bbox: 0.1944, d4.loss_cls: 0.0573, d4.loss_bbox: 0.1862, loss: 1.7611, grad_norm: 19.6100
2025-06-11 20:23:33,368 - mmdet - INFO - Epoch [6][4300/7033]	lr: 1.358e-05, eta: 1:15:47, time: 1.934, data_time: 0.274, memory: 17624, loss_cls: 0.0582, loss_bbox: 0.1927, d0.loss_cls: 0.1463, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2288, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0646, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1955, loss: 1.8228, grad_norm: 15.7912
2025-06-11 20:24:53,138 - mmdet - INFO - Epoch [6][4350/7033]	lr: 1.358e-05, eta: 1:14:24, time: 1.595, data_time: 0.079, memory: 17624, loss_cls: 0.0593, loss_bbox: 0.2000, d0.loss_cls: 0.1493, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0647, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0602, d4.loss_bbox: 0.2030, loss: 1.8753, grad_norm: 26.1500
2025-06-11 20:26:17,088 - mmdet - INFO - Epoch [6][4400/7033]	lr: 1.358e-05, eta: 1:13:01, time: 1.679, data_time: 0.081, memory: 17624, loss_cls: 0.0583, loss_bbox: 0.1911, d0.loss_cls: 0.1486, d0.loss_bbox: 0.3021, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2284, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0632, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1948, loss: 1.8242, grad_norm: 17.4415
2025-06-11 20:27:38,833 - mmdet - INFO - Epoch [6][4450/7033]	lr: 1.358e-05, eta: 1:11:37, time: 1.635, data_time: 0.085, memory: 17624, loss_cls: 0.0656, loss_bbox: 0.1959, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2084, d4.loss_cls: 0.0662, d4.loss_bbox: 0.2003, loss: 1.9046, grad_norm: 15.7581
2025-06-11 20:29:02,065 - mmdet - INFO - Epoch [6][4500/7033]	lr: 1.358e-05, eta: 1:10:14, time: 1.665, data_time: 0.130, memory: 17624, loss_cls: 0.0606, loss_bbox: 0.1945, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3119, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2313, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2113, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2068, d4.loss_cls: 0.0630, d4.loss_bbox: 0.1979, loss: 1.8688, grad_norm: 49.4827
2025-06-11 20:30:25,862 - mmdet - INFO - Epoch [6][4550/7033]	lr: 1.358e-05, eta: 1:08:51, time: 1.676, data_time: 0.094, memory: 17624, loss_cls: 0.0640, loss_bbox: 0.1922, d0.loss_cls: 0.1472, d0.loss_bbox: 0.3002, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2277, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1957, loss: 1.8426, grad_norm: 18.9579
2025-06-11 20:31:46,329 - mmdet - INFO - Epoch [6][4600/7033]	lr: 1.358e-05, eta: 1:07:28, time: 1.609, data_time: 0.072, memory: 17624, loss_cls: 0.0605, loss_bbox: 0.1929, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3036, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2270, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2033, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1961, loss: 1.8503, grad_norm: 58.3860
2025-06-11 20:33:01,103 - mmdet - INFO - Epoch [6][4650/7033]	lr: 1.358e-05, eta: 1:06:04, time: 1.495, data_time: 0.055, memory: 17624, loss_cls: 0.0572, loss_bbox: 0.1976, d0.loss_cls: 0.1519, d0.loss_bbox: 0.3039, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2312, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2114, d3.loss_cls: 0.0637, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0581, d4.loss_bbox: 0.2005, loss: 1.8512, grad_norm: 18.2524
2025-06-11 20:34:27,258 - mmdet - INFO - Epoch [6][4700/7033]	lr: 1.358e-05, eta: 1:04:41, time: 1.723, data_time: 0.180, memory: 17624, loss_cls: 0.0631, loss_bbox: 0.1943, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2048, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1979, loss: 1.8685, grad_norm: 17.4175
2025-06-11 20:35:51,677 - mmdet - INFO - Epoch [6][4750/7033]	lr: 1.358e-05, eta: 1:03:18, time: 1.688, data_time: 0.055, memory: 17624, loss_cls: 0.0571, loss_bbox: 0.1890, d0.loss_cls: 0.1496, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2246, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2053, d3.loss_cls: 0.0628, d3.loss_bbox: 0.2013, d4.loss_cls: 0.0597, d4.loss_bbox: 0.1921, loss: 1.8062, grad_norm: 38.9357
2025-06-11 20:37:15,746 - mmdet - INFO - Epoch [6][4800/7033]	lr: 1.358e-05, eta: 1:01:55, time: 1.681, data_time: 0.112, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1921, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3112, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2332, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1958, loss: 1.8529, grad_norm: 23.4208
2025-06-11 20:38:41,128 - mmdet - INFO - Epoch [6][4850/7033]	lr: 1.358e-05, eta: 1:00:32, time: 1.708, data_time: 0.115, memory: 17624, loss_cls: 0.0543, loss_bbox: 0.1870, d0.loss_cls: 0.1496, d0.loss_bbox: 0.2943, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2223, d2.loss_cls: 0.0718, d2.loss_bbox: 0.2016, d3.loss_cls: 0.0603, d3.loss_bbox: 0.1970, d4.loss_cls: 0.0562, d4.loss_bbox: 0.1890, loss: 1.7725, grad_norm: 18.0758
2025-06-11 20:39:58,353 - mmdet - INFO - Epoch [6][4900/7033]	lr: 1.358e-05, eta: 0:59:08, time: 1.544, data_time: 0.064, memory: 17624, loss_cls: 0.0635, loss_bbox: 0.1965, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3199, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2125, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2084, d4.loss_cls: 0.0676, d4.loss_bbox: 0.1985, loss: 1.9160, grad_norm: 49.1381
2025-06-11 20:41:15,468 - mmdet - INFO - Epoch [6][4950/7033]	lr: 1.358e-05, eta: 0:57:45, time: 1.542, data_time: 0.086, memory: 17624, loss_cls: 0.0558, loss_bbox: 0.1897, d0.loss_cls: 0.1456, d0.loss_bbox: 0.3014, d1.loss_cls: 0.0865, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2053, d3.loss_cls: 0.0629, d3.loss_bbox: 0.2000, d4.loss_cls: 0.0567, d4.loss_bbox: 0.1924, loss: 1.7983, grad_norm: 31.2448
2025-06-11 20:42:32,490 - mmdet - INFO - Epoch [6][5000/7033]	lr: 1.358e-05, eta: 0:56:21, time: 1.540, data_time: 0.094, memory: 17624, loss_cls: 0.0647, loss_bbox: 0.2033, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3192, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0676, d4.loss_bbox: 0.2054, loss: 1.9507, grad_norm: 17.7766
2025-06-11 20:43:54,891 - mmdet - INFO - Epoch [6][5050/7033]	lr: 1.358e-05, eta: 0:54:58, time: 1.648, data_time: 0.089, memory: 17624, loss_cls: 0.0568, loss_bbox: 0.1900, d0.loss_cls: 0.1475, d0.loss_bbox: 0.2976, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2243, d2.loss_cls: 0.0739, d2.loss_bbox: 0.2047, d3.loss_cls: 0.0628, d3.loss_bbox: 0.1999, d4.loss_cls: 0.0581, d4.loss_bbox: 0.1929, loss: 1.7981, grad_norm: 15.6023
2025-06-11 20:45:15,872 - mmdet - INFO - Epoch [6][5100/7033]	lr: 1.358e-05, eta: 0:53:35, time: 1.620, data_time: 0.061, memory: 17624, loss_cls: 0.0618, loss_bbox: 0.1910, d0.loss_cls: 0.1538, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0949, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2058, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2018, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1936, loss: 1.8479, grad_norm: 18.9297
2025-06-11 20:46:40,138 - mmdet - INFO - Epoch [6][5150/7033]	lr: 1.358e-05, eta: 0:52:11, time: 1.685, data_time: 0.087, memory: 17624, loss_cls: 0.0594, loss_bbox: 0.1892, d0.loss_cls: 0.1472, d0.loss_bbox: 0.2956, d1.loss_cls: 0.0883, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0744, d2.loss_bbox: 0.2065, d3.loss_cls: 0.0640, d3.loss_bbox: 0.2022, d4.loss_cls: 0.0614, d4.loss_bbox: 0.1928, loss: 1.8065, grad_norm: 31.5349
2025-06-11 20:48:04,684 - mmdet - INFO - Epoch [6][5200/7033]	lr: 1.358e-05, eta: 0:50:48, time: 1.691, data_time: 0.066, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1880, d0.loss_cls: 0.1535, d0.loss_bbox: 0.3085, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2284, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2050, d3.loss_cls: 0.0648, d3.loss_bbox: 0.1979, d4.loss_cls: 0.0607, d4.loss_bbox: 0.1918, loss: 1.8251, grad_norm: 16.9030
2025-06-11 20:49:25,605 - mmdet - INFO - Epoch [6][5250/7033]	lr: 1.358e-05, eta: 0:49:25, time: 1.619, data_time: 0.062, memory: 17624, loss_cls: 0.0540, loss_bbox: 0.1867, d0.loss_cls: 0.1479, d0.loss_bbox: 0.2990, d1.loss_cls: 0.0878, d1.loss_bbox: 0.2226, d2.loss_cls: 0.0708, d2.loss_bbox: 0.2020, d3.loss_cls: 0.0594, d3.loss_bbox: 0.1988, d4.loss_cls: 0.0550, d4.loss_bbox: 0.1895, loss: 1.7735, grad_norm: 31.6786
2025-06-11 20:50:44,061 - mmdet - INFO - Epoch [6][5300/7033]	lr: 1.358e-05, eta: 0:48:02, time: 1.568, data_time: 0.063, memory: 17624, loss_cls: 0.0540, loss_bbox: 0.1866, d0.loss_cls: 0.1407, d0.loss_bbox: 0.2956, d1.loss_cls: 0.0875, d1.loss_bbox: 0.2235, d2.loss_cls: 0.0724, d2.loss_bbox: 0.2032, d3.loss_cls: 0.0617, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0563, d4.loss_bbox: 0.1899, loss: 1.7692, grad_norm: 27.6455
2025-06-11 20:52:06,526 - mmdet - INFO - Epoch [6][5350/7033]	lr: 1.358e-05, eta: 0:46:39, time: 1.650, data_time: 0.066, memory: 17624, loss_cls: 0.0566, loss_bbox: 0.1921, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3018, d1.loss_cls: 0.0908, d1.loss_bbox: 0.2269, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2085, d3.loss_cls: 0.0629, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1947, loss: 1.8257, grad_norm: 23.2123
2025-06-11 20:53:27,689 - mmdet - INFO - Epoch [6][5400/7033]	lr: 1.358e-05, eta: 0:45:15, time: 1.622, data_time: 0.100, memory: 17624, loss_cls: 0.0672, loss_bbox: 0.1959, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2069, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1989, loss: 1.8950, grad_norm: 25.9603
2025-06-11 20:54:47,432 - mmdet - INFO - Epoch [6][5450/7033]	lr: 1.358e-05, eta: 0:43:52, time: 1.596, data_time: 0.062, memory: 17624, loss_cls: 0.0564, loss_bbox: 0.1872, d0.loss_cls: 0.1431, d0.loss_bbox: 0.2899, d1.loss_cls: 0.0866, d1.loss_bbox: 0.2221, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2028, d3.loss_cls: 0.0619, d3.loss_bbox: 0.1961, d4.loss_cls: 0.0573, d4.loss_bbox: 0.1906, loss: 1.7680, grad_norm: 66.4141
2025-06-11 20:56:09,126 - mmdet - INFO - Epoch [6][5500/7033]	lr: 1.358e-05, eta: 0:42:29, time: 1.634, data_time: 0.074, memory: 17624, loss_cls: 0.0636, loss_bbox: 0.1962, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0645, d4.loss_bbox: 0.2000, loss: 1.8961, grad_norm: 22.8753
2025-06-11 20:57:35,560 - mmdet - INFO - Epoch [6][5550/7033]	lr: 1.358e-05, eta: 0:41:06, time: 1.729, data_time: 0.145, memory: 17624, loss_cls: 0.0654, loss_bbox: 0.1968, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2003, loss: 1.9037, grad_norm: 17.8556
2025-06-11 20:59:00,509 - mmdet - INFO - Epoch [6][5600/7033]	lr: 1.358e-05, eta: 0:39:43, time: 1.697, data_time: 0.150, memory: 17624, loss_cls: 0.0649, loss_bbox: 0.1940, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1982, loss: 1.8888, grad_norm: 43.6448
2025-06-11 21:00:19,371 - mmdet - INFO - Epoch [6][5650/7033]	lr: 1.358e-05, eta: 0:38:19, time: 1.579, data_time: 0.105, memory: 17624, loss_cls: 0.0608, loss_bbox: 0.1936, d0.loss_cls: 0.1496, d0.loss_bbox: 0.3025, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2072, d3.loss_cls: 0.0654, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1963, loss: 1.8402, grad_norm: 50.7288
2025-06-11 21:01:42,705 - mmdet - INFO - Epoch [6][5700/7033]	lr: 1.358e-05, eta: 0:36:56, time: 1.666, data_time: 0.105, memory: 17624, loss_cls: 0.0579, loss_bbox: 0.1870, d0.loss_cls: 0.1470, d0.loss_bbox: 0.3056, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2048, d3.loss_cls: 0.0617, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0587, d4.loss_bbox: 0.1909, loss: 1.8056, grad_norm: 35.1001
2025-06-11 21:03:04,544 - mmdet - INFO - Epoch [6][5750/7033]	lr: 1.358e-05, eta: 0:35:33, time: 1.638, data_time: 0.061, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1898, d0.loss_cls: 0.1475, d0.loss_bbox: 0.3071, d1.loss_cls: 0.0921, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2082, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1932, loss: 1.8375, grad_norm: 31.7257
2025-06-11 21:04:24,059 - mmdet - INFO - Epoch [6][5800/7033]	lr: 1.358e-05, eta: 0:34:10, time: 1.590, data_time: 0.077, memory: 17624, loss_cls: 0.0564, loss_bbox: 0.1989, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3079, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0629, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0598, d4.loss_bbox: 0.2022, loss: 1.8656, grad_norm: 22.4928
2025-06-11 21:05:45,667 - mmdet - INFO - Epoch [6][5850/7033]	lr: 1.358e-05, eta: 0:32:47, time: 1.632, data_time: 0.079, memory: 17624, loss_cls: 0.0615, loss_bbox: 0.1978, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3092, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2078, d4.loss_cls: 0.0641, d4.loss_bbox: 0.2007, loss: 1.8895, grad_norm: 16.0221
2025-06-11 21:07:10,672 - mmdet - INFO - Epoch [6][5900/7033]	lr: 1.358e-05, eta: 0:31:24, time: 1.700, data_time: 0.054, memory: 17624, loss_cls: 0.0596, loss_bbox: 0.1849, d0.loss_cls: 0.1480, d0.loss_bbox: 0.2994, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2243, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2008, d3.loss_cls: 0.0663, d3.loss_bbox: 0.1960, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1888, loss: 1.7971, grad_norm: 14.2339
2025-06-11 21:08:32,540 - mmdet - INFO - Epoch [6][5950/7033]	lr: 1.358e-05, eta: 0:30:00, time: 1.637, data_time: 0.058, memory: 17624, loss_cls: 0.0588, loss_bbox: 0.1887, d0.loss_cls: 0.1468, d0.loss_bbox: 0.2967, d1.loss_cls: 0.0923, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2041, d3.loss_cls: 0.0643, d3.loss_bbox: 0.2001, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1924, loss: 1.8076, grad_norm: 32.8373
2025-06-11 21:09:50,674 - mmdet - INFO - Epoch [6][6000/7033]	lr: 1.358e-05, eta: 0:28:37, time: 1.563, data_time: 0.059, memory: 17624, loss_cls: 0.0589, loss_bbox: 0.1926, d0.loss_cls: 0.1546, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0764, d2.loss_bbox: 0.2083, d3.loss_cls: 0.0649, d3.loss_bbox: 0.2033, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1951, loss: 1.8404, grad_norm: 28.4059
2025-06-11 21:12:01,880 - mmdet - INFO - Epoch [6][6050/7033]	lr: 1.358e-05, eta: 0:27:15, time: 2.622, data_time: 0.076, memory: 17624, loss_cls: 0.0580, loss_bbox: 0.1879, d0.loss_cls: 0.1493, d0.loss_bbox: 0.3011, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2246, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2050, d3.loss_cls: 0.0644, d3.loss_bbox: 0.1995, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1906, loss: 1.8099, grad_norm: 139.1613
2025-06-11 21:13:23,409 - mmdet - INFO - Epoch [6][6100/7033]	lr: 1.358e-05, eta: 0:25:52, time: 1.632, data_time: 0.140, memory: 17624, loss_cls: 0.0568, loss_bbox: 0.1852, d0.loss_cls: 0.1436, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0866, d1.loss_bbox: 0.2230, d2.loss_cls: 0.0736, d2.loss_bbox: 0.2001, d3.loss_cls: 0.0617, d3.loss_bbox: 0.1952, d4.loss_cls: 0.0578, d4.loss_bbox: 0.1880, loss: 1.7696, grad_norm: 18.0213
2025-06-11 21:14:43,388 - mmdet - INFO - Epoch [6][6150/7033]	lr: 1.358e-05, eta: 0:24:29, time: 1.599, data_time: 0.096, memory: 17624, loss_cls: 0.0587, loss_bbox: 0.1891, d0.loss_cls: 0.1487, d0.loss_bbox: 0.3006, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2046, d3.loss_cls: 0.0637, d3.loss_bbox: 0.2002, d4.loss_cls: 0.0589, d4.loss_bbox: 0.1930, loss: 1.8155, grad_norm: 25.4889
2025-06-11 21:16:04,510 - mmdet - INFO - Epoch [6][6200/7033]	lr: 1.358e-05, eta: 0:23:05, time: 1.623, data_time: 0.073, memory: 17624, loss_cls: 0.0587, loss_bbox: 0.1890, d0.loss_cls: 0.1444, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2272, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2032, d3.loss_cls: 0.0650, d3.loss_bbox: 0.1987, d4.loss_cls: 0.0595, d4.loss_bbox: 0.1927, loss: 1.8129, grad_norm: 19.7489
2025-06-11 21:17:25,191 - mmdet - INFO - Epoch [6][6250/7033]	lr: 1.358e-05, eta: 0:21:42, time: 1.614, data_time: 0.066, memory: 17624, loss_cls: 0.0629, loss_bbox: 0.1876, d0.loss_cls: 0.1485, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2270, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2052, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2010, d4.loss_cls: 0.0631, d4.loss_bbox: 0.1925, loss: 1.8266, grad_norm: 21.9027
2025-06-11 21:18:46,261 - mmdet - INFO - Epoch [6][6300/7033]	lr: 1.358e-05, eta: 0:20:19, time: 1.621, data_time: 0.062, memory: 17624, loss_cls: 0.0596, loss_bbox: 0.1869, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3019, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2267, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0653, d3.loss_bbox: 0.1986, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1894, loss: 1.8159, grad_norm: 25.1972
2025-06-11 21:20:08,970 - mmdet - INFO - Epoch [6][6350/7033]	lr: 1.358e-05, eta: 0:18:56, time: 1.654, data_time: 0.052, memory: 17624, loss_cls: 0.0598, loss_bbox: 0.1916, d0.loss_cls: 0.1474, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2031, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1947, loss: 1.8372, grad_norm: 36.9611
2025-06-11 21:21:32,077 - mmdet - INFO - Epoch [6][6400/7033]	lr: 1.358e-05, eta: 0:17:33, time: 1.662, data_time: 0.058, memory: 17624, loss_cls: 0.0641, loss_bbox: 0.1992, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2023, loss: 1.8978, grad_norm: 13.4999
2025-06-11 21:22:53,081 - mmdet - INFO - Epoch [6][6450/7033]	lr: 1.358e-05, eta: 0:16:09, time: 1.620, data_time: 0.062, memory: 17624, loss_cls: 0.0563, loss_bbox: 0.1981, d0.loss_cls: 0.1454, d0.loss_bbox: 0.3162, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0625, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0591, d4.loss_bbox: 0.2007, loss: 1.8638, grad_norm: 16.1134
2025-06-11 21:24:16,437 - mmdet - INFO - Epoch [6][6500/7033]	lr: 1.358e-05, eta: 0:14:46, time: 1.666, data_time: 0.069, memory: 17624, loss_cls: 0.0632, loss_bbox: 0.1929, d0.loss_cls: 0.1504, d0.loss_bbox: 0.2974, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2247, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2073, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1944, loss: 1.8425, grad_norm: 22.6371
2025-06-11 21:25:38,872 - mmdet - INFO - Epoch [6][6550/7033]	lr: 1.358e-05, eta: 0:13:23, time: 1.648, data_time: 0.064, memory: 17624, loss_cls: 0.0648, loss_bbox: 0.1938, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2098, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1971, loss: 1.8788, grad_norm: 24.0808
2025-06-11 21:27:01,995 - mmdet - INFO - Epoch [6][6600/7033]	lr: 1.358e-05, eta: 0:12:00, time: 1.663, data_time: 0.070, memory: 17624, loss_cls: 0.0655, loss_bbox: 0.1911, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2056, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2014, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1936, loss: 1.8690, grad_norm: 20.7809
2025-06-11 21:28:25,352 - mmdet - INFO - Epoch [6][6650/7033]	lr: 1.358e-05, eta: 0:10:37, time: 1.669, data_time: 0.081, memory: 17624, loss_cls: 0.0624, loss_bbox: 0.1947, d0.loss_cls: 0.1460, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2282, d2.loss_cls: 0.0778, d2.loss_bbox: 0.2089, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2045, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1981, loss: 1.8460, grad_norm: 23.0891
2025-06-11 21:29:45,704 - mmdet - INFO - Epoch [6][6700/7033]	lr: 1.358e-05, eta: 0:09:13, time: 1.607, data_time: 0.063, memory: 17624, loss_cls: 0.0587, loss_bbox: 0.1918, d0.loss_cls: 0.1484, d0.loss_bbox: 0.3033, d1.loss_cls: 0.0918, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2056, d3.loss_cls: 0.0649, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1938, loss: 1.8210, grad_norm: 29.7971
2025-06-11 21:31:08,244 - mmdet - INFO - Epoch [6][6750/7033]	lr: 1.358e-05, eta: 0:07:50, time: 1.651, data_time: 0.059, memory: 17624, loss_cls: 0.0650, loss_bbox: 0.1972, d0.loss_cls: 0.1511, d0.loss_bbox: 0.3106, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2069, d4.loss_cls: 0.0677, d4.loss_bbox: 0.1984, loss: 1.8938, grad_norm: 17.4618
2025-06-11 21:32:29,669 - mmdet - INFO - Epoch [6][6800/7033]	lr: 1.358e-05, eta: 0:06:27, time: 1.629, data_time: 0.063, memory: 17624, loss_cls: 0.0602, loss_bbox: 0.1894, d0.loss_cls: 0.1467, d0.loss_bbox: 0.2965, d1.loss_cls: 0.0885, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2040, d3.loss_cls: 0.0639, d3.loss_bbox: 0.1995, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1909, loss: 1.8030, grad_norm: 20.7449
2025-06-11 21:33:52,568 - mmdet - INFO - Epoch [6][6850/7033]	lr: 1.358e-05, eta: 0:05:04, time: 1.658, data_time: 0.079, memory: 17624, loss_cls: 0.0651, loss_bbox: 0.1931, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3126, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2038, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1971, loss: 1.8714, grad_norm: 21.6264
2025-06-11 21:35:15,223 - mmdet - INFO - Epoch [6][6900/7033]	lr: 1.358e-05, eta: 0:03:41, time: 1.653, data_time: 0.069, memory: 17624, loss_cls: 0.0574, loss_bbox: 0.1873, d0.loss_cls: 0.1427, d0.loss_bbox: 0.3044, d1.loss_cls: 0.0895, d1.loss_bbox: 0.2254, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2033, d3.loss_cls: 0.0606, d3.loss_bbox: 0.1990, d4.loss_cls: 0.0573, d4.loss_bbox: 0.1907, loss: 1.7918, grad_norm: 17.5967
2025-06-11 21:36:36,283 - mmdet - INFO - Epoch [6][6950/7033]	lr: 1.358e-05, eta: 0:02:18, time: 1.620, data_time: 0.068, memory: 17624, loss_cls: 0.0622, loss_bbox: 0.1967, d0.loss_cls: 0.1497, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2070, d4.loss_cls: 0.0639, d4.loss_bbox: 0.1993, loss: 1.8665, grad_norm: 64.3281
2025-06-11 21:37:57,338 - mmdet - INFO - Epoch [6][7000/7033]	lr: 1.358e-05, eta: 0:00:54, time: 1.622, data_time: 0.084, memory: 17624, loss_cls: 0.0580, loss_bbox: 0.1860, d0.loss_cls: 0.1465, d0.loss_bbox: 0.3000, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2224, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2008, d3.loss_cls: 0.0639, d3.loss_bbox: 0.1968, d4.loss_cls: 0.0594, d4.loss_bbox: 0.1891, loss: 1.7924, grad_norm: 18.1507
2025-06-11 21:38:50,031 - mmdet - INFO - Saving checkpoint at 6 epochs
2025-06-11 22:13:38,806 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-11 22:13:38,807 - mmdet - INFO - Epoch(val) [6][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7957, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8837, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9103, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9226, pts_bbox_NuScenes/car_trans_err: 0.1739, pts_bbox_NuScenes/car_scale_err: 0.1514, pts_bbox_NuScenes/car_orient_err: 0.0403, pts_bbox_NuScenes/car_vel_err: 0.2995, pts_bbox_NuScenes/car_attr_err: 0.1860, pts_bbox_NuScenes/mATE: 0.2800, pts_bbox_NuScenes/mASE: 0.2618, pts_bbox_NuScenes/mAOE: 0.2599, pts_bbox_NuScenes/mAVE: 0.2770, pts_bbox_NuScenes/mAAE: 0.1859, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4294, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6104, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7214, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7539, pts_bbox_NuScenes/truck_trans_err: 0.3418, pts_bbox_NuScenes/truck_scale_err: 0.1957, pts_bbox_NuScenes/truck_orient_err: 0.0446, pts_bbox_NuScenes/truck_vel_err: 0.2590, pts_bbox_NuScenes/truck_attr_err: 0.2038, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0579, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2081, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4048, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4785, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6443, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4332, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8211, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1128, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2975, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5484, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7575, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9044, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9252, pts_bbox_NuScenes/bus_trans_err: 0.3120, pts_bbox_NuScenes/bus_scale_err: 0.1904, pts_bbox_NuScenes/bus_orient_err: 0.0405, pts_bbox_NuScenes/bus_vel_err: 0.4717, pts_bbox_NuScenes/bus_attr_err: 0.2723, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1858, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4324, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.6000, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6804, pts_bbox_NuScenes/trailer_trans_err: 0.4694, pts_bbox_NuScenes/trailer_scale_err: 0.2328, pts_bbox_NuScenes/trailer_orient_err: 0.4678, pts_bbox_NuScenes/trailer_vel_err: 0.2391, pts_bbox_NuScenes/trailer_attr_err: 0.1736, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6259, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7269, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7734, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7880, pts_bbox_NuScenes/barrier_trans_err: 0.2038, pts_bbox_NuScenes/barrier_scale_err: 0.2864, pts_bbox_NuScenes/barrier_orient_err: 0.0458, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6455, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7755, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8028, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8105, pts_bbox_NuScenes/motorcycle_trans_err: 0.2094, pts_bbox_NuScenes/motorcycle_scale_err: 0.2435, pts_bbox_NuScenes/motorcycle_orient_err: 0.2276, pts_bbox_NuScenes/motorcycle_vel_err: 0.4011, pts_bbox_NuScenes/motorcycle_attr_err: 0.2334, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5404, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5906, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5988, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6077, pts_bbox_NuScenes/bicycle_trans_err: 0.1729, pts_bbox_NuScenes/bicycle_scale_err: 0.2644, pts_bbox_NuScenes/bicycle_orient_err: 0.3244, pts_bbox_NuScenes/bicycle_vel_err: 0.2147, pts_bbox_NuScenes/bicycle_attr_err: 0.0047, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8128, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8546, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8756, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8881, pts_bbox_NuScenes/pedestrian_trans_err: 0.1442, pts_bbox_NuScenes/pedestrian_scale_err: 0.2940, pts_bbox_NuScenes/pedestrian_orient_err: 0.3269, pts_bbox_NuScenes/pedestrian_vel_err: 0.2183, pts_bbox_NuScenes/pedestrian_attr_err: 0.1156, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7434, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7730, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7966, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8171, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1286, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3264, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7118, pts_bbox_NuScenes/mAP: 0.6765
