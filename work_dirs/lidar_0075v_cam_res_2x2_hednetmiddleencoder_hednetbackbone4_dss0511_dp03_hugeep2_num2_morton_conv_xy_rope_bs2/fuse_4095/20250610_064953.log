2025-06-10 06:49:53,362 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+unknown
spconv2.0: True
------------------------------------------------------------

2025-06-10 06:49:54,076 - mmdet - INFO - 分布式训练: True
2025-06-10 06:49:54,778 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/fuse_4095'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
gpu_ids = range(0, 2)

2025-06-10 06:49:54,778 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-10 06:49:56,547 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-10 06:49:56,811 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-10 06:49:56,899 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:56,900 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:56,900 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:56,954 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,004 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,055 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,055 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,113 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,164 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,215 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,273 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,323 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,374 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,431 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,481 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,532 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,590 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,640 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,691 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,749 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,807 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,857 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,908 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:57,957 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,008 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,066 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,107 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,158 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,208 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,258 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,315 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,364 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,414 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-10 06:49:58,468 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-10 06:49:58,538 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-10 06:50:17,633 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-10 06:50:18,523 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias

2025-06-10 06:50:18,536 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/fuse_4095
2025-06-10 06:50:18,608 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-10 06:50:18,608 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-10 06:50:18,609 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/fuse_4095 by HardDiskBackend.
2025-06-10 06:51:27,566 - mmdet - INFO - Epoch [1][50/7033]	lr: 3.987e-05, eta: 16:02:29, time: 1.370, data_time: 0.109, memory: 16750, loss_cls: 1.1629, loss_bbox: 1.6061, d0.loss_cls: 1.1139, d0.loss_bbox: 1.6455, d1.loss_cls: 1.1928, d1.loss_bbox: 1.6106, d2.loss_cls: 1.0739, d2.loss_bbox: 1.5773, d3.loss_cls: 1.0304, d3.loss_bbox: 1.5995, d4.loss_cls: 1.0280, d4.loss_bbox: 1.5617, loss: 16.2024, grad_norm: 16.6642
2025-06-10 06:52:18,939 - mmdet - INFO - Epoch [1][100/7033]	lr: 4.653e-05, eta: 14:01:07, time: 1.027, data_time: 0.027, memory: 16901, loss_cls: 0.8583, loss_bbox: 1.2860, d0.loss_cls: 0.9274, d0.loss_bbox: 1.3852, d1.loss_cls: 0.9206, d1.loss_bbox: 1.3371, d2.loss_cls: 0.8529, d2.loss_bbox: 1.3019, d3.loss_cls: 0.8413, d3.loss_bbox: 1.2993, d4.loss_cls: 0.8282, d4.loss_bbox: 1.2658, loss: 13.1041, grad_norm: 13.4240
2025-06-10 06:53:10,251 - mmdet - INFO - Epoch [1][150/7033]	lr: 5.320e-05, eta: 13:19:48, time: 1.026, data_time: 0.026, memory: 16901, loss_cls: 0.7515, loss_bbox: 1.1777, d0.loss_cls: 0.8547, d0.loss_bbox: 1.3166, d1.loss_cls: 0.7884, d1.loss_bbox: 1.2482, d2.loss_cls: 0.7633, d2.loss_bbox: 1.2047, d3.loss_cls: 0.7495, d3.loss_bbox: 1.2001, d4.loss_cls: 0.7333, d4.loss_bbox: 1.1783, loss: 11.9663, grad_norm: 12.2329
2025-06-10 06:54:01,908 - mmdet - INFO - Epoch [1][200/7033]	lr: 5.987e-05, eta: 12:59:55, time: 1.033, data_time: 0.027, memory: 16901, loss_cls: 0.6909, loss_bbox: 1.1173, d0.loss_cls: 0.7981, d0.loss_bbox: 1.2782, d1.loss_cls: 0.7300, d1.loss_bbox: 1.2017, d2.loss_cls: 0.7049, d2.loss_bbox: 1.1617, d3.loss_cls: 0.6990, d3.loss_bbox: 1.1529, d4.loss_cls: 0.6811, d4.loss_bbox: 1.1328, loss: 11.3487, grad_norm: 11.0208
2025-06-10 06:54:53,386 - mmdet - INFO - Epoch [1][250/7033]	lr: 6.653e-05, eta: 12:47:09, time: 1.030, data_time: 0.030, memory: 16901, loss_cls: 0.6539, loss_bbox: 1.0654, d0.loss_cls: 0.7606, d0.loss_bbox: 1.2341, d1.loss_cls: 0.6948, d1.loss_bbox: 1.1411, d2.loss_cls: 0.6840, d2.loss_bbox: 1.1014, d3.loss_cls: 0.6795, d3.loss_bbox: 1.0863, d4.loss_cls: 0.6493, d4.loss_bbox: 1.0786, loss: 10.8291, grad_norm: 11.0309
2025-06-10 06:55:44,554 - mmdet - INFO - Epoch [1][300/7033]	lr: 7.320e-05, eta: 12:37:38, time: 1.023, data_time: 0.031, memory: 16901, loss_cls: 0.6096, loss_bbox: 1.0253, d0.loss_cls: 0.7331, d0.loss_bbox: 1.1965, d1.loss_cls: 0.6614, d1.loss_bbox: 1.0978, d2.loss_cls: 0.6439, d2.loss_bbox: 1.0621, d3.loss_cls: 0.6335, d3.loss_bbox: 1.0487, d4.loss_cls: 0.6035, d4.loss_bbox: 1.0354, loss: 10.3507, grad_norm: 10.6973
2025-06-10 06:56:35,727 - mmdet - INFO - Epoch [1][350/7033]	lr: 7.987e-05, eta: 12:30:36, time: 1.023, data_time: 0.030, memory: 16901, loss_cls: 0.5653, loss_bbox: 1.0348, d0.loss_cls: 0.7056, d0.loss_bbox: 1.1973, d1.loss_cls: 0.6209, d1.loss_bbox: 1.0985, d2.loss_cls: 0.6120, d2.loss_bbox: 1.0617, d3.loss_cls: 0.5995, d3.loss_bbox: 1.0558, d4.loss_cls: 0.5621, d4.loss_bbox: 1.0440, loss: 10.1575, grad_norm: 10.9317
2025-06-10 06:57:28,100 - mmdet - INFO - Epoch [1][400/7033]	lr: 8.653e-05, eta: 12:27:12, time: 1.047, data_time: 0.051, memory: 16939, loss_cls: 0.5227, loss_bbox: 0.9986, d0.loss_cls: 0.6730, d0.loss_bbox: 1.1697, d1.loss_cls: 0.5854, d1.loss_bbox: 1.0726, d2.loss_cls: 0.5721, d2.loss_bbox: 1.0301, d3.loss_cls: 0.5589, d3.loss_bbox: 1.0203, d4.loss_cls: 0.5161, d4.loss_bbox: 1.0101, loss: 9.7298, grad_norm: 9.7094
2025-06-10 06:58:22,593 - mmdet - INFO - Epoch [1][450/7033]	lr: 9.320e-05, eta: 12:27:38, time: 1.090, data_time: 0.026, memory: 16939, loss_cls: 0.5022, loss_bbox: 0.9806, d0.loss_cls: 0.6407, d0.loss_bbox: 1.1426, d1.loss_cls: 0.5545, d1.loss_bbox: 1.0472, d2.loss_cls: 0.5530, d2.loss_bbox: 1.0098, d3.loss_cls: 0.5431, d3.loss_bbox: 0.9985, d4.loss_cls: 0.4940, d4.loss_bbox: 0.9945, loss: 9.4607, grad_norm: 10.2116
2025-06-10 06:59:14,297 - mmdet - INFO - Epoch [1][500/7033]	lr: 9.987e-05, eta: 12:23:56, time: 1.034, data_time: 0.028, memory: 16939, loss_cls: 0.4770, loss_bbox: 0.9539, d0.loss_cls: 0.6386, d0.loss_bbox: 1.1346, d1.loss_cls: 0.5299, d1.loss_bbox: 1.0298, d2.loss_cls: 0.5211, d2.loss_bbox: 0.9831, d3.loss_cls: 0.5110, d3.loss_bbox: 0.9846, d4.loss_cls: 0.4689, d4.loss_bbox: 0.9714, loss: 9.2037, grad_norm: 10.9816
2025-06-10 07:00:05,858 - mmdet - INFO - Epoch [1][550/7033]	lr: 1.000e-04, eta: 12:20:33, time: 1.031, data_time: 0.030, memory: 16985, loss_cls: 0.4528, loss_bbox: 0.9420, d0.loss_cls: 0.6012, d0.loss_bbox: 1.1242, d1.loss_cls: 0.4970, d1.loss_bbox: 1.0130, d2.loss_cls: 0.4873, d2.loss_bbox: 0.9786, d3.loss_cls: 0.4770, d3.loss_bbox: 0.9722, d4.loss_cls: 0.4407, d4.loss_bbox: 0.9606, loss: 8.9467, grad_norm: 9.6239
2025-06-10 07:00:58,072 - mmdet - INFO - Epoch [1][600/7033]	lr: 1.000e-04, eta: 12:18:22, time: 1.044, data_time: 0.030, memory: 17173, loss_cls: 0.4117, loss_bbox: 0.9319, d0.loss_cls: 0.5712, d0.loss_bbox: 1.1228, d1.loss_cls: 0.4718, d1.loss_bbox: 1.0057, d2.loss_cls: 0.4618, d2.loss_bbox: 0.9600, d3.loss_cls: 0.4486, d3.loss_bbox: 0.9510, d4.loss_cls: 0.4099, d4.loss_bbox: 0.9381, loss: 8.6846, grad_norm: 10.0480
2025-06-10 07:01:49,364 - mmdet - INFO - Epoch [1][650/7033]	lr: 1.000e-04, eta: 12:15:23, time: 1.026, data_time: 0.029, memory: 17173, loss_cls: 0.4057, loss_bbox: 0.9254, d0.loss_cls: 0.5566, d0.loss_bbox: 1.1205, d1.loss_cls: 0.4444, d1.loss_bbox: 1.0033, d2.loss_cls: 0.4340, d2.loss_bbox: 0.9578, d3.loss_cls: 0.4235, d3.loss_bbox: 0.9484, d4.loss_cls: 0.4009, d4.loss_bbox: 0.9306, loss: 8.5509, grad_norm: 10.7594
2025-06-10 07:02:43,894 - mmdet - INFO - Epoch [1][700/7033]	lr: 1.000e-04, eta: 12:15:55, time: 1.091, data_time: 0.030, memory: 17173, loss_cls: 0.4165, loss_bbox: 0.8945, d0.loss_cls: 0.5506, d0.loss_bbox: 1.0931, d1.loss_cls: 0.4501, d1.loss_bbox: 0.9691, d2.loss_cls: 0.4350, d2.loss_bbox: 0.9284, d3.loss_cls: 0.4289, d3.loss_bbox: 0.9127, d4.loss_cls: 0.4071, d4.loss_bbox: 0.8985, loss: 8.3845, grad_norm: 10.8437
2025-06-10 07:03:36,209 - mmdet - INFO - Epoch [1][750/7033]	lr: 1.000e-04, eta: 12:14:13, time: 1.046, data_time: 0.032, memory: 17173, loss_cls: 0.3982, loss_bbox: 0.8969, d0.loss_cls: 0.5476, d0.loss_bbox: 1.1167, d1.loss_cls: 0.4228, d1.loss_bbox: 0.9817, d2.loss_cls: 0.4151, d2.loss_bbox: 0.9367, d3.loss_cls: 0.4052, d3.loss_bbox: 0.9185, d4.loss_cls: 0.3902, d4.loss_bbox: 0.9022, loss: 8.3318, grad_norm: 11.4520
2025-06-10 07:04:28,152 - mmdet - INFO - Epoch [1][800/7033]	lr: 1.000e-04, eta: 12:12:17, time: 1.039, data_time: 0.031, memory: 17173, loss_cls: 0.3478, loss_bbox: 0.8639, d0.loss_cls: 0.5226, d0.loss_bbox: 1.0776, d1.loss_cls: 0.3662, d1.loss_bbox: 0.9375, d2.loss_cls: 0.3471, d2.loss_bbox: 0.8921, d3.loss_cls: 0.3406, d3.loss_bbox: 0.8761, d4.loss_cls: 0.3367, d4.loss_bbox: 0.8547, loss: 7.7629, grad_norm: 11.5360
2025-06-10 07:05:20,605 - mmdet - INFO - Epoch [1][850/7033]	lr: 1.000e-04, eta: 12:10:54, time: 1.049, data_time: 0.035, memory: 17173, loss_cls: 0.3107, loss_bbox: 0.8353, d0.loss_cls: 0.5172, d0.loss_bbox: 1.0762, d1.loss_cls: 0.3235, d1.loss_bbox: 0.9354, d2.loss_cls: 0.3087, d2.loss_bbox: 0.8791, d3.loss_cls: 0.3053, d3.loss_bbox: 0.8516, d4.loss_cls: 0.3037, d4.loss_bbox: 0.8330, loss: 7.4796, grad_norm: 12.5233
2025-06-10 07:06:12,453 - mmdet - INFO - Epoch [1][900/7033]	lr: 1.000e-04, eta: 12:09:07, time: 1.037, data_time: 0.033, memory: 17173, loss_cls: 0.2981, loss_bbox: 0.8200, d0.loss_cls: 0.5089, d0.loss_bbox: 1.0725, d1.loss_cls: 0.3284, d1.loss_bbox: 0.9221, d2.loss_cls: 0.3086, d2.loss_bbox: 0.8593, d3.loss_cls: 0.2996, d3.loss_bbox: 0.8323, d4.loss_cls: 0.2950, d4.loss_bbox: 0.8118, loss: 7.3567, grad_norm: 13.9238
2025-06-10 07:07:04,169 - mmdet - INFO - Epoch [1][950/7033]	lr: 1.000e-04, eta: 12:07:20, time: 1.034, data_time: 0.035, memory: 17173, loss_cls: 0.2650, loss_bbox: 0.7759, d0.loss_cls: 0.4832, d0.loss_bbox: 1.0385, d1.loss_cls: 0.2827, d1.loss_bbox: 0.8570, d2.loss_cls: 0.2671, d2.loss_bbox: 0.7945, d3.loss_cls: 0.2580, d3.loss_bbox: 0.7818, d4.loss_cls: 0.2620, d4.loss_bbox: 0.7659, loss: 6.8317, grad_norm: 16.5758
2025-06-10 07:07:55,779 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 07:07:55,780 - mmdet - INFO - Epoch [1][1000/7033]	lr: 1.000e-04, eta: 12:05:34, time: 1.032, data_time: 0.032, memory: 17173, loss_cls: 0.2677, loss_bbox: 0.7278, d0.loss_cls: 0.4485, d0.loss_bbox: 1.0131, d1.loss_cls: 0.2907, d1.loss_bbox: 0.7938, d2.loss_cls: 0.2679, d2.loss_bbox: 0.7329, d3.loss_cls: 0.2570, d3.loss_bbox: 0.7269, d4.loss_cls: 0.2601, d4.loss_bbox: 0.7137, loss: 6.4999, grad_norm: 15.7842
2025-06-10 07:08:47,752 - mmdet - INFO - Epoch [1][1050/7033]	lr: 1.000e-04, eta: 12:04:07, time: 1.039, data_time: 0.034, memory: 17173, loss_cls: 0.2544, loss_bbox: 0.6815, d0.loss_cls: 0.4406, d0.loss_bbox: 1.0342, d1.loss_cls: 0.3041, d1.loss_bbox: 0.7362, d2.loss_cls: 0.2655, d2.loss_bbox: 0.6676, d3.loss_cls: 0.2474, d3.loss_bbox: 0.6719, d4.loss_cls: 0.2486, d4.loss_bbox: 0.6608, loss: 6.2128, grad_norm: 18.3990
2025-06-10 07:09:40,272 - mmdet - INFO - Epoch [1][1100/7033]	lr: 1.000e-04, eta: 12:03:03, time: 1.050, data_time: 0.033, memory: 17173, loss_cls: 0.2245, loss_bbox: 0.6018, d0.loss_cls: 0.4123, d0.loss_bbox: 0.9697, d1.loss_cls: 0.2974, d1.loss_bbox: 0.6211, d2.loss_cls: 0.2506, d2.loss_bbox: 0.5805, d3.loss_cls: 0.2286, d3.loss_bbox: 0.5872, d4.loss_cls: 0.2239, d4.loss_bbox: 0.5814, loss: 5.5789, grad_norm: 19.8218
2025-06-10 07:10:32,076 - mmdet - INFO - Epoch [1][1150/7033]	lr: 1.000e-04, eta: 12:01:35, time: 1.036, data_time: 0.029, memory: 17173, loss_cls: 0.2140, loss_bbox: 0.5677, d0.loss_cls: 0.3852, d0.loss_bbox: 0.9326, d1.loss_cls: 0.2983, d1.loss_bbox: 0.5605, d2.loss_cls: 0.2410, d2.loss_bbox: 0.5289, d3.loss_cls: 0.2177, d3.loss_bbox: 0.5480, d4.loss_cls: 0.2146, d4.loss_bbox: 0.5467, loss: 5.2553, grad_norm: 27.3066
2025-06-10 07:11:27,829 - mmdet - INFO - Epoch [1][1200/7033]	lr: 1.000e-04, eta: 12:02:27, time: 1.116, data_time: 0.044, memory: 17173, loss_cls: 0.2071, loss_bbox: 0.5403, d0.loss_cls: 0.3563, d0.loss_bbox: 0.8974, d1.loss_cls: 0.2783, d1.loss_bbox: 0.5365, d2.loss_cls: 0.2239, d2.loss_bbox: 0.5221, d3.loss_cls: 0.2094, d3.loss_bbox: 0.5283, d4.loss_cls: 0.2083, d4.loss_bbox: 0.5239, loss: 5.0317, grad_norm: 18.3985
2025-06-10 07:12:20,584 - mmdet - INFO - Epoch [1][1250/7033]	lr: 1.000e-04, eta: 12:01:30, time: 1.055, data_time: 0.032, memory: 17173, loss_cls: 0.1928, loss_bbox: 0.4870, d0.loss_cls: 0.3167, d0.loss_bbox: 0.7899, d1.loss_cls: 0.2708, d1.loss_bbox: 0.4780, d2.loss_cls: 0.2146, d2.loss_bbox: 0.4698, d3.loss_cls: 0.1936, d3.loss_bbox: 0.4819, d4.loss_cls: 0.1910, d4.loss_bbox: 0.4796, loss: 4.5658, grad_norm: 22.7969
2025-06-10 07:13:13,736 - mmdet - INFO - Epoch [1][1300/7033]	lr: 1.000e-04, eta: 12:00:46, time: 1.063, data_time: 0.032, memory: 17173, loss_cls: 0.1796, loss_bbox: 0.4483, d0.loss_cls: 0.2783, d0.loss_bbox: 0.6828, d1.loss_cls: 0.2553, d1.loss_bbox: 0.4253, d2.loss_cls: 0.1961, d2.loss_bbox: 0.4208, d3.loss_cls: 0.1811, d3.loss_bbox: 0.4272, d4.loss_cls: 0.1803, d4.loss_bbox: 0.4341, loss: 4.1092, grad_norm: 22.9490
2025-06-10 07:14:09,384 - mmdet - INFO - Epoch [1][1350/7033]	lr: 1.000e-04, eta: 12:01:17, time: 1.113, data_time: 0.033, memory: 17173, loss_cls: 0.1601, loss_bbox: 0.4233, d0.loss_cls: 0.2492, d0.loss_bbox: 0.6146, d1.loss_cls: 0.2217, d1.loss_bbox: 0.4092, d2.loss_cls: 0.1743, d2.loss_bbox: 0.3966, d3.loss_cls: 0.1604, d3.loss_bbox: 0.4034, d4.loss_cls: 0.1580, d4.loss_bbox: 0.4089, loss: 3.7796, grad_norm: 26.6644
2025-06-10 07:15:30,913 - mmdet - INFO - Epoch [1][1400/7033]	lr: 1.000e-04, eta: 12:14:17, time: 1.631, data_time: 0.453, memory: 17173, loss_cls: 0.1541, loss_bbox: 0.4134, d0.loss_cls: 0.2434, d0.loss_bbox: 0.5693, d1.loss_cls: 0.2269, d1.loss_bbox: 0.3929, d2.loss_cls: 0.1714, d2.loss_bbox: 0.3835, d3.loss_cls: 0.1550, d3.loss_bbox: 0.3954, d4.loss_cls: 0.1498, d4.loss_bbox: 0.4030, loss: 3.6581, grad_norm: 33.0705
2025-06-10 07:16:47,661 - mmdet - INFO - Epoch [1][1450/7033]	lr: 1.000e-04, eta: 12:24:02, time: 1.535, data_time: 0.393, memory: 17173, loss_cls: 0.1496, loss_bbox: 0.4060, d0.loss_cls: 0.2318, d0.loss_bbox: 0.5412, d1.loss_cls: 0.2114, d1.loss_bbox: 0.3762, d2.loss_cls: 0.1592, d2.loss_bbox: 0.3779, d3.loss_cls: 0.1498, d3.loss_bbox: 0.3852, d4.loss_cls: 0.1491, d4.loss_bbox: 0.3968, loss: 3.5342, grad_norm: 31.9133
2025-06-10 07:17:43,062 - mmdet - INFO - Epoch [1][1500/7033]	lr: 1.000e-04, eta: 12:23:24, time: 1.108, data_time: 0.121, memory: 17173, loss_cls: 0.1489, loss_bbox: 0.3909, d0.loss_cls: 0.2380, d0.loss_bbox: 0.5153, d1.loss_cls: 0.2085, d1.loss_bbox: 0.3724, d2.loss_cls: 0.1625, d2.loss_bbox: 0.3636, d3.loss_cls: 0.1521, d3.loss_bbox: 0.3732, d4.loss_cls: 0.1488, d4.loss_bbox: 0.3823, loss: 3.4565, grad_norm: 28.5733
2025-06-10 07:18:35,642 - mmdet - INFO - Epoch [1][1550/7033]	lr: 1.000e-04, eta: 12:21:31, time: 1.051, data_time: 0.030, memory: 17173, loss_cls: 0.1505, loss_bbox: 0.3812, d0.loss_cls: 0.2332, d0.loss_bbox: 0.4967, d1.loss_cls: 0.2123, d1.loss_bbox: 0.3574, d2.loss_cls: 0.1641, d2.loss_bbox: 0.3553, d3.loss_cls: 0.1520, d3.loss_bbox: 0.3625, d4.loss_cls: 0.1504, d4.loss_bbox: 0.3689, loss: 3.3846, grad_norm: 29.4860
2025-06-10 07:19:28,517 - mmdet - INFO - Epoch [1][1600/7033]	lr: 1.000e-04, eta: 12:19:49, time: 1.058, data_time: 0.030, memory: 17173, loss_cls: 0.1560, loss_bbox: 0.3870, d0.loss_cls: 0.2281, d0.loss_bbox: 0.4863, d1.loss_cls: 0.1997, d1.loss_bbox: 0.3661, d2.loss_cls: 0.1588, d2.loss_bbox: 0.3572, d3.loss_cls: 0.1513, d3.loss_bbox: 0.3637, d4.loss_cls: 0.1516, d4.loss_bbox: 0.3725, loss: 3.3784, grad_norm: 31.6499
2025-06-10 07:20:26,172 - mmdet - INFO - Epoch [1][1650/7033]	lr: 1.000e-04, eta: 12:20:08, time: 1.153, data_time: 0.060, memory: 17173, loss_cls: 0.1368, loss_bbox: 0.3543, d0.loss_cls: 0.2177, d0.loss_bbox: 0.4591, d1.loss_cls: 0.1854, d1.loss_bbox: 0.3390, d2.loss_cls: 0.1514, d2.loss_bbox: 0.3318, d3.loss_cls: 0.1399, d3.loss_bbox: 0.3386, d4.loss_cls: 0.1344, d4.loss_bbox: 0.3475, loss: 3.1359, grad_norm: 29.0170
2025-06-10 07:21:20,946 - mmdet - INFO - Epoch [1][1700/7033]	lr: 1.000e-04, eta: 12:19:13, time: 1.095, data_time: 0.088, memory: 17173, loss_cls: 0.1409, loss_bbox: 0.3710, d0.loss_cls: 0.2169, d0.loss_bbox: 0.4800, d1.loss_cls: 0.1789, d1.loss_bbox: 0.3564, d2.loss_cls: 0.1457, d2.loss_bbox: 0.3445, d3.loss_cls: 0.1397, d3.loss_bbox: 0.3492, d4.loss_cls: 0.1369, d4.loss_bbox: 0.3610, loss: 3.2210, grad_norm: 29.7173
2025-06-10 07:22:15,757 - mmdet - INFO - Epoch [1][1750/7033]	lr: 1.000e-04, eta: 12:18:20, time: 1.096, data_time: 0.110, memory: 17203, loss_cls: 0.1344, loss_bbox: 0.3555, d0.loss_cls: 0.2103, d0.loss_bbox: 0.4559, d1.loss_cls: 0.1683, d1.loss_bbox: 0.3352, d2.loss_cls: 0.1367, d2.loss_bbox: 0.3309, d3.loss_cls: 0.1318, d3.loss_bbox: 0.3387, d4.loss_cls: 0.1335, d4.loss_bbox: 0.3444, loss: 3.0756, grad_norm: 29.0933
2025-06-10 07:23:12,327 - mmdet - INFO - Epoch [1][1800/7033]	lr: 1.000e-04, eta: 12:18:06, time: 1.131, data_time: 0.028, memory: 17203, loss_cls: 0.1257, loss_bbox: 0.3640, d0.loss_cls: 0.2045, d0.loss_bbox: 0.4502, d1.loss_cls: 0.1664, d1.loss_bbox: 0.3371, d2.loss_cls: 0.1320, d2.loss_bbox: 0.3359, d3.loss_cls: 0.1258, d3.loss_bbox: 0.3410, d4.loss_cls: 0.1231, d4.loss_bbox: 0.3500, loss: 3.0557, grad_norm: 34.4523
2025-06-10 07:24:09,526 - mmdet - INFO - Epoch [1][1850/7033]	lr: 1.000e-04, eta: 12:18:03, time: 1.144, data_time: 0.069, memory: 17203, loss_cls: 0.1259, loss_bbox: 0.3322, d0.loss_cls: 0.2098, d0.loss_bbox: 0.4342, d1.loss_cls: 0.1643, d1.loss_bbox: 0.3227, d2.loss_cls: 0.1309, d2.loss_bbox: 0.3123, d3.loss_cls: 0.1253, d3.loss_bbox: 0.3189, d4.loss_cls: 0.1204, d4.loss_bbox: 0.3245, loss: 2.9214, grad_norm: 36.4383
2025-06-10 07:25:04,452 - mmdet - INFO - Epoch [1][1900/7033]	lr: 1.000e-04, eta: 12:17:09, time: 1.099, data_time: 0.028, memory: 17203, loss_cls: 0.1296, loss_bbox: 0.3436, d0.loss_cls: 0.2044, d0.loss_bbox: 0.4272, d1.loss_cls: 0.1656, d1.loss_bbox: 0.3183, d2.loss_cls: 0.1381, d2.loss_bbox: 0.3115, d3.loss_cls: 0.1296, d3.loss_bbox: 0.3221, d4.loss_cls: 0.1268, d4.loss_bbox: 0.3299, loss: 2.9467, grad_norm: 90.6214
2025-06-10 07:26:02,178 - mmdet - INFO - Epoch [1][1950/7033]	lr: 1.000e-04, eta: 12:17:13, time: 1.154, data_time: 0.137, memory: 17203, loss_cls: 0.1233, loss_bbox: 0.3425, d0.loss_cls: 0.2069, d0.loss_bbox: 0.4446, d1.loss_cls: 0.1578, d1.loss_bbox: 0.3292, d2.loss_cls: 0.1331, d2.loss_bbox: 0.3196, d3.loss_cls: 0.1265, d3.loss_bbox: 0.3239, d4.loss_cls: 0.1229, d4.loss_bbox: 0.3306, loss: 2.9611, grad_norm: 33.5123
2025-06-10 07:26:59,803 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 07:26:59,803 - mmdet - INFO - Epoch [1][2000/7033]	lr: 1.000e-04, eta: 12:17:12, time: 1.152, data_time: 0.119, memory: 17203, loss_cls: 0.1364, loss_bbox: 0.3628, d0.loss_cls: 0.2177, d0.loss_bbox: 0.4521, d1.loss_cls: 0.1711, d1.loss_bbox: 0.3426, d2.loss_cls: 0.1413, d2.loss_bbox: 0.3363, d3.loss_cls: 0.1384, d3.loss_bbox: 0.3393, d4.loss_cls: 0.1361, d4.loss_bbox: 0.3512, loss: 3.1253, grad_norm: 78.7001
2025-06-10 07:27:58,384 - mmdet - INFO - Epoch [1][2050/7033]	lr: 1.000e-04, eta: 12:17:27, time: 1.172, data_time: 0.159, memory: 17203, loss_cls: 0.1282, loss_bbox: 0.3490, d0.loss_cls: 0.2089, d0.loss_bbox: 0.4330, d1.loss_cls: 0.1559, d1.loss_bbox: 0.3256, d2.loss_cls: 0.1326, d2.loss_bbox: 0.3204, d3.loss_cls: 0.1292, d3.loss_bbox: 0.3260, d4.loss_cls: 0.1267, d4.loss_bbox: 0.3378, loss: 2.9732, grad_norm: 38.3856
2025-06-10 07:28:50,679 - mmdet - INFO - Epoch [1][2100/7033]	lr: 1.000e-04, eta: 12:15:38, time: 1.046, data_time: 0.032, memory: 17203, loss_cls: 0.1309, loss_bbox: 0.3433, d0.loss_cls: 0.2024, d0.loss_bbox: 0.4357, d1.loss_cls: 0.1575, d1.loss_bbox: 0.3296, d2.loss_cls: 0.1373, d2.loss_bbox: 0.3187, d3.loss_cls: 0.1292, d3.loss_bbox: 0.3244, d4.loss_cls: 0.1275, d4.loss_bbox: 0.3314, loss: 2.9681, grad_norm: 26.1377
2025-06-10 07:29:45,341 - mmdet - INFO - Epoch [1][2150/7033]	lr: 1.000e-04, eta: 12:14:36, time: 1.093, data_time: 0.089, memory: 17203, loss_cls: 0.1275, loss_bbox: 0.3393, d0.loss_cls: 0.2106, d0.loss_bbox: 0.4322, d1.loss_cls: 0.1534, d1.loss_bbox: 0.3224, d2.loss_cls: 0.1322, d2.loss_bbox: 0.3160, d3.loss_cls: 0.1261, d3.loss_bbox: 0.3244, d4.loss_cls: 0.1248, d4.loss_bbox: 0.3307, loss: 2.9397, grad_norm: 83.9269
2025-06-10 07:30:41,205 - mmdet - INFO - Epoch [1][2200/7033]	lr: 1.000e-04, eta: 12:13:56, time: 1.117, data_time: 0.114, memory: 17203, loss_cls: 0.1305, loss_bbox: 0.3299, d0.loss_cls: 0.1958, d0.loss_bbox: 0.4304, d1.loss_cls: 0.1489, d1.loss_bbox: 0.3207, d2.loss_cls: 0.1307, d2.loss_bbox: 0.3093, d3.loss_cls: 0.1285, d3.loss_bbox: 0.3150, d4.loss_cls: 0.1259, d4.loss_bbox: 0.3198, loss: 2.8852, grad_norm: 34.7640
2025-06-10 07:31:35,404 - mmdet - INFO - Epoch [1][2250/7033]	lr: 1.000e-04, eta: 12:12:46, time: 1.084, data_time: 0.046, memory: 17203, loss_cls: 0.1215, loss_bbox: 0.3287, d0.loss_cls: 0.2033, d0.loss_bbox: 0.4204, d1.loss_cls: 0.1446, d1.loss_bbox: 0.3188, d2.loss_cls: 0.1249, d2.loss_bbox: 0.3131, d3.loss_cls: 0.1204, d3.loss_bbox: 0.3169, d4.loss_cls: 0.1206, d4.loss_bbox: 0.3193, loss: 2.8524, grad_norm: 42.8328
2025-06-10 07:32:29,691 - mmdet - INFO - Epoch [1][2300/7033]	lr: 1.000e-04, eta: 12:11:38, time: 1.086, data_time: 0.033, memory: 17203, loss_cls: 0.1201, loss_bbox: 0.3219, d0.loss_cls: 0.2033, d0.loss_bbox: 0.4136, d1.loss_cls: 0.1487, d1.loss_bbox: 0.3094, d2.loss_cls: 0.1244, d2.loss_bbox: 0.3037, d3.loss_cls: 0.1222, d3.loss_bbox: 0.3077, d4.loss_cls: 0.1198, d4.loss_bbox: 0.3132, loss: 2.8079, grad_norm: 37.1570
2025-06-10 07:33:25,271 - mmdet - INFO - Epoch [1][2350/7033]	lr: 1.000e-04, eta: 12:10:52, time: 1.112, data_time: 0.073, memory: 17291, loss_cls: 0.1246, loss_bbox: 0.3324, d0.loss_cls: 0.1977, d0.loss_bbox: 0.4248, d1.loss_cls: 0.1495, d1.loss_bbox: 0.3190, d2.loss_cls: 0.1251, d2.loss_bbox: 0.3121, d3.loss_cls: 0.1218, d3.loss_bbox: 0.3172, d4.loss_cls: 0.1226, d4.loss_bbox: 0.3191, loss: 2.8660, grad_norm: 24.1693
2025-06-10 07:34:20,172 - mmdet - INFO - Epoch [1][2400/7033]	lr: 1.000e-04, eta: 12:09:55, time: 1.098, data_time: 0.064, memory: 17291, loss_cls: 0.1172, loss_bbox: 0.3271, d0.loss_cls: 0.2043, d0.loss_bbox: 0.4172, d1.loss_cls: 0.1411, d1.loss_bbox: 0.3162, d2.loss_cls: 0.1223, d2.loss_bbox: 0.3075, d3.loss_cls: 0.1171, d3.loss_bbox: 0.3113, d4.loss_cls: 0.1153, d4.loss_bbox: 0.3169, loss: 2.8137, grad_norm: 46.8553
2025-06-10 07:35:15,176 - mmdet - INFO - Epoch [1][2450/7033]	lr: 1.000e-04, eta: 12:09:00, time: 1.100, data_time: 0.030, memory: 17291, loss_cls: 0.1186, loss_bbox: 0.3175, d0.loss_cls: 0.1975, d0.loss_bbox: 0.4127, d1.loss_cls: 0.1451, d1.loss_bbox: 0.3105, d2.loss_cls: 0.1203, d2.loss_bbox: 0.3038, d3.loss_cls: 0.1159, d3.loss_bbox: 0.3077, d4.loss_cls: 0.1147, d4.loss_bbox: 0.3096, loss: 2.7739, grad_norm: 29.1624
2025-06-10 07:36:09,514 - mmdet - INFO - Epoch [1][2500/7033]	lr: 1.000e-04, eta: 12:07:54, time: 1.087, data_time: 0.031, memory: 17291, loss_cls: 0.1166, loss_bbox: 0.3250, d0.loss_cls: 0.1966, d0.loss_bbox: 0.4195, d1.loss_cls: 0.1407, d1.loss_bbox: 0.3132, d2.loss_cls: 0.1244, d2.loss_bbox: 0.3051, d3.loss_cls: 0.1211, d3.loss_bbox: 0.3080, d4.loss_cls: 0.1173, d4.loss_bbox: 0.3165, loss: 2.8039, grad_norm: 47.5270
2025-06-10 07:37:06,230 - mmdet - INFO - Epoch [1][2550/7033]	lr: 1.000e-04, eta: 12:07:26, time: 1.134, data_time: 0.146, memory: 17291, loss_cls: 0.1146, loss_bbox: 0.3142, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3962, d1.loss_cls: 0.1347, d1.loss_bbox: 0.3043, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2968, d3.loss_cls: 0.1135, d3.loss_bbox: 0.3018, d4.loss_cls: 0.1124, d4.loss_bbox: 0.3066, loss: 2.7059, grad_norm: 32.5419
2025-06-10 07:38:04,182 - mmdet - INFO - Epoch [1][2600/7033]	lr: 1.000e-04, eta: 12:07:15, time: 1.159, data_time: 0.033, memory: 17291, loss_cls: 0.1250, loss_bbox: 0.3186, d0.loss_cls: 0.2032, d0.loss_bbox: 0.4159, d1.loss_cls: 0.1481, d1.loss_bbox: 0.3171, d2.loss_cls: 0.1298, d2.loss_bbox: 0.3028, d3.loss_cls: 0.1227, d3.loss_bbox: 0.3059, d4.loss_cls: 0.1232, d4.loss_bbox: 0.3082, loss: 2.8206, grad_norm: 29.9231
2025-06-10 07:38:56,563 - mmdet - INFO - Epoch [1][2650/7033]	lr: 1.000e-04, eta: 12:05:39, time: 1.048, data_time: 0.033, memory: 17291, loss_cls: 0.1170, loss_bbox: 0.3329, d0.loss_cls: 0.1992, d0.loss_bbox: 0.4169, d1.loss_cls: 0.1451, d1.loss_bbox: 0.3202, d2.loss_cls: 0.1248, d2.loss_bbox: 0.3123, d3.loss_cls: 0.1201, d3.loss_bbox: 0.3180, d4.loss_cls: 0.1162, d4.loss_bbox: 0.3253, loss: 2.8481, grad_norm: 34.0822
2025-06-10 07:39:48,815 - mmdet - INFO - Epoch [1][2700/7033]	lr: 1.000e-04, eta: 12:04:03, time: 1.045, data_time: 0.034, memory: 17291, loss_cls: 0.1211, loss_bbox: 0.3025, d0.loss_cls: 0.1939, d0.loss_bbox: 0.4056, d1.loss_cls: 0.1403, d1.loss_bbox: 0.3034, d2.loss_cls: 0.1207, d2.loss_bbox: 0.2925, d3.loss_cls: 0.1162, d3.loss_bbox: 0.2937, d4.loss_cls: 0.1172, d4.loss_bbox: 0.2963, loss: 2.7036, grad_norm: 40.1372
2025-06-10 07:40:40,864 - mmdet - INFO - Epoch [1][2750/7033]	lr: 1.000e-04, eta: 12:02:26, time: 1.041, data_time: 0.033, memory: 17291, loss_cls: 0.1195, loss_bbox: 0.3192, d0.loss_cls: 0.2038, d0.loss_bbox: 0.4171, d1.loss_cls: 0.1466, d1.loss_bbox: 0.3190, d2.loss_cls: 0.1251, d2.loss_bbox: 0.3077, d3.loss_cls: 0.1177, d3.loss_bbox: 0.3109, d4.loss_cls: 0.1174, d4.loss_bbox: 0.3140, loss: 2.8181, grad_norm: 28.8465
2025-06-10 07:41:32,889 - mmdet - INFO - Epoch [1][2800/7033]	lr: 1.000e-04, eta: 12:00:50, time: 1.040, data_time: 0.036, memory: 17291, loss_cls: 0.1119, loss_bbox: 0.3043, d0.loss_cls: 0.1947, d0.loss_bbox: 0.4002, d1.loss_cls: 0.1383, d1.loss_bbox: 0.2998, d2.loss_cls: 0.1189, d2.loss_bbox: 0.2907, d3.loss_cls: 0.1134, d3.loss_bbox: 0.2941, d4.loss_cls: 0.1119, d4.loss_bbox: 0.2974, loss: 2.6756, grad_norm: 41.9419
2025-06-10 07:42:25,799 - mmdet - INFO - Epoch [1][2850/7033]	lr: 1.000e-04, eta: 11:59:28, time: 1.058, data_time: 0.034, memory: 17291, loss_cls: 0.1106, loss_bbox: 0.3159, d0.loss_cls: 0.1995, d0.loss_bbox: 0.4022, d1.loss_cls: 0.1415, d1.loss_bbox: 0.3022, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2940, d3.loss_cls: 0.1113, d3.loss_bbox: 0.3009, d4.loss_cls: 0.1114, d4.loss_bbox: 0.3062, loss: 2.7178, grad_norm: 36.5147
2025-06-10 07:43:18,286 - mmdet - INFO - Epoch [1][2900/7033]	lr: 1.000e-04, eta: 11:58:01, time: 1.050, data_time: 0.032, memory: 17291, loss_cls: 0.1031, loss_bbox: 0.3056, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3821, d1.loss_cls: 0.1303, d1.loss_bbox: 0.2873, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2815, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2877, d4.loss_cls: 0.0997, d4.loss_bbox: 0.2933, loss: 2.5631, grad_norm: 28.4585
2025-06-10 07:44:10,262 - mmdet - INFO - Epoch [1][2950/7033]	lr: 1.000e-04, eta: 11:56:28, time: 1.040, data_time: 0.029, memory: 17291, loss_cls: 0.1194, loss_bbox: 0.3230, d0.loss_cls: 0.2016, d0.loss_bbox: 0.4021, d1.loss_cls: 0.1450, d1.loss_bbox: 0.3093, d2.loss_cls: 0.1279, d2.loss_bbox: 0.3026, d3.loss_cls: 0.1194, d3.loss_bbox: 0.3077, d4.loss_cls: 0.1191, d4.loss_bbox: 0.3115, loss: 2.7887, grad_norm: 32.7314
2025-06-10 07:45:02,198 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 07:45:02,198 - mmdet - INFO - Epoch [1][3000/7033]	lr: 1.000e-04, eta: 11:54:57, time: 1.039, data_time: 0.031, memory: 17291, loss_cls: 0.1214, loss_bbox: 0.3277, d0.loss_cls: 0.2034, d0.loss_bbox: 0.4138, d1.loss_cls: 0.1547, d1.loss_bbox: 0.3172, d2.loss_cls: 0.1290, d2.loss_bbox: 0.3127, d3.loss_cls: 0.1223, d3.loss_bbox: 0.3156, d4.loss_cls: 0.1215, d4.loss_bbox: 0.3183, loss: 2.8576, grad_norm: 31.1513
2025-06-10 07:45:54,442 - mmdet - INFO - Epoch [1][3050/7033]	lr: 1.000e-04, eta: 11:53:30, time: 1.045, data_time: 0.032, memory: 17291, loss_cls: 0.1147, loss_bbox: 0.3284, d0.loss_cls: 0.1958, d0.loss_bbox: 0.4128, d1.loss_cls: 0.1411, d1.loss_bbox: 0.3214, d2.loss_cls: 0.1236, d2.loss_bbox: 0.3121, d3.loss_cls: 0.1174, d3.loss_bbox: 0.3163, d4.loss_cls: 0.1135, d4.loss_bbox: 0.3210, loss: 2.8180, grad_norm: 40.3703
2025-06-10 07:46:46,590 - mmdet - INFO - Epoch [1][3100/7033]	lr: 1.000e-04, eta: 11:52:03, time: 1.043, data_time: 0.033, memory: 17291, loss_cls: 0.1091, loss_bbox: 0.3193, d0.loss_cls: 0.1909, d0.loss_bbox: 0.4008, d1.loss_cls: 0.1388, d1.loss_bbox: 0.3107, d2.loss_cls: 0.1147, d2.loss_bbox: 0.3004, d3.loss_cls: 0.1075, d3.loss_bbox: 0.3037, d4.loss_cls: 0.1061, d4.loss_bbox: 0.3077, loss: 2.7097, grad_norm: 90.0376
2025-06-10 07:47:39,668 - mmdet - INFO - Epoch [1][3150/7033]	lr: 1.000e-04, eta: 11:50:49, time: 1.062, data_time: 0.050, memory: 17291, loss_cls: 0.1200, loss_bbox: 0.3376, d0.loss_cls: 0.1973, d0.loss_bbox: 0.4321, d1.loss_cls: 0.1496, d1.loss_bbox: 0.3380, d2.loss_cls: 0.1279, d2.loss_bbox: 0.3252, d3.loss_cls: 0.1232, d3.loss_bbox: 0.3267, d4.loss_cls: 0.1191, d4.loss_bbox: 0.3313, loss: 2.9279, grad_norm: 47.6968
2025-06-10 07:48:31,902 - mmdet - INFO - Epoch [1][3200/7033]	lr: 1.000e-04, eta: 11:49:26, time: 1.045, data_time: 0.030, memory: 17291, loss_cls: 0.1134, loss_bbox: 0.3169, d0.loss_cls: 0.1971, d0.loss_bbox: 0.4025, d1.loss_cls: 0.1415, d1.loss_bbox: 0.3098, d2.loss_cls: 0.1201, d2.loss_bbox: 0.3013, d3.loss_cls: 0.1122, d3.loss_bbox: 0.3040, d4.loss_cls: 0.1118, d4.loss_bbox: 0.3071, loss: 2.7376, grad_norm: 109.0369
2025-06-10 07:49:23,733 - mmdet - INFO - Epoch [1][3250/7033]	lr: 1.000e-04, eta: 11:47:58, time: 1.037, data_time: 0.032, memory: 17291, loss_cls: 0.1121, loss_bbox: 0.3095, d0.loss_cls: 0.1858, d0.loss_bbox: 0.3976, d1.loss_cls: 0.1371, d1.loss_bbox: 0.2994, d2.loss_cls: 0.1175, d2.loss_bbox: 0.2929, d3.loss_cls: 0.1099, d3.loss_bbox: 0.2981, d4.loss_cls: 0.1092, d4.loss_bbox: 0.3035, loss: 2.6725, grad_norm: 35.2695
2025-06-10 07:50:16,265 - mmdet - INFO - Epoch [1][3300/7033]	lr: 1.000e-04, eta: 11:46:40, time: 1.051, data_time: 0.039, memory: 17291, loss_cls: 0.1091, loss_bbox: 0.3167, d0.loss_cls: 0.1904, d0.loss_bbox: 0.4043, d1.loss_cls: 0.1356, d1.loss_bbox: 0.3040, d2.loss_cls: 0.1171, d2.loss_bbox: 0.2944, d3.loss_cls: 0.1091, d3.loss_bbox: 0.2999, d4.loss_cls: 0.1074, d4.loss_bbox: 0.3069, loss: 2.6950, grad_norm: 36.6423
2025-06-10 07:51:08,142 - mmdet - INFO - Epoch [1][3350/7033]	lr: 1.000e-04, eta: 11:45:15, time: 1.038, data_time: 0.038, memory: 17291, loss_cls: 0.1104, loss_bbox: 0.3082, d0.loss_cls: 0.1906, d0.loss_bbox: 0.4045, d1.loss_cls: 0.1351, d1.loss_bbox: 0.3010, d2.loss_cls: 0.1151, d2.loss_bbox: 0.2913, d3.loss_cls: 0.1085, d3.loss_bbox: 0.2971, d4.loss_cls: 0.1098, d4.loss_bbox: 0.3021, loss: 2.6737, grad_norm: 33.7772
2025-06-10 07:52:00,582 - mmdet - INFO - Epoch [1][3400/7033]	lr: 1.000e-04, eta: 11:43:58, time: 1.049, data_time: 0.032, memory: 17291, loss_cls: 0.1091, loss_bbox: 0.3046, d0.loss_cls: 0.1908, d0.loss_bbox: 0.3887, d1.loss_cls: 0.1375, d1.loss_bbox: 0.2958, d2.loss_cls: 0.1195, d2.loss_bbox: 0.2860, d3.loss_cls: 0.1105, d3.loss_bbox: 0.2902, d4.loss_cls: 0.1093, d4.loss_bbox: 0.2938, loss: 2.6359, grad_norm: 32.9483
2025-06-10 07:52:52,587 - mmdet - INFO - Epoch [1][3450/7033]	lr: 1.000e-04, eta: 11:42:36, time: 1.040, data_time: 0.029, memory: 17291, loss_cls: 0.1045, loss_bbox: 0.3043, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3835, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2893, d2.loss_cls: 0.1101, d2.loss_bbox: 0.2819, d3.loss_cls: 0.1037, d3.loss_bbox: 0.2884, d4.loss_cls: 0.1028, d4.loss_bbox: 0.2949, loss: 2.5824, grad_norm: 35.3254
2025-06-10 07:53:44,377 - mmdet - INFO - Epoch [1][3500/7033]	lr: 1.000e-04, eta: 11:41:13, time: 1.036, data_time: 0.033, memory: 17291, loss_cls: 0.1080, loss_bbox: 0.2998, d0.loss_cls: 0.1978, d0.loss_bbox: 0.3808, d1.loss_cls: 0.1382, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1191, d2.loss_bbox: 0.2760, d3.loss_cls: 0.1112, d3.loss_bbox: 0.2837, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2887, loss: 2.5978, grad_norm: 35.7070
2025-06-10 07:54:35,903 - mmdet - INFO - Epoch [1][3550/7033]	lr: 1.000e-04, eta: 11:39:48, time: 1.030, data_time: 0.037, memory: 17291, loss_cls: 0.1036, loss_bbox: 0.2875, d0.loss_cls: 0.1951, d0.loss_bbox: 0.3796, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2830, d2.loss_cls: 0.1128, d2.loss_bbox: 0.2734, d3.loss_cls: 0.1038, d3.loss_bbox: 0.2774, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2800, loss: 2.5322, grad_norm: 26.0105
2025-06-10 07:55:27,067 - mmdet - INFO - Epoch [1][3600/7033]	lr: 1.000e-04, eta: 11:38:19, time: 1.023, data_time: 0.033, memory: 17291, loss_cls: 0.1044, loss_bbox: 0.3079, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3768, d1.loss_cls: 0.1275, d1.loss_bbox: 0.2880, d2.loss_cls: 0.1084, d2.loss_bbox: 0.2801, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2895, d4.loss_cls: 0.1027, d4.loss_bbox: 0.2962, loss: 2.5687, grad_norm: 46.2960
2025-06-10 07:56:17,964 - mmdet - INFO - Epoch [1][3650/7033]	lr: 1.000e-04, eta: 11:36:49, time: 1.018, data_time: 0.034, memory: 17291, loss_cls: 0.1075, loss_bbox: 0.2989, d0.loss_cls: 0.1964, d0.loss_bbox: 0.3969, d1.loss_cls: 0.1376, d1.loss_bbox: 0.3015, d2.loss_cls: 0.1169, d2.loss_bbox: 0.2892, d3.loss_cls: 0.1093, d3.loss_bbox: 0.2880, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2906, loss: 2.6404, grad_norm: 27.2165
2025-06-10 07:57:09,272 - mmdet - INFO - Epoch [1][3700/7033]	lr: 1.000e-04, eta: 11:35:25, time: 1.026, data_time: 0.032, memory: 17291, loss_cls: 0.1065, loss_bbox: 0.3036, d0.loss_cls: 0.1947, d0.loss_bbox: 0.4041, d1.loss_cls: 0.1354, d1.loss_bbox: 0.3028, d2.loss_cls: 0.1157, d2.loss_bbox: 0.2865, d3.loss_cls: 0.1097, d3.loss_bbox: 0.2921, d4.loss_cls: 0.1072, d4.loss_bbox: 0.2972, loss: 2.6555, grad_norm: 41.5521
2025-06-10 07:58:00,342 - mmdet - INFO - Epoch [1][3750/7033]	lr: 1.000e-04, eta: 11:33:59, time: 1.021, data_time: 0.032, memory: 17291, loss_cls: 0.1127, loss_bbox: 0.3067, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3820, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2934, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2836, d3.loss_cls: 0.1106, d3.loss_bbox: 0.2911, d4.loss_cls: 0.1118, d4.loss_bbox: 0.2956, loss: 2.6263, grad_norm: 40.7216
2025-06-10 07:58:51,881 - mmdet - INFO - Epoch [1][3800/7033]	lr: 1.000e-04, eta: 11:32:38, time: 1.031, data_time: 0.033, memory: 17291, loss_cls: 0.1064, loss_bbox: 0.2978, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3802, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2869, d2.loss_cls: 0.1166, d2.loss_bbox: 0.2805, d3.loss_cls: 0.1115, d3.loss_bbox: 0.2849, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2891, loss: 2.5910, grad_norm: 72.3355
2025-06-10 07:59:43,874 - mmdet - INFO - Epoch [1][3850/7033]	lr: 1.000e-04, eta: 11:31:23, time: 1.040, data_time: 0.034, memory: 17291, loss_cls: 0.0997, loss_bbox: 0.3010, d0.loss_cls: 0.1837, d0.loss_bbox: 0.3852, d1.loss_cls: 0.1285, d1.loss_bbox: 0.2939, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2830, d3.loss_cls: 0.1018, d3.loss_bbox: 0.2880, d4.loss_cls: 0.1004, d4.loss_bbox: 0.2896, loss: 2.5628, grad_norm: 37.7459
2025-06-10 08:00:35,757 - mmdet - INFO - Epoch [1][3900/7033]	lr: 1.000e-04, eta: 11:30:07, time: 1.038, data_time: 0.028, memory: 17291, loss_cls: 0.1043, loss_bbox: 0.2876, d0.loss_cls: 0.1928, d0.loss_bbox: 0.3771, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2843, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2739, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2777, d4.loss_cls: 0.1018, d4.loss_bbox: 0.2824, loss: 2.5262, grad_norm: 26.2822
2025-06-10 08:01:27,822 - mmdet - INFO - Epoch [1][3950/7033]	lr: 1.000e-04, eta: 11:28:54, time: 1.041, data_time: 0.029, memory: 17291, loss_cls: 0.1008, loss_bbox: 0.2949, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3913, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2913, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2813, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2861, d4.loss_cls: 0.1005, d4.loss_bbox: 0.2876, loss: 2.5563, grad_norm: 31.1861
2025-06-10 08:02:19,753 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 08:02:19,753 - mmdet - INFO - Epoch [1][4000/7033]	lr: 1.000e-04, eta: 11:27:40, time: 1.039, data_time: 0.032, memory: 17291, loss_cls: 0.1049, loss_bbox: 0.2917, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3738, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2873, d2.loss_cls: 0.1126, d2.loss_bbox: 0.2753, d3.loss_cls: 0.1076, d3.loss_bbox: 0.2787, d4.loss_cls: 0.1044, d4.loss_bbox: 0.2817, loss: 2.5352, grad_norm: 30.5339
2025-06-10 08:03:11,592 - mmdet - INFO - Epoch [1][4050/7033]	lr: 1.000e-04, eta: 11:26:25, time: 1.037, data_time: 0.033, memory: 17291, loss_cls: 0.1076, loss_bbox: 0.2861, d0.loss_cls: 0.1969, d0.loss_bbox: 0.3827, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2861, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2736, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2757, d4.loss_cls: 0.1055, d4.loss_bbox: 0.2782, loss: 2.5522, grad_norm: 44.9038
2025-06-10 08:04:03,436 - mmdet - INFO - Epoch [1][4100/7033]	lr: 1.000e-04, eta: 11:25:11, time: 1.037, data_time: 0.032, memory: 17291, loss_cls: 0.1030, loss_bbox: 0.2977, d0.loss_cls: 0.1965, d0.loss_bbox: 0.3845, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2896, d2.loss_cls: 0.1081, d2.loss_bbox: 0.2808, d3.loss_cls: 0.1042, d3.loss_bbox: 0.2837, d4.loss_cls: 0.1032, d4.loss_bbox: 0.2888, loss: 2.5652, grad_norm: 48.1650
2025-06-10 08:04:55,625 - mmdet - INFO - Epoch [1][4150/7033]	lr: 1.000e-04, eta: 11:24:01, time: 1.044, data_time: 0.031, memory: 17291, loss_cls: 0.0979, loss_bbox: 0.2878, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3715, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2708, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2757, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2810, loss: 2.4670, grad_norm: 38.9899
2025-06-10 08:05:47,643 - mmdet - INFO - Epoch [1][4200/7033]	lr: 1.000e-04, eta: 11:22:50, time: 1.040, data_time: 0.034, memory: 17291, loss_cls: 0.0977, loss_bbox: 0.2981, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3784, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2906, d2.loss_cls: 0.1064, d2.loss_bbox: 0.2798, d3.loss_cls: 0.1015, d3.loss_bbox: 0.2850, d4.loss_cls: 0.0980, d4.loss_bbox: 0.2893, loss: 2.5375, grad_norm: 25.1284
2025-06-10 08:06:39,688 - mmdet - INFO - Epoch [1][4250/7033]	lr: 1.000e-04, eta: 11:21:39, time: 1.041, data_time: 0.031, memory: 17291, loss_cls: 0.1015, loss_bbox: 0.2826, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3661, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2793, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2666, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2729, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2764, loss: 2.4613, grad_norm: 32.4010
2025-06-10 08:07:31,572 - mmdet - INFO - Epoch [1][4300/7033]	lr: 1.000e-04, eta: 11:20:28, time: 1.038, data_time: 0.029, memory: 17291, loss_cls: 0.1013, loss_bbox: 0.2991, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3921, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2941, d2.loss_cls: 0.1126, d2.loss_bbox: 0.2799, d3.loss_cls: 0.1053, d3.loss_bbox: 0.2838, d4.loss_cls: 0.1003, d4.loss_bbox: 0.2903, loss: 2.5779, grad_norm: 34.7010
2025-06-10 08:08:23,435 - mmdet - INFO - Epoch [1][4350/7033]	lr: 1.000e-04, eta: 11:19:16, time: 1.037, data_time: 0.029, memory: 17291, loss_cls: 0.0967, loss_bbox: 0.3015, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3797, d1.loss_cls: 0.1225, d1.loss_bbox: 0.2910, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2810, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2846, d4.loss_cls: 0.0965, d4.loss_bbox: 0.2923, loss: 2.5345, grad_norm: 41.2060
2025-06-10 08:09:15,852 - mmdet - INFO - Epoch [1][4400/7033]	lr: 1.000e-04, eta: 11:18:10, time: 1.048, data_time: 0.032, memory: 17291, loss_cls: 0.0984, loss_bbox: 0.2956, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3936, d1.loss_cls: 0.1290, d1.loss_bbox: 0.3006, d2.loss_cls: 0.1106, d2.loss_bbox: 0.2871, d3.loss_cls: 0.0999, d3.loss_bbox: 0.2873, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2892, loss: 2.5769, grad_norm: 39.1775
2025-06-10 08:10:07,882 - mmdet - INFO - Epoch [1][4450/7033]	lr: 1.000e-04, eta: 11:17:01, time: 1.041, data_time: 0.031, memory: 17291, loss_cls: 0.0985, loss_bbox: 0.2781, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2833, d2.loss_cls: 0.1072, d2.loss_bbox: 0.2684, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2711, d4.loss_cls: 0.0959, d4.loss_bbox: 0.2716, loss: 2.4606, grad_norm: 31.9352
2025-06-10 08:11:00,392 - mmdet - INFO - Epoch [1][4500/7033]	lr: 1.000e-04, eta: 11:15:57, time: 1.050, data_time: 0.029, memory: 17291, loss_cls: 0.1036, loss_bbox: 0.2912, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2805, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2745, d3.loss_cls: 0.1026, d3.loss_bbox: 0.2781, d4.loss_cls: 0.1008, d4.loss_bbox: 0.2811, loss: 2.4838, grad_norm: 33.0912
2025-06-10 08:11:52,583 - mmdet - INFO - Epoch [1][4550/7033]	lr: 1.000e-04, eta: 11:14:50, time: 1.044, data_time: 0.031, memory: 17291, loss_cls: 0.1023, loss_bbox: 0.2936, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3840, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2930, d2.loss_cls: 0.1116, d2.loss_bbox: 0.2814, d3.loss_cls: 0.1030, d3.loss_bbox: 0.2853, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2883, loss: 2.5697, grad_norm: 76.8459
2025-06-10 08:12:44,480 - mmdet - INFO - Epoch [1][4600/7033]	lr: 1.000e-04, eta: 11:13:40, time: 1.038, data_time: 0.031, memory: 17291, loss_cls: 0.1082, loss_bbox: 0.2954, d0.loss_cls: 0.1904, d0.loss_bbox: 0.3786, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2901, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2812, d3.loss_cls: 0.1060, d3.loss_bbox: 0.2870, d4.loss_cls: 0.1058, d4.loss_bbox: 0.2884, loss: 2.5791, grad_norm: 29.7125
2025-06-10 08:13:36,230 - mmdet - INFO - Epoch [1][4650/7033]	lr: 1.000e-04, eta: 11:12:30, time: 1.035, data_time: 0.031, memory: 17291, loss_cls: 0.1075, loss_bbox: 0.2825, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3722, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2682, d3.loss_cls: 0.1088, d3.loss_bbox: 0.2721, d4.loss_cls: 0.1051, d4.loss_bbox: 0.2744, loss: 2.4959, grad_norm: 39.8180
2025-06-10 08:14:27,657 - mmdet - INFO - Epoch [1][4700/7033]	lr: 1.000e-04, eta: 11:11:18, time: 1.029, data_time: 0.027, memory: 17291, loss_cls: 0.0959, loss_bbox: 0.2948, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2915, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2793, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2831, d4.loss_cls: 0.0954, d4.loss_bbox: 0.2857, loss: 2.5127, grad_norm: 36.9612
2025-06-10 08:15:18,909 - mmdet - INFO - Epoch [1][4750/7033]	lr: 1.000e-04, eta: 11:10:05, time: 1.025, data_time: 0.025, memory: 17291, loss_cls: 0.0966, loss_bbox: 0.2879, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3678, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2747, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2778, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2809, loss: 2.4681, grad_norm: 35.4341
2025-06-10 08:16:10,212 - mmdet - INFO - Epoch [1][4800/7033]	lr: 1.000e-04, eta: 11:08:53, time: 1.026, data_time: 0.027, memory: 17291, loss_cls: 0.0904, loss_bbox: 0.3001, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3621, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2790, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2679, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2760, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2835, loss: 2.4377, grad_norm: 30.6537
2025-06-10 08:17:02,244 - mmdet - INFO - Epoch [1][4850/7033]	lr: 1.000e-04, eta: 11:07:47, time: 1.041, data_time: 0.030, memory: 17291, loss_cls: 0.1041, loss_bbox: 0.2863, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3696, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2690, d3.loss_cls: 0.1038, d3.loss_bbox: 0.2728, d4.loss_cls: 0.1023, d4.loss_bbox: 0.2771, loss: 2.4952, grad_norm: 25.7751
2025-06-10 08:17:54,105 - mmdet - INFO - Epoch [1][4900/7033]	lr: 1.000e-04, eta: 11:06:40, time: 1.037, data_time: 0.028, memory: 17291, loss_cls: 0.0979, loss_bbox: 0.2887, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2859, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2738, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2791, d4.loss_cls: 0.0985, d4.loss_bbox: 0.2814, loss: 2.5007, grad_norm: 26.7926
2025-06-10 08:18:45,821 - mmdet - INFO - Epoch [1][4950/7033]	lr: 1.000e-04, eta: 11:05:32, time: 1.034, data_time: 0.029, memory: 17291, loss_cls: 0.1052, loss_bbox: 0.2833, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3826, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2749, d3.loss_cls: 0.1050, d3.loss_bbox: 0.2760, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2760, loss: 2.5209, grad_norm: 31.2005
2025-06-10 08:19:41,254 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 08:19:41,254 - mmdet - INFO - Epoch [1][5000/7033]	lr: 1.000e-04, eta: 11:04:52, time: 1.109, data_time: 0.033, memory: 17291, loss_cls: 0.0879, loss_bbox: 0.2806, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2687, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2717, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2735, loss: 2.4004, grad_norm: 33.7213
2025-06-10 08:20:33,477 - mmdet - INFO - Epoch [1][5050/7033]	lr: 1.000e-04, eta: 11:03:48, time: 1.045, data_time: 0.033, memory: 17291, loss_cls: 0.1001, loss_bbox: 0.2817, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3713, d1.loss_cls: 0.1319, d1.loss_bbox: 0.2775, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2654, d3.loss_cls: 0.1036, d3.loss_bbox: 0.2682, d4.loss_cls: 0.1003, d4.loss_bbox: 0.2727, loss: 2.4718, grad_norm: 36.8775
2025-06-10 08:21:28,688 - mmdet - INFO - Epoch [1][5100/7033]	lr: 1.000e-04, eta: 11:03:06, time: 1.104, data_time: 0.036, memory: 17291, loss_cls: 0.0985, loss_bbox: 0.2791, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3756, d1.loss_cls: 0.1281, d1.loss_bbox: 0.2881, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2712, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2723, d4.loss_cls: 0.0996, d4.loss_bbox: 0.2733, loss: 2.4834, grad_norm: 28.2064
2025-06-10 08:22:20,169 - mmdet - INFO - Epoch [1][5150/7033]	lr: 1.000e-04, eta: 11:01:57, time: 1.030, data_time: 0.029, memory: 17291, loss_cls: 0.1096, loss_bbox: 0.2867, d0.loss_cls: 0.1917, d0.loss_bbox: 0.3727, d1.loss_cls: 0.1328, d1.loss_bbox: 0.2864, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2756, d3.loss_cls: 0.1097, d3.loss_bbox: 0.2773, d4.loss_cls: 0.1076, d4.loss_bbox: 0.2803, loss: 2.5468, grad_norm: 31.8421
2025-06-10 08:23:11,405 - mmdet - INFO - Epoch [1][5200/7033]	lr: 1.000e-04, eta: 11:00:46, time: 1.025, data_time: 0.031, memory: 17291, loss_cls: 0.1007, loss_bbox: 0.2855, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3750, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2854, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2719, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2755, d4.loss_cls: 0.0992, d4.loss_bbox: 0.2785, loss: 2.4797, grad_norm: 26.4685
2025-06-10 08:24:02,795 - mmdet - INFO - Epoch [1][5250/7033]	lr: 1.000e-04, eta: 10:59:37, time: 1.028, data_time: 0.034, memory: 17291, loss_cls: 0.0933, loss_bbox: 0.2728, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3629, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2751, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2629, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2641, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2668, loss: 2.3793, grad_norm: 26.2224
2025-06-10 08:24:54,389 - mmdet - INFO - Epoch [1][5300/7033]	lr: 1.000e-04, eta: 10:58:30, time: 1.032, data_time: 0.033, memory: 17291, loss_cls: 0.0916, loss_bbox: 0.2663, d0.loss_cls: 0.1820, d0.loss_bbox: 0.3632, d1.loss_cls: 0.1168, d1.loss_bbox: 0.2720, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2584, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2608, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2594, loss: 2.3514, grad_norm: 32.8792
2025-06-10 08:25:46,380 - mmdet - INFO - Epoch [1][5350/7033]	lr: 1.000e-04, eta: 10:57:26, time: 1.040, data_time: 0.039, memory: 17291, loss_cls: 0.1033, loss_bbox: 0.2876, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3873, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2945, d2.loss_cls: 0.1153, d2.loss_bbox: 0.2766, d3.loss_cls: 0.1056, d3.loss_bbox: 0.2818, d4.loss_cls: 0.1042, d4.loss_bbox: 0.2827, loss: 2.5632, grad_norm: 28.3218
2025-06-10 08:26:38,113 - mmdet - INFO - Epoch [1][5400/7033]	lr: 1.000e-04, eta: 10:56:20, time: 1.035, data_time: 0.035, memory: 17291, loss_cls: 0.0964, loss_bbox: 0.2764, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2790, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2656, d3.loss_cls: 0.0964, d3.loss_bbox: 0.2694, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2711, loss: 2.4293, grad_norm: 28.6963
2025-06-10 08:27:29,466 - mmdet - INFO - Epoch [1][5450/7033]	lr: 1.000e-04, eta: 10:55:12, time: 1.027, data_time: 0.032, memory: 17291, loss_cls: 0.0969, loss_bbox: 0.2781, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3771, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2864, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2727, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2731, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2736, loss: 2.4590, grad_norm: 46.7215
2025-06-10 08:28:20,794 - mmdet - INFO - Epoch [1][5500/7033]	lr: 1.000e-04, eta: 10:54:04, time: 1.027, data_time: 0.032, memory: 17291, loss_cls: 0.0999, loss_bbox: 0.2718, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3557, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1090, d2.loss_bbox: 0.2634, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2669, d4.loss_cls: 0.1003, d4.loss_bbox: 0.2672, loss: 2.4145, grad_norm: 25.8957
2025-06-10 08:29:11,991 - mmdet - INFO - Epoch [1][5550/7033]	lr: 1.000e-04, eta: 10:52:56, time: 1.024, data_time: 0.030, memory: 17291, loss_cls: 0.0952, loss_bbox: 0.2783, d0.loss_cls: 0.1909, d0.loss_bbox: 0.3714, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2704, d3.loss_cls: 0.0996, d3.loss_bbox: 0.2715, d4.loss_cls: 0.0952, d4.loss_bbox: 0.2705, loss: 2.4584, grad_norm: 46.2080
2025-06-10 08:30:03,117 - mmdet - INFO - Epoch [1][5600/7033]	lr: 1.000e-04, eta: 10:51:47, time: 1.023, data_time: 0.031, memory: 17291, loss_cls: 0.0934, loss_bbox: 0.2750, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3655, d1.loss_cls: 0.1254, d1.loss_bbox: 0.2792, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2649, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2685, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2682, loss: 2.4112, grad_norm: 42.0583
2025-06-10 08:30:54,596 - mmdet - INFO - Epoch [1][5650/7033]	lr: 1.000e-04, eta: 10:50:41, time: 1.030, data_time: 0.030, memory: 17291, loss_cls: 0.0978, loss_bbox: 0.2832, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2833, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2735, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2766, d4.loss_cls: 0.0954, d4.loss_bbox: 0.2783, loss: 2.4623, grad_norm: 50.1010
2025-06-10 08:31:46,100 - mmdet - INFO - Epoch [1][5700/7033]	lr: 1.000e-04, eta: 10:49:35, time: 1.030, data_time: 0.033, memory: 17291, loss_cls: 0.0891, loss_bbox: 0.2703, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3626, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2756, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2632, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2649, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2640, loss: 2.3640, grad_norm: 27.2783
2025-06-10 08:32:37,532 - mmdet - INFO - Epoch [1][5750/7033]	lr: 1.000e-04, eta: 10:48:30, time: 1.029, data_time: 0.030, memory: 17291, loss_cls: 0.0878, loss_bbox: 0.2648, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3659, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2724, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2587, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2611, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2587, loss: 2.3401, grad_norm: 23.7925
2025-06-10 08:33:29,081 - mmdet - INFO - Epoch [1][5800/7033]	lr: 1.000e-04, eta: 10:47:25, time: 1.031, data_time: 0.031, memory: 17291, loss_cls: 0.0879, loss_bbox: 0.2690, d0.loss_cls: 0.1790, d0.loss_bbox: 0.3680, d1.loss_cls: 0.1155, d1.loss_bbox: 0.2730, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2598, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2626, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2614, loss: 2.3516, grad_norm: 26.8328
2025-06-10 08:34:20,780 - mmdet - INFO - Epoch [1][5850/7033]	lr: 1.000e-04, eta: 10:46:21, time: 1.034, data_time: 0.034, memory: 17291, loss_cls: 0.0942, loss_bbox: 0.2660, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3687, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2787, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2608, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2611, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2612, loss: 2.3888, grad_norm: 41.5075
2025-06-10 08:35:12,662 - mmdet - INFO - Epoch [1][5900/7033]	lr: 1.000e-04, eta: 10:45:19, time: 1.038, data_time: 0.030, memory: 17291, loss_cls: 0.0919, loss_bbox: 0.2769, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3666, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2662, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2673, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2684, loss: 2.4131, grad_norm: 142.6153
2025-06-10 08:36:03,946 - mmdet - INFO - Epoch [1][5950/7033]	lr: 1.000e-04, eta: 10:44:13, time: 1.026, data_time: 0.030, memory: 17291, loss_cls: 0.0997, loss_bbox: 0.2752, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3650, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2765, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2648, d3.loss_cls: 0.1007, d3.loss_bbox: 0.2673, d4.loss_cls: 0.0997, d4.loss_bbox: 0.2670, loss: 2.4266, grad_norm: 41.3676
2025-06-10 08:36:55,805 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 08:36:55,805 - mmdet - INFO - Epoch [1][6000/7033]	lr: 1.000e-04, eta: 10:43:11, time: 1.037, data_time: 0.030, memory: 17617, loss_cls: 0.0989, loss_bbox: 0.2803, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2823, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2695, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2732, d4.loss_cls: 0.0980, d4.loss_bbox: 0.2734, loss: 2.4658, grad_norm: 38.9471
2025-06-10 08:37:47,423 - mmdet - INFO - Epoch [1][6050/7033]	lr: 1.000e-04, eta: 10:42:07, time: 1.032, data_time: 0.031, memory: 17617, loss_cls: 0.0906, loss_bbox: 0.2652, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3579, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2726, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2592, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2580, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2577, loss: 2.3372, grad_norm: 29.7080
2025-06-10 08:38:39,050 - mmdet - INFO - Epoch [1][6100/7033]	lr: 1.000e-04, eta: 10:41:04, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.1055, loss_bbox: 0.2690, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1339, d1.loss_bbox: 0.2701, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2573, d3.loss_cls: 0.1105, d3.loss_bbox: 0.2605, d4.loss_cls: 0.1069, d4.loss_bbox: 0.2598, loss: 2.4266, grad_norm: 36.8378
2025-06-10 08:39:30,908 - mmdet - INFO - Epoch [1][6150/7033]	lr: 1.000e-04, eta: 10:40:03, time: 1.037, data_time: 0.027, memory: 17617, loss_cls: 0.0944, loss_bbox: 0.2697, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2745, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2651, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2676, d4.loss_cls: 0.0948, d4.loss_bbox: 0.2626, loss: 2.3735, grad_norm: 29.6987
2025-06-10 08:40:22,462 - mmdet - INFO - Epoch [1][6200/7033]	lr: 1.000e-04, eta: 10:38:59, time: 1.031, data_time: 0.031, memory: 17617, loss_cls: 0.0853, loss_bbox: 0.2602, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2624, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2531, d3.loss_cls: 0.0870, d3.loss_bbox: 0.2547, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2532, loss: 2.2771, grad_norm: 73.0862
2025-06-10 08:41:13,532 - mmdet - INFO - Epoch [1][6250/7033]	lr: 1.000e-04, eta: 10:37:54, time: 1.021, data_time: 0.029, memory: 17617, loss_cls: 0.0980, loss_bbox: 0.2605, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3668, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2744, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2569, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2614, d4.loss_cls: 0.0978, d4.loss_bbox: 0.2555, loss: 2.3863, grad_norm: 62.3344
2025-06-10 08:42:04,964 - mmdet - INFO - Epoch [1][6300/7033]	lr: 1.000e-04, eta: 10:36:50, time: 1.029, data_time: 0.028, memory: 17617, loss_cls: 0.0943, loss_bbox: 0.2829, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3829, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2909, d2.loss_cls: 0.1044, d2.loss_bbox: 0.2769, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2800, d4.loss_cls: 0.0960, d4.loss_bbox: 0.2748, loss: 2.4828, grad_norm: 32.0710
2025-06-10 08:42:56,319 - mmdet - INFO - Epoch [1][6350/7033]	lr: 1.000e-04, eta: 10:35:46, time: 1.027, data_time: 0.031, memory: 17617, loss_cls: 0.1021, loss_bbox: 0.2655, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3647, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2594, d3.loss_cls: 0.1053, d3.loss_bbox: 0.2598, d4.loss_cls: 0.1011, d4.loss_bbox: 0.2594, loss: 2.4171, grad_norm: 28.3132
2025-06-10 08:43:47,678 - mmdet - INFO - Epoch [1][6400/7033]	lr: 1.000e-04, eta: 10:34:43, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.1054, loss_bbox: 0.2754, d0.loss_cls: 0.1945, d0.loss_bbox: 0.3802, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2807, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2698, d3.loss_cls: 0.1023, d3.loss_bbox: 0.2721, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2703, loss: 2.4928, grad_norm: 52.4778
2025-06-10 08:44:38,622 - mmdet - INFO - Epoch [1][6450/7033]	lr: 1.000e-04, eta: 10:33:37, time: 1.019, data_time: 0.025, memory: 17617, loss_cls: 0.0956, loss_bbox: 0.2715, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2769, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2671, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2694, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2666, loss: 2.3981, grad_norm: 48.3350
2025-06-10 08:45:29,766 - mmdet - INFO - Epoch [1][6500/7033]	lr: 1.000e-04, eta: 10:32:33, time: 1.023, data_time: 0.028, memory: 17617, loss_cls: 0.1061, loss_bbox: 0.2851, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3824, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2906, d2.loss_cls: 0.1145, d2.loss_bbox: 0.2782, d3.loss_cls: 0.1065, d3.loss_bbox: 0.2828, d4.loss_cls: 0.1061, d4.loss_bbox: 0.2784, loss: 2.5478, grad_norm: 72.2377
2025-06-10 08:46:21,015 - mmdet - INFO - Epoch [1][6550/7033]	lr: 1.000e-04, eta: 10:31:29, time: 1.025, data_time: 0.028, memory: 17617, loss_cls: 0.1004, loss_bbox: 0.2818, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3798, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2838, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2722, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2767, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2722, loss: 2.5032, grad_norm: 32.6492
2025-06-10 08:47:12,317 - mmdet - INFO - Epoch [1][6600/7033]	lr: 1.000e-04, eta: 10:30:26, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0891, loss_bbox: 0.2686, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3605, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2721, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2625, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2661, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2617, loss: 2.3556, grad_norm: 40.1427
2025-06-10 08:48:03,584 - mmdet - INFO - Epoch [1][6650/7033]	lr: 1.000e-04, eta: 10:29:23, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0960, loss_bbox: 0.2772, d0.loss_cls: 0.1790, d0.loss_bbox: 0.3693, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2768, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2703, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2759, d4.loss_cls: 0.0966, d4.loss_bbox: 0.2689, loss: 2.4406, grad_norm: 30.3878
2025-06-10 08:48:54,961 - mmdet - INFO - Epoch [1][6700/7033]	lr: 1.000e-04, eta: 10:28:21, time: 1.028, data_time: 0.028, memory: 17617, loss_cls: 0.0910, loss_bbox: 0.2696, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3554, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2672, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2618, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2590, loss: 2.3422, grad_norm: 347.7212
2025-06-10 08:49:46,545 - mmdet - INFO - Epoch [1][6750/7033]	lr: 1.000e-04, eta: 10:27:20, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0910, loss_bbox: 0.2664, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2781, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2619, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2638, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2600, loss: 2.3844, grad_norm: 27.6378
2025-06-10 08:50:37,850 - mmdet - INFO - Epoch [1][6800/7033]	lr: 1.000e-04, eta: 10:26:17, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0941, loss_bbox: 0.2703, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3646, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2563, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2618, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2606, loss: 2.3833, grad_norm: 36.2285
2025-06-10 08:51:29,155 - mmdet - INFO - Epoch [1][6850/7033]	lr: 1.000e-04, eta: 10:25:15, time: 1.026, data_time: 0.028, memory: 17617, loss_cls: 0.0834, loss_bbox: 0.2543, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3580, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2697, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2557, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2567, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2505, loss: 2.2877, grad_norm: 33.1285
2025-06-10 08:52:20,250 - mmdet - INFO - Epoch [1][6900/7033]	lr: 1.000e-04, eta: 10:24:12, time: 1.022, data_time: 0.027, memory: 17617, loss_cls: 0.0911, loss_bbox: 0.2651, d0.loss_cls: 0.1851, d0.loss_bbox: 0.3660, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2816, d2.loss_cls: 0.1015, d2.loss_bbox: 0.2652, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2670, d4.loss_cls: 0.0920, d4.loss_bbox: 0.2621, loss: 2.3922, grad_norm: 22.4904
2025-06-10 08:53:11,799 - mmdet - INFO - Epoch [1][6950/7033]	lr: 1.000e-04, eta: 10:23:11, time: 1.031, data_time: 0.030, memory: 17617, loss_cls: 0.0891, loss_bbox: 0.2560, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3587, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2678, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2561, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2568, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2506, loss: 2.3114, grad_norm: 48.6247
2025-06-10 08:54:03,269 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 08:54:03,270 - mmdet - INFO - Epoch [1][7000/7033]	lr: 1.000e-04, eta: 10:22:10, time: 1.029, data_time: 0.029, memory: 17617, loss_cls: 0.0928, loss_bbox: 0.2589, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2718, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2592, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2618, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2544, loss: 2.3497, grad_norm: 58.8080
2025-06-10 08:54:37,178 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-10 09:19:28,010 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 09:19:28,010 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7596, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8783, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9111, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9250, pts_bbox_NuScenes/car_trans_err: 0.2201, pts_bbox_NuScenes/car_scale_err: 0.1526, pts_bbox_NuScenes/car_orient_err: 0.0675, pts_bbox_NuScenes/car_vel_err: 0.4695, pts_bbox_NuScenes/car_attr_err: 0.1756, pts_bbox_NuScenes/mATE: 0.3330, pts_bbox_NuScenes/mASE: 0.2730, pts_bbox_NuScenes/mAOE: 0.2768, pts_bbox_NuScenes/mAVE: 0.4287, pts_bbox_NuScenes/mAAE: 0.1831, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.3846, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6109, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7200, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7591, pts_bbox_NuScenes/truck_trans_err: 0.3896, pts_bbox_NuScenes/truck_scale_err: 0.2174, pts_bbox_NuScenes/truck_orient_err: 0.1029, pts_bbox_NuScenes/truck_vel_err: 0.4795, pts_bbox_NuScenes/truck_attr_err: 0.2168, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0416, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1833, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3959, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4572, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6997, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4664, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8417, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1096, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3025, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.3846, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7062, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8833, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9130, pts_bbox_NuScenes/bus_trans_err: 0.4273, pts_bbox_NuScenes/bus_scale_err: 0.2130, pts_bbox_NuScenes/bus_orient_err: 0.0657, pts_bbox_NuScenes/bus_vel_err: 0.8397, pts_bbox_NuScenes/bus_attr_err: 0.2394, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1264, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3864, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5745, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6694, pts_bbox_NuScenes/trailer_trans_err: 0.5554, pts_bbox_NuScenes/trailer_scale_err: 0.2250, pts_bbox_NuScenes/trailer_orient_err: 0.4089, pts_bbox_NuScenes/trailer_vel_err: 0.2755, pts_bbox_NuScenes/trailer_attr_err: 0.1658, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5564, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6725, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7206, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7402, pts_bbox_NuScenes/barrier_trans_err: 0.2457, pts_bbox_NuScenes/barrier_scale_err: 0.2910, pts_bbox_NuScenes/barrier_orient_err: 0.0957, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5931, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7479, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7774, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7860, pts_bbox_NuScenes/motorcycle_trans_err: 0.2412, pts_bbox_NuScenes/motorcycle_scale_err: 0.2563, pts_bbox_NuScenes/motorcycle_orient_err: 0.2319, pts_bbox_NuScenes/motorcycle_vel_err: 0.7160, pts_bbox_NuScenes/motorcycle_attr_err: 0.2377, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5416, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5978, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6098, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6205, pts_bbox_NuScenes/bicycle_trans_err: 0.2010, pts_bbox_NuScenes/bicycle_scale_err: 0.2776, pts_bbox_NuScenes/bicycle_orient_err: 0.3415, pts_bbox_NuScenes/bicycle_vel_err: 0.2749, pts_bbox_NuScenes/bicycle_attr_err: 0.0041, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7998, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8551, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8798, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8948, pts_bbox_NuScenes/pedestrian_trans_err: 0.1711, pts_bbox_NuScenes/pedestrian_scale_err: 0.2990, pts_bbox_NuScenes/pedestrian_orient_err: 0.3359, pts_bbox_NuScenes/pedestrian_vel_err: 0.2645, pts_bbox_NuScenes/pedestrian_attr_err: 0.1228, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6950, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7607, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7881, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8130, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1790, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3321, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6770, pts_bbox_NuScenes/mAP: 0.6530
2025-06-10 09:20:27,718 - mmdet - INFO - Epoch [2][50/7033]	lr: 9.331e-05, eta: 10:18:02, time: 1.113, data_time: 0.123, memory: 17617, loss_cls: 0.0922, loss_bbox: 0.2684, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3633, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2738, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2624, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2675, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2613, loss: 2.3893, grad_norm: 35.3401
2025-06-10 09:21:18,620 - mmdet - INFO - Epoch [2][100/7033]	lr: 9.331e-05, eta: 10:17:00, time: 1.018, data_time: 0.033, memory: 17617, loss_cls: 0.0972, loss_bbox: 0.2692, d0.loss_cls: 0.1941, d0.loss_bbox: 0.3727, d1.loss_cls: 0.1267, d1.loss_bbox: 0.2818, d2.loss_cls: 0.1079, d2.loss_bbox: 0.2686, d3.loss_cls: 0.0984, d3.loss_bbox: 0.2718, d4.loss_cls: 0.0993, d4.loss_bbox: 0.2631, loss: 2.4507, grad_norm: 58.7963
2025-06-10 09:22:09,861 - mmdet - INFO - Epoch [2][150/7033]	lr: 9.331e-05, eta: 10:15:59, time: 1.025, data_time: 0.034, memory: 17617, loss_cls: 0.0843, loss_bbox: 0.2476, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2647, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2514, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2515, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2432, loss: 2.2611, grad_norm: 33.0675
2025-06-10 09:23:01,035 - mmdet - INFO - Epoch [2][200/7033]	lr: 9.331e-05, eta: 10:14:59, time: 1.023, data_time: 0.033, memory: 17617, loss_cls: 0.0902, loss_bbox: 0.2596, d0.loss_cls: 0.1859, d0.loss_bbox: 0.3660, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2751, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2592, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2602, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2540, loss: 2.3611, grad_norm: 23.4227
2025-06-10 09:23:52,486 - mmdet - INFO - Epoch [2][250/7033]	lr: 9.331e-05, eta: 10:14:00, time: 1.029, data_time: 0.035, memory: 17617, loss_cls: 0.0906, loss_bbox: 0.2493, d0.loss_cls: 0.1767, d0.loss_bbox: 0.3463, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2593, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2502, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2440, loss: 2.2691, grad_norm: 34.5096
2025-06-10 09:24:43,802 - mmdet - INFO - Epoch [2][300/7033]	lr: 9.331e-05, eta: 10:13:00, time: 1.026, data_time: 0.034, memory: 17617, loss_cls: 0.0809, loss_bbox: 0.2565, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3518, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2609, d2.loss_cls: 0.0933, d2.loss_bbox: 0.2514, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2571, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2507, loss: 2.2565, grad_norm: 37.6380
2025-06-10 09:25:35,264 - mmdet - INFO - Epoch [2][350/7033]	lr: 9.331e-05, eta: 10:12:01, time: 1.029, data_time: 0.034, memory: 17617, loss_cls: 0.0974, loss_bbox: 0.2578, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1333, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1111, d2.loss_bbox: 0.2569, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2609, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2529, loss: 2.3979, grad_norm: 65.2049
2025-06-10 09:26:26,432 - mmdet - INFO - Epoch [2][400/7033]	lr: 9.331e-05, eta: 10:11:01, time: 1.023, data_time: 0.033, memory: 17617, loss_cls: 0.0867, loss_bbox: 0.2461, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2632, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2484, d4.loss_cls: 0.0888, d4.loss_bbox: 0.2421, loss: 2.2482, grad_norm: 282.1217
2025-06-10 09:27:18,004 - mmdet - INFO - Epoch [2][450/7033]	lr: 9.331e-05, eta: 10:10:03, time: 1.031, data_time: 0.036, memory: 17617, loss_cls: 0.0880, loss_bbox: 0.2617, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2787, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2645, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2668, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2583, loss: 2.3893, grad_norm: 48.9005
2025-06-10 09:28:09,770 - mmdet - INFO - Epoch [2][500/7033]	lr: 9.331e-05, eta: 10:09:06, time: 1.035, data_time: 0.036, memory: 17617, loss_cls: 0.0877, loss_bbox: 0.2471, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2618, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2464, d3.loss_cls: 0.0897, d3.loss_bbox: 0.2474, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2418, loss: 2.2410, grad_norm: 45.6963
2025-06-10 09:29:01,250 - mmdet - INFO - Epoch [2][550/7033]	lr: 9.331e-05, eta: 10:08:08, time: 1.030, data_time: 0.036, memory: 17617, loss_cls: 0.0915, loss_bbox: 0.2626, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3681, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2749, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2624, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2643, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2565, loss: 2.3811, grad_norm: 25.0066
2025-06-10 09:29:56,236 - mmdet - INFO - Epoch [2][600/7033]	lr: 9.331e-05, eta: 10:07:25, time: 1.100, data_time: 0.035, memory: 17617, loss_cls: 0.0896, loss_bbox: 0.2667, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2778, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2657, d3.loss_cls: 0.0916, d3.loss_bbox: 0.2698, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2608, loss: 2.3769, grad_norm: 86.8307
2025-06-10 09:30:48,126 - mmdet - INFO - Epoch [2][650/7033]	lr: 9.331e-05, eta: 10:06:29, time: 1.038, data_time: 0.034, memory: 17617, loss_cls: 0.0904, loss_bbox: 0.2662, d0.loss_cls: 0.1825, d0.loss_bbox: 0.3720, d1.loss_cls: 0.1234, d1.loss_bbox: 0.2786, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2686, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2711, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2607, loss: 2.4025, grad_norm: 51.1518
2025-06-10 09:31:39,342 - mmdet - INFO - Epoch [2][700/7033]	lr: 9.331e-05, eta: 10:05:29, time: 1.024, data_time: 0.033, memory: 17617, loss_cls: 0.0950, loss_bbox: 0.2668, d0.loss_cls: 0.1858, d0.loss_bbox: 0.3736, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2674, d3.loss_cls: 0.1007, d3.loss_bbox: 0.2684, d4.loss_cls: 0.0976, d4.loss_bbox: 0.2612, loss: 2.4322, grad_norm: 68.3789
2025-06-10 09:32:30,244 - mmdet - INFO - Epoch [2][750/7033]	lr: 9.331e-05, eta: 10:04:29, time: 1.018, data_time: 0.030, memory: 17617, loss_cls: 0.0948, loss_bbox: 0.2557, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3667, d1.loss_cls: 0.1289, d1.loss_bbox: 0.2705, d2.loss_cls: 0.1072, d2.loss_bbox: 0.2563, d3.loss_cls: 0.0994, d3.loss_bbox: 0.2587, d4.loss_cls: 0.0965, d4.loss_bbox: 0.2504, loss: 2.3751, grad_norm: 43.8480
2025-06-10 09:33:21,393 - mmdet - INFO - Epoch [2][800/7033]	lr: 9.331e-05, eta: 10:03:29, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.1008, loss_bbox: 0.2462, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1305, d1.loss_bbox: 0.2632, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2510, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2527, d4.loss_cls: 0.1001, d4.loss_bbox: 0.2427, loss: 2.3385, grad_norm: 24.2430
2025-06-10 09:34:12,755 - mmdet - INFO - Epoch [2][850/7033]	lr: 9.331e-05, eta: 10:02:31, time: 1.027, data_time: 0.035, memory: 17617, loss_cls: 0.0883, loss_bbox: 0.2520, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3527, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2677, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2568, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2465, loss: 2.3098, grad_norm: 58.1077
2025-06-10 09:35:04,257 - mmdet - INFO - Epoch [2][900/7033]	lr: 9.331e-05, eta: 10:01:33, time: 1.030, data_time: 0.035, memory: 17617, loss_cls: 0.0954, loss_bbox: 0.2491, d0.loss_cls: 0.2060, d0.loss_bbox: 0.3529, d1.loss_cls: 0.1348, d1.loss_bbox: 0.2655, d2.loss_cls: 0.1117, d2.loss_bbox: 0.2551, d3.loss_cls: 0.1006, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0956, d4.loss_bbox: 0.2475, loss: 2.3709, grad_norm: 42.4449
2025-06-10 09:36:06,049 - mmdet - INFO - Epoch [2][950/7033]	lr: 9.331e-05, eta: 10:01:19, time: 1.236, data_time: 0.038, memory: 17617, loss_cls: 0.0883, loss_bbox: 0.2447, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3448, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0982, d2.loss_bbox: 0.2481, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2400, loss: 2.2517, grad_norm: 35.7156
2025-06-10 09:36:57,023 - mmdet - INFO - Epoch [2][1000/7033]	lr: 9.331e-05, eta: 10:00:19, time: 1.020, data_time: 0.032, memory: 17617, loss_cls: 0.0844, loss_bbox: 0.2406, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2643, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2521, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2516, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2387, loss: 2.2331, grad_norm: 25.7675
2025-06-10 09:37:48,132 - mmdet - INFO - Epoch [2][1050/7033]	lr: 9.331e-05, eta: 9:59:20, time: 1.022, data_time: 0.032, memory: 17617, loss_cls: 0.0897, loss_bbox: 0.2506, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2545, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2572, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2466, loss: 2.3152, grad_norm: 59.4671
2025-06-10 09:38:39,028 - mmdet - INFO - Epoch [2][1100/7033]	lr: 9.331e-05, eta: 9:58:19, time: 1.018, data_time: 0.033, memory: 17617, loss_cls: 0.0910, loss_bbox: 0.2491, d0.loss_cls: 0.1837, d0.loss_bbox: 0.3511, d1.loss_cls: 0.1225, d1.loss_bbox: 0.2664, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2556, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2581, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2447, loss: 2.3102, grad_norm: 31.8769
2025-06-10 09:39:29,997 - mmdet - INFO - Epoch [2][1150/7033]	lr: 9.331e-05, eta: 9:57:20, time: 1.019, data_time: 0.031, memory: 17617, loss_cls: 0.0839, loss_bbox: 0.2398, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3446, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2560, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2472, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2501, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2344, loss: 2.2122, grad_norm: 67.2858
2025-06-10 09:40:20,873 - mmdet - INFO - Epoch [2][1200/7033]	lr: 9.331e-05, eta: 9:56:19, time: 1.018, data_time: 0.032, memory: 17617, loss_cls: 0.0882, loss_bbox: 0.2370, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2601, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2458, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2351, loss: 2.2298, grad_norm: 30.9234
2025-06-10 09:41:12,262 - mmdet - INFO - Epoch [2][1250/7033]	lr: 9.331e-05, eta: 9:55:22, time: 1.028, data_time: 0.034, memory: 17617, loss_cls: 0.0860, loss_bbox: 0.2478, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3497, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2676, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2555, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2610, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2430, loss: 2.2601, grad_norm: 29.8691
2025-06-10 09:42:03,476 - mmdet - INFO - Epoch [2][1300/7033]	lr: 9.331e-05, eta: 9:54:23, time: 1.024, data_time: 0.032, memory: 17617, loss_cls: 0.0821, loss_bbox: 0.2410, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3485, d1.loss_cls: 0.1107, d1.loss_bbox: 0.2598, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2483, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2522, d4.loss_cls: 0.0869, d4.loss_bbox: 0.2363, loss: 2.2138, grad_norm: 130.3735
2025-06-10 09:42:54,437 - mmdet - INFO - Epoch [2][1350/7033]	lr: 9.331e-05, eta: 9:53:23, time: 1.019, data_time: 0.031, memory: 17617, loss_cls: 0.0885, loss_bbox: 0.2409, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3427, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2555, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2448, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2473, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2364, loss: 2.2396, grad_norm: 45.2448
2025-06-10 09:43:45,943 - mmdet - INFO - Epoch [2][1400/7033]	lr: 9.331e-05, eta: 9:52:26, time: 1.030, data_time: 0.034, memory: 17617, loss_cls: 0.0937, loss_bbox: 0.2375, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2602, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2486, d3.loss_cls: 0.0939, d3.loss_bbox: 0.2512, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2385, loss: 2.2712, grad_norm: 28.6676
2025-06-10 09:44:37,539 - mmdet - INFO - Epoch [2][1450/7033]	lr: 9.331e-05, eta: 9:51:29, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0884, loss_bbox: 0.2385, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2602, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2505, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0919, d4.loss_bbox: 0.2367, loss: 2.2542, grad_norm: 45.2784
2025-06-10 09:45:29,211 - mmdet - INFO - Epoch [2][1500/7033]	lr: 9.331e-05, eta: 9:50:33, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.0879, loss_bbox: 0.2410, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1162, d1.loss_bbox: 0.2661, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2522, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2539, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2383, loss: 2.2705, grad_norm: 35.8912
2025-06-10 09:46:20,826 - mmdet - INFO - Epoch [2][1550/7033]	lr: 9.331e-05, eta: 9:49:36, time: 1.032, data_time: 0.036, memory: 17617, loss_cls: 0.0860, loss_bbox: 0.2389, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2579, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2475, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2506, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2367, loss: 2.2380, grad_norm: 52.9439
2025-06-10 09:47:12,234 - mmdet - INFO - Epoch [2][1600/7033]	lr: 9.331e-05, eta: 9:48:39, time: 1.028, data_time: 0.033, memory: 17617, loss_cls: 0.0858, loss_bbox: 0.2282, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2411, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2261, loss: 2.1824, grad_norm: 34.3603
2025-06-10 09:48:03,941 - mmdet - INFO - Epoch [2][1650/7033]	lr: 9.331e-05, eta: 9:47:43, time: 1.034, data_time: 0.032, memory: 17617, loss_cls: 0.0879, loss_bbox: 0.2446, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3561, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2696, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2547, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2571, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2416, loss: 2.2981, grad_norm: 53.7225
2025-06-10 09:48:55,445 - mmdet - INFO - Epoch [2][1700/7033]	lr: 9.331e-05, eta: 9:46:46, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0819, loss_bbox: 0.2398, d0.loss_cls: 0.1829, d0.loss_bbox: 0.3537, d1.loss_cls: 0.1133, d1.loss_bbox: 0.2614, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2501, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2537, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2372, loss: 2.2426, grad_norm: 22.5431
2025-06-10 09:49:46,953 - mmdet - INFO - Epoch [2][1750/7033]	lr: 9.331e-05, eta: 9:45:49, time: 1.030, data_time: 0.033, memory: 17617, loss_cls: 0.0865, loss_bbox: 0.2392, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3557, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2631, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2493, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2507, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2381, loss: 2.2568, grad_norm: 32.9325
2025-06-10 09:50:38,637 - mmdet - INFO - Epoch [2][1800/7033]	lr: 9.331e-05, eta: 9:44:53, time: 1.034, data_time: 0.031, memory: 17617, loss_cls: 0.0840, loss_bbox: 0.2361, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3487, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2597, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2473, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2486, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2348, loss: 2.2107, grad_norm: 49.6453
2025-06-10 09:51:30,172 - mmdet - INFO - Epoch [2][1850/7033]	lr: 9.331e-05, eta: 9:43:57, time: 1.031, data_time: 0.031, memory: 17617, loss_cls: 0.0857, loss_bbox: 0.2368, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2617, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2363, loss: 2.2322, grad_norm: 53.2245
2025-06-10 09:52:21,607 - mmdet - INFO - Epoch [2][1900/7033]	lr: 9.331e-05, eta: 9:43:00, time: 1.029, data_time: 0.033, memory: 17617, loss_cls: 0.0854, loss_bbox: 0.2290, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3445, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2384, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2263, loss: 2.1918, grad_norm: 32.0037
2025-06-10 09:53:13,093 - mmdet - INFO - Epoch [2][1950/7033]	lr: 9.331e-05, eta: 9:42:03, time: 1.030, data_time: 0.031, memory: 17617, loss_cls: 0.0835, loss_bbox: 0.2351, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2600, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2473, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2482, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2339, loss: 2.2177, grad_norm: 53.2103
2025-06-10 09:54:04,866 - mmdet - INFO - Epoch [2][2000/7033]	lr: 9.331e-05, eta: 9:41:08, time: 1.035, data_time: 0.033, memory: 17617, loss_cls: 0.0822, loss_bbox: 0.2326, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1106, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2445, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2451, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2299, loss: 2.1879, grad_norm: 28.4144
2025-06-10 09:54:56,677 - mmdet - INFO - Epoch [2][2050/7033]	lr: 9.331e-05, eta: 9:40:12, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0853, loss_bbox: 0.2411, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2652, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2537, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2541, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2404, loss: 2.2499, grad_norm: 27.6842
2025-06-10 09:55:48,546 - mmdet - INFO - Epoch [2][2100/7033]	lr: 9.331e-05, eta: 9:39:17, time: 1.037, data_time: 0.035, memory: 17617, loss_cls: 0.0855, loss_bbox: 0.2348, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3430, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2601, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2444, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2464, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2313, loss: 2.1983, grad_norm: 25.5551
2025-06-10 09:56:40,293 - mmdet - INFO - Epoch [2][2150/7033]	lr: 9.331e-05, eta: 9:38:22, time: 1.035, data_time: 0.035, memory: 17617, loss_cls: 0.0855, loss_bbox: 0.2354, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0965, d2.loss_bbox: 0.2479, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2334, loss: 2.2216, grad_norm: 26.9868
2025-06-10 09:57:40,215 - mmdet - INFO - Epoch [2][2200/7033]	lr: 9.331e-05, eta: 9:37:56, time: 1.198, data_time: 0.072, memory: 17617, loss_cls: 0.0859, loss_bbox: 0.2401, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2662, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2554, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2394, loss: 2.2577, grad_norm: 33.7108
2025-06-10 09:58:36,600 - mmdet - INFO - Epoch [2][2250/7033]	lr: 9.331e-05, eta: 9:37:16, time: 1.127, data_time: 0.028, memory: 17617, loss_cls: 0.0878, loss_bbox: 0.2444, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3622, d1.loss_cls: 0.1162, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2601, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2631, d4.loss_cls: 0.0905, d4.loss_bbox: 0.2431, loss: 2.3051, grad_norm: 69.8863
2025-06-10 09:59:28,724 - mmdet - INFO - Epoch [2][2300/7033]	lr: 9.331e-05, eta: 9:36:22, time: 1.043, data_time: 0.058, memory: 17617, loss_cls: 0.0913, loss_bbox: 0.2416, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3602, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2575, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2416, loss: 2.3143, grad_norm: 37.7878
2025-06-10 10:00:23,839 - mmdet - INFO - Epoch [2][2350/7033]	lr: 9.331e-05, eta: 9:35:38, time: 1.102, data_time: 0.055, memory: 17617, loss_cls: 0.0866, loss_bbox: 0.2353, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3428, d1.loss_cls: 0.1185, d1.loss_bbox: 0.2602, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2455, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2476, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2322, loss: 2.2340, grad_norm: 104.3057
2025-06-10 10:01:18,005 - mmdet - INFO - Epoch [2][2400/7033]	lr: 9.331e-05, eta: 9:34:51, time: 1.083, data_time: 0.033, memory: 17617, loss_cls: 0.0915, loss_bbox: 0.2421, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3553, d1.loss_cls: 0.1172, d1.loss_bbox: 0.2688, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2526, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2553, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2389, loss: 2.2952, grad_norm: 27.0206
2025-06-10 10:02:14,059 - mmdet - INFO - Epoch [2][2450/7033]	lr: 9.331e-05, eta: 9:34:10, time: 1.121, data_time: 0.032, memory: 17617, loss_cls: 0.0877, loss_bbox: 0.2362, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1166, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2494, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2339, loss: 2.2312, grad_norm: 40.8464
2025-06-10 10:03:07,502 - mmdet - INFO - Epoch [2][2500/7033]	lr: 9.331e-05, eta: 9:33:20, time: 1.069, data_time: 0.032, memory: 17617, loss_cls: 0.0855, loss_bbox: 0.2347, d0.loss_cls: 0.1707, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2613, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2485, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2510, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2343, loss: 2.2237, grad_norm: 68.0403
2025-06-10 10:04:00,119 - mmdet - INFO - Epoch [2][2550/7033]	lr: 9.331e-05, eta: 9:32:27, time: 1.052, data_time: 0.034, memory: 17617, loss_cls: 0.0891, loss_bbox: 0.2516, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2750, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2640, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2668, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2501, loss: 2.3349, grad_norm: 29.8840
2025-06-10 10:04:51,819 - mmdet - INFO - Epoch [2][2600/7033]	lr: 9.331e-05, eta: 9:31:32, time: 1.034, data_time: 0.029, memory: 17617, loss_cls: 0.0960, loss_bbox: 0.2529, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1220, d1.loss_bbox: 0.2830, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2691, d3.loss_cls: 0.1009, d3.loss_bbox: 0.2709, d4.loss_cls: 0.0972, d4.loss_bbox: 0.2530, loss: 2.3987, grad_norm: 67.4075
2025-06-10 10:05:43,138 - mmdet - INFO - Epoch [2][2650/7033]	lr: 9.331e-05, eta: 9:30:34, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0842, loss_bbox: 0.2350, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1132, d1.loss_bbox: 0.2611, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2496, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2510, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2345, loss: 2.2149, grad_norm: 35.0787
2025-06-10 10:06:34,960 - mmdet - INFO - Epoch [2][2700/7033]	lr: 9.331e-05, eta: 9:29:39, time: 1.037, data_time: 0.046, memory: 17617, loss_cls: 0.0866, loss_bbox: 0.2400, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2628, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2548, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2564, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2382, loss: 2.2635, grad_norm: 80.6971
2025-06-10 10:07:28,179 - mmdet - INFO - Epoch [2][2750/7033]	lr: 9.331e-05, eta: 9:28:48, time: 1.064, data_time: 0.033, memory: 17617, loss_cls: 0.0898, loss_bbox: 0.2357, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2620, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2498, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2517, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2348, loss: 2.2532, grad_norm: 81.2370
2025-06-10 10:08:19,462 - mmdet - INFO - Epoch [2][2800/7033]	lr: 9.331e-05, eta: 9:27:51, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0824, loss_bbox: 0.2334, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3472, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2612, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2474, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2486, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2314, loss: 2.1957, grad_norm: 38.3906
2025-06-10 10:09:11,204 - mmdet - INFO - Epoch [2][2850/7033]	lr: 9.331e-05, eta: 9:26:56, time: 1.035, data_time: 0.031, memory: 17617, loss_cls: 0.0927, loss_bbox: 0.2386, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2673, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2546, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2566, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2376, loss: 2.2844, grad_norm: 86.8910
2025-06-10 10:10:03,462 - mmdet - INFO - Epoch [2][2900/7033]	lr: 9.331e-05, eta: 9:26:02, time: 1.045, data_time: 0.028, memory: 17617, loss_cls: 0.0847, loss_bbox: 0.2331, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3432, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2618, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2475, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2482, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2314, loss: 2.2091, grad_norm: 352.0461
2025-06-10 10:10:56,772 - mmdet - INFO - Epoch [2][2950/7033]	lr: 9.331e-05, eta: 9:25:11, time: 1.066, data_time: 0.073, memory: 17617, loss_cls: 0.0847, loss_bbox: 0.2225, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2366, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2381, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2203, loss: 2.1514, grad_norm: 38.2591
2025-06-10 10:11:50,949 - mmdet - INFO - Epoch [2][3000/7033]	lr: 9.331e-05, eta: 9:24:24, time: 1.084, data_time: 0.091, memory: 17617, loss_cls: 0.0742, loss_bbox: 0.2278, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2564, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2416, d3.loss_cls: 0.0785, d3.loss_bbox: 0.2427, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2272, loss: 2.1205, grad_norm: 29.2492
2025-06-10 10:12:42,746 - mmdet - INFO - Epoch [2][3050/7033]	lr: 9.331e-05, eta: 9:23:28, time: 1.036, data_time: 0.042, memory: 17617, loss_cls: 0.0852, loss_bbox: 0.2351, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2653, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2508, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2511, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2344, loss: 2.2443, grad_norm: 29.2827
2025-06-10 10:13:35,190 - mmdet - INFO - Epoch [2][3100/7033]	lr: 9.331e-05, eta: 9:22:35, time: 1.049, data_time: 0.049, memory: 17617, loss_cls: 0.0855, loss_bbox: 0.2443, d0.loss_cls: 0.1865, d0.loss_bbox: 0.3607, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2718, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2573, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2604, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2417, loss: 2.3111, grad_norm: 33.3920
2025-06-10 10:14:26,503 - mmdet - INFO - Epoch [2][3150/7033]	lr: 9.331e-05, eta: 9:21:38, time: 1.026, data_time: 0.037, memory: 17617, loss_cls: 0.0859, loss_bbox: 0.2372, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2673, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2498, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2529, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2356, loss: 2.2712, grad_norm: 29.9629
2025-06-10 10:15:20,548 - mmdet - INFO - Epoch [2][3200/7033]	lr: 9.331e-05, eta: 9:20:50, time: 1.081, data_time: 0.040, memory: 17617, loss_cls: 0.0963, loss_bbox: 0.2430, d0.loss_cls: 0.1901, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2696, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2565, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2571, d4.loss_cls: 0.0963, d4.loss_bbox: 0.2421, loss: 2.3342, grad_norm: 62.9727
2025-06-10 10:16:11,102 - mmdet - INFO - Epoch [2][3250/7033]	lr: 9.331e-05, eta: 9:19:51, time: 1.011, data_time: 0.032, memory: 17617, loss_cls: 0.0887, loss_bbox: 0.2366, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2653, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2492, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2537, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2344, loss: 2.2659, grad_norm: 39.7884
2025-06-10 10:17:07,839 - mmdet - INFO - Epoch [2][3300/7033]	lr: 9.331e-05, eta: 9:19:11, time: 1.135, data_time: 0.030, memory: 17617, loss_cls: 0.0854, loss_bbox: 0.2315, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3419, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2604, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2449, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2487, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2300, loss: 2.2159, grad_norm: 91.0727
2025-06-10 10:18:01,449 - mmdet - INFO - Epoch [2][3350/7033]	lr: 9.331e-05, eta: 9:18:21, time: 1.072, data_time: 0.030, memory: 17617, loss_cls: 0.0772, loss_bbox: 0.2287, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3420, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0938, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2459, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2261, loss: 2.1567, grad_norm: 111.2251
2025-06-10 10:18:56,857 - mmdet - INFO - Epoch [2][3400/7033]	lr: 9.331e-05, eta: 9:17:37, time: 1.108, data_time: 0.030, memory: 17617, loss_cls: 0.0856, loss_bbox: 0.2390, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3553, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2754, d2.loss_cls: 0.1015, d2.loss_bbox: 0.2558, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2577, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2380, loss: 2.2872, grad_norm: 30.7862
2025-06-10 10:19:49,359 - mmdet - INFO - Epoch [2][3450/7033]	lr: 9.331e-05, eta: 9:16:44, time: 1.050, data_time: 0.056, memory: 17617, loss_cls: 0.0880, loss_bbox: 0.2266, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3420, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2566, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2436, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2453, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2259, loss: 2.2034, grad_norm: 234.4333
2025-06-10 10:20:40,197 - mmdet - INFO - Epoch [2][3500/7033]	lr: 9.331e-05, eta: 9:15:46, time: 1.017, data_time: 0.026, memory: 17617, loss_cls: 0.0868, loss_bbox: 0.2340, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2597, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2488, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2327, loss: 2.2242, grad_norm: 28.8855
2025-06-10 10:21:31,075 - mmdet - INFO - Epoch [2][3550/7033]	lr: 9.331e-05, eta: 9:14:48, time: 1.018, data_time: 0.027, memory: 17617, loss_cls: 0.0801, loss_bbox: 0.2325, d0.loss_cls: 0.1780, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2598, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2502, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2536, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2321, loss: 2.2039, grad_norm: 30.1423
2025-06-10 10:22:22,555 - mmdet - INFO - Epoch [2][3600/7033]	lr: 9.331e-05, eta: 9:13:52, time: 1.030, data_time: 0.038, memory: 17617, loss_cls: 0.0878, loss_bbox: 0.2365, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2606, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2489, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2501, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2332, loss: 2.2357, grad_norm: 39.4687
2025-06-10 10:23:13,950 - mmdet - INFO - Epoch [2][3650/7033]	lr: 9.331e-05, eta: 9:12:55, time: 1.028, data_time: 0.034, memory: 17617, loss_cls: 0.0796, loss_bbox: 0.2303, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3465, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2596, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2453, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2453, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2278, loss: 2.1860, grad_norm: 26.6524
2025-06-10 10:24:05,010 - mmdet - INFO - Epoch [2][3700/7033]	lr: 9.331e-05, eta: 9:11:58, time: 1.021, data_time: 0.031, memory: 17617, loss_cls: 0.0905, loss_bbox: 0.2323, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2601, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2477, d3.loss_cls: 0.0939, d3.loss_bbox: 0.2512, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2302, loss: 2.2413, grad_norm: 71.8740
2025-06-10 10:24:58,154 - mmdet - INFO - Epoch [2][3750/7033]	lr: 9.331e-05, eta: 9:11:07, time: 1.063, data_time: 0.042, memory: 17617, loss_cls: 0.0810, loss_bbox: 0.2272, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2403, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2415, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2255, loss: 2.1414, grad_norm: 38.0442
2025-06-10 10:25:50,913 - mmdet - INFO - Epoch [2][3800/7033]	lr: 9.331e-05, eta: 9:10:15, time: 1.055, data_time: 0.032, memory: 17617, loss_cls: 0.0787, loss_bbox: 0.2329, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3513, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2635, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2494, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2322, loss: 2.1892, grad_norm: 29.5728
2025-06-10 10:26:42,220 - mmdet - INFO - Epoch [2][3850/7033]	lr: 9.331e-05, eta: 9:09:18, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0841, loss_bbox: 0.2377, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3469, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2605, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2500, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2519, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2352, loss: 2.2255, grad_norm: 40.3898
2025-06-10 10:27:33,974 - mmdet - INFO - Epoch [2][3900/7033]	lr: 9.331e-05, eta: 9:08:23, time: 1.035, data_time: 0.026, memory: 17617, loss_cls: 0.0836, loss_bbox: 0.2332, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3458, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2480, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2492, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2319, loss: 2.2208, grad_norm: 29.7399
2025-06-10 10:28:26,295 - mmdet - INFO - Epoch [2][3950/7033]	lr: 9.331e-05, eta: 9:07:30, time: 1.046, data_time: 0.051, memory: 17617, loss_cls: 0.0822, loss_bbox: 0.2342, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3434, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2617, d2.loss_cls: 0.0932, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2500, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2342, loss: 2.2006, grad_norm: 22.9656
2025-06-10 10:29:17,627 - mmdet - INFO - Epoch [2][4000/7033]	lr: 9.331e-05, eta: 9:06:33, time: 1.027, data_time: 0.033, memory: 17617, loss_cls: 0.0889, loss_bbox: 0.2393, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3545, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2680, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2521, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2563, d4.loss_cls: 0.0897, d4.loss_bbox: 0.2382, loss: 2.2869, grad_norm: 26.7891
2025-06-10 10:30:08,736 - mmdet - INFO - Epoch [2][4050/7033]	lr: 9.331e-05, eta: 9:05:37, time: 1.022, data_time: 0.029, memory: 17617, loss_cls: 0.0852, loss_bbox: 0.2397, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3579, d1.loss_cls: 0.1128, d1.loss_bbox: 0.2707, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2550, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2560, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2379, loss: 2.2714, grad_norm: 22.3044
2025-06-10 10:30:59,635 - mmdet - INFO - Epoch [2][4100/7033]	lr: 9.331e-05, eta: 9:04:39, time: 1.018, data_time: 0.027, memory: 17617, loss_cls: 0.0863, loss_bbox: 0.2375, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3589, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2698, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2522, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2533, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2368, loss: 2.2572, grad_norm: 28.0361
2025-06-10 10:31:53,493 - mmdet - INFO - Epoch [2][4150/7033]	lr: 9.331e-05, eta: 9:03:50, time: 1.077, data_time: 0.027, memory: 17617, loss_cls: 0.0866, loss_bbox: 0.2280, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2617, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2426, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2428, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2270, loss: 2.2246, grad_norm: 83.8517
2025-06-10 10:32:44,407 - mmdet - INFO - Epoch [2][4200/7033]	lr: 9.331e-05, eta: 9:02:53, time: 1.018, data_time: 0.028, memory: 17617, loss_cls: 0.0861, loss_bbox: 0.2439, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3508, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2590, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2622, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2440, loss: 2.2924, grad_norm: 60.0608
2025-06-10 10:33:38,205 - mmdet - INFO - Epoch [2][4250/7033]	lr: 9.331e-05, eta: 9:02:04, time: 1.076, data_time: 0.024, memory: 17617, loss_cls: 0.0867, loss_bbox: 0.2341, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3455, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2625, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2471, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2497, d4.loss_cls: 0.0870, d4.loss_bbox: 0.2332, loss: 2.2303, grad_norm: 35.2226
2025-06-10 10:34:29,589 - mmdet - INFO - Epoch [2][4300/7033]	lr: 9.331e-05, eta: 9:01:08, time: 1.028, data_time: 0.028, memory: 17617, loss_cls: 0.0831, loss_bbox: 0.2365, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2661, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2487, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2495, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2360, loss: 2.2327, grad_norm: 27.6258
2025-06-10 10:35:23,875 - mmdet - INFO - Epoch [2][4350/7033]	lr: 9.331e-05, eta: 9:00:20, time: 1.086, data_time: 0.028, memory: 17617, loss_cls: 0.0842, loss_bbox: 0.2354, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3472, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2654, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2500, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2512, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2344, loss: 2.2334, grad_norm: 46.3318
2025-06-10 10:36:22,770 - mmdet - INFO - Epoch [2][4400/7033]	lr: 9.331e-05, eta: 8:59:44, time: 1.178, data_time: 0.032, memory: 17617, loss_cls: 0.0756, loss_bbox: 0.2229, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2525, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2392, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2389, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2226, loss: 2.1062, grad_norm: 41.9071
2025-06-10 10:37:14,445 - mmdet - INFO - Epoch [2][4450/7033]	lr: 9.331e-05, eta: 8:58:49, time: 1.033, data_time: 0.028, memory: 17617, loss_cls: 0.0869, loss_bbox: 0.2323, d0.loss_cls: 0.1861, d0.loss_bbox: 0.3503, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2642, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2467, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2478, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2307, loss: 2.2445, grad_norm: 52.8329
2025-06-10 10:38:08,652 - mmdet - INFO - Epoch [2][4500/7033]	lr: 9.331e-05, eta: 8:58:00, time: 1.084, data_time: 0.028, memory: 17617, loss_cls: 0.0853, loss_bbox: 0.2327, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2640, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2480, d3.loss_cls: 0.0870, d3.loss_bbox: 0.2493, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2320, loss: 2.2240, grad_norm: 30.6449
2025-06-10 10:39:01,167 - mmdet - INFO - Epoch [2][4550/7033]	lr: 9.331e-05, eta: 8:57:07, time: 1.050, data_time: 0.058, memory: 17617, loss_cls: 0.0849, loss_bbox: 0.2312, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2601, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2465, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2475, d4.loss_cls: 0.0870, d4.loss_bbox: 0.2309, loss: 2.2180, grad_norm: 26.7241
2025-06-10 10:39:54,571 - mmdet - INFO - Epoch [2][4600/7033]	lr: 9.331e-05, eta: 8:56:17, time: 1.068, data_time: 0.035, memory: 17617, loss_cls: 0.0844, loss_bbox: 0.2322, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2456, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2481, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2310, loss: 2.1931, grad_norm: 76.6506
2025-06-10 10:40:57,886 - mmdet - INFO - Epoch [2][4650/7033]	lr: 9.331e-05, eta: 8:55:52, time: 1.266, data_time: 0.032, memory: 17617, loss_cls: 0.0863, loss_bbox: 0.2268, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3376, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2404, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2430, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2264, loss: 2.1750, grad_norm: 22.6446
2025-06-10 10:41:53,993 - mmdet - INFO - Epoch [2][4700/7033]	lr: 9.331e-05, eta: 8:55:08, time: 1.122, data_time: 0.025, memory: 17617, loss_cls: 0.0807, loss_bbox: 0.2292, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3372, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2553, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2397, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2428, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2271, loss: 2.1592, grad_norm: 62.1340
2025-06-10 10:42:46,777 - mmdet - INFO - Epoch [2][4750/7033]	lr: 9.331e-05, eta: 8:54:16, time: 1.056, data_time: 0.028, memory: 17617, loss_cls: 0.0803, loss_bbox: 0.2278, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2628, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2466, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2448, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2272, loss: 2.1727, grad_norm: 40.7049
2025-06-10 10:43:37,846 - mmdet - INFO - Epoch [2][4800/7033]	lr: 9.331e-05, eta: 8:53:19, time: 1.022, data_time: 0.027, memory: 17617, loss_cls: 0.0841, loss_bbox: 0.2254, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3376, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2246, loss: 2.1567, grad_norm: 27.2522
2025-06-10 10:44:28,659 - mmdet - INFO - Epoch [2][4850/7033]	lr: 9.331e-05, eta: 8:52:21, time: 1.016, data_time: 0.028, memory: 17617, loss_cls: 0.0750, loss_bbox: 0.2207, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2207, loss: 2.0945, grad_norm: 31.7468
2025-06-10 10:45:19,846 - mmdet - INFO - Epoch [2][4900/7033]	lr: 9.331e-05, eta: 8:51:25, time: 1.024, data_time: 0.032, memory: 17617, loss_cls: 0.0900, loss_bbox: 0.2426, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3493, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2708, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2568, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2603, d4.loss_cls: 0.0921, d4.loss_bbox: 0.2410, loss: 2.2991, grad_norm: 34.1568
2025-06-10 10:46:10,860 - mmdet - INFO - Epoch [2][4950/7033]	lr: 9.331e-05, eta: 8:50:28, time: 1.020, data_time: 0.029, memory: 17617, loss_cls: 0.0862, loss_bbox: 0.2292, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2617, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2457, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2279, loss: 2.2297, grad_norm: 34.7080
2025-06-10 10:47:01,830 - mmdet - INFO - Epoch [2][5000/7033]	lr: 9.331e-05, eta: 8:49:31, time: 1.019, data_time: 0.026, memory: 17617, loss_cls: 0.0849, loss_bbox: 0.2236, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1201, d1.loss_bbox: 0.2503, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2363, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2220, loss: 2.1605, grad_norm: 28.4475
2025-06-10 10:47:53,589 - mmdet - INFO - Epoch [2][5050/7033]	lr: 9.331e-05, eta: 8:48:36, time: 1.035, data_time: 0.029, memory: 17617, loss_cls: 0.0902, loss_bbox: 0.2243, d0.loss_cls: 0.1901, d0.loss_bbox: 0.3518, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2583, d2.loss_cls: 0.1043, d2.loss_bbox: 0.2418, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2405, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2257, loss: 2.2311, grad_norm: 40.9046
2025-06-10 10:48:48,095 - mmdet - INFO - Epoch [2][5100/7033]	lr: 9.331e-05, eta: 8:47:48, time: 1.090, data_time: 0.032, memory: 17617, loss_cls: 0.0817, loss_bbox: 0.2299, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2428, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2296, loss: 2.1955, grad_norm: 47.7673
2025-06-10 10:49:38,799 - mmdet - INFO - Epoch [2][5150/7033]	lr: 9.331e-05, eta: 8:46:51, time: 1.014, data_time: 0.030, memory: 17617, loss_cls: 0.0854, loss_bbox: 0.2412, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2710, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2560, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2555, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2412, loss: 2.2809, grad_norm: 32.5219
2025-06-10 10:50:29,607 - mmdet - INFO - Epoch [2][5200/7033]	lr: 9.331e-05, eta: 8:45:53, time: 1.016, data_time: 0.029, memory: 17617, loss_cls: 0.0912, loss_bbox: 0.2279, d0.loss_cls: 0.1835, d0.loss_bbox: 0.3466, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2566, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2446, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2276, loss: 2.2299, grad_norm: 40.5814
2025-06-10 10:51:20,903 - mmdet - INFO - Epoch [2][5250/7033]	lr: 9.331e-05, eta: 8:44:57, time: 1.026, data_time: 0.033, memory: 17617, loss_cls: 0.0883, loss_bbox: 0.2335, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2645, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2495, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2508, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2332, loss: 2.2629, grad_norm: 33.9070
2025-06-10 10:52:12,509 - mmdet - INFO - Epoch [2][5300/7033]	lr: 9.331e-05, eta: 8:44:02, time: 1.032, data_time: 0.028, memory: 17617, loss_cls: 0.0796, loss_bbox: 0.2385, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3513, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2669, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2517, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2537, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2371, loss: 2.2173, grad_norm: 29.1244
2025-06-10 10:53:03,784 - mmdet - INFO - Epoch [2][5350/7033]	lr: 9.331e-05, eta: 8:43:06, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0789, loss_bbox: 0.2263, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3350, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2529, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2243, loss: 2.1457, grad_norm: 41.6224
2025-06-10 10:53:55,290 - mmdet - INFO - Epoch [2][5400/7033]	lr: 9.331e-05, eta: 8:42:11, time: 1.030, data_time: 0.027, memory: 17617, loss_cls: 0.0777, loss_bbox: 0.2247, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3459, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2590, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2230, loss: 2.1420, grad_norm: 31.5803
2025-06-10 10:54:46,951 - mmdet - INFO - Epoch [2][5450/7033]	lr: 9.331e-05, eta: 8:41:16, time: 1.033, data_time: 0.027, memory: 17617, loss_cls: 0.0753, loss_bbox: 0.2338, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2651, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2484, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2309, loss: 2.1744, grad_norm: 26.3266
2025-06-10 10:55:38,845 - mmdet - INFO - Epoch [2][5500/7033]	lr: 9.331e-05, eta: 8:40:22, time: 1.038, data_time: 0.027, memory: 17617, loss_cls: 0.0866, loss_bbox: 0.2338, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2654, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2487, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2494, d4.loss_cls: 0.0863, d4.loss_bbox: 0.2337, loss: 2.2381, grad_norm: 28.4945
2025-06-10 10:56:30,550 - mmdet - INFO - Epoch [2][5550/7033]	lr: 9.331e-05, eta: 8:39:27, time: 1.034, data_time: 0.027, memory: 17617, loss_cls: 0.0829, loss_bbox: 0.2366, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2685, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2520, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2512, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2362, loss: 2.2638, grad_norm: 24.4963
2025-06-10 10:57:22,344 - mmdet - INFO - Epoch [2][5600/7033]	lr: 9.331e-05, eta: 8:38:32, time: 1.036, data_time: 0.028, memory: 17617, loss_cls: 0.0880, loss_bbox: 0.2285, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3477, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1012, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2474, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2271, loss: 2.2230, grad_norm: 39.7483
2025-06-10 10:58:14,447 - mmdet - INFO - Epoch [2][5650/7033]	lr: 9.331e-05, eta: 8:37:38, time: 1.042, data_time: 0.029, memory: 17617, loss_cls: 0.0816, loss_bbox: 0.2343, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2689, d2.loss_cls: 0.0965, d2.loss_bbox: 0.2506, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2503, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2330, loss: 2.2514, grad_norm: 23.3361
2025-06-10 10:59:06,230 - mmdet - INFO - Epoch [2][5700/7033]	lr: 9.331e-05, eta: 8:36:44, time: 1.036, data_time: 0.029, memory: 17617, loss_cls: 0.0715, loss_bbox: 0.2215, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2212, loss: 2.0744, grad_norm: 58.1280
2025-06-10 10:59:58,097 - mmdet - INFO - Epoch [2][5750/7033]	lr: 9.331e-05, eta: 8:35:50, time: 1.037, data_time: 0.027, memory: 17617, loss_cls: 0.0908, loss_bbox: 0.2312, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3528, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2626, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2479, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2304, loss: 2.2759, grad_norm: 33.3064
2025-06-10 11:00:50,017 - mmdet - INFO - Epoch [2][5800/7033]	lr: 9.331e-05, eta: 8:34:55, time: 1.038, data_time: 0.027, memory: 17617, loss_cls: 0.0779, loss_bbox: 0.2211, d0.loss_cls: 0.1804, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2216, loss: 2.1280, grad_norm: 94.3283
2025-06-10 11:01:41,750 - mmdet - INFO - Epoch [2][5850/7033]	lr: 9.331e-05, eta: 8:34:01, time: 1.035, data_time: 0.029, memory: 17617, loss_cls: 0.0842, loss_bbox: 0.2319, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1133, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2439, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2463, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2286, loss: 2.1980, grad_norm: 37.2244
2025-06-10 11:02:33,493 - mmdet - INFO - Epoch [2][5900/7033]	lr: 9.331e-05, eta: 8:33:06, time: 1.035, data_time: 0.026, memory: 17617, loss_cls: 0.0802, loss_bbox: 0.2260, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3387, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2552, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2236, loss: 2.1696, grad_norm: 77.3638
2025-06-10 11:03:25,133 - mmdet - INFO - Epoch [2][5950/7033]	lr: 9.331e-05, eta: 8:32:12, time: 1.033, data_time: 0.030, memory: 17617, loss_cls: 0.0819, loss_bbox: 0.2365, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3555, d1.loss_cls: 0.1165, d1.loss_bbox: 0.2674, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2514, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2511, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2361, loss: 2.2386, grad_norm: 47.6584
2025-06-10 11:04:17,194 - mmdet - INFO - Epoch [2][6000/7033]	lr: 9.331e-05, eta: 8:31:18, time: 1.041, data_time: 0.038, memory: 17617, loss_cls: 0.0701, loss_bbox: 0.2212, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3348, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2192, loss: 2.0643, grad_norm: 28.2819
2025-06-10 11:05:11,541 - mmdet - INFO - Epoch [2][6050/7033]	lr: 9.331e-05, eta: 8:30:29, time: 1.087, data_time: 0.032, memory: 17617, loss_cls: 0.0821, loss_bbox: 0.2376, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3493, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2652, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2545, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2356, loss: 2.2394, grad_norm: 23.1629
2025-06-10 11:06:02,762 - mmdet - INFO - Epoch [2][6100/7033]	lr: 9.331e-05, eta: 8:29:33, time: 1.024, data_time: 0.029, memory: 17617, loss_cls: 0.0772, loss_bbox: 0.2216, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2518, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2370, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2197, loss: 2.1218, grad_norm: 74.3906
2025-06-10 11:06:54,552 - mmdet - INFO - Epoch [2][6150/7033]	lr: 9.331e-05, eta: 8:28:39, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0810, loss_bbox: 0.2255, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2416, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2426, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2248, loss: 2.1528, grad_norm: 39.0041
2025-06-10 11:07:46,774 - mmdet - INFO - Epoch [2][6200/7033]	lr: 9.331e-05, eta: 8:27:46, time: 1.044, data_time: 0.033, memory: 17617, loss_cls: 0.0718, loss_bbox: 0.2270, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3430, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2417, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2272, loss: 2.1251, grad_norm: 25.6945
2025-06-10 11:08:38,751 - mmdet - INFO - Epoch [2][6250/7033]	lr: 9.331e-05, eta: 8:26:52, time: 1.040, data_time: 0.033, memory: 17617, loss_cls: 0.0820, loss_bbox: 0.2291, d0.loss_cls: 0.1711, d0.loss_bbox: 0.3467, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2600, d2.loss_cls: 0.0963, d2.loss_bbox: 0.2464, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2476, d4.loss_cls: 0.0818, d4.loss_bbox: 0.2297, loss: 2.1916, grad_norm: 53.7062
2025-06-10 11:09:30,663 - mmdet - INFO - Epoch [2][6300/7033]	lr: 9.331e-05, eta: 8:25:58, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0797, loss_bbox: 0.2288, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2597, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2251, loss: 2.1647, grad_norm: 26.1187
2025-06-10 11:10:22,609 - mmdet - INFO - Epoch [2][6350/7033]	lr: 9.331e-05, eta: 8:25:04, time: 1.039, data_time: 0.032, memory: 17617, loss_cls: 0.0836, loss_bbox: 0.2296, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3497, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2638, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2451, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2281, loss: 2.2039, grad_norm: 28.5107
2025-06-10 11:11:14,831 - mmdet - INFO - Epoch [2][6400/7033]	lr: 9.331e-05, eta: 8:24:10, time: 1.044, data_time: 0.031, memory: 17617, loss_cls: 0.0808, loss_bbox: 0.2274, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3380, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2563, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2465, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2281, loss: 2.1565, grad_norm: 30.7256
2025-06-10 11:12:06,859 - mmdet - INFO - Epoch [2][6450/7033]	lr: 9.331e-05, eta: 8:23:17, time: 1.041, data_time: 0.029, memory: 17617, loss_cls: 0.0768, loss_bbox: 0.2254, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2404, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2383, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2248, loss: 2.1114, grad_norm: 79.0126
2025-06-10 11:12:58,998 - mmdet - INFO - Epoch [2][6500/7033]	lr: 9.331e-05, eta: 8:22:23, time: 1.043, data_time: 0.033, memory: 17617, loss_cls: 0.0810, loss_bbox: 0.2223, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3329, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2372, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2205, loss: 2.1256, grad_norm: 50.5532
2025-06-10 11:13:51,030 - mmdet - INFO - Epoch [2][6550/7033]	lr: 9.331e-05, eta: 8:21:29, time: 1.041, data_time: 0.033, memory: 17617, loss_cls: 0.0717, loss_bbox: 0.2216, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2526, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2372, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2382, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2206, loss: 2.0723, grad_norm: 180.0175
2025-06-10 11:14:42,919 - mmdet - INFO - Epoch [2][6600/7033]	lr: 9.331e-05, eta: 8:20:35, time: 1.038, data_time: 0.033, memory: 17617, loss_cls: 0.0767, loss_bbox: 0.2190, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2172, loss: 2.0938, grad_norm: 24.4048
2025-06-10 11:15:34,666 - mmdet - INFO - Epoch [2][6650/7033]	lr: 9.331e-05, eta: 8:19:41, time: 1.035, data_time: 0.030, memory: 17617, loss_cls: 0.0849, loss_bbox: 0.2253, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3377, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2573, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2400, d4.loss_cls: 0.0846, d4.loss_bbox: 0.2248, loss: 2.1605, grad_norm: 486.7482
2025-06-10 11:16:26,582 - mmdet - INFO - Epoch [2][6700/7033]	lr: 9.331e-05, eta: 8:18:47, time: 1.038, data_time: 0.033, memory: 17617, loss_cls: 0.0796, loss_bbox: 0.2264, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3396, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2582, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2433, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2438, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2272, loss: 2.1495, grad_norm: 25.6601
2025-06-10 11:17:18,474 - mmdet - INFO - Epoch [2][6750/7033]	lr: 9.331e-05, eta: 8:17:53, time: 1.038, data_time: 0.030, memory: 17617, loss_cls: 0.0773, loss_bbox: 0.2333, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3346, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2463, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2497, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2325, loss: 2.1587, grad_norm: 25.2855
2025-06-10 11:18:10,306 - mmdet - INFO - Epoch [2][6800/7033]	lr: 9.331e-05, eta: 8:16:59, time: 1.037, data_time: 0.033, memory: 17617, loss_cls: 0.0804, loss_bbox: 0.2228, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3380, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2534, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2404, d3.loss_cls: 0.0851, d3.loss_bbox: 0.2398, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2213, loss: 2.1384, grad_norm: 47.4970
2025-06-10 11:19:02,520 - mmdet - INFO - Epoch [2][6850/7033]	lr: 9.331e-05, eta: 8:16:06, time: 1.044, data_time: 0.032, memory: 17617, loss_cls: 0.0802, loss_bbox: 0.2319, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3475, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2639, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2459, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2458, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2300, loss: 2.2001, grad_norm: 36.4141
2025-06-10 11:19:57,751 - mmdet - INFO - Epoch [2][6900/7033]	lr: 9.331e-05, eta: 8:15:19, time: 1.105, data_time: 0.029, memory: 17617, loss_cls: 0.0843, loss_bbox: 0.2383, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2683, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2507, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2365, loss: 2.2520, grad_norm: 160.5211
2025-06-10 11:20:49,682 - mmdet - INFO - Epoch [2][6950/7033]	lr: 9.331e-05, eta: 8:14:25, time: 1.039, data_time: 0.029, memory: 17617, loss_cls: 0.0773, loss_bbox: 0.2215, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3372, d1.loss_cls: 0.1057, d1.loss_bbox: 0.2527, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2207, loss: 2.1172, grad_norm: 100.2496
2025-06-10 11:21:41,880 - mmdet - INFO - Epoch [2][7000/7033]	lr: 9.331e-05, eta: 8:13:31, time: 1.044, data_time: 0.032, memory: 17617, loss_cls: 0.0781, loss_bbox: 0.2250, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2228, loss: 2.1156, grad_norm: 28.6646
2025-06-10 11:22:16,763 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-10 11:49:03,159 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 11:49:03,160 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7817, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8791, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9086, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9215, pts_bbox_NuScenes/car_trans_err: 0.1870, pts_bbox_NuScenes/car_scale_err: 0.1554, pts_bbox_NuScenes/car_orient_err: 0.0477, pts_bbox_NuScenes/car_vel_err: 0.3641, pts_bbox_NuScenes/car_attr_err: 0.1612, pts_bbox_NuScenes/mATE: 0.2995, pts_bbox_NuScenes/mASE: 0.2728, pts_bbox_NuScenes/mAOE: 0.2614, pts_bbox_NuScenes/mAVE: 0.3210, pts_bbox_NuScenes/mAAE: 0.1814, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4159, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6226, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7279, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7703, pts_bbox_NuScenes/truck_trans_err: 0.3511, pts_bbox_NuScenes/truck_scale_err: 0.2145, pts_bbox_NuScenes/truck_orient_err: 0.0571, pts_bbox_NuScenes/truck_vel_err: 0.2975, pts_bbox_NuScenes/truck_attr_err: 0.2077, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0633, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2019, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4053, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4757, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6566, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4471, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8210, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1143, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2949, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4917, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7269, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8922, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9182, pts_bbox_NuScenes/bus_trans_err: 0.3534, pts_bbox_NuScenes/bus_scale_err: 0.2051, pts_bbox_NuScenes/bus_orient_err: 0.0526, pts_bbox_NuScenes/bus_vel_err: 0.5808, pts_bbox_NuScenes/bus_attr_err: 0.2857, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1820, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4265, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5906, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6809, pts_bbox_NuScenes/trailer_trans_err: 0.4759, pts_bbox_NuScenes/trailer_scale_err: 0.2708, pts_bbox_NuScenes/trailer_orient_err: 0.3993, pts_bbox_NuScenes/trailer_vel_err: 0.2347, pts_bbox_NuScenes/trailer_attr_err: 0.1656, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6028, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7110, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7586, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7747, pts_bbox_NuScenes/barrier_trans_err: 0.2213, pts_bbox_NuScenes/barrier_scale_err: 0.2901, pts_bbox_NuScenes/barrier_orient_err: 0.0536, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6338, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7630, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7960, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8022, pts_bbox_NuScenes/motorcycle_trans_err: 0.2341, pts_bbox_NuScenes/motorcycle_scale_err: 0.2510, pts_bbox_NuScenes/motorcycle_orient_err: 0.2606, pts_bbox_NuScenes/motorcycle_vel_err: 0.5008, pts_bbox_NuScenes/motorcycle_attr_err: 0.2230, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5415, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5970, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6081, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6154, pts_bbox_NuScenes/bicycle_trans_err: 0.1864, pts_bbox_NuScenes/bicycle_scale_err: 0.2676, pts_bbox_NuScenes/bicycle_orient_err: 0.3384, pts_bbox_NuScenes/bicycle_vel_err: 0.2484, pts_bbox_NuScenes/bicycle_attr_err: 0.0036, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8051, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8536, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8770, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8902, pts_bbox_NuScenes/pedestrian_trans_err: 0.1627, pts_bbox_NuScenes/pedestrian_scale_err: 0.2948, pts_bbox_NuScenes/pedestrian_orient_err: 0.3222, pts_bbox_NuScenes/pedestrian_vel_err: 0.2275, pts_bbox_NuScenes/pedestrian_attr_err: 0.1093, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7040, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7513, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7805, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8033, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1665, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3317, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7008, pts_bbox_NuScenes/mAP: 0.6688
2025-06-10 11:50:03,161 - mmdet - INFO - Epoch [3][50/7033]	lr: 7.503e-05, eta: 8:11:01, time: 1.112, data_time: 0.114, memory: 17617, loss_cls: 0.0812, loss_bbox: 0.2221, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2554, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2225, loss: 2.1457, grad_norm: 34.5980
2025-06-10 11:50:55,062 - mmdet - INFO - Epoch [3][100/7033]	lr: 7.503e-05, eta: 8:10:08, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0771, loss_bbox: 0.2277, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2546, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2428, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2271, loss: 2.1382, grad_norm: 32.8831
2025-06-10 11:51:46,546 - mmdet - INFO - Epoch [3][150/7033]	lr: 7.503e-05, eta: 8:09:13, time: 1.030, data_time: 0.029, memory: 17617, loss_cls: 0.0683, loss_bbox: 0.2107, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2458, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2276, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2104, loss: 2.0248, grad_norm: 65.2938
2025-06-10 11:52:38,074 - mmdet - INFO - Epoch [3][200/7033]	lr: 7.503e-05, eta: 8:08:19, time: 1.031, data_time: 0.028, memory: 17617, loss_cls: 0.0796, loss_bbox: 0.2232, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3421, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2575, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2405, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2380, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2242, loss: 2.1441, grad_norm: 27.5915
2025-06-10 11:53:29,444 - mmdet - INFO - Epoch [3][250/7033]	lr: 7.503e-05, eta: 8:07:24, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.0748, loss_bbox: 0.2138, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2408, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2142, loss: 2.0397, grad_norm: 89.8802
2025-06-10 11:54:21,596 - mmdet - INFO - Epoch [3][300/7033]	lr: 7.503e-05, eta: 8:06:31, time: 1.043, data_time: 0.049, memory: 17617, loss_cls: 0.0739, loss_bbox: 0.2190, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3331, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2188, loss: 2.0898, grad_norm: 21.3016
2025-06-10 11:55:12,569 - mmdet - INFO - Epoch [3][350/7033]	lr: 7.503e-05, eta: 8:05:36, time: 1.019, data_time: 0.027, memory: 17617, loss_cls: 0.0748, loss_bbox: 0.2251, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2239, loss: 2.1069, grad_norm: 39.9451
2025-06-10 11:56:03,468 - mmdet - INFO - Epoch [3][400/7033]	lr: 7.503e-05, eta: 8:04:41, time: 1.018, data_time: 0.027, memory: 17617, loss_cls: 0.0778, loss_bbox: 0.2171, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3249, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2312, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2171, loss: 2.0755, grad_norm: 25.6309
2025-06-10 11:56:54,172 - mmdet - INFO - Epoch [3][450/7033]	lr: 7.503e-05, eta: 8:03:45, time: 1.014, data_time: 0.028, memory: 17617, loss_cls: 0.0769, loss_bbox: 0.2177, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3270, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2322, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2176, loss: 2.0660, grad_norm: 47.2493
2025-06-10 11:57:45,168 - mmdet - INFO - Epoch [3][500/7033]	lr: 7.503e-05, eta: 8:02:50, time: 1.020, data_time: 0.030, memory: 17617, loss_cls: 0.0797, loss_bbox: 0.2163, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3227, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2342, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2169, loss: 2.0911, grad_norm: 42.4347
2025-06-10 11:58:36,180 - mmdet - INFO - Epoch [3][550/7033]	lr: 7.503e-05, eta: 8:01:55, time: 1.020, data_time: 0.029, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2220, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3293, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2389, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2220, loss: 2.0593, grad_norm: 33.8009
2025-06-10 11:59:27,492 - mmdet - INFO - Epoch [3][600/7033]	lr: 7.503e-05, eta: 8:01:00, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0785, loss_bbox: 0.2279, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3382, d1.loss_cls: 0.1078, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2410, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2280, loss: 2.1510, grad_norm: 31.4459
2025-06-10 12:00:18,597 - mmdet - INFO - Epoch [3][650/7033]	lr: 7.503e-05, eta: 8:00:05, time: 1.022, data_time: 0.031, memory: 17617, loss_cls: 0.0705, loss_bbox: 0.2143, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3257, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2138, loss: 2.0295, grad_norm: 38.1642
2025-06-10 12:01:09,437 - mmdet - INFO - Epoch [3][700/7033]	lr: 7.503e-05, eta: 7:59:10, time: 1.017, data_time: 0.026, memory: 17617, loss_cls: 0.0830, loss_bbox: 0.2206, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3346, d1.loss_cls: 0.1104, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2204, loss: 2.1300, grad_norm: 78.7647
2025-06-10 12:02:00,394 - mmdet - INFO - Epoch [3][750/7033]	lr: 7.503e-05, eta: 7:58:15, time: 1.019, data_time: 0.026, memory: 17617, loss_cls: 0.0780, loss_bbox: 0.2237, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3427, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2567, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2410, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2249, loss: 2.1285, grad_norm: 54.9876
2025-06-10 12:02:51,591 - mmdet - INFO - Epoch [3][800/7033]	lr: 7.503e-05, eta: 7:57:20, time: 1.024, data_time: 0.030, memory: 17617, loss_cls: 0.0818, loss_bbox: 0.2249, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2521, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2245, loss: 2.1513, grad_norm: 33.0392
2025-06-10 12:03:42,975 - mmdet - INFO - Epoch [3][850/7033]	lr: 7.503e-05, eta: 7:56:26, time: 1.028, data_time: 0.030, memory: 17617, loss_cls: 0.0730, loss_bbox: 0.2176, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3282, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2493, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2332, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2182, loss: 2.0562, grad_norm: 226.8833
2025-06-10 12:04:34,405 - mmdet - INFO - Epoch [3][900/7033]	lr: 7.503e-05, eta: 7:55:32, time: 1.029, data_time: 0.031, memory: 17617, loss_cls: 0.0801, loss_bbox: 0.2236, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2573, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2381, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2243, loss: 2.1491, grad_norm: 59.9553
2025-06-10 12:05:25,465 - mmdet - INFO - Epoch [3][950/7033]	lr: 7.503e-05, eta: 7:54:37, time: 1.021, data_time: 0.029, memory: 17617, loss_cls: 0.0703, loss_bbox: 0.2178, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3330, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2180, loss: 2.0483, grad_norm: 56.3734
2025-06-10 12:06:16,421 - mmdet - INFO - Epoch [3][1000/7033]	lr: 7.503e-05, eta: 7:53:42, time: 1.019, data_time: 0.024, memory: 17617, loss_cls: 0.0753, loss_bbox: 0.2155, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2472, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2166, loss: 2.0660, grad_norm: 47.1784
2025-06-10 12:07:10,534 - mmdet - INFO - Epoch [3][1050/7033]	lr: 7.503e-05, eta: 7:52:53, time: 1.082, data_time: 0.026, memory: 17617, loss_cls: 0.0724, loss_bbox: 0.2161, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2158, loss: 2.0224, grad_norm: 29.0510
2025-06-10 12:08:01,542 - mmdet - INFO - Epoch [3][1100/7033]	lr: 7.503e-05, eta: 7:51:58, time: 1.020, data_time: 0.029, memory: 17617, loss_cls: 0.0778, loss_bbox: 0.2168, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3302, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2493, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2158, loss: 2.0686, grad_norm: 25.8543
2025-06-10 12:08:52,639 - mmdet - INFO - Epoch [3][1150/7033]	lr: 7.503e-05, eta: 7:51:03, time: 1.022, data_time: 0.028, memory: 17617, loss_cls: 0.0789, loss_bbox: 0.2338, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3442, d1.loss_cls: 0.1102, d1.loss_bbox: 0.2631, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2487, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2331, loss: 2.1801, grad_norm: 69.5550
2025-06-10 12:09:43,951 - mmdet - INFO - Epoch [3][1200/7033]	lr: 7.503e-05, eta: 7:50:09, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0757, loss_bbox: 0.2174, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2345, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2334, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2191, loss: 2.0798, grad_norm: 37.4219
2025-06-10 12:10:35,236 - mmdet - INFO - Epoch [3][1250/7033]	lr: 7.503e-05, eta: 7:49:15, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0785, loss_bbox: 0.2268, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2435, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2429, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2258, loss: 2.1563, grad_norm: 66.9150
2025-06-10 12:11:26,553 - mmdet - INFO - Epoch [3][1300/7033]	lr: 7.503e-05, eta: 7:48:21, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0807, loss_bbox: 0.2218, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3347, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2536, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2378, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2222, loss: 2.1308, grad_norm: 26.0488
2025-06-10 12:12:17,915 - mmdet - INFO - Epoch [3][1350/7033]	lr: 7.503e-05, eta: 7:47:27, time: 1.027, data_time: 0.033, memory: 17617, loss_cls: 0.0809, loss_bbox: 0.2167, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3279, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2178, loss: 2.0863, grad_norm: 25.0970
2025-06-10 12:13:09,518 - mmdet - INFO - Epoch [3][1400/7033]	lr: 7.503e-05, eta: 7:46:33, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0791, loss_bbox: 0.2179, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2329, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2174, loss: 2.0922, grad_norm: 163.3569
2025-06-10 12:14:01,345 - mmdet - INFO - Epoch [3][1450/7033]	lr: 7.503e-05, eta: 7:45:40, time: 1.037, data_time: 0.034, memory: 17617, loss_cls: 0.0840, loss_bbox: 0.2236, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3434, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2552, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2401, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2391, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2244, loss: 2.1561, grad_norm: 36.3566
2025-06-10 12:14:53,027 - mmdet - INFO - Epoch [3][1500/7033]	lr: 7.503e-05, eta: 7:44:46, time: 1.034, data_time: 0.033, memory: 17617, loss_cls: 0.0784, loss_bbox: 0.2186, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2179, loss: 2.0940, grad_norm: 31.3765
2025-06-10 12:15:44,848 - mmdet - INFO - Epoch [3][1550/7033]	lr: 7.503e-05, eta: 7:43:53, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0763, loss_bbox: 0.2217, d0.loss_cls: 0.1643, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2525, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0779, d4.loss_bbox: 0.2222, loss: 2.1044, grad_norm: 31.3944
2025-06-10 12:16:36,573 - mmdet - INFO - Epoch [3][1600/7033]	lr: 7.503e-05, eta: 7:42:59, time: 1.034, data_time: 0.036, memory: 17617, loss_cls: 0.0745, loss_bbox: 0.2252, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3337, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2539, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2381, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2238, loss: 2.1174, grad_norm: 24.4589
2025-06-10 12:17:39,373 - mmdet - INFO - Epoch [3][1650/7033]	lr: 7.503e-05, eta: 7:42:25, time: 1.256, data_time: 0.035, memory: 17617, loss_cls: 0.0736, loss_bbox: 0.2225, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3366, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2537, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2226, loss: 2.0948, grad_norm: 41.7983
2025-06-10 12:18:31,161 - mmdet - INFO - Epoch [3][1700/7033]	lr: 7.503e-05, eta: 7:41:31, time: 1.036, data_time: 0.036, memory: 17617, loss_cls: 0.0701, loss_bbox: 0.2076, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3202, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2086, loss: 1.9845, grad_norm: 22.3890
2025-06-10 12:19:22,658 - mmdet - INFO - Epoch [3][1750/7033]	lr: 7.503e-05, eta: 7:40:37, time: 1.030, data_time: 0.033, memory: 17617, loss_cls: 0.0722, loss_bbox: 0.2128, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3284, d1.loss_cls: 0.1012, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2274, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2130, loss: 2.0244, grad_norm: 30.5717
2025-06-10 12:20:13,990 - mmdet - INFO - Epoch [3][1800/7033]	lr: 7.503e-05, eta: 7:39:43, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.0749, loss_bbox: 0.2208, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2519, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2206, loss: 2.0926, grad_norm: 19.2015
2025-06-10 12:21:05,636 - mmdet - INFO - Epoch [3][1850/7033]	lr: 7.503e-05, eta: 7:38:50, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.0775, loss_bbox: 0.2161, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3339, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2317, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2156, loss: 2.0681, grad_norm: 29.4787
2025-06-10 12:21:56,932 - mmdet - INFO - Epoch [3][1900/7033]	lr: 7.503e-05, eta: 7:37:56, time: 1.026, data_time: 0.033, memory: 17617, loss_cls: 0.0755, loss_bbox: 0.2216, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3376, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2529, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2346, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2210, loss: 2.1135, grad_norm: 23.5678
2025-06-10 12:22:48,577 - mmdet - INFO - Epoch [3][1950/7033]	lr: 7.503e-05, eta: 7:37:02, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0727, loss_bbox: 0.2228, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3306, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2210, loss: 2.0731, grad_norm: 25.0958
2025-06-10 12:23:39,656 - mmdet - INFO - Epoch [3][2000/7033]	lr: 7.503e-05, eta: 7:36:08, time: 1.022, data_time: 0.033, memory: 17617, loss_cls: 0.0755, loss_bbox: 0.2135, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2312, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2134, loss: 2.0483, grad_norm: 35.5016
2025-06-10 12:24:31,146 - mmdet - INFO - Epoch [3][2050/7033]	lr: 7.503e-05, eta: 7:35:14, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0742, loss_bbox: 0.2182, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3304, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2181, loss: 2.0658, grad_norm: 38.7891
2025-06-10 12:25:22,314 - mmdet - INFO - Epoch [3][2100/7033]	lr: 7.503e-05, eta: 7:34:20, time: 1.023, data_time: 0.031, memory: 17617, loss_cls: 0.0812, loss_bbox: 0.2302, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3574, d1.loss_cls: 0.1106, d1.loss_bbox: 0.2671, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2470, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2459, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2300, loss: 2.2121, grad_norm: 21.8699
2025-06-10 12:27:09,504 - mmdet - INFO - Epoch [3][2150/7033]	lr: 7.503e-05, eta: 7:34:55, time: 2.144, data_time: 0.646, memory: 17617, loss_cls: 0.0729, loss_bbox: 0.2157, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3249, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2154, loss: 2.0359, grad_norm: 23.5719
2025-06-10 12:28:53,893 - mmdet - INFO - Epoch [3][2200/7033]	lr: 7.503e-05, eta: 7:35:25, time: 2.088, data_time: 0.123, memory: 17617, loss_cls: 0.0798, loss_bbox: 0.2104, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2472, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2115, loss: 2.0621, grad_norm: 24.9526
2025-06-10 12:30:34,215 - mmdet - INFO - Epoch [3][2250/7033]	lr: 7.503e-05, eta: 7:35:48, time: 2.006, data_time: 0.026, memory: 17617, loss_cls: 0.0729, loss_bbox: 0.2157, d0.loss_cls: 0.1644, d0.loss_bbox: 0.3294, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2158, loss: 2.0490, grad_norm: 47.4018
2025-06-10 12:32:10,316 - mmdet - INFO - Epoch [3][2300/7033]	lr: 7.503e-05, eta: 7:36:04, time: 1.922, data_time: 0.177, memory: 17617, loss_cls: 0.0821, loss_bbox: 0.2212, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2197, loss: 2.1214, grad_norm: 30.2930
2025-06-10 12:33:16,937 - mmdet - INFO - Epoch [3][2350/7033]	lr: 7.503e-05, eta: 7:35:32, time: 1.332, data_time: 0.030, memory: 17617, loss_cls: 0.0814, loss_bbox: 0.2281, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3407, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2599, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2264, loss: 2.1676, grad_norm: 29.7297
2025-06-10 12:34:12,030 - mmdet - INFO - Epoch [3][2400/7033]	lr: 7.503e-05, eta: 7:34:42, time: 1.102, data_time: 0.103, memory: 17617, loss_cls: 0.0776, loss_bbox: 0.2215, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2581, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2404, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2398, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2220, loss: 2.1124, grad_norm: 27.3443
2025-06-10 12:35:05,683 - mmdet - INFO - Epoch [3][2450/7033]	lr: 7.503e-05, eta: 7:33:50, time: 1.073, data_time: 0.077, memory: 17617, loss_cls: 0.0742, loss_bbox: 0.2117, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3226, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2117, loss: 2.0306, grad_norm: 46.1928
2025-06-10 12:35:56,883 - mmdet - INFO - Epoch [3][2500/7033]	lr: 7.503e-05, eta: 7:32:54, time: 1.024, data_time: 0.032, memory: 17617, loss_cls: 0.0748, loss_bbox: 0.2153, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2165, loss: 2.0644, grad_norm: 38.9629
2025-06-10 12:36:48,982 - mmdet - INFO - Epoch [3][2550/7033]	lr: 7.503e-05, eta: 7:32:00, time: 1.042, data_time: 0.035, memory: 17617, loss_cls: 0.0753, loss_bbox: 0.2215, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2525, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2382, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2377, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2201, loss: 2.0794, grad_norm: 43.0108
2025-06-10 12:37:40,644 - mmdet - INFO - Epoch [3][2600/7033]	lr: 7.503e-05, eta: 7:31:05, time: 1.033, data_time: 0.034, memory: 17617, loss_cls: 0.0802, loss_bbox: 0.2233, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3269, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2218, loss: 2.1036, grad_norm: 30.6116
2025-06-10 12:38:32,238 - mmdet - INFO - Epoch [3][2650/7033]	lr: 7.503e-05, eta: 7:30:10, time: 1.032, data_time: 0.034, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2171, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3223, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2153, loss: 2.0313, grad_norm: 26.9626
2025-06-10 12:39:23,864 - mmdet - INFO - Epoch [3][2700/7033]	lr: 7.503e-05, eta: 7:29:15, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0799, loss_bbox: 0.2267, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2578, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2413, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2401, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2270, loss: 2.1414, grad_norm: 180.8039
2025-06-10 12:40:15,101 - mmdet - INFO - Epoch [3][2750/7033]	lr: 7.503e-05, eta: 7:28:19, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0765, loss_bbox: 0.2166, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2463, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2171, loss: 2.0637, grad_norm: 70.4463
2025-06-10 12:41:06,525 - mmdet - INFO - Epoch [3][2800/7033]	lr: 7.503e-05, eta: 7:27:24, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0797, loss_bbox: 0.2231, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2527, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2228, loss: 2.1204, grad_norm: 41.8631
2025-06-10 12:41:57,611 - mmdet - INFO - Epoch [3][2850/7033]	lr: 7.503e-05, eta: 7:26:28, time: 1.022, data_time: 0.031, memory: 17617, loss_cls: 0.0784, loss_bbox: 0.2213, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1074, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2201, loss: 2.1261, grad_norm: 37.9693
2025-06-10 12:42:48,776 - mmdet - INFO - Epoch [3][2900/7033]	lr: 7.503e-05, eta: 7:25:32, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.0757, loss_bbox: 0.2132, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3167, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0908, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2124, loss: 2.0363, grad_norm: 21.7591
2025-06-10 12:43:40,036 - mmdet - INFO - Epoch [3][2950/7033]	lr: 7.503e-05, eta: 7:24:37, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0708, loss_bbox: 0.2121, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2111, loss: 2.0054, grad_norm: 172.1938
2025-06-10 12:44:33,919 - mmdet - INFO - Epoch [3][3000/7033]	lr: 7.503e-05, eta: 7:23:45, time: 1.078, data_time: 0.029, memory: 17617, loss_cls: 0.0754, loss_bbox: 0.2145, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2137, loss: 2.0797, grad_norm: 32.5138
2025-06-10 12:45:25,241 - mmdet - INFO - Epoch [3][3050/7033]	lr: 7.503e-05, eta: 7:22:50, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0776, loss_bbox: 0.2185, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3377, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2191, loss: 2.1030, grad_norm: 29.1205
2025-06-10 12:46:16,041 - mmdet - INFO - Epoch [3][3100/7033]	lr: 7.503e-05, eta: 7:21:54, time: 1.016, data_time: 0.028, memory: 17617, loss_cls: 0.0761, loss_bbox: 0.2158, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3311, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2488, d2.loss_cls: 0.0904, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2148, loss: 2.0851, grad_norm: 36.6850
2025-06-10 12:47:06,963 - mmdet - INFO - Epoch [3][3150/7033]	lr: 7.503e-05, eta: 7:20:58, time: 1.018, data_time: 0.028, memory: 17617, loss_cls: 0.0744, loss_bbox: 0.2108, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3273, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2107, loss: 2.0448, grad_norm: 35.7479
2025-06-10 12:47:57,846 - mmdet - INFO - Epoch [3][3200/7033]	lr: 7.503e-05, eta: 7:20:02, time: 1.018, data_time: 0.029, memory: 17617, loss_cls: 0.0815, loss_bbox: 0.2169, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3403, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2397, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0832, d4.loss_bbox: 0.2173, loss: 2.1276, grad_norm: 22.6503
2025-06-10 12:48:48,956 - mmdet - INFO - Epoch [3][3250/7033]	lr: 7.503e-05, eta: 7:19:06, time: 1.022, data_time: 0.028, memory: 17617, loss_cls: 0.0730, loss_bbox: 0.2132, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3180, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2125, loss: 2.0185, grad_norm: 25.6197
2025-06-10 12:49:39,895 - mmdet - INFO - Epoch [3][3300/7033]	lr: 7.503e-05, eta: 7:18:10, time: 1.019, data_time: 0.030, memory: 17617, loss_cls: 0.0770, loss_bbox: 0.2172, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2496, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0816, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2180, loss: 2.0949, grad_norm: 27.2665
2025-06-10 12:50:31,050 - mmdet - INFO - Epoch [3][3350/7033]	lr: 7.503e-05, eta: 7:17:15, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.0703, loss_bbox: 0.2161, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3309, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2490, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2162, loss: 2.0427, grad_norm: 22.6985
2025-06-10 12:51:21,963 - mmdet - INFO - Epoch [3][3400/7033]	lr: 7.503e-05, eta: 7:16:19, time: 1.018, data_time: 0.026, memory: 17617, loss_cls: 0.0738, loss_bbox: 0.2131, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1014, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2151, loss: 2.0530, grad_norm: 55.4162
2025-06-10 12:52:13,040 - mmdet - INFO - Epoch [3][3450/7033]	lr: 7.503e-05, eta: 7:15:23, time: 1.022, data_time: 0.028, memory: 17617, loss_cls: 0.0763, loss_bbox: 0.2194, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2347, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2179, loss: 2.1001, grad_norm: 25.2259
2025-06-10 12:53:03,882 - mmdet - INFO - Epoch [3][3500/7033]	lr: 7.503e-05, eta: 7:14:28, time: 1.017, data_time: 0.029, memory: 17617, loss_cls: 0.0729, loss_bbox: 0.2149, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3268, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2140, loss: 2.0265, grad_norm: 35.5823
2025-06-10 12:53:54,729 - mmdet - INFO - Epoch [3][3550/7033]	lr: 7.503e-05, eta: 7:13:32, time: 1.017, data_time: 0.031, memory: 17617, loss_cls: 0.0691, loss_bbox: 0.2132, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3231, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2263, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2115, loss: 2.0056, grad_norm: 68.4608
2025-06-10 12:54:48,772 - mmdet - INFO - Epoch [3][3600/7033]	lr: 7.503e-05, eta: 7:12:41, time: 1.081, data_time: 0.031, memory: 17617, loss_cls: 0.0730, loss_bbox: 0.2170, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2167, loss: 2.0714, grad_norm: 23.5579
2025-06-10 12:55:39,720 - mmdet - INFO - Epoch [3][3650/7033]	lr: 7.503e-05, eta: 7:11:45, time: 1.019, data_time: 0.030, memory: 17617, loss_cls: 0.0765, loss_bbox: 0.2183, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2531, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2206, loss: 2.0838, grad_norm: 37.9894
2025-06-10 12:56:31,226 - mmdet - INFO - Epoch [3][3700/7033]	lr: 7.503e-05, eta: 7:10:50, time: 1.030, data_time: 0.031, memory: 17617, loss_cls: 0.0930, loss_bbox: 0.2197, d0.loss_cls: 0.1852, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2517, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2367, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2197, loss: 2.1926, grad_norm: 47.8164
2025-06-10 12:57:22,546 - mmdet - INFO - Epoch [3][3750/7033]	lr: 7.503e-05, eta: 7:09:55, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0807, loss_bbox: 0.2303, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1102, d1.loss_bbox: 0.2556, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2443, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2277, loss: 2.1556, grad_norm: 29.4689
2025-06-10 12:58:14,096 - mmdet - INFO - Epoch [3][3800/7033]	lr: 7.503e-05, eta: 7:09:00, time: 1.031, data_time: 0.032, memory: 17617, loss_cls: 0.0744, loss_bbox: 0.2214, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2199, loss: 2.0862, grad_norm: 34.6752
2025-06-10 12:59:05,168 - mmdet - INFO - Epoch [3][3850/7033]	lr: 7.503e-05, eta: 7:08:05, time: 1.021, data_time: 0.030, memory: 17617, loss_cls: 0.0840, loss_bbox: 0.2230, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0957, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0864, d4.loss_bbox: 0.2224, loss: 2.1256, grad_norm: 21.6164
2025-06-10 12:59:56,281 - mmdet - INFO - Epoch [3][3900/7033]	lr: 7.503e-05, eta: 7:07:10, time: 1.022, data_time: 0.031, memory: 17617, loss_cls: 0.0779, loss_bbox: 0.2214, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2357, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2218, loss: 2.1067, grad_norm: 40.7292
2025-06-10 13:00:47,539 - mmdet - INFO - Epoch [3][3950/7033]	lr: 7.503e-05, eta: 7:06:15, time: 1.025, data_time: 0.032, memory: 17617, loss_cls: 0.0727, loss_bbox: 0.2221, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2544, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2220, loss: 2.0873, grad_norm: 59.4534
2025-06-10 13:01:38,714 - mmdet - INFO - Epoch [3][4000/7033]	lr: 7.503e-05, eta: 7:05:19, time: 1.023, data_time: 0.031, memory: 17617, loss_cls: 0.0738, loss_bbox: 0.2142, d0.loss_cls: 0.1569, d0.loss_bbox: 0.3305, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2157, loss: 2.0417, grad_norm: 20.4684
2025-06-10 13:02:29,545 - mmdet - INFO - Epoch [3][4050/7033]	lr: 7.503e-05, eta: 7:04:24, time: 1.017, data_time: 0.028, memory: 17617, loss_cls: 0.0692, loss_bbox: 0.2081, d0.loss_cls: 0.1619, d0.loss_bbox: 0.3142, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2247, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2086, loss: 1.9720, grad_norm: 23.1005
2025-06-10 13:03:20,607 - mmdet - INFO - Epoch [3][4100/7033]	lr: 7.503e-05, eta: 7:03:28, time: 1.021, data_time: 0.027, memory: 17617, loss_cls: 0.0735, loss_bbox: 0.2196, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3311, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2179, loss: 2.0725, grad_norm: 24.6306
2025-06-10 13:04:11,593 - mmdet - INFO - Epoch [3][4150/7033]	lr: 7.503e-05, eta: 7:02:33, time: 1.020, data_time: 0.027, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2182, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3262, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2320, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2158, loss: 2.0319, grad_norm: 34.0682
2025-06-10 13:05:02,838 - mmdet - INFO - Epoch [3][4200/7033]	lr: 7.503e-05, eta: 7:01:38, time: 1.025, data_time: 0.035, memory: 17617, loss_cls: 0.0721, loss_bbox: 0.2131, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3288, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2140, loss: 2.0284, grad_norm: 37.1059
2025-06-10 13:05:54,105 - mmdet - INFO - Epoch [3][4250/7033]	lr: 7.503e-05, eta: 7:00:43, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0853, loss_bbox: 0.2238, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3397, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2592, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2395, d4.loss_cls: 0.0873, d4.loss_bbox: 0.2250, loss: 2.1828, grad_norm: 29.6940
2025-06-10 13:06:45,511 - mmdet - INFO - Epoch [3][4300/7033]	lr: 7.503e-05, eta: 6:59:49, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0764, loss_bbox: 0.2323, d0.loss_cls: 0.1584, d0.loss_bbox: 0.3528, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2664, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2475, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2460, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2325, loss: 2.1637, grad_norm: 1461.7576
2025-06-10 13:07:39,845 - mmdet - INFO - Epoch [3][4350/7033]	lr: 7.503e-05, eta: 6:58:58, time: 1.087, data_time: 0.035, memory: 17617, loss_cls: 0.0799, loss_bbox: 0.2228, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1086, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2363, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2229, loss: 2.1299, grad_norm: 30.8334
2025-06-10 13:08:31,029 - mmdet - INFO - Epoch [3][4400/7033]	lr: 7.503e-05, eta: 6:58:03, time: 1.024, data_time: 0.033, memory: 17617, loss_cls: 0.0729, loss_bbox: 0.2180, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3251, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2182, loss: 2.0496, grad_norm: 28.0558
2025-06-10 13:09:22,298 - mmdet - INFO - Epoch [3][4450/7033]	lr: 7.503e-05, eta: 6:57:08, time: 1.025, data_time: 0.033, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2064, d0.loss_cls: 0.1546, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2071, loss: 1.9578, grad_norm: 31.3053
2025-06-10 13:10:13,231 - mmdet - INFO - Epoch [3][4500/7033]	lr: 7.503e-05, eta: 6:56:12, time: 1.019, data_time: 0.030, memory: 17617, loss_cls: 0.0724, loss_bbox: 0.2146, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2458, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2141, loss: 2.0366, grad_norm: 25.5239
2025-06-10 13:11:04,417 - mmdet - INFO - Epoch [3][4550/7033]	lr: 7.503e-05, eta: 6:55:18, time: 1.024, data_time: 0.034, memory: 17617, loss_cls: 0.0733, loss_bbox: 0.2210, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3346, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2555, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2207, loss: 2.0823, grad_norm: 102.7628
2025-06-10 13:11:55,852 - mmdet - INFO - Epoch [3][4600/7033]	lr: 7.503e-05, eta: 6:54:23, time: 1.029, data_time: 0.035, memory: 17617, loss_cls: 0.0683, loss_bbox: 0.2170, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3275, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2312, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2165, loss: 2.0175, grad_norm: 27.1511
2025-06-10 13:12:47,012 - mmdet - INFO - Epoch [3][4650/7033]	lr: 7.503e-05, eta: 6:53:28, time: 1.023, data_time: 0.033, memory: 17617, loss_cls: 0.0787, loss_bbox: 0.2194, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3333, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2197, loss: 2.0969, grad_norm: 52.0424
2025-06-10 13:13:38,492 - mmdet - INFO - Epoch [3][4700/7033]	lr: 7.503e-05, eta: 6:52:34, time: 1.030, data_time: 0.035, memory: 17617, loss_cls: 0.0796, loss_bbox: 0.2206, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3262, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2541, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2212, loss: 2.0889, grad_norm: 25.4051
2025-06-10 13:14:29,856 - mmdet - INFO - Epoch [3][4750/7033]	lr: 7.503e-05, eta: 6:51:39, time: 1.027, data_time: 0.034, memory: 17617, loss_cls: 0.0734, loss_bbox: 0.2101, d0.loss_cls: 0.1707, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2097, loss: 2.0157, grad_norm: 106.2392
2025-06-10 13:15:21,019 - mmdet - INFO - Epoch [3][4800/7033]	lr: 7.503e-05, eta: 6:50:44, time: 1.023, data_time: 0.032, memory: 17617, loss_cls: 0.0790, loss_bbox: 0.2150, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2153, loss: 2.0661, grad_norm: 38.3777
2025-06-10 13:16:12,081 - mmdet - INFO - Epoch [3][4850/7033]	lr: 7.503e-05, eta: 6:49:49, time: 1.021, data_time: 0.031, memory: 17617, loss_cls: 0.0683, loss_bbox: 0.2056, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2068, loss: 1.9475, grad_norm: 19.5587
2025-06-10 13:17:03,364 - mmdet - INFO - Epoch [3][4900/7033]	lr: 7.503e-05, eta: 6:48:54, time: 1.026, data_time: 0.034, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.2112, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3232, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2110, loss: 1.9900, grad_norm: 27.3736
2025-06-10 13:17:54,541 - mmdet - INFO - Epoch [3][4950/7033]	lr: 7.503e-05, eta: 6:48:00, time: 1.024, data_time: 0.035, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2056, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3165, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2191, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2061, loss: 1.9567, grad_norm: 48.7203
2025-06-10 13:18:45,710 - mmdet - INFO - Epoch [3][5000/7033]	lr: 7.503e-05, eta: 6:47:05, time: 1.023, data_time: 0.034, memory: 17617, loss_cls: 0.0746, loss_bbox: 0.2147, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2160, loss: 2.0434, grad_norm: 38.8117
2025-06-10 13:19:37,078 - mmdet - INFO - Epoch [3][5050/7033]	lr: 7.503e-05, eta: 6:46:10, time: 1.027, data_time: 0.034, memory: 17617, loss_cls: 0.0807, loss_bbox: 0.2197, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3231, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2338, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2188, loss: 2.0957, grad_norm: 39.0084
2025-06-10 13:20:28,771 - mmdet - INFO - Epoch [3][5100/7033]	lr: 7.503e-05, eta: 6:45:16, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0681, loss_bbox: 0.2056, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3214, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2056, loss: 1.9632, grad_norm: 29.0825
2025-06-10 13:21:20,321 - mmdet - INFO - Epoch [3][5150/7033]	lr: 7.503e-05, eta: 6:44:22, time: 1.031, data_time: 0.032, memory: 17617, loss_cls: 0.0683, loss_bbox: 0.2101, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2093, loss: 1.9767, grad_norm: 24.2273
2025-06-10 13:22:11,340 - mmdet - INFO - Epoch [3][5200/7033]	lr: 7.503e-05, eta: 6:43:27, time: 1.020, data_time: 0.029, memory: 17617, loss_cls: 0.0733, loss_bbox: 0.2127, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3274, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2459, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2131, loss: 2.0431, grad_norm: 44.9674
2025-06-10 13:23:02,582 - mmdet - INFO - Epoch [3][5250/7033]	lr: 7.503e-05, eta: 6:42:32, time: 1.025, data_time: 0.034, memory: 17617, loss_cls: 0.0745, loss_bbox: 0.2128, d0.loss_cls: 0.1755, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1107, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2269, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2122, loss: 2.0511, grad_norm: 38.2670
2025-06-10 13:23:53,432 - mmdet - INFO - Epoch [3][5300/7033]	lr: 7.503e-05, eta: 6:41:37, time: 1.017, data_time: 0.030, memory: 17617, loss_cls: 0.0719, loss_bbox: 0.2210, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3326, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2211, loss: 2.0815, grad_norm: 55.1499
2025-06-10 13:24:44,744 - mmdet - INFO - Epoch [3][5350/7033]	lr: 7.503e-05, eta: 6:40:43, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0705, loss_bbox: 0.2047, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3138, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2385, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2064, loss: 1.9683, grad_norm: 1395.3751
2025-06-10 13:25:35,947 - mmdet - INFO - Epoch [3][5400/7033]	lr: 7.503e-05, eta: 6:39:48, time: 1.024, data_time: 0.031, memory: 17617, loss_cls: 0.0784, loss_bbox: 0.2176, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3237, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2154, loss: 2.0687, grad_norm: 50.4917
2025-06-10 13:26:27,015 - mmdet - INFO - Epoch [3][5450/7033]	lr: 7.503e-05, eta: 6:38:54, time: 1.021, data_time: 0.033, memory: 17617, loss_cls: 0.0841, loss_bbox: 0.2178, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2321, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2164, loss: 2.1054, grad_norm: 37.7271
2025-06-10 13:27:17,719 - mmdet - INFO - Epoch [3][5500/7033]	lr: 7.503e-05, eta: 6:37:58, time: 1.014, data_time: 0.027, memory: 17617, loss_cls: 0.0707, loss_bbox: 0.2180, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3274, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2346, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2342, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2186, loss: 2.0551, grad_norm: 21.1092
2025-06-10 13:28:08,163 - mmdet - INFO - Epoch [3][5550/7033]	lr: 7.503e-05, eta: 6:37:03, time: 1.009, data_time: 0.024, memory: 17617, loss_cls: 0.0669, loss_bbox: 0.2153, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3167, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0699, d4.loss_bbox: 0.2150, loss: 1.9877, grad_norm: 50.1217
2025-06-10 13:28:58,978 - mmdet - INFO - Epoch [3][5600/7033]	lr: 7.503e-05, eta: 6:36:08, time: 1.016, data_time: 0.030, memory: 17617, loss_cls: 0.0714, loss_bbox: 0.2135, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3266, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2446, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2285, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2135, loss: 2.0211, grad_norm: 40.7900
2025-06-10 13:29:49,693 - mmdet - INFO - Epoch [3][5650/7033]	lr: 7.503e-05, eta: 6:35:13, time: 1.014, data_time: 0.028, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.2106, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3254, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2117, loss: 1.9939, grad_norm: 28.8410
2025-06-10 13:30:40,629 - mmdet - INFO - Epoch [3][5700/7033]	lr: 7.503e-05, eta: 6:34:18, time: 1.019, data_time: 0.029, memory: 17617, loss_cls: 0.0731, loss_bbox: 0.2122, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3264, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2127, loss: 2.0229, grad_norm: 27.5669
2025-06-10 13:31:31,084 - mmdet - INFO - Epoch [3][5750/7033]	lr: 7.503e-05, eta: 6:33:23, time: 1.009, data_time: 0.029, memory: 17617, loss_cls: 0.0807, loss_bbox: 0.2130, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3310, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0830, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2141, loss: 2.0751, grad_norm: 41.2069
2025-06-10 13:32:21,746 - mmdet - INFO - Epoch [3][5800/7033]	lr: 7.503e-05, eta: 6:32:28, time: 1.013, data_time: 0.031, memory: 17617, loss_cls: 0.0755, loss_bbox: 0.2187, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2183, loss: 2.0702, grad_norm: 20.7200
2025-06-10 13:33:12,809 - mmdet - INFO - Epoch [3][5850/7033]	lr: 7.503e-05, eta: 6:31:33, time: 1.021, data_time: 0.031, memory: 17617, loss_cls: 0.0737, loss_bbox: 0.2244, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3435, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2615, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2404, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2239, loss: 2.1210, grad_norm: 27.5971
2025-06-10 13:34:04,531 - mmdet - INFO - Epoch [3][5900/7033]	lr: 7.503e-05, eta: 6:30:39, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0770, loss_bbox: 0.2226, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3373, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2375, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2237, loss: 2.1198, grad_norm: 26.5716
2025-06-10 13:34:56,255 - mmdet - INFO - Epoch [3][5950/7033]	lr: 7.503e-05, eta: 6:29:46, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0714, loss_bbox: 0.2170, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2481, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2289, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2139, loss: 2.0575, grad_norm: 261.4437
2025-06-10 13:35:48,369 - mmdet - INFO - Epoch [3][6000/7033]	lr: 7.503e-05, eta: 6:28:52, time: 1.042, data_time: 0.035, memory: 17617, loss_cls: 0.0786, loss_bbox: 0.2185, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3314, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2185, loss: 2.0858, grad_norm: 52.6993
2025-06-10 13:36:40,327 - mmdet - INFO - Epoch [3][6050/7033]	lr: 7.503e-05, eta: 6:27:59, time: 1.039, data_time: 0.032, memory: 17617, loss_cls: 0.0753, loss_bbox: 0.2066, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3187, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2071, loss: 2.0033, grad_norm: 35.4298
2025-06-10 13:37:34,826 - mmdet - INFO - Epoch [3][6100/7033]	lr: 7.503e-05, eta: 6:27:08, time: 1.090, data_time: 0.031, memory: 17617, loss_cls: 0.0756, loss_bbox: 0.2105, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3139, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2091, loss: 2.0175, grad_norm: 49.6579
2025-06-10 13:38:29,005 - mmdet - INFO - Epoch [3][6150/7033]	lr: 7.503e-05, eta: 6:26:17, time: 1.084, data_time: 0.031, memory: 17617, loss_cls: 0.0813, loss_bbox: 0.2236, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3306, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2375, d4.loss_cls: 0.0835, d4.loss_bbox: 0.2234, loss: 2.1305, grad_norm: 25.2622
2025-06-10 13:39:20,512 - mmdet - INFO - Epoch [3][6200/7033]	lr: 7.503e-05, eta: 6:25:23, time: 1.030, data_time: 0.029, memory: 17617, loss_cls: 0.0765, loss_bbox: 0.2170, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3238, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0822, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2158, loss: 2.0697, grad_norm: 24.4776
2025-06-10 13:40:11,973 - mmdet - INFO - Epoch [3][6250/7033]	lr: 7.503e-05, eta: 6:24:29, time: 1.029, data_time: 0.032, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2218, d0.loss_cls: 0.1712, d0.loss_bbox: 0.3310, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0908, d2.loss_bbox: 0.2352, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2211, loss: 2.0945, grad_norm: 31.7447
2025-06-10 13:41:02,658 - mmdet - INFO - Epoch [3][6300/7033]	lr: 7.503e-05, eta: 6:23:34, time: 1.014, data_time: 0.031, memory: 17617, loss_cls: 0.0706, loss_bbox: 0.2198, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2187, loss: 2.0697, grad_norm: 30.2120
2025-06-10 13:41:53,282 - mmdet - INFO - Epoch [3][6350/7033]	lr: 7.503e-05, eta: 6:22:39, time: 1.013, data_time: 0.031, memory: 17617, loss_cls: 0.0770, loss_bbox: 0.2289, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2584, d2.loss_cls: 0.0905, d2.loss_bbox: 0.2447, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2284, loss: 2.1616, grad_norm: 30.2430
2025-06-10 13:42:44,041 - mmdet - INFO - Epoch [3][6400/7033]	lr: 7.503e-05, eta: 6:21:44, time: 1.015, data_time: 0.031, memory: 17617, loss_cls: 0.0698, loss_bbox: 0.2168, d0.loss_cls: 0.1643, d0.loss_bbox: 0.3252, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2174, loss: 2.0325, grad_norm: 25.9112
2025-06-10 13:43:34,693 - mmdet - INFO - Epoch [3][6450/7033]	lr: 7.503e-05, eta: 6:20:49, time: 1.013, data_time: 0.031, memory: 17617, loss_cls: 0.0763, loss_bbox: 0.2214, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3383, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2563, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2389, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2220, loss: 2.1176, grad_norm: 24.4774
2025-06-10 13:44:25,190 - mmdet - INFO - Epoch [3][6500/7033]	lr: 7.503e-05, eta: 6:19:54, time: 1.010, data_time: 0.032, memory: 17617, loss_cls: 0.0773, loss_bbox: 0.2270, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3413, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2610, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2435, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2275, loss: 2.1433, grad_norm: 23.0863
2025-06-10 13:45:15,613 - mmdet - INFO - Epoch [3][6550/7033]	lr: 7.503e-05, eta: 6:18:59, time: 1.008, data_time: 0.028, memory: 17617, loss_cls: 0.0779, loss_bbox: 0.2222, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2566, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2386, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2363, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2232, loss: 2.1237, grad_norm: 22.8953
2025-06-10 13:46:06,048 - mmdet - INFO - Epoch [3][6600/7033]	lr: 7.503e-05, eta: 6:18:04, time: 1.009, data_time: 0.031, memory: 17617, loss_cls: 0.0711, loss_bbox: 0.2202, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3275, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2383, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2217, loss: 2.0701, grad_norm: 56.8178
2025-06-10 13:46:56,784 - mmdet - INFO - Epoch [3][6650/7033]	lr: 7.503e-05, eta: 6:17:09, time: 1.015, data_time: 0.031, memory: 17617, loss_cls: 0.0696, loss_bbox: 0.2090, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3324, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2250, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2081, loss: 2.0007, grad_norm: 23.8959
2025-06-10 13:47:47,325 - mmdet - INFO - Epoch [3][6700/7033]	lr: 7.503e-05, eta: 6:16:15, time: 1.011, data_time: 0.031, memory: 17617, loss_cls: 0.0710, loss_bbox: 0.2127, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0741, d4.loss_bbox: 0.2114, loss: 2.0004, grad_norm: 24.7446
2025-06-10 13:48:37,835 - mmdet - INFO - Epoch [3][6750/7033]	lr: 7.503e-05, eta: 6:15:20, time: 1.010, data_time: 0.033, memory: 17617, loss_cls: 0.0765, loss_bbox: 0.2156, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3308, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2528, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2319, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2159, loss: 2.0743, grad_norm: 25.4667
2025-06-10 13:49:28,438 - mmdet - INFO - Epoch [3][6800/7033]	lr: 7.503e-05, eta: 6:14:25, time: 1.012, data_time: 0.030, memory: 17617, loss_cls: 0.0739, loss_bbox: 0.2118, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3249, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2117, loss: 2.0412, grad_norm: 41.9082
2025-06-10 13:50:19,192 - mmdet - INFO - Epoch [3][6850/7033]	lr: 7.503e-05, eta: 6:13:30, time: 1.015, data_time: 0.031, memory: 17617, loss_cls: 0.0787, loss_bbox: 0.2242, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2555, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2417, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2251, loss: 2.1371, grad_norm: 24.3687
2025-06-10 13:51:10,620 - mmdet - INFO - Epoch [3][6900/7033]	lr: 7.503e-05, eta: 6:12:36, time: 1.029, data_time: 0.035, memory: 17617, loss_cls: 0.0789, loss_bbox: 0.2202, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2208, loss: 2.1014, grad_norm: 21.9069
2025-06-10 13:52:01,753 - mmdet - INFO - Epoch [3][6950/7033]	lr: 7.503e-05, eta: 6:11:42, time: 1.023, data_time: 0.033, memory: 17617, loss_cls: 0.0741, loss_bbox: 0.2161, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3241, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2457, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2313, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2150, loss: 2.0414, grad_norm: 929.3406
2025-06-10 13:52:53,181 - mmdet - INFO - Epoch [3][7000/7033]	lr: 7.503e-05, eta: 6:10:48, time: 1.029, data_time: 0.036, memory: 17617, loss_cls: 0.0697, loss_bbox: 0.2121, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3229, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2114, loss: 2.0059, grad_norm: 28.0514
2025-06-10 13:53:27,438 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-10 14:23:12,673 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 14:23:12,673 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7789, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8815, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9103, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9236, pts_bbox_NuScenes/car_trans_err: 0.1965, pts_bbox_NuScenes/car_scale_err: 0.1543, pts_bbox_NuScenes/car_orient_err: 0.0469, pts_bbox_NuScenes/car_vel_err: 0.3288, pts_bbox_NuScenes/car_attr_err: 0.1692, pts_bbox_NuScenes/mATE: 0.3028, pts_bbox_NuScenes/mASE: 0.2648, pts_bbox_NuScenes/mAOE: 0.2707, pts_bbox_NuScenes/mAVE: 0.3101, pts_bbox_NuScenes/mAAE: 0.1818, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4164, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6200, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7237, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7654, pts_bbox_NuScenes/truck_trans_err: 0.3496, pts_bbox_NuScenes/truck_scale_err: 0.2001, pts_bbox_NuScenes/truck_orient_err: 0.0504, pts_bbox_NuScenes/truck_vel_err: 0.2851, pts_bbox_NuScenes/truck_attr_err: 0.1985, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0613, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2059, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3991, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4661, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6453, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4338, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8166, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1243, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3052, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4858, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7429, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8937, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9194, pts_bbox_NuScenes/bus_trans_err: 0.3544, pts_bbox_NuScenes/bus_scale_err: 0.1965, pts_bbox_NuScenes/bus_orient_err: 0.0432, pts_bbox_NuScenes/bus_vel_err: 0.5440, pts_bbox_NuScenes/bus_attr_err: 0.3080, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1771, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4440, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5912, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6824, pts_bbox_NuScenes/trailer_trans_err: 0.4956, pts_bbox_NuScenes/trailer_scale_err: 0.2373, pts_bbox_NuScenes/trailer_orient_err: 0.4961, pts_bbox_NuScenes/trailer_vel_err: 0.3139, pts_bbox_NuScenes/trailer_attr_err: 0.1373, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5873, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6942, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7437, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7590, pts_bbox_NuScenes/barrier_trans_err: 0.2285, pts_bbox_NuScenes/barrier_scale_err: 0.2873, pts_bbox_NuScenes/barrier_orient_err: 0.0497, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6018, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7630, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8033, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8097, pts_bbox_NuScenes/motorcycle_trans_err: 0.2400, pts_bbox_NuScenes/motorcycle_scale_err: 0.2540, pts_bbox_NuScenes/motorcycle_orient_err: 0.2315, pts_bbox_NuScenes/motorcycle_vel_err: 0.4235, pts_bbox_NuScenes/motorcycle_attr_err: 0.2203, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5228, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5850, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5959, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6065, pts_bbox_NuScenes/bicycle_trans_err: 0.1925, pts_bbox_NuScenes/bicycle_scale_err: 0.2619, pts_bbox_NuScenes/bicycle_orient_err: 0.3674, pts_bbox_NuScenes/bicycle_vel_err: 0.2227, pts_bbox_NuScenes/bicycle_attr_err: 0.0028, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7996, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8517, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8766, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8901, pts_bbox_NuScenes/pedestrian_trans_err: 0.1709, pts_bbox_NuScenes/pedestrian_scale_err: 0.2962, pts_bbox_NuScenes/pedestrian_orient_err: 0.3346, pts_bbox_NuScenes/pedestrian_vel_err: 0.2385, pts_bbox_NuScenes/pedestrian_attr_err: 0.1134, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7224, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7689, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7921, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8168, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1543, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3264, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7005, pts_bbox_NuScenes/mAP: 0.6670
2025-06-10 14:24:12,264 - mmdet - INFO - Epoch [4][50/7033]	lr: 5.005e-05, eta: 6:08:50, time: 1.115, data_time: 0.119, memory: 17617, loss_cls: 0.0737, loss_bbox: 0.2111, d0.loss_cls: 0.1595, d0.loss_bbox: 0.3269, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2111, loss: 2.0198, grad_norm: 34.7296
2025-06-10 14:25:03,676 - mmdet - INFO - Epoch [4][100/7033]	lr: 5.005e-05, eta: 6:07:56, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0716, loss_bbox: 0.2097, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2106, loss: 1.9796, grad_norm: 27.7892
2025-06-10 14:25:54,879 - mmdet - INFO - Epoch [4][150/7033]	lr: 5.005e-05, eta: 6:07:02, time: 1.024, data_time: 0.029, memory: 17617, loss_cls: 0.0661, loss_bbox: 0.2036, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3155, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0670, d4.loss_bbox: 0.2056, loss: 1.9344, grad_norm: 38.2388
2025-06-10 14:26:46,484 - mmdet - INFO - Epoch [4][200/7033]	lr: 5.005e-05, eta: 6:06:08, time: 1.032, data_time: 0.034, memory: 17617, loss_cls: 0.0676, loss_bbox: 0.2113, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3182, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2118, loss: 1.9813, grad_norm: 174.5949
2025-06-10 14:27:38,188 - mmdet - INFO - Epoch [4][250/7033]	lr: 5.005e-05, eta: 6:05:15, time: 1.034, data_time: 0.035, memory: 17617, loss_cls: 0.0689, loss_bbox: 0.2055, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3177, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2164, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2053, loss: 1.9560, grad_norm: 20.8348
2025-06-10 14:28:29,522 - mmdet - INFO - Epoch [4][300/7033]	lr: 5.005e-05, eta: 6:04:21, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.0692, loss_bbox: 0.2118, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3299, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2133, loss: 2.0095, grad_norm: 21.2784
2025-06-10 14:29:20,747 - mmdet - INFO - Epoch [4][350/7033]	lr: 5.005e-05, eta: 6:03:27, time: 1.024, data_time: 0.028, memory: 17617, loss_cls: 0.0696, loss_bbox: 0.2128, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3141, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2128, loss: 1.9972, grad_norm: 37.0853
2025-06-10 14:30:12,107 - mmdet - INFO - Epoch [4][400/7033]	lr: 5.005e-05, eta: 6:02:34, time: 1.027, data_time: 0.029, memory: 17617, loss_cls: 0.0768, loss_bbox: 0.2129, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2512, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2142, loss: 2.0688, grad_norm: 39.2044
2025-06-10 14:31:03,564 - mmdet - INFO - Epoch [4][450/7033]	lr: 5.005e-05, eta: 6:01:40, time: 1.029, data_time: 0.030, memory: 17617, loss_cls: 0.0631, loss_bbox: 0.2046, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3135, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0662, d4.loss_bbox: 0.2053, loss: 1.9305, grad_norm: 40.3936
2025-06-10 14:31:54,859 - mmdet - INFO - Epoch [4][500/7033]	lr: 5.005e-05, eta: 6:00:46, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0713, loss_bbox: 0.2091, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3209, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2232, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2105, loss: 2.0167, grad_norm: 109.2847
2025-06-10 14:32:45,981 - mmdet - INFO - Epoch [4][550/7033]	lr: 5.005e-05, eta: 5:59:52, time: 1.022, data_time: 0.031, memory: 17617, loss_cls: 0.0733, loss_bbox: 0.2136, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2151, loss: 2.0327, grad_norm: 26.7694
2025-06-10 14:33:37,132 - mmdet - INFO - Epoch [4][600/7033]	lr: 5.005e-05, eta: 5:58:59, time: 1.023, data_time: 0.029, memory: 17617, loss_cls: 0.0689, loss_bbox: 0.2032, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2038, loss: 1.9415, grad_norm: 18.8990
2025-06-10 14:34:28,419 - mmdet - INFO - Epoch [4][650/7033]	lr: 5.005e-05, eta: 5:58:05, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0682, loss_bbox: 0.2153, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1021, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2143, loss: 2.0191, grad_norm: 26.2057
2025-06-10 14:35:19,701 - mmdet - INFO - Epoch [4][700/7033]	lr: 5.005e-05, eta: 5:57:11, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0766, loss_bbox: 0.2093, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2119, loss: 2.0375, grad_norm: 30.1707
2025-06-10 14:36:10,824 - mmdet - INFO - Epoch [4][750/7033]	lr: 5.005e-05, eta: 5:56:17, time: 1.022, data_time: 0.029, memory: 17617, loss_cls: 0.0699, loss_bbox: 0.2071, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3178, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2399, d2.loss_cls: 0.0840, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2078, loss: 1.9766, grad_norm: 29.0996
2025-06-10 14:37:01,935 - mmdet - INFO - Epoch [4][800/7033]	lr: 5.005e-05, eta: 5:55:24, time: 1.022, data_time: 0.028, memory: 17617, loss_cls: 0.0714, loss_bbox: 0.2089, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2228, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2101, loss: 1.9989, grad_norm: 31.2889
2025-06-10 14:37:53,122 - mmdet - INFO - Epoch [4][850/7033]	lr: 5.005e-05, eta: 5:54:30, time: 1.024, data_time: 0.029, memory: 17617, loss_cls: 0.0709, loss_bbox: 0.2107, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3212, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2115, loss: 2.0114, grad_norm: 26.9198
2025-06-10 14:38:44,460 - mmdet - INFO - Epoch [4][900/7033]	lr: 5.005e-05, eta: 5:53:36, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.0646, loss_bbox: 0.2053, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0690, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2046, loss: 1.9272, grad_norm: 27.4242
2025-06-10 14:39:35,257 - mmdet - INFO - Epoch [4][950/7033]	lr: 5.005e-05, eta: 5:52:42, time: 1.016, data_time: 0.028, memory: 17617, loss_cls: 0.0728, loss_bbox: 0.2144, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3298, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2152, loss: 2.0438, grad_norm: 46.1844
2025-06-10 14:40:25,985 - mmdet - INFO - Epoch [4][1000/7033]	lr: 5.005e-05, eta: 5:51:48, time: 1.015, data_time: 0.028, memory: 17617, loss_cls: 0.0715, loss_bbox: 0.2146, d0.loss_cls: 0.1584, d0.loss_bbox: 0.3198, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2470, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2166, loss: 2.0258, grad_norm: 37.9934
2025-06-10 14:41:16,858 - mmdet - INFO - Epoch [4][1050/7033]	lr: 5.005e-05, eta: 5:50:54, time: 1.017, data_time: 0.026, memory: 17617, loss_cls: 0.0616, loss_bbox: 0.2022, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0693, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2016, loss: 1.8928, grad_norm: 26.5494
2025-06-10 14:42:07,306 - mmdet - INFO - Epoch [4][1100/7033]	lr: 5.005e-05, eta: 5:50:00, time: 1.009, data_time: 0.027, memory: 17617, loss_cls: 0.0667, loss_bbox: 0.2086, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3175, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2071, loss: 1.9640, grad_norm: 20.5441
2025-06-10 14:42:57,904 - mmdet - INFO - Epoch [4][1150/7033]	lr: 5.005e-05, eta: 5:49:05, time: 1.012, data_time: 0.028, memory: 17617, loss_cls: 0.0695, loss_bbox: 0.2083, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3188, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2099, loss: 1.9818, grad_norm: 25.8743
2025-06-10 14:43:48,536 - mmdet - INFO - Epoch [4][1200/7033]	lr: 5.005e-05, eta: 5:48:11, time: 1.013, data_time: 0.029, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.2070, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3187, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0842, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0698, d4.loss_bbox: 0.2067, loss: 1.9670, grad_norm: 32.7197
2025-06-10 14:44:39,030 - mmdet - INFO - Epoch [4][1250/7033]	lr: 5.005e-05, eta: 5:47:17, time: 1.010, data_time: 0.030, memory: 17617, loss_cls: 0.0778, loss_bbox: 0.2171, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3354, d1.loss_cls: 0.1084, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2196, loss: 2.0937, grad_norm: 39.3973
2025-06-10 14:45:41,559 - mmdet - INFO - Epoch [4][1300/7033]	lr: 5.005e-05, eta: 5:46:33, time: 1.251, data_time: 0.269, memory: 17617, loss_cls: 0.0746, loss_bbox: 0.2115, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2283, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2134, loss: 2.0401, grad_norm: 22.5964
2025-06-10 14:46:32,068 - mmdet - INFO - Epoch [4][1350/7033]	lr: 5.005e-05, eta: 5:45:39, time: 1.010, data_time: 0.029, memory: 17617, loss_cls: 0.0709, loss_bbox: 0.2036, d0.loss_cls: 0.1624, d0.loss_bbox: 0.3184, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2057, loss: 1.9764, grad_norm: 18.7473
2025-06-10 14:47:22,329 - mmdet - INFO - Epoch [4][1400/7033]	lr: 5.005e-05, eta: 5:44:45, time: 1.005, data_time: 0.028, memory: 17617, loss_cls: 0.0687, loss_bbox: 0.2131, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3164, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2271, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2134, loss: 1.9996, grad_norm: 30.3449
2025-06-10 14:48:16,155 - mmdet - INFO - Epoch [4][1450/7033]	lr: 5.005e-05, eta: 5:43:53, time: 1.077, data_time: 0.031, memory: 17617, loss_cls: 0.0692, loss_bbox: 0.2005, d0.loss_cls: 0.1630, d0.loss_bbox: 0.3205, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0872, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2030, loss: 1.9674, grad_norm: 21.9672
2025-06-10 14:49:07,104 - mmdet - INFO - Epoch [4][1500/7033]	lr: 5.005e-05, eta: 5:42:59, time: 1.019, data_time: 0.029, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2099, d0.loss_cls: 0.1531, d0.loss_bbox: 0.3177, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2111, loss: 1.9792, grad_norm: 33.9116
2025-06-10 14:49:58,466 - mmdet - INFO - Epoch [4][1550/7033]	lr: 5.005e-05, eta: 5:42:06, time: 1.027, data_time: 0.031, memory: 17617, loss_cls: 0.0660, loss_bbox: 0.2097, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3106, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2245, d4.loss_cls: 0.0674, d4.loss_bbox: 0.2104, loss: 1.9471, grad_norm: 19.1745
2025-06-10 14:50:49,696 - mmdet - INFO - Epoch [4][1600/7033]	lr: 5.005e-05, eta: 5:41:12, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0649, loss_bbox: 0.2081, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3209, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0667, d4.loss_bbox: 0.2099, loss: 1.9557, grad_norm: 28.0372
2025-06-10 14:51:40,972 - mmdet - INFO - Epoch [4][1650/7033]	lr: 5.005e-05, eta: 5:40:19, time: 1.026, data_time: 0.028, memory: 17617, loss_cls: 0.0651, loss_bbox: 0.2068, d0.loss_cls: 0.1539, d0.loss_bbox: 0.3259, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0661, d4.loss_bbox: 0.2078, loss: 1.9608, grad_norm: 35.0900
2025-06-10 14:52:32,035 - mmdet - INFO - Epoch [4][1700/7033]	lr: 5.005e-05, eta: 5:39:25, time: 1.021, data_time: 0.029, memory: 17617, loss_cls: 0.0688, loss_bbox: 0.2160, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2501, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2164, loss: 2.0319, grad_norm: 22.8704
2025-06-10 14:53:23,372 - mmdet - INFO - Epoch [4][1750/7033]	lr: 5.005e-05, eta: 5:38:32, time: 1.027, data_time: 0.030, memory: 17617, loss_cls: 0.0691, loss_bbox: 0.2059, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3183, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2071, loss: 1.9570, grad_norm: 22.8171
2025-06-10 14:54:14,339 - mmdet - INFO - Epoch [4][1800/7033]	lr: 5.005e-05, eta: 5:37:38, time: 1.019, data_time: 0.030, memory: 17617, loss_cls: 0.0654, loss_bbox: 0.2026, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3149, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2191, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0666, d4.loss_bbox: 0.2043, loss: 1.9271, grad_norm: 24.3170
2025-06-10 14:55:05,827 - mmdet - INFO - Epoch [4][1850/7033]	lr: 5.005e-05, eta: 5:36:44, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0697, loss_bbox: 0.2064, d0.loss_cls: 0.1573, d0.loss_bbox: 0.3126, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2069, loss: 1.9632, grad_norm: 32.1655
2025-06-10 14:55:57,365 - mmdet - INFO - Epoch [4][1900/7033]	lr: 5.005e-05, eta: 5:35:51, time: 1.031, data_time: 0.034, memory: 17617, loss_cls: 0.0712, loss_bbox: 0.2087, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3178, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2402, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2228, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2105, loss: 1.9871, grad_norm: 33.5512
2025-06-10 14:56:48,746 - mmdet - INFO - Epoch [4][1950/7033]	lr: 5.005e-05, eta: 5:34:58, time: 1.028, data_time: 0.034, memory: 17617, loss_cls: 0.0737, loss_bbox: 0.2131, d0.loss_cls: 0.1606, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1020, d1.loss_bbox: 0.2463, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2146, loss: 2.0365, grad_norm: 20.1334
2025-06-10 14:57:40,260 - mmdet - INFO - Epoch [4][2000/7033]	lr: 5.005e-05, eta: 5:34:05, time: 1.030, data_time: 0.033, memory: 17617, loss_cls: 0.0672, loss_bbox: 0.2118, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3235, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2247, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2098, loss: 1.9872, grad_norm: 24.4587
2025-06-10 14:58:31,707 - mmdet - INFO - Epoch [4][2050/7033]	lr: 5.005e-05, eta: 5:33:11, time: 1.029, data_time: 0.035, memory: 17617, loss_cls: 0.0669, loss_bbox: 0.2110, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3258, d1.loss_cls: 0.0972, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2115, loss: 1.9883, grad_norm: 29.9039
2025-06-10 14:59:23,370 - mmdet - INFO - Epoch [4][2100/7033]	lr: 5.005e-05, eta: 5:32:18, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0669, loss_bbox: 0.2021, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3122, d1.loss_cls: 0.0970, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2034, loss: 1.9303, grad_norm: 21.9407
2025-06-10 15:00:14,722 - mmdet - INFO - Epoch [4][2150/7033]	lr: 5.005e-05, eta: 5:31:25, time: 1.027, data_time: 0.032, memory: 17617, loss_cls: 0.0767, loss_bbox: 0.2108, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3145, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2123, loss: 2.0263, grad_norm: 18.7463
2025-06-10 15:01:06,656 - mmdet - INFO - Epoch [4][2200/7033]	lr: 5.005e-05, eta: 5:30:32, time: 1.039, data_time: 0.032, memory: 17617, loss_cls: 0.0703, loss_bbox: 0.2097, d0.loss_cls: 0.1594, d0.loss_bbox: 0.3321, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2228, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2122, loss: 2.0151, grad_norm: 21.4285
2025-06-10 15:01:58,911 - mmdet - INFO - Epoch [4][2250/7033]	lr: 5.005e-05, eta: 5:29:39, time: 1.045, data_time: 0.032, memory: 17617, loss_cls: 0.0651, loss_bbox: 0.1999, d0.loss_cls: 0.1541, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2299, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2155, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2000, loss: 1.8971, grad_norm: 18.5564
2025-06-10 15:02:51,056 - mmdet - INFO - Epoch [4][2300/7033]	lr: 5.005e-05, eta: 5:28:46, time: 1.043, data_time: 0.034, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2153, d0.loss_cls: 0.1614, d0.loss_bbox: 0.3188, d1.loss_cls: 0.0997, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2159, loss: 2.0259, grad_norm: 124.7434
2025-06-10 15:03:43,404 - mmdet - INFO - Epoch [4][2350/7033]	lr: 5.005e-05, eta: 5:27:54, time: 1.047, data_time: 0.039, memory: 17617, loss_cls: 0.0640, loss_bbox: 0.2051, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0778, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2053, loss: 1.9222, grad_norm: 28.4260
2025-06-10 15:04:35,264 - mmdet - INFO - Epoch [4][2400/7033]	lr: 5.005e-05, eta: 5:27:01, time: 1.037, data_time: 0.032, memory: 17617, loss_cls: 0.0684, loss_bbox: 0.2139, d0.loss_cls: 0.1606, d0.loss_bbox: 0.3179, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0691, d4.loss_bbox: 0.2137, loss: 2.0053, grad_norm: 19.8027
2025-06-10 15:05:26,862 - mmdet - INFO - Epoch [4][2450/7033]	lr: 5.005e-05, eta: 5:26:08, time: 1.032, data_time: 0.029, memory: 17617, loss_cls: 0.0704, loss_bbox: 0.2142, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3200, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0768, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2134, loss: 2.0050, grad_norm: 25.8955
2025-06-10 15:06:18,805 - mmdet - INFO - Epoch [4][2500/7033]	lr: 5.005e-05, eta: 5:25:15, time: 1.039, data_time: 0.033, memory: 17617, loss_cls: 0.0704, loss_bbox: 0.2157, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2168, loss: 2.0456, grad_norm: 24.5015
2025-06-10 15:07:10,617 - mmdet - INFO - Epoch [4][2550/7033]	lr: 5.005e-05, eta: 5:24:22, time: 1.036, data_time: 0.037, memory: 17617, loss_cls: 0.0716, loss_bbox: 0.2160, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2158, loss: 2.0484, grad_norm: 43.7378
2025-06-10 15:08:02,402 - mmdet - INFO - Epoch [4][2600/7033]	lr: 5.005e-05, eta: 5:23:29, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0687, loss_bbox: 0.2154, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3227, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2274, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0683, d4.loss_bbox: 0.2161, loss: 2.0011, grad_norm: 27.3144
2025-06-10 15:08:54,576 - mmdet - INFO - Epoch [4][2650/7033]	lr: 5.005e-05, eta: 5:22:36, time: 1.043, data_time: 0.032, memory: 17617, loss_cls: 0.0687, loss_bbox: 0.2101, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2124, loss: 1.9834, grad_norm: 32.1450
2025-06-10 15:09:47,004 - mmdet - INFO - Epoch [4][2700/7033]	lr: 5.005e-05, eta: 5:21:44, time: 1.049, data_time: 0.031, memory: 17617, loss_cls: 0.0701, loss_bbox: 0.2024, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3177, d1.loss_cls: 0.0987, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2035, loss: 1.9496, grad_norm: 19.3615
2025-06-10 15:10:39,180 - mmdet - INFO - Epoch [4][2750/7033]	lr: 5.005e-05, eta: 5:20:51, time: 1.044, data_time: 0.032, memory: 17617, loss_cls: 0.0711, loss_bbox: 0.2071, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0971, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2083, loss: 1.9774, grad_norm: 31.9683
2025-06-10 15:11:31,436 - mmdet - INFO - Epoch [4][2800/7033]	lr: 5.005e-05, eta: 5:19:58, time: 1.045, data_time: 0.032, memory: 17617, loss_cls: 0.0698, loss_bbox: 0.2056, d0.loss_cls: 0.1538, d0.loss_bbox: 0.3250, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2063, loss: 1.9807, grad_norm: 25.4333
2025-06-10 15:12:23,869 - mmdet - INFO - Epoch [4][2850/7033]	lr: 5.005e-05, eta: 5:19:06, time: 1.049, data_time: 0.032, memory: 17617, loss_cls: 0.0676, loss_bbox: 0.2146, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3307, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2152, loss: 2.0241, grad_norm: 21.1739
2025-06-10 15:13:16,217 - mmdet - INFO - Epoch [4][2900/7033]	lr: 5.005e-05, eta: 5:18:13, time: 1.047, data_time: 0.034, memory: 17617, loss_cls: 0.0678, loss_bbox: 0.2145, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3250, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2275, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2146, loss: 1.9953, grad_norm: 39.2549
2025-06-10 15:14:08,637 - mmdet - INFO - Epoch [4][2950/7033]	lr: 5.005e-05, eta: 5:17:21, time: 1.048, data_time: 0.035, memory: 17617, loss_cls: 0.0638, loss_bbox: 0.2075, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3162, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2085, loss: 1.9502, grad_norm: 42.0138
2025-06-10 15:15:00,780 - mmdet - INFO - Epoch [4][3000/7033]	lr: 5.005e-05, eta: 5:16:28, time: 1.043, data_time: 0.033, memory: 17617, loss_cls: 0.0708, loss_bbox: 0.2149, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2493, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2163, loss: 2.0427, grad_norm: 38.8785
2025-06-10 15:15:52,809 - mmdet - INFO - Epoch [4][3050/7033]	lr: 5.005e-05, eta: 5:15:35, time: 1.041, data_time: 0.032, memory: 17617, loss_cls: 0.0749, loss_bbox: 0.2120, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3256, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2471, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2272, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2127, loss: 2.0530, grad_norm: 20.1875
2025-06-10 15:16:44,800 - mmdet - INFO - Epoch [4][3100/7033]	lr: 5.005e-05, eta: 5:14:43, time: 1.040, data_time: 0.033, memory: 17617, loss_cls: 0.0681, loss_bbox: 0.2109, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3229, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2103, loss: 1.9857, grad_norm: 22.7684
2025-06-10 15:17:36,950 - mmdet - INFO - Epoch [4][3150/7033]	lr: 5.005e-05, eta: 5:13:50, time: 1.043, data_time: 0.031, memory: 17617, loss_cls: 0.0739, loss_bbox: 0.2109, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2271, d3.loss_cls: 0.0781, d3.loss_bbox: 0.2247, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2121, loss: 2.0174, grad_norm: 354.3664
2025-06-10 15:18:29,076 - mmdet - INFO - Epoch [4][3200/7033]	lr: 5.005e-05, eta: 5:12:57, time: 1.043, data_time: 0.031, memory: 17617, loss_cls: 0.0709, loss_bbox: 0.2111, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3211, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2276, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2129, loss: 1.9982, grad_norm: 21.4461
2025-06-10 15:19:21,492 - mmdet - INFO - Epoch [4][3250/7033]	lr: 5.005e-05, eta: 5:12:05, time: 1.048, data_time: 0.033, memory: 17617, loss_cls: 0.0684, loss_bbox: 0.2179, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3342, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2557, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2188, loss: 2.0542, grad_norm: 113.5872
2025-06-10 15:20:13,413 - mmdet - INFO - Epoch [4][3300/7033]	lr: 5.005e-05, eta: 5:11:12, time: 1.038, data_time: 0.033, memory: 17617, loss_cls: 0.0660, loss_bbox: 0.2012, d0.loss_cls: 0.1566, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0684, d4.loss_bbox: 0.2009, loss: 1.9018, grad_norm: 31.2924
2025-06-10 15:21:05,191 - mmdet - INFO - Epoch [4][3350/7033]	lr: 5.005e-05, eta: 5:10:19, time: 1.036, data_time: 0.031, memory: 17617, loss_cls: 0.0741, loss_bbox: 0.2149, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3212, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2144, loss: 2.0380, grad_norm: 18.1589
2025-06-10 15:21:56,672 - mmdet - INFO - Epoch [4][3400/7033]	lr: 5.005e-05, eta: 5:09:26, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0657, loss_bbox: 0.2098, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3222, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2100, loss: 1.9664, grad_norm: 64.7779
2025-06-10 15:22:48,327 - mmdet - INFO - Epoch [4][3450/7033]	lr: 5.005e-05, eta: 5:08:33, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0727, loss_bbox: 0.2148, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0789, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2155, loss: 2.0341, grad_norm: 26.9763
2025-06-10 15:23:39,510 - mmdet - INFO - Epoch [4][3500/7033]	lr: 5.005e-05, eta: 5:07:39, time: 1.024, data_time: 0.030, memory: 17617, loss_cls: 0.0699, loss_bbox: 0.2070, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3241, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2077, loss: 1.9906, grad_norm: 24.2453
2025-06-10 15:24:30,785 - mmdet - INFO - Epoch [4][3550/7033]	lr: 5.005e-05, eta: 5:06:46, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0768, loss_bbox: 0.2127, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1033, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2137, loss: 2.0526, grad_norm: 28.7558
2025-06-10 15:25:22,102 - mmdet - INFO - Epoch [4][3600/7033]	lr: 5.005e-05, eta: 5:05:53, time: 1.026, data_time: 0.033, memory: 17617, loss_cls: 0.0765, loss_bbox: 0.2134, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3178, d1.loss_cls: 0.1051, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0879, d2.loss_bbox: 0.2331, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2153, loss: 2.0507, grad_norm: 30.6092
2025-06-10 15:26:13,778 - mmdet - INFO - Epoch [4][3650/7033]	lr: 5.005e-05, eta: 5:05:00, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0687, loss_bbox: 0.2055, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3170, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2053, loss: 1.9686, grad_norm: 43.3426
2025-06-10 15:27:05,324 - mmdet - INFO - Epoch [4][3700/7033]	lr: 5.005e-05, eta: 5:04:07, time: 1.031, data_time: 0.033, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2094, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3218, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2090, loss: 1.9898, grad_norm: 22.6723
2025-06-10 15:27:57,723 - mmdet - INFO - Epoch [4][3750/7033]	lr: 5.005e-05, eta: 5:03:14, time: 1.048, data_time: 0.053, memory: 17617, loss_cls: 0.0685, loss_bbox: 0.2045, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3179, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2048, loss: 1.9507, grad_norm: 18.8422
2025-06-10 15:28:49,207 - mmdet - INFO - Epoch [4][3800/7033]	lr: 5.005e-05, eta: 5:02:21, time: 1.030, data_time: 0.031, memory: 17617, loss_cls: 0.0744, loss_bbox: 0.2064, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2389, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2076, loss: 1.9836, grad_norm: 37.5805
2025-06-10 15:29:40,936 - mmdet - INFO - Epoch [4][3850/7033]	lr: 5.005e-05, eta: 5:01:28, time: 1.035, data_time: 0.032, memory: 17617, loss_cls: 0.0659, loss_bbox: 0.1989, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3028, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2303, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1985, loss: 1.8839, grad_norm: 24.7538
2025-06-10 15:30:32,723 - mmdet - INFO - Epoch [4][3900/7033]	lr: 5.005e-05, eta: 5:00:36, time: 1.036, data_time: 0.031, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.2001, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3180, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0733, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2000, loss: 1.9327, grad_norm: 48.8016
2025-06-10 15:31:24,665 - mmdet - INFO - Epoch [4][3950/7033]	lr: 5.005e-05, eta: 4:59:43, time: 1.039, data_time: 0.035, memory: 17617, loss_cls: 0.0721, loss_bbox: 0.2090, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3208, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2221, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2086, loss: 1.9997, grad_norm: 25.1457
2025-06-10 15:32:16,362 - mmdet - INFO - Epoch [4][4000/7033]	lr: 5.005e-05, eta: 4:58:50, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0720, loss_bbox: 0.2097, d0.loss_cls: 0.1681, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2470, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2107, loss: 2.0307, grad_norm: 26.3740
2025-06-10 15:33:08,164 - mmdet - INFO - Epoch [4][4050/7033]	lr: 5.005e-05, eta: 4:57:57, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0667, loss_bbox: 0.2064, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3238, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2077, loss: 1.9674, grad_norm: 23.4053
2025-06-10 15:33:59,779 - mmdet - INFO - Epoch [4][4100/7033]	lr: 5.005e-05, eta: 4:57:04, time: 1.032, data_time: 0.034, memory: 17617, loss_cls: 0.0719, loss_bbox: 0.2139, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3188, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0768, d3.loss_bbox: 0.2272, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2157, loss: 2.0215, grad_norm: 22.7655
2025-06-10 15:34:51,629 - mmdet - INFO - Epoch [4][4150/7033]	lr: 5.005e-05, eta: 4:56:11, time: 1.037, data_time: 0.036, memory: 17617, loss_cls: 0.0702, loss_bbox: 0.2053, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3095, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0854, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2054, loss: 1.9745, grad_norm: 557.0398
2025-06-10 15:35:45,642 - mmdet - INFO - Epoch [4][4200/7033]	lr: 5.005e-05, eta: 4:55:20, time: 1.080, data_time: 0.034, memory: 17617, loss_cls: 0.0666, loss_bbox: 0.2115, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3279, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2126, loss: 1.9869, grad_norm: 22.1162
2025-06-10 15:36:36,854 - mmdet - INFO - Epoch [4][4250/7033]	lr: 5.005e-05, eta: 4:54:27, time: 1.024, data_time: 0.032, memory: 17617, loss_cls: 0.0731, loss_bbox: 0.2091, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3215, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0864, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2085, loss: 2.0130, grad_norm: 30.8347
2025-06-10 15:37:27,520 - mmdet - INFO - Epoch [4][4300/7033]	lr: 5.005e-05, eta: 4:53:33, time: 1.013, data_time: 0.033, memory: 17617, loss_cls: 0.0778, loss_bbox: 0.2128, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3244, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2142, loss: 2.0654, grad_norm: 23.6942
2025-06-10 15:38:18,924 - mmdet - INFO - Epoch [4][4350/7033]	lr: 5.005e-05, eta: 4:52:40, time: 1.028, data_time: 0.034, memory: 17617, loss_cls: 0.0716, loss_bbox: 0.2117, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2126, loss: 2.0054, grad_norm: 34.1612
2025-06-10 15:39:10,561 - mmdet - INFO - Epoch [4][4400/7033]	lr: 5.005e-05, eta: 4:51:47, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0767, loss_bbox: 0.2119, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1029, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2281, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2141, loss: 2.0526, grad_norm: 22.8731
2025-06-10 15:40:01,902 - mmdet - INFO - Epoch [4][4450/7033]	lr: 5.005e-05, eta: 4:50:54, time: 1.027, data_time: 0.032, memory: 17617, loss_cls: 0.0660, loss_bbox: 0.2017, d0.loss_cls: 0.1505, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2033, loss: 1.8965, grad_norm: 28.9372
2025-06-10 15:40:53,672 - mmdet - INFO - Epoch [4][4500/7033]	lr: 5.005e-05, eta: 4:50:01, time: 1.035, data_time: 0.034, memory: 17617, loss_cls: 0.0656, loss_bbox: 0.2051, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2047, loss: 1.9126, grad_norm: 89.9788
2025-06-10 15:41:45,057 - mmdet - INFO - Epoch [4][4550/7033]	lr: 5.005e-05, eta: 4:49:08, time: 1.028, data_time: 0.031, memory: 17617, loss_cls: 0.0674, loss_bbox: 0.2066, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3247, d1.loss_cls: 0.0991, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2075, loss: 1.9800, grad_norm: 19.2577
2025-06-10 15:42:36,283 - mmdet - INFO - Epoch [4][4600/7033]	lr: 5.005e-05, eta: 4:48:15, time: 1.025, data_time: 0.028, memory: 17617, loss_cls: 0.0734, loss_bbox: 0.2181, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3287, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2505, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2195, loss: 2.0628, grad_norm: 67.4258
2025-06-10 15:43:27,401 - mmdet - INFO - Epoch [4][4650/7033]	lr: 5.005e-05, eta: 4:47:21, time: 1.022, data_time: 0.032, memory: 17617, loss_cls: 0.0704, loss_bbox: 0.2087, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3168, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2411, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2265, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2100, loss: 1.9708, grad_norm: 21.3594
2025-06-10 15:44:18,874 - mmdet - INFO - Epoch [4][4700/7033]	lr: 5.005e-05, eta: 4:46:28, time: 1.029, data_time: 0.033, memory: 17617, loss_cls: 0.0716, loss_bbox: 0.2126, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3234, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2141, loss: 2.0250, grad_norm: 48.0694
2025-06-10 15:45:10,138 - mmdet - INFO - Epoch [4][4750/7033]	lr: 5.005e-05, eta: 4:45:35, time: 1.025, data_time: 0.031, memory: 17617, loss_cls: 0.0718, loss_bbox: 0.2112, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3175, d1.loss_cls: 0.1046, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2276, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2120, loss: 2.0226, grad_norm: 24.0647
2025-06-10 15:46:01,477 - mmdet - INFO - Epoch [4][4800/7033]	lr: 5.005e-05, eta: 4:44:42, time: 1.027, data_time: 0.033, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2133, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3149, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2139, loss: 2.0219, grad_norm: 42.5967
2025-06-10 15:46:52,524 - mmdet - INFO - Epoch [4][4850/7033]	lr: 5.005e-05, eta: 4:43:49, time: 1.021, data_time: 0.032, memory: 17617, loss_cls: 0.0678, loss_bbox: 0.2007, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0982, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2015, loss: 1.9143, grad_norm: 45.3506
2025-06-10 15:47:43,602 - mmdet - INFO - Epoch [4][4900/7033]	lr: 5.005e-05, eta: 4:42:56, time: 1.022, data_time: 0.030, memory: 17617, loss_cls: 0.0789, loss_bbox: 0.2122, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2441, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2125, loss: 2.0516, grad_norm: 44.8062
2025-06-10 15:48:34,782 - mmdet - INFO - Epoch [4][4950/7033]	lr: 5.005e-05, eta: 4:42:03, time: 1.024, data_time: 0.032, memory: 17617, loss_cls: 0.0622, loss_bbox: 0.2071, d0.loss_cls: 0.1569, d0.loss_bbox: 0.3248, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2412, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0674, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0634, d4.loss_bbox: 0.2090, loss: 1.9482, grad_norm: 26.3578
2025-06-10 15:49:26,043 - mmdet - INFO - Epoch [4][5000/7033]	lr: 5.005e-05, eta: 4:41:09, time: 1.025, data_time: 0.035, memory: 17617, loss_cls: 0.0771, loss_bbox: 0.2155, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2149, loss: 2.0633, grad_norm: 26.6175
2025-06-10 15:50:17,287 - mmdet - INFO - Epoch [4][5050/7033]	lr: 5.005e-05, eta: 4:40:16, time: 1.025, data_time: 0.035, memory: 17617, loss_cls: 0.0727, loss_bbox: 0.2063, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3084, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2071, loss: 1.9691, grad_norm: 109.1471
2025-06-10 15:51:08,910 - mmdet - INFO - Epoch [4][5100/7033]	lr: 5.005e-05, eta: 4:39:24, time: 1.032, data_time: 0.035, memory: 17617, loss_cls: 0.0802, loss_bbox: 0.2257, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3397, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2615, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2428, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2415, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2259, loss: 2.1599, grad_norm: 41.4485
2025-06-10 15:52:00,119 - mmdet - INFO - Epoch [4][5150/7033]	lr: 5.005e-05, eta: 4:38:30, time: 1.024, data_time: 0.035, memory: 17617, loss_cls: 0.0698, loss_bbox: 0.2005, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3155, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2013, loss: 1.9618, grad_norm: 47.3580
2025-06-10 15:52:51,041 - mmdet - INFO - Epoch [4][5200/7033]	lr: 5.005e-05, eta: 4:37:37, time: 1.018, data_time: 0.033, memory: 17617, loss_cls: 0.0655, loss_bbox: 0.2007, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2152, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2017, loss: 1.9220, grad_norm: 25.7173
2025-06-10 15:53:42,216 - mmdet - INFO - Epoch [4][5250/7033]	lr: 5.005e-05, eta: 4:36:44, time: 1.024, data_time: 0.034, memory: 17617, loss_cls: 0.0705, loss_bbox: 0.2012, d0.loss_cls: 0.1587, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2009, loss: 1.9146, grad_norm: 28.7145
2025-06-10 15:54:33,582 - mmdet - INFO - Epoch [4][5300/7033]	lr: 5.005e-05, eta: 4:35:51, time: 1.027, data_time: 0.034, memory: 17617, loss_cls: 0.0737, loss_bbox: 0.2065, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3149, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0753, d4.loss_bbox: 0.2073, loss: 1.9836, grad_norm: 37.6576
2025-06-10 15:55:36,031 - mmdet - INFO - Epoch [4][5350/7033]	lr: 5.005e-05, eta: 4:35:05, time: 1.249, data_time: 0.037, memory: 17617, loss_cls: 0.0646, loss_bbox: 0.2033, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2383, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2034, loss: 1.9219, grad_norm: 36.1992
2025-06-10 15:56:27,317 - mmdet - INFO - Epoch [4][5400/7033]	lr: 5.005e-05, eta: 4:34:12, time: 1.026, data_time: 0.035, memory: 17617, loss_cls: 0.0718, loss_bbox: 0.2063, d0.loss_cls: 0.1562, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2069, loss: 1.9771, grad_norm: 18.9442
2025-06-10 15:57:18,663 - mmdet - INFO - Epoch [4][5450/7033]	lr: 5.005e-05, eta: 4:33:19, time: 1.027, data_time: 0.033, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.1938, d0.loss_cls: 0.1514, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2111, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2074, d4.loss_cls: 0.0688, d4.loss_bbox: 0.1951, loss: 1.8741, grad_norm: 20.9430
2025-06-10 15:58:09,931 - mmdet - INFO - Epoch [4][5500/7033]	lr: 5.005e-05, eta: 4:32:26, time: 1.025, data_time: 0.033, memory: 17617, loss_cls: 0.0655, loss_bbox: 0.2072, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2089, loss: 1.9458, grad_norm: 53.9093
2025-06-10 15:59:00,873 - mmdet - INFO - Epoch [4][5550/7033]	lr: 5.005e-05, eta: 4:31:32, time: 1.019, data_time: 0.033, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.2033, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3133, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2360, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2040, loss: 1.9418, grad_norm: 25.3513
2025-06-10 15:59:52,056 - mmdet - INFO - Epoch [4][5600/7033]	lr: 5.005e-05, eta: 4:30:39, time: 1.024, data_time: 0.034, memory: 17617, loss_cls: 0.0627, loss_bbox: 0.1946, d0.loss_cls: 0.1510, d0.loss_bbox: 0.3063, d1.loss_cls: 0.0907, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2109, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0644, d4.loss_bbox: 0.1965, loss: 1.8599, grad_norm: 27.9277
2025-06-10 16:00:43,339 - mmdet - INFO - Epoch [4][5650/7033]	lr: 5.005e-05, eta: 4:29:46, time: 1.026, data_time: 0.032, memory: 17617, loss_cls: 0.0758, loss_bbox: 0.2135, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2133, loss: 2.0463, grad_norm: 37.6088
2025-06-10 16:01:34,437 - mmdet - INFO - Epoch [4][5700/7033]	lr: 5.005e-05, eta: 4:28:53, time: 1.022, data_time: 0.032, memory: 17617, loss_cls: 0.0752, loss_bbox: 0.2114, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2274, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2118, loss: 2.0324, grad_norm: 21.4764
2025-06-10 16:02:25,857 - mmdet - INFO - Epoch [4][5750/7033]	lr: 5.005e-05, eta: 4:28:00, time: 1.028, data_time: 0.034, memory: 17617, loss_cls: 0.0749, loss_bbox: 0.2130, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3237, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2289, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2165, loss: 2.0249, grad_norm: 116.6334
2025-06-10 16:03:17,184 - mmdet - INFO - Epoch [4][5800/7033]	lr: 5.005e-05, eta: 4:27:07, time: 1.027, data_time: 0.034, memory: 17617, loss_cls: 0.0746, loss_bbox: 0.2017, d0.loss_cls: 0.1575, d0.loss_bbox: 0.3089, d1.loss_cls: 0.1003, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2028, loss: 1.9620, grad_norm: 20.4974
2025-06-10 16:04:08,857 - mmdet - INFO - Epoch [4][5850/7033]	lr: 5.005e-05, eta: 4:26:14, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0739, loss_bbox: 0.2142, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3234, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2508, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2292, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2159, loss: 2.0465, grad_norm: 27.3314
2025-06-10 16:05:00,370 - mmdet - INFO - Epoch [4][5900/7033]	lr: 5.005e-05, eta: 4:25:21, time: 1.030, data_time: 0.034, memory: 17617, loss_cls: 0.0699, loss_bbox: 0.2076, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2080, loss: 1.9741, grad_norm: 32.6289
2025-06-10 16:05:54,607 - mmdet - INFO - Epoch [4][5950/7033]	lr: 5.005e-05, eta: 4:24:30, time: 1.085, data_time: 0.035, memory: 17617, loss_cls: 0.0703, loss_bbox: 0.2035, d0.loss_cls: 0.1530, d0.loss_bbox: 0.3185, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2373, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2034, loss: 1.9464, grad_norm: 25.1671
2025-06-10 16:06:45,850 - mmdet - INFO - Epoch [4][6000/7033]	lr: 5.005e-05, eta: 4:23:37, time: 1.025, data_time: 0.031, memory: 17617, loss_cls: 0.0708, loss_bbox: 0.2036, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3184, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2220, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2055, loss: 1.9686, grad_norm: 62.6231
2025-06-10 16:07:37,024 - mmdet - INFO - Epoch [4][6050/7033]	lr: 5.005e-05, eta: 4:22:44, time: 1.023, data_time: 0.032, memory: 17617, loss_cls: 0.0699, loss_bbox: 0.2036, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3172, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2052, loss: 1.9633, grad_norm: 20.6684
2025-06-10 16:08:28,743 - mmdet - INFO - Epoch [4][6100/7033]	lr: 5.005e-05, eta: 4:21:51, time: 1.034, data_time: 0.034, memory: 17617, loss_cls: 0.0634, loss_bbox: 0.2022, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3146, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0761, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2037, loss: 1.9220, grad_norm: 24.0311
2025-06-10 16:09:20,355 - mmdet - INFO - Epoch [4][6150/7033]	lr: 5.005e-05, eta: 4:20:59, time: 1.032, data_time: 0.035, memory: 17617, loss_cls: 0.0740, loss_bbox: 0.2137, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1053, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2137, loss: 2.0351, grad_norm: 26.8719
2025-06-10 16:10:12,165 - mmdet - INFO - Epoch [4][6200/7033]	lr: 5.005e-05, eta: 4:20:06, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.2049, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3142, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2385, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2052, loss: 1.9547, grad_norm: 21.9997
2025-06-10 16:11:03,819 - mmdet - INFO - Epoch [4][6250/7033]	lr: 5.005e-05, eta: 4:19:13, time: 1.033, data_time: 0.036, memory: 17617, loss_cls: 0.0684, loss_bbox: 0.2043, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3187, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2048, loss: 1.9573, grad_norm: 2515.0693
2025-06-10 16:11:55,204 - mmdet - INFO - Epoch [4][6300/7033]	lr: 5.005e-05, eta: 4:18:20, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0723, loss_bbox: 0.2098, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3257, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2099, loss: 2.0166, grad_norm: 32.0567
2025-06-10 16:12:47,007 - mmdet - INFO - Epoch [4][6350/7033]	lr: 5.005e-05, eta: 4:17:27, time: 1.036, data_time: 0.036, memory: 17617, loss_cls: 0.0668, loss_bbox: 0.2083, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3215, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2101, loss: 1.9716, grad_norm: 55.5760
2025-06-10 16:13:38,759 - mmdet - INFO - Epoch [4][6400/7033]	lr: 5.005e-05, eta: 4:16:35, time: 1.035, data_time: 0.033, memory: 17617, loss_cls: 0.0744, loss_bbox: 0.2055, d0.loss_cls: 0.1588, d0.loss_bbox: 0.3189, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2191, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2065, loss: 1.9845, grad_norm: 38.8181
2025-06-10 16:14:30,490 - mmdet - INFO - Epoch [4][6450/7033]	lr: 5.005e-05, eta: 4:15:42, time: 1.035, data_time: 0.034, memory: 17617, loss_cls: 0.0709, loss_bbox: 0.2030, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2191, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2039, loss: 1.9406, grad_norm: 66.9438
2025-06-10 16:15:22,556 - mmdet - INFO - Epoch [4][6500/7033]	lr: 5.005e-05, eta: 4:14:50, time: 1.041, data_time: 0.034, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2111, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3245, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2283, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2131, loss: 2.0243, grad_norm: 32.4942
2025-06-10 16:16:14,490 - mmdet - INFO - Epoch [4][6550/7033]	lr: 5.005e-05, eta: 4:13:57, time: 1.039, data_time: 0.035, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2047, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0719, d4.loss_bbox: 0.2068, loss: 1.9461, grad_norm: 27.6158
2025-06-10 16:17:06,138 - mmdet - INFO - Epoch [4][6600/7033]	lr: 5.005e-05, eta: 4:13:04, time: 1.033, data_time: 0.034, memory: 17617, loss_cls: 0.0775, loss_bbox: 0.2063, d0.loss_cls: 0.1522, d0.loss_bbox: 0.3138, d1.loss_cls: 0.1007, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2072, loss: 1.9903, grad_norm: 30.0755
2025-06-10 16:17:57,955 - mmdet - INFO - Epoch [4][6650/7033]	lr: 5.005e-05, eta: 4:12:12, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0688, loss_bbox: 0.2037, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3114, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2048, loss: 1.9362, grad_norm: 24.2536
2025-06-10 16:18:49,252 - mmdet - INFO - Epoch [4][6700/7033]	lr: 5.005e-05, eta: 4:11:19, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0789, loss_bbox: 0.2068, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0837, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2077, loss: 2.0294, grad_norm: 42.9695
2025-06-10 16:19:41,159 - mmdet - INFO - Epoch [4][6750/7033]	lr: 5.005e-05, eta: 4:10:26, time: 1.038, data_time: 0.032, memory: 17617, loss_cls: 0.0777, loss_bbox: 0.2123, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3332, d1.loss_cls: 0.1088, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2312, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2125, loss: 2.0839, grad_norm: 52.4757
2025-06-10 16:20:33,038 - mmdet - INFO - Epoch [4][6800/7033]	lr: 5.005e-05, eta: 4:09:33, time: 1.038, data_time: 0.032, memory: 17617, loss_cls: 0.0623, loss_bbox: 0.2012, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2369, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0653, d4.loss_bbox: 0.2018, loss: 1.9112, grad_norm: 36.3392
2025-06-10 16:21:24,699 - mmdet - INFO - Epoch [4][6850/7033]	lr: 5.005e-05, eta: 4:08:41, time: 1.033, data_time: 0.034, memory: 17617, loss_cls: 0.0604, loss_bbox: 0.1999, d0.loss_cls: 0.1462, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0885, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1996, loss: 1.8717, grad_norm: 37.5681
2025-06-10 16:22:16,414 - mmdet - INFO - Epoch [4][6900/7033]	lr: 5.005e-05, eta: 4:07:48, time: 1.034, data_time: 0.033, memory: 17617, loss_cls: 0.0701, loss_bbox: 0.2106, d0.loss_cls: 0.1545, d0.loss_bbox: 0.3290, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2114, loss: 1.9965, grad_norm: 81.5571
2025-06-10 16:23:08,081 - mmdet - INFO - Epoch [4][6950/7033]	lr: 5.005e-05, eta: 4:06:55, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0694, loss_bbox: 0.2100, d0.loss_cls: 0.1514, d0.loss_bbox: 0.3195, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2411, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2256, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2111, loss: 1.9768, grad_norm: 108.7602
2025-06-10 16:24:00,505 - mmdet - INFO - Epoch [4][7000/7033]	lr: 5.005e-05, eta: 4:06:03, time: 1.049, data_time: 0.034, memory: 17617, loss_cls: 0.0670, loss_bbox: 0.2031, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3111, d1.loss_cls: 0.0926, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0674, d4.loss_bbox: 0.2048, loss: 1.9319, grad_norm: 38.5117
2025-06-10 16:24:35,143 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-06-10 16:56:27,498 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 16:56:27,498 - mmdet - INFO - Epoch(val) [4][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7911, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8839, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9112, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9246, pts_bbox_NuScenes/car_trans_err: 0.1807, pts_bbox_NuScenes/car_scale_err: 0.1497, pts_bbox_NuScenes/car_orient_err: 0.0427, pts_bbox_NuScenes/car_vel_err: 0.3264, pts_bbox_NuScenes/car_attr_err: 0.1863, pts_bbox_NuScenes/mATE: 0.2941, pts_bbox_NuScenes/mASE: 0.2622, pts_bbox_NuScenes/mAOE: 0.2676, pts_bbox_NuScenes/mAVE: 0.3014, pts_bbox_NuScenes/mAAE: 0.1878, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4231, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6186, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7274, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7649, pts_bbox_NuScenes/truck_trans_err: 0.3474, pts_bbox_NuScenes/truck_scale_err: 0.2013, pts_bbox_NuScenes/truck_orient_err: 0.0478, pts_bbox_NuScenes/truck_vel_err: 0.3244, pts_bbox_NuScenes/truck_attr_err: 0.2007, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0629, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2035, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4112, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4820, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6558, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4397, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8456, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1187, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3078, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5148, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7551, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8983, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9251, pts_bbox_NuScenes/bus_trans_err: 0.3339, pts_bbox_NuScenes/bus_scale_err: 0.1851, pts_bbox_NuScenes/bus_orient_err: 0.0481, pts_bbox_NuScenes/bus_vel_err: 0.4838, pts_bbox_NuScenes/bus_attr_err: 0.2931, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1774, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4251, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5916, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6752, pts_bbox_NuScenes/trailer_trans_err: 0.4837, pts_bbox_NuScenes/trailer_scale_err: 0.2174, pts_bbox_NuScenes/trailer_orient_err: 0.5170, pts_bbox_NuScenes/trailer_vel_err: 0.3033, pts_bbox_NuScenes/trailer_attr_err: 0.1613, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5988, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7057, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7562, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7698, pts_bbox_NuScenes/barrier_trans_err: 0.2295, pts_bbox_NuScenes/barrier_scale_err: 0.2870, pts_bbox_NuScenes/barrier_orient_err: 0.0474, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6249, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7701, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7991, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8062, pts_bbox_NuScenes/motorcycle_trans_err: 0.2243, pts_bbox_NuScenes/motorcycle_scale_err: 0.2519, pts_bbox_NuScenes/motorcycle_orient_err: 0.2081, pts_bbox_NuScenes/motorcycle_vel_err: 0.4165, pts_bbox_NuScenes/motorcycle_attr_err: 0.2387, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5324, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5860, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5979, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6086, pts_bbox_NuScenes/bicycle_trans_err: 0.1878, pts_bbox_NuScenes/bicycle_scale_err: 0.2625, pts_bbox_NuScenes/bicycle_orient_err: 0.3106, pts_bbox_NuScenes/bicycle_vel_err: 0.2117, pts_bbox_NuScenes/bicycle_attr_err: 0.0018, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8179, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8603, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8807, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8930, pts_bbox_NuScenes/pedestrian_trans_err: 0.1479, pts_bbox_NuScenes/pedestrian_scale_err: 0.2976, pts_bbox_NuScenes/pedestrian_orient_err: 0.3410, pts_bbox_NuScenes/pedestrian_vel_err: 0.2263, pts_bbox_NuScenes/pedestrian_attr_err: 0.1127, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7285, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7707, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7976, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8195, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1500, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3302, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7048, pts_bbox_NuScenes/mAP: 0.6723
2025-06-10 16:57:28,421 - mmdet - INFO - Epoch [5][50/7033]	lr: 2.508e-05, eta: 4:04:21, time: 1.130, data_time: 0.128, memory: 17617, loss_cls: 0.0717, loss_bbox: 0.2088, d0.loss_cls: 0.1587, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2096, loss: 2.0025, grad_norm: 67.2949
2025-06-10 16:58:20,171 - mmdet - INFO - Epoch [5][100/7033]	lr: 2.508e-05, eta: 4:03:28, time: 1.035, data_time: 0.034, memory: 17617, loss_cls: 0.0654, loss_bbox: 0.1995, d0.loss_cls: 0.1501, d0.loss_bbox: 0.3086, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2001, loss: 1.8948, grad_norm: 38.4238
2025-06-10 16:59:11,906 - mmdet - INFO - Epoch [5][150/7033]	lr: 2.508e-05, eta: 4:02:36, time: 1.035, data_time: 0.035, memory: 17617, loss_cls: 0.0749, loss_bbox: 0.2099, d0.loss_cls: 0.1583, d0.loss_bbox: 0.3253, d1.loss_cls: 0.1035, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2109, loss: 2.0287, grad_norm: 19.1197
2025-06-10 17:00:03,562 - mmdet - INFO - Epoch [5][200/7033]	lr: 2.508e-05, eta: 4:01:43, time: 1.033, data_time: 0.034, memory: 17617, loss_cls: 0.0688, loss_bbox: 0.1998, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2360, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2017, loss: 1.9288, grad_norm: 33.3824
2025-06-10 17:00:55,164 - mmdet - INFO - Epoch [5][250/7033]	lr: 2.508e-05, eta: 4:00:51, time: 1.032, data_time: 0.034, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.2041, d0.loss_cls: 0.1495, d0.loss_bbox: 0.3094, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0687, d4.loss_bbox: 0.2068, loss: 1.9328, grad_norm: 22.3780
2025-06-10 17:01:47,155 - mmdet - INFO - Epoch [5][300/7033]	lr: 2.508e-05, eta: 3:59:58, time: 1.040, data_time: 0.034, memory: 17617, loss_cls: 0.0710, loss_bbox: 0.2102, d0.loss_cls: 0.1578, d0.loss_bbox: 0.3113, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2117, loss: 1.9865, grad_norm: 27.9513
2025-06-10 17:02:38,807 - mmdet - INFO - Epoch [5][350/7033]	lr: 2.508e-05, eta: 3:59:05, time: 1.033, data_time: 0.032, memory: 17617, loss_cls: 0.0659, loss_bbox: 0.2041, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3139, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2061, loss: 1.9357, grad_norm: 17.4799
2025-06-10 17:03:30,714 - mmdet - INFO - Epoch [5][400/7033]	lr: 2.508e-05, eta: 3:58:13, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0570, loss_bbox: 0.1960, d0.loss_cls: 0.1475, d0.loss_bbox: 0.3083, d1.loss_cls: 0.0864, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0733, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0638, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0597, d4.loss_bbox: 0.1972, loss: 1.8460, grad_norm: 21.2210
2025-06-10 17:04:22,221 - mmdet - INFO - Epoch [5][450/7033]	lr: 2.508e-05, eta: 3:57:20, time: 1.030, data_time: 0.030, memory: 17617, loss_cls: 0.0718, loss_bbox: 0.2040, d0.loss_cls: 0.1530, d0.loss_bbox: 0.3116, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2051, loss: 1.9642, grad_norm: 29.0731
2025-06-10 17:05:14,002 - mmdet - INFO - Epoch [5][500/7033]	lr: 2.508e-05, eta: 3:56:28, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0719, loss_bbox: 0.2123, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3238, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2269, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2148, loss: 2.0476, grad_norm: 41.2217
2025-06-10 17:06:06,207 - mmdet - INFO - Epoch [5][550/7033]	lr: 2.508e-05, eta: 3:55:35, time: 1.044, data_time: 0.035, memory: 17617, loss_cls: 0.0662, loss_bbox: 0.1975, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2324, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0700, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0682, d4.loss_bbox: 0.1981, loss: 1.8974, grad_norm: 52.1131
2025-06-10 17:06:58,014 - mmdet - INFO - Epoch [5][600/7033]	lr: 2.508e-05, eta: 3:54:43, time: 1.036, data_time: 0.034, memory: 17617, loss_cls: 0.0709, loss_bbox: 0.2042, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3175, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2224, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2050, loss: 1.9636, grad_norm: 21.3258
2025-06-10 17:07:50,375 - mmdet - INFO - Epoch [5][650/7033]	lr: 2.508e-05, eta: 3:53:51, time: 1.047, data_time: 0.035, memory: 17617, loss_cls: 0.0714, loss_bbox: 0.2061, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3126, d1.loss_cls: 0.0992, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2072, loss: 1.9701, grad_norm: 54.6434
2025-06-10 17:08:42,438 - mmdet - INFO - Epoch [5][700/7033]	lr: 2.508e-05, eta: 3:52:58, time: 1.041, data_time: 0.033, memory: 17617, loss_cls: 0.0588, loss_bbox: 0.1991, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0914, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0674, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0616, d4.loss_bbox: 0.1996, loss: 1.8801, grad_norm: 24.7394
2025-06-10 17:09:34,294 - mmdet - INFO - Epoch [5][750/7033]	lr: 2.508e-05, eta: 3:52:06, time: 1.037, data_time: 0.032, memory: 17617, loss_cls: 0.0637, loss_bbox: 0.1963, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3083, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0658, d4.loss_bbox: 0.1975, loss: 1.8825, grad_norm: 47.0442
2025-06-10 17:10:26,004 - mmdet - INFO - Epoch [5][800/7033]	lr: 2.508e-05, eta: 3:51:13, time: 1.034, data_time: 0.030, memory: 17617, loss_cls: 0.0638, loss_bbox: 0.2030, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3179, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2043, loss: 1.9201, grad_norm: 31.7118
2025-06-10 17:11:17,394 - mmdet - INFO - Epoch [5][850/7033]	lr: 2.508e-05, eta: 3:50:21, time: 1.028, data_time: 0.031, memory: 17617, loss_cls: 0.0640, loss_bbox: 0.2076, d0.loss_cls: 0.1500, d0.loss_bbox: 0.3202, d1.loss_cls: 0.0922, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2096, loss: 1.9567, grad_norm: 44.9541
2025-06-10 17:12:08,879 - mmdet - INFO - Epoch [5][900/7033]	lr: 2.508e-05, eta: 3:49:28, time: 1.030, data_time: 0.031, memory: 17617, loss_cls: 0.0645, loss_bbox: 0.2021, d0.loss_cls: 0.1539, d0.loss_bbox: 0.3038, d1.loss_cls: 0.0921, d1.loss_bbox: 0.2359, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0669, d4.loss_bbox: 0.2031, loss: 1.8978, grad_norm: 18.6059
2025-06-10 17:13:03,555 - mmdet - INFO - Epoch [5][950/7033]	lr: 2.508e-05, eta: 3:48:37, time: 1.093, data_time: 0.030, memory: 17617, loss_cls: 0.0553, loss_bbox: 0.1905, d0.loss_cls: 0.1435, d0.loss_bbox: 0.3066, d1.loss_cls: 0.0839, d1.loss_bbox: 0.2237, d2.loss_cls: 0.0690, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0599, d3.loss_bbox: 0.2050, d4.loss_cls: 0.0574, d4.loss_bbox: 0.1929, loss: 1.7954, grad_norm: 41.7556
2025-06-10 17:13:54,780 - mmdet - INFO - Epoch [5][1000/7033]	lr: 2.508e-05, eta: 3:47:44, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0672, loss_bbox: 0.2066, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3113, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2193, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2070, loss: 1.9469, grad_norm: 31.9349
2025-06-10 17:14:45,983 - mmdet - INFO - Epoch [5][1050/7033]	lr: 2.508e-05, eta: 3:46:51, time: 1.024, data_time: 0.027, memory: 17617, loss_cls: 0.0646, loss_bbox: 0.2051, d0.loss_cls: 0.1549, d0.loss_bbox: 0.3155, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0802, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0670, d4.loss_bbox: 0.2058, loss: 1.9378, grad_norm: 60.1274
2025-06-10 17:15:37,398 - mmdet - INFO - Epoch [5][1100/7033]	lr: 2.508e-05, eta: 3:45:58, time: 1.028, data_time: 0.029, memory: 17617, loss_cls: 0.0621, loss_bbox: 0.1983, d0.loss_cls: 0.1481, d0.loss_bbox: 0.3125, d1.loss_cls: 0.0873, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0741, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1997, loss: 1.8794, grad_norm: 21.9183
2025-06-10 17:16:29,078 - mmdet - INFO - Epoch [5][1150/7033]	lr: 2.508e-05, eta: 3:45:06, time: 1.034, data_time: 0.029, memory: 17617, loss_cls: 0.0612, loss_bbox: 0.1955, d0.loss_cls: 0.1406, d0.loss_bbox: 0.3021, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0747, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1971, loss: 1.8426, grad_norm: 19.1302
2025-06-10 17:17:20,696 - mmdet - INFO - Epoch [5][1200/7033]	lr: 2.508e-05, eta: 3:44:13, time: 1.032, data_time: 0.032, memory: 17617, loss_cls: 0.0657, loss_bbox: 0.1944, d0.loss_cls: 0.1527, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2132, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0675, d4.loss_bbox: 0.1963, loss: 1.8763, grad_norm: 26.2362
2025-06-10 17:18:12,005 - mmdet - INFO - Epoch [5][1250/7033]	lr: 2.508e-05, eta: 3:43:21, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0625, loss_bbox: 0.2003, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0927, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0631, d4.loss_bbox: 0.2027, loss: 1.9008, grad_norm: 15.8688
2025-06-10 17:19:03,395 - mmdet - INFO - Epoch [5][1300/7033]	lr: 2.508e-05, eta: 3:42:28, time: 1.028, data_time: 0.030, memory: 17617, loss_cls: 0.0629, loss_bbox: 0.2010, d0.loss_cls: 0.1469, d0.loss_bbox: 0.3092, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0771, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0644, d4.loss_bbox: 0.2023, loss: 1.8906, grad_norm: 218.3274
2025-06-10 17:19:54,780 - mmdet - INFO - Epoch [5][1350/7033]	lr: 2.508e-05, eta: 3:41:35, time: 1.028, data_time: 0.029, memory: 17617, loss_cls: 0.0738, loss_bbox: 0.2060, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3212, d1.loss_cls: 0.0993, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2082, loss: 1.9909, grad_norm: 77.7843
2025-06-10 17:20:46,384 - mmdet - INFO - Epoch [5][1400/7033]	lr: 2.508e-05, eta: 3:40:43, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0629, loss_bbox: 0.1922, d0.loss_cls: 0.1464, d0.loss_bbox: 0.3065, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2305, d2.loss_cls: 0.0755, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0677, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1961, loss: 1.8541, grad_norm: 32.1822
2025-06-10 17:21:37,914 - mmdet - INFO - Epoch [5][1450/7033]	lr: 2.508e-05, eta: 3:39:50, time: 1.031, data_time: 0.031, memory: 17617, loss_cls: 0.0701, loss_bbox: 0.2103, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3231, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2118, loss: 2.0054, grad_norm: 41.0631
2025-06-10 17:22:29,127 - mmdet - INFO - Epoch [5][1500/7033]	lr: 2.508e-05, eta: 3:38:57, time: 1.024, data_time: 0.027, memory: 17617, loss_cls: 0.0686, loss_bbox: 0.2033, d0.loss_cls: 0.1519, d0.loss_bbox: 0.3049, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2317, d2.loss_cls: 0.0822, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2044, loss: 1.9177, grad_norm: 23.6466
2025-06-10 17:23:20,593 - mmdet - INFO - Epoch [5][1550/7033]	lr: 2.508e-05, eta: 3:38:05, time: 1.029, data_time: 0.030, memory: 17617, loss_cls: 0.0662, loss_bbox: 0.2060, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2074, loss: 1.9465, grad_norm: 17.6962
2025-06-10 17:24:11,891 - mmdet - INFO - Epoch [5][1600/7033]	lr: 2.508e-05, eta: 3:37:12, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0712, loss_bbox: 0.2012, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2029, loss: 1.9404, grad_norm: 34.8667
2025-06-10 17:25:03,160 - mmdet - INFO - Epoch [5][1650/7033]	lr: 2.508e-05, eta: 3:36:19, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0648, loss_bbox: 0.2045, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3178, d1.loss_cls: 0.0981, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2058, loss: 1.9486, grad_norm: 38.3963
2025-06-10 17:25:54,652 - mmdet - INFO - Epoch [5][1700/7033]	lr: 2.508e-05, eta: 3:35:27, time: 1.030, data_time: 0.033, memory: 17617, loss_cls: 0.0684, loss_bbox: 0.2005, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3111, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0735, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2019, loss: 1.9298, grad_norm: 27.5695
2025-06-10 17:26:45,923 - mmdet - INFO - Epoch [5][1750/7033]	lr: 2.508e-05, eta: 3:34:34, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0699, loss_bbox: 0.1985, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3030, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0720, d4.loss_bbox: 0.1989, loss: 1.9159, grad_norm: 23.5943
2025-06-10 17:27:37,280 - mmdet - INFO - Epoch [5][1800/7033]	lr: 2.508e-05, eta: 3:33:41, time: 1.027, data_time: 0.027, memory: 17617, loss_cls: 0.0682, loss_bbox: 0.2034, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3137, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2177, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2063, loss: 1.9507, grad_norm: 37.6338
2025-06-10 17:28:28,231 - mmdet - INFO - Epoch [5][1850/7033]	lr: 2.508e-05, eta: 3:32:49, time: 1.019, data_time: 0.027, memory: 17617, loss_cls: 0.0616, loss_bbox: 0.2003, d0.loss_cls: 0.1464, d0.loss_bbox: 0.3039, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0736, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0650, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0624, d4.loss_bbox: 0.2015, loss: 1.8657, grad_norm: 39.4508
2025-06-10 17:29:19,509 - mmdet - INFO - Epoch [5][1900/7033]	lr: 2.508e-05, eta: 3:31:56, time: 1.026, data_time: 0.030, memory: 17617, loss_cls: 0.0675, loss_bbox: 0.2109, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3169, d1.loss_cls: 0.0980, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2119, loss: 1.9841, grad_norm: 72.8612
2025-06-10 17:30:11,099 - mmdet - INFO - Epoch [5][1950/7033]	lr: 2.508e-05, eta: 3:31:03, time: 1.032, data_time: 0.031, memory: 17617, loss_cls: 0.0666, loss_bbox: 0.2083, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3149, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0685, d4.loss_bbox: 0.2087, loss: 1.9564, grad_norm: 19.9148
2025-06-10 17:31:02,697 - mmdet - INFO - Epoch [5][2000/7033]	lr: 2.508e-05, eta: 3:30:11, time: 1.032, data_time: 0.031, memory: 17617, loss_cls: 0.0643, loss_bbox: 0.2035, d0.loss_cls: 0.1533, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2051, loss: 1.9290, grad_norm: 34.9695
2025-06-10 17:31:54,264 - mmdet - INFO - Epoch [5][2050/7033]	lr: 2.508e-05, eta: 3:29:18, time: 1.031, data_time: 0.030, memory: 17617, loss_cls: 0.0661, loss_bbox: 0.2051, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3063, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0718, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2068, loss: 1.9278, grad_norm: 46.8271
2025-06-10 17:32:45,575 - mmdet - INFO - Epoch [5][2100/7033]	lr: 2.508e-05, eta: 3:28:26, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.2003, d0.loss_cls: 0.1541, d0.loss_bbox: 0.3224, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2023, loss: 1.9381, grad_norm: 168.9372
2025-06-10 17:33:36,649 - mmdet - INFO - Epoch [5][2150/7033]	lr: 2.508e-05, eta: 3:27:33, time: 1.021, data_time: 0.026, memory: 17617, loss_cls: 0.0649, loss_bbox: 0.2022, d0.loss_cls: 0.1540, d0.loss_bbox: 0.3181, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0783, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2043, loss: 1.9303, grad_norm: 19.2831
2025-06-10 17:34:27,584 - mmdet - INFO - Epoch [5][2200/7033]	lr: 2.508e-05, eta: 3:26:40, time: 1.019, data_time: 0.031, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.1984, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2005, loss: 1.9038, grad_norm: 59.8025
2025-06-10 17:35:18,709 - mmdet - INFO - Epoch [5][2250/7033]	lr: 2.508e-05, eta: 3:25:48, time: 1.023, data_time: 0.032, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.2037, d0.loss_cls: 0.1517, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0926, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2044, loss: 1.9058, grad_norm: 23.0693
2025-06-10 17:36:09,736 - mmdet - INFO - Epoch [5][2300/7033]	lr: 2.508e-05, eta: 3:24:55, time: 1.021, data_time: 0.032, memory: 17617, loss_cls: 0.0647, loss_bbox: 0.2031, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3208, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2046, loss: 1.9360, grad_norm: 59.4015
2025-06-10 17:37:00,969 - mmdet - INFO - Epoch [5][2350/7033]	lr: 2.508e-05, eta: 3:24:02, time: 1.025, data_time: 0.033, memory: 17617, loss_cls: 0.0629, loss_bbox: 0.2024, d0.loss_cls: 0.1472, d0.loss_bbox: 0.3082, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2028, loss: 1.8937, grad_norm: 35.0510
2025-06-10 17:37:51,975 - mmdet - INFO - Epoch [5][2400/7033]	lr: 2.508e-05, eta: 3:23:09, time: 1.020, data_time: 0.031, memory: 17617, loss_cls: 0.0655, loss_bbox: 0.1974, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3140, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0673, d4.loss_bbox: 0.1993, loss: 1.8931, grad_norm: 91.2855
2025-06-10 17:38:43,121 - mmdet - INFO - Epoch [5][2450/7033]	lr: 2.508e-05, eta: 3:22:17, time: 1.023, data_time: 0.029, memory: 17617, loss_cls: 0.0692, loss_bbox: 0.2095, d0.loss_cls: 0.1568, d0.loss_bbox: 0.3193, d1.loss_cls: 0.0976, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2096, loss: 1.9797, grad_norm: 47.7251
2025-06-10 17:39:34,838 - mmdet - INFO - Epoch [5][2500/7033]	lr: 2.508e-05, eta: 3:21:24, time: 1.034, data_time: 0.031, memory: 17617, loss_cls: 0.0600, loss_bbox: 0.1914, d0.loss_cls: 0.1382, d0.loss_bbox: 0.3002, d1.loss_cls: 0.0839, d1.loss_bbox: 0.2256, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0642, d3.loss_bbox: 0.2041, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1936, loss: 1.8034, grad_norm: 27.4818
2025-06-10 17:40:26,097 - mmdet - INFO - Epoch [5][2550/7033]	lr: 2.508e-05, eta: 3:20:32, time: 1.025, data_time: 0.030, memory: 17617, loss_cls: 0.0651, loss_bbox: 0.1952, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3059, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0684, d4.loss_bbox: 0.1966, loss: 1.8829, grad_norm: 27.0524
2025-06-10 17:41:18,277 - mmdet - INFO - Epoch [5][2600/7033]	lr: 2.508e-05, eta: 3:19:39, time: 1.044, data_time: 0.033, memory: 17617, loss_cls: 0.0747, loss_bbox: 0.2045, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3172, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2366, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0763, d4.loss_bbox: 0.2061, loss: 1.9913, grad_norm: 33.5588
2025-06-10 17:42:10,067 - mmdet - INFO - Epoch [5][2650/7033]	lr: 2.508e-05, eta: 3:18:47, time: 1.036, data_time: 0.030, memory: 17617, loss_cls: 0.0670, loss_bbox: 0.2028, d0.loss_cls: 0.1542, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0954, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0811, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2141, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2036, loss: 1.9220, grad_norm: 33.1510
2025-06-10 17:43:01,518 - mmdet - INFO - Epoch [5][2700/7033]	lr: 2.508e-05, eta: 3:17:55, time: 1.029, data_time: 0.029, memory: 17617, loss_cls: 0.0658, loss_bbox: 0.1986, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3102, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2180, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2139, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2007, loss: 1.9090, grad_norm: 20.5925
2025-06-10 17:43:52,800 - mmdet - INFO - Epoch [5][2750/7033]	lr: 2.508e-05, eta: 3:17:02, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0706, loss_bbox: 0.2031, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3130, d1.loss_cls: 0.0985, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2154, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2050, loss: 1.9600, grad_norm: 22.0055
2025-06-10 17:44:44,007 - mmdet - INFO - Epoch [5][2800/7033]	lr: 2.508e-05, eta: 3:16:09, time: 1.024, data_time: 0.030, memory: 17617, loss_cls: 0.0673, loss_bbox: 0.2048, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2067, loss: 1.9409, grad_norm: 24.1071
2025-06-10 17:45:35,632 - mmdet - INFO - Epoch [5][2850/7033]	lr: 2.508e-05, eta: 3:15:17, time: 1.033, data_time: 0.027, memory: 17617, loss_cls: 0.0715, loss_bbox: 0.2115, d0.loss_cls: 0.1595, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2223, d4.loss_cls: 0.0740, d4.loss_bbox: 0.2123, loss: 1.9907, grad_norm: 20.6523
2025-06-10 17:46:27,447 - mmdet - INFO - Epoch [5][2900/7033]	lr: 2.508e-05, eta: 3:14:25, time: 1.036, data_time: 0.027, memory: 17617, loss_cls: 0.0623, loss_bbox: 0.1908, d0.loss_cls: 0.1475, d0.loss_bbox: 0.3021, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2268, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1939, loss: 1.8343, grad_norm: 25.3114
2025-06-10 17:47:19,233 - mmdet - INFO - Epoch [5][2950/7033]	lr: 2.508e-05, eta: 3:13:32, time: 1.036, data_time: 0.033, memory: 17617, loss_cls: 0.0691, loss_bbox: 0.2119, d0.loss_cls: 0.1555, d0.loss_bbox: 0.3251, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2250, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2136, loss: 2.0071, grad_norm: 23.6912
2025-06-10 17:48:11,137 - mmdet - INFO - Epoch [5][3000/7033]	lr: 2.508e-05, eta: 3:12:40, time: 1.038, data_time: 0.029, memory: 17617, loss_cls: 0.0646, loss_bbox: 0.1973, d0.loss_cls: 0.1541, d0.loss_bbox: 0.3041, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0791, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1994, loss: 1.8826, grad_norm: 17.8584
2025-06-10 17:49:03,010 - mmdet - INFO - Epoch [5][3050/7033]	lr: 2.508e-05, eta: 3:11:47, time: 1.038, data_time: 0.029, memory: 17617, loss_cls: 0.0614, loss_bbox: 0.2040, d0.loss_cls: 0.1486, d0.loss_bbox: 0.3130, d1.loss_cls: 0.0901, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0674, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0646, d4.loss_bbox: 0.2046, loss: 1.9099, grad_norm: 55.2611
2025-06-10 17:49:54,621 - mmdet - INFO - Epoch [5][3100/7033]	lr: 2.508e-05, eta: 3:10:55, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0684, loss_bbox: 0.2101, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3222, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2488, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2111, loss: 1.9959, grad_norm: 21.2131
2025-06-10 17:50:45,887 - mmdet - INFO - Epoch [5][3150/7033]	lr: 2.508e-05, eta: 3:10:02, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0593, loss_bbox: 0.2031, d0.loss_cls: 0.1486, d0.loss_bbox: 0.3054, d1.loss_cls: 0.0899, d1.loss_bbox: 0.2309, d2.loss_cls: 0.0773, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0606, d4.loss_bbox: 0.2048, loss: 1.8774, grad_norm: 24.4709
2025-06-10 17:51:37,530 - mmdet - INFO - Epoch [5][3200/7033]	lr: 2.508e-05, eta: 3:09:10, time: 1.033, data_time: 0.028, memory: 17617, loss_cls: 0.0631, loss_bbox: 0.1952, d0.loss_cls: 0.1481, d0.loss_bbox: 0.3030, d1.loss_cls: 0.0930, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1973, loss: 1.8592, grad_norm: 35.4349
2025-06-10 17:52:29,413 - mmdet - INFO - Epoch [5][3250/7033]	lr: 2.508e-05, eta: 3:08:18, time: 1.038, data_time: 0.027, memory: 17617, loss_cls: 0.0635, loss_bbox: 0.2036, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0958, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2202, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2165, d4.loss_cls: 0.0648, d4.loss_bbox: 0.2061, loss: 1.9283, grad_norm: 81.1021
2025-06-10 17:53:20,961 - mmdet - INFO - Epoch [5][3300/7033]	lr: 2.508e-05, eta: 3:07:25, time: 1.031, data_time: 0.030, memory: 17617, loss_cls: 0.0696, loss_bbox: 0.2063, d0.loss_cls: 0.1563, d0.loss_bbox: 0.3167, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2240, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2083, loss: 1.9762, grad_norm: 260.4904
2025-06-10 17:54:12,269 - mmdet - INFO - Epoch [5][3350/7033]	lr: 2.508e-05, eta: 3:06:33, time: 1.026, data_time: 0.032, memory: 17617, loss_cls: 0.0636, loss_bbox: 0.2084, d0.loss_cls: 0.1515, d0.loss_bbox: 0.3191, d1.loss_cls: 0.0923, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2094, loss: 1.9430, grad_norm: 21.7436
2025-06-10 17:55:03,533 - mmdet - INFO - Epoch [5][3400/7033]	lr: 2.508e-05, eta: 3:05:40, time: 1.025, data_time: 0.029, memory: 17617, loss_cls: 0.0661, loss_bbox: 0.1981, d0.loss_cls: 0.1500, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2330, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0679, d4.loss_bbox: 0.1988, loss: 1.8928, grad_norm: 26.8197
2025-06-10 17:55:57,822 - mmdet - INFO - Epoch [5][3450/7033]	lr: 2.508e-05, eta: 3:04:49, time: 1.086, data_time: 0.030, memory: 17617, loss_cls: 0.0637, loss_bbox: 0.1982, d0.loss_cls: 0.1454, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0908, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2115, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1988, loss: 1.8769, grad_norm: 14.6870
2025-06-10 17:56:49,282 - mmdet - INFO - Epoch [5][3500/7033]	lr: 2.508e-05, eta: 3:03:56, time: 1.029, data_time: 0.031, memory: 17617, loss_cls: 0.0663, loss_bbox: 0.1969, d0.loss_cls: 0.1444, d0.loss_bbox: 0.3019, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0815, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0695, d4.loss_bbox: 0.1979, loss: 1.8777, grad_norm: 19.5593
2025-06-10 17:57:40,785 - mmdet - INFO - Epoch [5][3550/7033]	lr: 2.508e-05, eta: 3:03:04, time: 1.030, data_time: 0.029, memory: 17617, loss_cls: 0.0643, loss_bbox: 0.1982, d0.loss_cls: 0.1472, d0.loss_bbox: 0.3060, d1.loss_cls: 0.0922, d1.loss_bbox: 0.2299, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1992, loss: 1.8754, grad_norm: 29.5309
2025-06-10 17:58:32,463 - mmdet - INFO - Epoch [5][3600/7033]	lr: 2.508e-05, eta: 3:02:11, time: 1.034, data_time: 0.033, memory: 17617, loss_cls: 0.0636, loss_bbox: 0.1985, d0.loss_cls: 0.1547, d0.loss_bbox: 0.3001, d1.loss_cls: 0.0967, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1990, loss: 1.8733, grad_norm: 23.7360
2025-06-10 17:59:23,988 - mmdet - INFO - Epoch [5][3650/7033]	lr: 2.508e-05, eta: 3:01:19, time: 1.030, data_time: 0.028, memory: 17617, loss_cls: 0.0681, loss_bbox: 0.1975, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0951, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0712, d4.loss_bbox: 0.1988, loss: 1.9015, grad_norm: 17.6024
2025-06-10 18:00:15,362 - mmdet - INFO - Epoch [5][3700/7033]	lr: 2.508e-05, eta: 3:00:26, time: 1.027, data_time: 0.027, memory: 17617, loss_cls: 0.0627, loss_bbox: 0.1960, d0.loss_cls: 0.1523, d0.loss_bbox: 0.3093, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2123, d3.loss_cls: 0.0676, d3.loss_bbox: 0.2088, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1993, loss: 1.8698, grad_norm: 48.5011
2025-06-10 18:01:07,343 - mmdet - INFO - Epoch [5][3750/7033]	lr: 2.508e-05, eta: 2:59:34, time: 1.040, data_time: 0.029, memory: 17617, loss_cls: 0.0647, loss_bbox: 0.2083, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3159, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2453, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2111, loss: 1.9621, grad_norm: 23.2003
2025-06-10 18:01:59,011 - mmdet - INFO - Epoch [5][3800/7033]	lr: 2.508e-05, eta: 2:58:42, time: 1.033, data_time: 0.026, memory: 17617, loss_cls: 0.0704, loss_bbox: 0.1951, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3016, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2284, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2112, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0725, d4.loss_bbox: 0.1965, loss: 1.8933, grad_norm: 23.6109
2025-06-10 18:02:55,534 - mmdet - INFO - Epoch [5][3850/7033]	lr: 2.508e-05, eta: 2:57:51, time: 1.130, data_time: 0.124, memory: 17617, loss_cls: 0.0596, loss_bbox: 0.1904, d0.loss_cls: 0.1498, d0.loss_bbox: 0.2983, d1.loss_cls: 0.0875, d1.loss_bbox: 0.2248, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2078, d3.loss_cls: 0.0633, d3.loss_bbox: 0.2039, d4.loss_cls: 0.0613, d4.loss_bbox: 0.1927, loss: 1.8120, grad_norm: 38.8887
2025-06-10 18:03:56,376 - mmdet - INFO - Epoch [5][3900/7033]	lr: 2.508e-05, eta: 2:57:01, time: 1.217, data_time: 0.214, memory: 17617, loss_cls: 0.0715, loss_bbox: 0.2005, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3130, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2135, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2027, loss: 1.9473, grad_norm: 24.2463
2025-06-10 18:04:47,953 - mmdet - INFO - Epoch [5][3950/7033]	lr: 2.508e-05, eta: 2:56:09, time: 1.032, data_time: 0.028, memory: 17617, loss_cls: 0.0634, loss_bbox: 0.1944, d0.loss_cls: 0.1476, d0.loss_bbox: 0.3056, d1.loss_cls: 0.0887, d1.loss_bbox: 0.2302, d2.loss_cls: 0.0763, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0648, d4.loss_bbox: 0.1963, loss: 1.8519, grad_norm: 24.1344
2025-06-10 18:05:39,425 - mmdet - INFO - Epoch [5][4000/7033]	lr: 2.508e-05, eta: 2:55:16, time: 1.029, data_time: 0.026, memory: 17617, loss_cls: 0.0651, loss_bbox: 0.1957, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3033, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2289, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2129, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2095, d4.loss_cls: 0.0671, d4.loss_bbox: 0.1972, loss: 1.8751, grad_norm: 18.0555
2025-06-10 18:06:31,185 - mmdet - INFO - Epoch [5][4050/7033]	lr: 2.508e-05, eta: 2:54:24, time: 1.035, data_time: 0.029, memory: 17617, loss_cls: 0.0671, loss_bbox: 0.2006, d0.loss_cls: 0.1514, d0.loss_bbox: 0.3074, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2169, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2029, loss: 1.9052, grad_norm: 31.8252
2025-06-10 18:07:22,779 - mmdet - INFO - Epoch [5][4100/7033]	lr: 2.508e-05, eta: 2:53:31, time: 1.032, data_time: 0.030, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.2066, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3146, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2408, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0690, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2076, loss: 1.9328, grad_norm: 22.2787
2025-06-10 18:08:14,587 - mmdet - INFO - Epoch [5][4150/7033]	lr: 2.508e-05, eta: 2:52:39, time: 1.036, data_time: 0.031, memory: 17617, loss_cls: 0.0625, loss_bbox: 0.2033, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3171, d1.loss_cls: 0.0956, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0794, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0650, d4.loss_bbox: 0.2056, loss: 1.9277, grad_norm: 26.6408
2025-06-10 18:09:06,306 - mmdet - INFO - Epoch [5][4200/7033]	lr: 2.508e-05, eta: 2:51:47, time: 1.034, data_time: 0.028, memory: 17617, loss_cls: 0.0723, loss_bbox: 0.2018, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3146, d1.loss_cls: 0.1009, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0859, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0764, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2035, loss: 1.9593, grad_norm: 19.1210
2025-06-10 18:09:58,019 - mmdet - INFO - Epoch [5][4250/7033]	lr: 2.508e-05, eta: 2:50:54, time: 1.034, data_time: 0.029, memory: 17617, loss_cls: 0.0652, loss_bbox: 0.2059, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3082, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0653, d4.loss_bbox: 0.2079, loss: 1.9280, grad_norm: 21.9751
2025-06-10 18:10:49,827 - mmdet - INFO - Epoch [5][4300/7033]	lr: 2.508e-05, eta: 2:50:02, time: 1.036, data_time: 0.033, memory: 17617, loss_cls: 0.0637, loss_bbox: 0.1926, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2256, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2101, d3.loss_cls: 0.0704, d3.loss_bbox: 0.2056, d4.loss_cls: 0.0669, d4.loss_bbox: 0.1940, loss: 1.8527, grad_norm: 19.7482
2025-06-10 18:11:42,063 - mmdet - INFO - Epoch [5][4350/7033]	lr: 2.508e-05, eta: 2:49:10, time: 1.045, data_time: 0.034, memory: 17617, loss_cls: 0.0564, loss_bbox: 0.1899, d0.loss_cls: 0.1436, d0.loss_bbox: 0.2925, d1.loss_cls: 0.0830, d1.loss_bbox: 0.2209, d2.loss_cls: 0.0693, d2.loss_bbox: 0.2068, d3.loss_cls: 0.0622, d3.loss_bbox: 0.2032, d4.loss_cls: 0.0585, d4.loss_bbox: 0.1915, loss: 1.7779, grad_norm: 21.3163
2025-06-10 18:12:33,700 - mmdet - INFO - Epoch [5][4400/7033]	lr: 2.508e-05, eta: 2:48:17, time: 1.033, data_time: 0.030, memory: 17617, loss_cls: 0.0715, loss_bbox: 0.1994, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3012, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2130, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2017, loss: 1.9278, grad_norm: 27.9161
2025-06-10 18:13:25,630 - mmdet - INFO - Epoch [5][4450/7033]	lr: 2.508e-05, eta: 2:47:25, time: 1.039, data_time: 0.031, memory: 17617, loss_cls: 0.0693, loss_bbox: 0.2031, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3035, d1.loss_cls: 0.0957, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0817, d2.loss_bbox: 0.2191, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2045, loss: 1.9189, grad_norm: 18.9430
2025-06-10 18:14:17,717 - mmdet - INFO - Epoch [5][4500/7033]	lr: 2.508e-05, eta: 2:46:33, time: 1.042, data_time: 0.031, memory: 17617, loss_cls: 0.0619, loss_bbox: 0.1947, d0.loss_cls: 0.1455, d0.loss_bbox: 0.3004, d1.loss_cls: 0.0874, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2101, d3.loss_cls: 0.0690, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0647, d4.loss_bbox: 0.1958, loss: 1.8360, grad_norm: 19.8161
2025-06-10 18:15:09,797 - mmdet - INFO - Epoch [5][4550/7033]	lr: 2.508e-05, eta: 2:45:40, time: 1.042, data_time: 0.032, memory: 17617, loss_cls: 0.0623, loss_bbox: 0.1999, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3108, d1.loss_cls: 0.0897, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0642, d4.loss_bbox: 0.2021, loss: 1.8934, grad_norm: 26.9700
2025-06-10 18:16:01,934 - mmdet - INFO - Epoch [5][4600/7033]	lr: 2.508e-05, eta: 2:44:48, time: 1.043, data_time: 0.032, memory: 17617, loss_cls: 0.0654, loss_bbox: 0.2017, d0.loss_cls: 0.1457, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0929, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2038, loss: 1.9020, grad_norm: 107.5337
2025-06-10 18:16:54,117 - mmdet - INFO - Epoch [5][4650/7033]	lr: 2.508e-05, eta: 2:43:56, time: 1.044, data_time: 0.034, memory: 17617, loss_cls: 0.0619, loss_bbox: 0.1972, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2333, d2.loss_cls: 0.0744, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1996, loss: 1.8706, grad_norm: 18.4092
2025-06-10 18:17:45,708 - mmdet - INFO - Epoch [5][4700/7033]	lr: 2.508e-05, eta: 2:43:03, time: 1.032, data_time: 0.029, memory: 17617, loss_cls: 0.0642, loss_bbox: 0.2005, d0.loss_cls: 0.1515, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2132, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2025, loss: 1.8965, grad_norm: 25.8105
2025-06-10 18:18:37,256 - mmdet - INFO - Epoch [5][4750/7033]	lr: 2.508e-05, eta: 2:42:11, time: 1.031, data_time: 0.032, memory: 17617, loss_cls: 0.0636, loss_bbox: 0.1998, d0.loss_cls: 0.1519, d0.loss_bbox: 0.2999, d1.loss_cls: 0.0926, d1.loss_bbox: 0.2303, d2.loss_cls: 0.0773, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1999, loss: 1.8769, grad_norm: 81.0981
2025-06-10 18:19:29,038 - mmdet - INFO - Epoch [5][4800/7033]	lr: 2.508e-05, eta: 2:41:19, time: 1.036, data_time: 0.033, memory: 17617, loss_cls: 0.0660, loss_bbox: 0.1979, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3106, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2316, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0683, d4.loss_bbox: 0.1987, loss: 1.9003, grad_norm: 56.6523
2025-06-10 18:20:20,748 - mmdet - INFO - Epoch [5][4850/7033]	lr: 2.508e-05, eta: 2:40:26, time: 1.034, data_time: 0.032, memory: 17617, loss_cls: 0.0664, loss_bbox: 0.2048, d0.loss_cls: 0.1550, d0.loss_bbox: 0.3147, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0667, d4.loss_bbox: 0.2069, loss: 1.9386, grad_norm: 17.3128
2025-06-10 18:21:12,765 - mmdet - INFO - Epoch [5][4900/7033]	lr: 2.508e-05, eta: 2:39:34, time: 1.040, data_time: 0.034, memory: 17617, loss_cls: 0.0663, loss_bbox: 0.1991, d0.loss_cls: 0.1568, d0.loss_bbox: 0.3095, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0807, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2016, loss: 1.9106, grad_norm: 30.2308
2025-06-10 18:22:04,676 - mmdet - INFO - Epoch [5][4950/7033]	lr: 2.508e-05, eta: 2:38:42, time: 1.038, data_time: 0.034, memory: 17617, loss_cls: 0.0624, loss_bbox: 0.1915, d0.loss_cls: 0.1511, d0.loss_bbox: 0.3085, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2287, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2088, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2054, d4.loss_cls: 0.0655, d4.loss_bbox: 0.1925, loss: 1.8483, grad_norm: 27.7461
2025-06-10 18:22:56,444 - mmdet - INFO - Epoch [5][5000/7033]	lr: 2.508e-05, eta: 2:37:49, time: 1.035, data_time: 0.034, memory: 17617, loss_cls: 0.0543, loss_bbox: 0.1936, d0.loss_cls: 0.1403, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0842, d1.loss_bbox: 0.2256, d2.loss_cls: 0.0698, d2.loss_bbox: 0.2100, d3.loss_cls: 0.0606, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0567, d4.loss_bbox: 0.1952, loss: 1.7987, grad_norm: 56.6454
2025-06-10 18:23:48,090 - mmdet - INFO - Epoch [5][5050/7033]	lr: 2.508e-05, eta: 2:36:57, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.0596, loss_bbox: 0.1970, d0.loss_cls: 0.1490, d0.loss_bbox: 0.2970, d1.loss_cls: 0.0890, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0721, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1991, loss: 1.8385, grad_norm: 34.0686
2025-06-10 18:24:39,749 - mmdet - INFO - Epoch [5][5100/7033]	lr: 2.508e-05, eta: 2:36:05, time: 1.033, data_time: 0.030, memory: 17617, loss_cls: 0.0685, loss_bbox: 0.2022, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3153, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0701, d4.loss_bbox: 0.2034, loss: 1.9520, grad_norm: 41.4703
2025-06-10 18:25:34,522 - mmdet - INFO - Epoch [5][5150/7033]	lr: 2.508e-05, eta: 2:35:13, time: 1.095, data_time: 0.032, memory: 17617, loss_cls: 0.0668, loss_bbox: 0.2001, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3069, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2130, d4.loss_cls: 0.0669, d4.loss_bbox: 0.2022, loss: 1.9035, grad_norm: 17.4879
2025-06-10 18:26:26,449 - mmdet - INFO - Epoch [5][5200/7033]	lr: 2.508e-05, eta: 2:34:21, time: 1.039, data_time: 0.035, memory: 17617, loss_cls: 0.0644, loss_bbox: 0.1992, d0.loss_cls: 0.1554, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2126, d4.loss_cls: 0.0660, d4.loss_bbox: 0.2004, loss: 1.8988, grad_norm: 22.7312
2025-06-10 18:27:19,117 - mmdet - INFO - Epoch [5][5250/7033]	lr: 2.508e-05, eta: 2:33:29, time: 1.053, data_time: 0.053, memory: 17617, loss_cls: 0.0616, loss_bbox: 0.2027, d0.loss_cls: 0.1560, d0.loss_bbox: 0.3162, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0664, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0633, d4.loss_bbox: 0.2049, loss: 1.9121, grad_norm: 53.9778
2025-06-10 18:28:11,117 - mmdet - INFO - Epoch [5][5300/7033]	lr: 2.508e-05, eta: 2:32:36, time: 1.040, data_time: 0.035, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.1949, d0.loss_cls: 0.1473, d0.loss_bbox: 0.3061, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2310, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2069, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1979, loss: 1.8518, grad_norm: 19.1087
2025-06-10 18:29:02,668 - mmdet - INFO - Epoch [5][5350/7033]	lr: 2.508e-05, eta: 2:31:44, time: 1.031, data_time: 0.033, memory: 17617, loss_cls: 0.0743, loss_bbox: 0.2136, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3239, d1.loss_cls: 0.1005, d1.loss_bbox: 0.2481, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2152, loss: 2.0389, grad_norm: 20.6042
2025-06-10 18:29:54,523 - mmdet - INFO - Epoch [5][5400/7033]	lr: 2.508e-05, eta: 2:30:52, time: 1.037, data_time: 0.033, memory: 17617, loss_cls: 0.0641, loss_bbox: 0.1975, d0.loss_cls: 0.1531, d0.loss_bbox: 0.3087, d1.loss_cls: 0.0924, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1991, loss: 1.8798, grad_norm: 25.6022
2025-06-10 18:30:46,628 - mmdet - INFO - Epoch [5][5450/7033]	lr: 2.508e-05, eta: 2:29:59, time: 1.042, data_time: 0.031, memory: 17617, loss_cls: 0.0665, loss_bbox: 0.2028, d0.loss_cls: 0.1533, d0.loss_bbox: 0.3113, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2151, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2031, loss: 1.9243, grad_norm: 45.5978
2025-06-10 18:31:38,911 - mmdet - INFO - Epoch [5][5500/7033]	lr: 2.508e-05, eta: 2:29:07, time: 1.046, data_time: 0.033, memory: 17617, loss_cls: 0.0686, loss_bbox: 0.2055, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3155, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2192, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2072, loss: 1.9608, grad_norm: 34.3032
2025-06-10 18:32:30,662 - mmdet - INFO - Epoch [5][5550/7033]	lr: 2.508e-05, eta: 2:28:15, time: 1.035, data_time: 0.033, memory: 17617, loss_cls: 0.0622, loss_bbox: 0.1953, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3083, d1.loss_cls: 0.0875, d1.loss_bbox: 0.2289, d2.loss_cls: 0.0752, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2100, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1968, loss: 1.8627, grad_norm: 32.7000
2025-06-10 18:33:22,673 - mmdet - INFO - Epoch [5][5600/7033]	lr: 2.508e-05, eta: 2:27:22, time: 1.040, data_time: 0.033, memory: 17617, loss_cls: 0.0634, loss_bbox: 0.1973, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3066, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0695, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0667, d4.loss_bbox: 0.1982, loss: 1.8884, grad_norm: 35.7503
2025-06-10 18:34:14,909 - mmdet - INFO - Epoch [5][5650/7033]	lr: 2.508e-05, eta: 2:26:30, time: 1.045, data_time: 0.034, memory: 17617, loss_cls: 0.0644, loss_bbox: 0.1982, d0.loss_cls: 0.1543, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2304, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0658, d4.loss_bbox: 0.2003, loss: 1.8858, grad_norm: 27.9504
2025-06-10 18:35:06,793 - mmdet - INFO - Epoch [5][5700/7033]	lr: 2.508e-05, eta: 2:25:38, time: 1.038, data_time: 0.035, memory: 17617, loss_cls: 0.0677, loss_bbox: 0.2068, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2217, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2085, loss: 1.9402, grad_norm: 23.4228
2025-06-10 18:35:58,785 - mmdet - INFO - Epoch [5][5750/7033]	lr: 2.508e-05, eta: 2:24:46, time: 1.040, data_time: 0.033, memory: 17617, loss_cls: 0.0717, loss_bbox: 0.1995, d0.loss_cls: 0.1540, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0771, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2007, loss: 1.9340, grad_norm: 236.5364
2025-06-10 18:36:50,612 - mmdet - INFO - Epoch [5][5800/7033]	lr: 2.508e-05, eta: 2:23:53, time: 1.037, data_time: 0.031, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.1886, d0.loss_cls: 0.1445, d0.loss_bbox: 0.2980, d1.loss_cls: 0.0885, d1.loss_bbox: 0.2230, d2.loss_cls: 0.0729, d2.loss_bbox: 0.2043, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2016, d4.loss_cls: 0.0627, d4.loss_bbox: 0.1902, loss: 1.7994, grad_norm: 65.4753
2025-06-10 18:37:42,361 - mmdet - INFO - Epoch [5][5850/7033]	lr: 2.508e-05, eta: 2:23:01, time: 1.035, data_time: 0.031, memory: 17617, loss_cls: 0.0718, loss_bbox: 0.2026, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3212, d1.loss_cls: 0.1052, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2072, loss: 1.9893, grad_norm: 20.1808
2025-06-10 18:38:34,298 - mmdet - INFO - Epoch [5][5900/7033]	lr: 2.508e-05, eta: 2:22:09, time: 1.039, data_time: 0.030, memory: 17617, loss_cls: 0.0657, loss_bbox: 0.1982, d0.loss_cls: 0.1544, d0.loss_bbox: 0.3078, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0707, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2002, loss: 1.8997, grad_norm: 42.0454
2025-06-10 18:39:26,247 - mmdet - INFO - Epoch [5][5950/7033]	lr: 2.508e-05, eta: 2:21:16, time: 1.039, data_time: 0.034, memory: 17617, loss_cls: 0.0660, loss_bbox: 0.1970, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0676, d4.loss_bbox: 0.1988, loss: 1.9026, grad_norm: 24.4313
2025-06-10 18:40:17,831 - mmdet - INFO - Epoch [5][6000/7033]	lr: 2.508e-05, eta: 2:20:24, time: 1.032, data_time: 0.032, memory: 17617, loss_cls: 0.0665, loss_bbox: 0.1961, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2288, d2.loss_cls: 0.0814, d2.loss_bbox: 0.2125, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2080, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1965, loss: 1.8774, grad_norm: 37.1922
2025-06-10 18:41:09,492 - mmdet - INFO - Epoch [5][6050/7033]	lr: 2.508e-05, eta: 2:19:32, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0598, loss_bbox: 0.1978, d0.loss_cls: 0.1468, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0884, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0632, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1991, loss: 1.8547, grad_norm: 26.4046
2025-06-10 18:42:00,914 - mmdet - INFO - Epoch [5][6100/7033]	lr: 2.508e-05, eta: 2:18:39, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0643, loss_bbox: 0.2035, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3097, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2161, d4.loss_cls: 0.0653, d4.loss_bbox: 0.2048, loss: 1.9133, grad_norm: 15.9045
2025-06-10 18:42:52,571 - mmdet - INFO - Epoch [5][6150/7033]	lr: 2.508e-05, eta: 2:17:47, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0702, loss_bbox: 0.2086, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3151, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2100, loss: 1.9731, grad_norm: 19.3669
2025-06-10 18:43:43,849 - mmdet - INFO - Epoch [5][6200/7033]	lr: 2.508e-05, eta: 2:16:54, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0631, loss_bbox: 0.2005, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0887, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0754, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0647, d4.loss_bbox: 0.2016, loss: 1.8880, grad_norm: 70.8509
2025-06-10 18:44:35,499 - mmdet - INFO - Epoch [5][6250/7033]	lr: 2.508e-05, eta: 2:16:02, time: 1.033, data_time: 0.033, memory: 17617, loss_cls: 0.0648, loss_bbox: 0.2021, d0.loss_cls: 0.1515, d0.loss_bbox: 0.3073, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2359, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2161, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2036, loss: 1.9116, grad_norm: 18.5846
2025-06-10 18:45:26,926 - mmdet - INFO - Epoch [5][6300/7033]	lr: 2.508e-05, eta: 2:15:10, time: 1.029, data_time: 0.034, memory: 17617, loss_cls: 0.0647, loss_bbox: 0.2004, d0.loss_cls: 0.1492, d0.loss_bbox: 0.2945, d1.loss_cls: 0.0931, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2010, loss: 1.8738, grad_norm: 33.3008
2025-06-10 18:46:19,186 - mmdet - INFO - Epoch [5][6350/7033]	lr: 2.508e-05, eta: 2:14:18, time: 1.045, data_time: 0.053, memory: 17617, loss_cls: 0.0640, loss_bbox: 0.1991, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0663, d4.loss_bbox: 0.2012, loss: 1.8842, grad_norm: 21.2712
2025-06-10 18:47:10,751 - mmdet - INFO - Epoch [5][6400/7033]	lr: 2.508e-05, eta: 2:13:25, time: 1.031, data_time: 0.033, memory: 17617, loss_cls: 0.0662, loss_bbox: 0.1960, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2072, d4.loss_cls: 0.0687, d4.loss_bbox: 0.1968, loss: 1.8820, grad_norm: 60.3001
2025-06-10 18:48:02,497 - mmdet - INFO - Epoch [5][6450/7033]	lr: 2.508e-05, eta: 2:12:33, time: 1.035, data_time: 0.034, memory: 17617, loss_cls: 0.0641, loss_bbox: 0.1929, d0.loss_cls: 0.1461, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0891, d1.loss_bbox: 0.2317, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1942, loss: 1.8520, grad_norm: 19.2394
2025-06-10 18:48:54,298 - mmdet - INFO - Epoch [5][6500/7033]	lr: 2.508e-05, eta: 2:11:41, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0650, loss_bbox: 0.1982, d0.loss_cls: 0.1530, d0.loss_bbox: 0.3157, d1.loss_cls: 0.0973, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0809, d2.loss_bbox: 0.2167, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0674, d4.loss_bbox: 0.2001, loss: 1.9131, grad_norm: 40.7361
2025-06-10 18:49:45,595 - mmdet - INFO - Epoch [5][6550/7033]	lr: 2.508e-05, eta: 2:10:48, time: 1.026, data_time: 0.031, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.1965, d0.loss_cls: 0.1492, d0.loss_bbox: 0.3094, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2309, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0684, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0654, d4.loss_bbox: 0.1976, loss: 1.8708, grad_norm: 18.7863
2025-06-10 18:50:37,171 - mmdet - INFO - Epoch [5][6600/7033]	lr: 2.508e-05, eta: 2:09:56, time: 1.031, data_time: 0.032, memory: 17617, loss_cls: 0.0623, loss_bbox: 0.2088, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3207, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0780, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0647, d4.loss_bbox: 0.2109, loss: 1.9505, grad_norm: 24.1813
2025-06-10 18:51:28,535 - mmdet - INFO - Epoch [5][6650/7033]	lr: 2.508e-05, eta: 2:09:03, time: 1.027, data_time: 0.032, memory: 17617, loss_cls: 0.0617, loss_bbox: 0.1973, d0.loss_cls: 0.1509, d0.loss_bbox: 0.3062, d1.loss_cls: 0.0932, d1.loss_bbox: 0.2305, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2139, d3.loss_cls: 0.0675, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1996, loss: 1.8714, grad_norm: 23.6977
2025-06-10 18:52:20,467 - mmdet - INFO - Epoch [5][6700/7033]	lr: 2.508e-05, eta: 2:08:11, time: 1.039, data_time: 0.035, memory: 17617, loss_cls: 0.0649, loss_bbox: 0.2016, d0.loss_cls: 0.1441, d0.loss_bbox: 0.3118, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2219, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0666, d4.loss_bbox: 0.2043, loss: 1.9009, grad_norm: 22.4621
2025-06-10 18:53:11,928 - mmdet - INFO - Epoch [5][6750/7033]	lr: 2.508e-05, eta: 2:07:19, time: 1.029, data_time: 0.033, memory: 17617, loss_cls: 0.0553, loss_bbox: 0.1949, d0.loss_cls: 0.1461, d0.loss_bbox: 0.3007, d1.loss_cls: 0.0818, d1.loss_bbox: 0.2260, d2.loss_cls: 0.0669, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0590, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0569, d4.loss_bbox: 0.1968, loss: 1.8047, grad_norm: 23.6835
2025-06-10 18:54:03,594 - mmdet - INFO - Epoch [5][6800/7033]	lr: 2.508e-05, eta: 2:06:26, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0646, loss_bbox: 0.2041, d0.loss_cls: 0.1514, d0.loss_bbox: 0.3147, d1.loss_cls: 0.0915, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2157, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2059, loss: 1.9252, grad_norm: 24.6666
2025-06-10 18:54:55,451 - mmdet - INFO - Epoch [5][6850/7033]	lr: 2.508e-05, eta: 2:05:34, time: 1.037, data_time: 0.031, memory: 17617, loss_cls: 0.0662, loss_bbox: 0.2059, d0.loss_cls: 0.1518, d0.loss_bbox: 0.3135, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0712, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0670, d4.loss_bbox: 0.2089, loss: 1.9443, grad_norm: 28.6739
2025-06-10 18:55:47,075 - mmdet - INFO - Epoch [5][6900/7033]	lr: 2.508e-05, eta: 2:04:42, time: 1.032, data_time: 0.027, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.2011, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3095, d1.loss_cls: 0.0963, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2169, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2139, d4.loss_cls: 0.0655, d4.loss_bbox: 0.2020, loss: 1.9058, grad_norm: 25.0344
2025-06-10 18:56:39,339 - mmdet - INFO - Epoch [5][6950/7033]	lr: 2.508e-05, eta: 2:03:50, time: 1.045, data_time: 0.032, memory: 17617, loss_cls: 0.0665, loss_bbox: 0.2021, d0.loss_cls: 0.1487, d0.loss_bbox: 0.3110, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0671, d4.loss_bbox: 0.2049, loss: 1.9197, grad_norm: 28.3973
2025-06-10 18:57:30,982 - mmdet - INFO - Epoch [5][7000/7033]	lr: 2.508e-05, eta: 2:02:57, time: 1.033, data_time: 0.029, memory: 17617, loss_cls: 0.0676, loss_bbox: 0.2010, d0.loss_cls: 0.1517, d0.loss_bbox: 0.3064, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2032, loss: 1.9151, grad_norm: 223.1840
2025-06-10 18:58:05,361 - mmdet - INFO - Saving checkpoint at 5 epochs
2025-06-10 19:26:47,607 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 19:26:47,607 - mmdet - INFO - Epoch(val) [5][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7912, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8806, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9083, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9206, pts_bbox_NuScenes/car_trans_err: 0.1764, pts_bbox_NuScenes/car_scale_err: 0.1498, pts_bbox_NuScenes/car_orient_err: 0.0417, pts_bbox_NuScenes/car_vel_err: 0.3524, pts_bbox_NuScenes/car_attr_err: 0.1834, pts_bbox_NuScenes/mATE: 0.2893, pts_bbox_NuScenes/mASE: 0.2637, pts_bbox_NuScenes/mAOE: 0.2617, pts_bbox_NuScenes/mAVE: 0.3032, pts_bbox_NuScenes/mAAE: 0.1842, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4168, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6120, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7182, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7539, pts_bbox_NuScenes/truck_trans_err: 0.3453, pts_bbox_NuScenes/truck_scale_err: 0.1998, pts_bbox_NuScenes/truck_orient_err: 0.0483, pts_bbox_NuScenes/truck_vel_err: 0.2957, pts_bbox_NuScenes/truck_attr_err: 0.2035, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0528, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2019, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3954, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4662, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6674, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4397, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8125, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1221, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2979, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5264, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7591, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9023, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9230, pts_bbox_NuScenes/bus_trans_err: 0.3280, pts_bbox_NuScenes/bus_scale_err: 0.1936, pts_bbox_NuScenes/bus_orient_err: 0.0533, pts_bbox_NuScenes/bus_vel_err: 0.5032, pts_bbox_NuScenes/bus_attr_err: 0.2991, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1798, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4182, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5936, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6753, pts_bbox_NuScenes/trailer_trans_err: 0.4821, pts_bbox_NuScenes/trailer_scale_err: 0.2281, pts_bbox_NuScenes/trailer_orient_err: 0.4933, pts_bbox_NuScenes/trailer_vel_err: 0.2582, pts_bbox_NuScenes/trailer_attr_err: 0.1678, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6130, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7171, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7645, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7776, pts_bbox_NuScenes/barrier_trans_err: 0.2150, pts_bbox_NuScenes/barrier_scale_err: 0.2867, pts_bbox_NuScenes/barrier_orient_err: 0.0473, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6528, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7680, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8031, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8083, pts_bbox_NuScenes/motorcycle_trans_err: 0.2152, pts_bbox_NuScenes/motorcycle_scale_err: 0.2524, pts_bbox_NuScenes/motorcycle_orient_err: 0.2304, pts_bbox_NuScenes/motorcycle_vel_err: 0.4448, pts_bbox_NuScenes/motorcycle_attr_err: 0.2059, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5311, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5896, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5996, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6097, pts_bbox_NuScenes/bicycle_trans_err: 0.1775, pts_bbox_NuScenes/bicycle_scale_err: 0.2646, pts_bbox_NuScenes/bicycle_orient_err: 0.3069, pts_bbox_NuScenes/bicycle_vel_err: 0.2216, pts_bbox_NuScenes/bicycle_attr_err: 0.0025, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8161, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8581, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8781, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8896, pts_bbox_NuScenes/pedestrian_trans_err: 0.1478, pts_bbox_NuScenes/pedestrian_scale_err: 0.2935, pts_bbox_NuScenes/pedestrian_orient_err: 0.3214, pts_bbox_NuScenes/pedestrian_vel_err: 0.2275, pts_bbox_NuScenes/pedestrian_attr_err: 0.1132, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7318, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7708, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7962, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8172, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1379, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3290, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7059, pts_bbox_NuScenes/mAP: 0.6722
2025-06-10 19:27:49,252 - mmdet - INFO - Epoch [6][50/7033]	lr: 6.792e-06, eta: 2:01:25, time: 1.138, data_time: 0.130, memory: 17617, loss_cls: 0.0666, loss_bbox: 0.1997, d0.loss_cls: 0.1465, d0.loss_bbox: 0.3203, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2138, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2021, loss: 1.9254, grad_norm: 45.8834
2025-06-10 19:28:40,364 - mmdet - INFO - Epoch [6][100/7033]	lr: 6.792e-06, eta: 2:00:32, time: 1.022, data_time: 0.031, memory: 17617, loss_cls: 0.0640, loss_bbox: 0.2007, d0.loss_cls: 0.1459, d0.loss_bbox: 0.3090, d1.loss_cls: 0.0908, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2174, d3.loss_cls: 0.0703, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2027, loss: 1.8929, grad_norm: 31.6324
2025-06-10 19:29:31,579 - mmdet - INFO - Epoch [6][150/7033]	lr: 6.792e-06, eta: 1:59:40, time: 1.024, data_time: 0.029, memory: 17617, loss_cls: 0.0634, loss_bbox: 0.1998, d0.loss_cls: 0.1471, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0952, d1.loss_bbox: 0.2313, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0660, d4.loss_bbox: 0.2014, loss: 1.8837, grad_norm: 25.0826
2025-06-10 19:30:22,980 - mmdet - INFO - Epoch [6][200/7033]	lr: 6.792e-06, eta: 1:58:48, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0710, loss_bbox: 0.2013, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3107, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2139, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2037, loss: 1.9463, grad_norm: 20.6781
2025-06-10 19:31:14,422 - mmdet - INFO - Epoch [6][250/7033]	lr: 6.792e-06, eta: 1:57:55, time: 1.029, data_time: 0.031, memory: 17617, loss_cls: 0.0603, loss_bbox: 0.1891, d0.loss_cls: 0.1378, d0.loss_bbox: 0.2987, d1.loss_cls: 0.0874, d1.loss_bbox: 0.2225, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2012, d4.loss_cls: 0.0628, d4.loss_bbox: 0.1913, loss: 1.7959, grad_norm: 17.3697
2025-06-10 19:32:05,969 - mmdet - INFO - Epoch [6][300/7033]	lr: 6.792e-06, eta: 1:57:03, time: 1.031, data_time: 0.030, memory: 17617, loss_cls: 0.0581, loss_bbox: 0.1969, d0.loss_cls: 0.1534, d0.loss_bbox: 0.3022, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2239, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0609, d4.loss_bbox: 0.1985, loss: 1.8428, grad_norm: 68.7908
2025-06-10 19:33:00,467 - mmdet - INFO - Epoch [6][350/7033]	lr: 6.792e-06, eta: 1:56:11, time: 1.090, data_time: 0.034, memory: 17617, loss_cls: 0.0583, loss_bbox: 0.1963, d0.loss_cls: 0.1449, d0.loss_bbox: 0.3089, d1.loss_cls: 0.0864, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0710, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0623, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0596, d4.loss_bbox: 0.1997, loss: 1.8456, grad_norm: 27.1759
2025-06-10 19:33:51,753 - mmdet - INFO - Epoch [6][400/7033]	lr: 6.792e-06, eta: 1:55:19, time: 1.026, data_time: 0.029, memory: 17617, loss_cls: 0.0713, loss_bbox: 0.2051, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2221, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2075, loss: 1.9660, grad_norm: 23.9951
2025-06-10 19:34:43,139 - mmdet - INFO - Epoch [6][450/7033]	lr: 6.792e-06, eta: 1:54:27, time: 1.028, data_time: 0.030, memory: 17617, loss_cls: 0.0728, loss_bbox: 0.2033, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3138, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2220, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0756, d4.loss_bbox: 0.2055, loss: 1.9728, grad_norm: 28.0384
2025-06-10 19:35:34,301 - mmdet - INFO - Epoch [6][500/7033]	lr: 6.792e-06, eta: 1:53:34, time: 1.023, data_time: 0.031, memory: 17617, loss_cls: 0.0589, loss_bbox: 0.1968, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3046, d1.loss_cls: 0.0910, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0608, d4.loss_bbox: 0.1984, loss: 1.8565, grad_norm: 25.4967
2025-06-10 19:36:26,350 - mmdet - INFO - Epoch [6][550/7033]	lr: 6.792e-06, eta: 1:52:42, time: 1.041, data_time: 0.033, memory: 17617, loss_cls: 0.0713, loss_bbox: 0.2031, d0.loss_cls: 0.1612, d0.loss_bbox: 0.3233, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2062, loss: 1.9883, grad_norm: 27.4645
2025-06-10 19:37:18,128 - mmdet - INFO - Epoch [6][600/7033]	lr: 6.792e-06, eta: 1:51:50, time: 1.036, data_time: 0.033, memory: 17617, loss_cls: 0.0664, loss_bbox: 0.1946, d0.loss_cls: 0.1497, d0.loss_bbox: 0.2996, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2313, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2092, d4.loss_cls: 0.0675, d4.loss_bbox: 0.1973, loss: 1.8716, grad_norm: 22.7304
2025-06-10 19:38:13,042 - mmdet - INFO - Epoch [6][650/7033]	lr: 6.792e-06, eta: 1:50:58, time: 1.098, data_time: 0.095, memory: 17617, loss_cls: 0.0596, loss_bbox: 0.1930, d0.loss_cls: 0.1492, d0.loss_bbox: 0.3013, d1.loss_cls: 0.0873, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0739, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0609, d4.loss_bbox: 0.1956, loss: 1.8319, grad_norm: 17.9741
2025-06-10 19:39:04,864 - mmdet - INFO - Epoch [6][700/7033]	lr: 6.792e-06, eta: 1:50:06, time: 1.036, data_time: 0.034, memory: 17617, loss_cls: 0.0581, loss_bbox: 0.1936, d0.loss_cls: 0.1479, d0.loss_bbox: 0.2960, d1.loss_cls: 0.0878, d1.loss_bbox: 0.2242, d2.loss_cls: 0.0719, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0646, d3.loss_bbox: 0.2042, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1948, loss: 1.8130, grad_norm: 123.4753
2025-06-10 19:39:57,523 - mmdet - INFO - Epoch [6][750/7033]	lr: 6.792e-06, eta: 1:49:14, time: 1.053, data_time: 0.055, memory: 17617, loss_cls: 0.0605, loss_bbox: 0.1945, d0.loss_cls: 0.1454, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0897, d1.loss_bbox: 0.2314, d2.loss_cls: 0.0753, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0661, d3.loss_bbox: 0.2085, d4.loss_cls: 0.0629, d4.loss_bbox: 0.1964, loss: 1.8485, grad_norm: 20.1611
2025-06-10 19:40:48,695 - mmdet - INFO - Epoch [6][800/7033]	lr: 6.792e-06, eta: 1:48:22, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.0615, loss_bbox: 0.2006, d0.loss_cls: 0.1449, d0.loss_bbox: 0.3144, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2354, d2.loss_cls: 0.0766, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0630, d4.loss_bbox: 0.2037, loss: 1.8874, grad_norm: 18.0332
2025-06-10 19:41:40,155 - mmdet - INFO - Epoch [6][850/7033]	lr: 6.792e-06, eta: 1:47:29, time: 1.029, data_time: 0.029, memory: 17617, loss_cls: 0.0639, loss_bbox: 0.1959, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3000, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2301, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2137, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0657, d4.loss_bbox: 0.1976, loss: 1.8787, grad_norm: 57.9332
2025-06-10 19:42:31,965 - mmdet - INFO - Epoch [6][900/7033]	lr: 6.792e-06, eta: 1:46:37, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0602, loss_bbox: 0.1869, d0.loss_cls: 0.1428, d0.loss_bbox: 0.2936, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2210, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2047, d3.loss_cls: 0.0665, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1888, loss: 1.7871, grad_norm: 49.4318
2025-06-10 19:43:23,764 - mmdet - INFO - Epoch [6][950/7033]	lr: 6.792e-06, eta: 1:45:45, time: 1.036, data_time: 0.034, memory: 17617, loss_cls: 0.0604, loss_bbox: 0.1924, d0.loss_cls: 0.1508, d0.loss_bbox: 0.2993, d1.loss_cls: 0.0897, d1.loss_bbox: 0.2262, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2070, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2037, d4.loss_cls: 0.0637, d4.loss_bbox: 0.1940, loss: 1.8319, grad_norm: 21.6739
2025-06-10 19:44:15,538 - mmdet - INFO - Epoch [6][1000/7033]	lr: 6.792e-06, eta: 1:44:53, time: 1.036, data_time: 0.035, memory: 17617, loss_cls: 0.0546, loss_bbox: 0.1851, d0.loss_cls: 0.1494, d0.loss_bbox: 0.2915, d1.loss_cls: 0.0850, d1.loss_bbox: 0.2202, d2.loss_cls: 0.0691, d2.loss_bbox: 0.2036, d3.loss_cls: 0.0602, d3.loss_bbox: 0.1977, d4.loss_cls: 0.0568, d4.loss_bbox: 0.1874, loss: 1.7605, grad_norm: 41.4485
2025-06-10 19:45:07,411 - mmdet - INFO - Epoch [6][1050/7033]	lr: 6.792e-06, eta: 1:44:01, time: 1.037, data_time: 0.034, memory: 17617, loss_cls: 0.0658, loss_bbox: 0.2013, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3069, d1.loss_cls: 0.0937, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0797, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2039, loss: 1.9152, grad_norm: 39.6995
2025-06-10 19:45:59,228 - mmdet - INFO - Epoch [6][1100/7033]	lr: 6.792e-06, eta: 1:43:08, time: 1.036, data_time: 0.032, memory: 17617, loss_cls: 0.0602, loss_bbox: 0.1973, d0.loss_cls: 0.1464, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0902, d1.loss_bbox: 0.2362, d2.loss_cls: 0.0756, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0618, d4.loss_bbox: 0.2002, loss: 1.8706, grad_norm: 19.9243
2025-06-10 19:46:50,821 - mmdet - INFO - Epoch [6][1150/7033]	lr: 6.792e-06, eta: 1:42:16, time: 1.032, data_time: 0.032, memory: 17617, loss_cls: 0.0616, loss_bbox: 0.2007, d0.loss_cls: 0.1523, d0.loss_bbox: 0.3013, d1.loss_cls: 0.0933, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0778, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0647, d4.loss_bbox: 0.2027, loss: 1.8875, grad_norm: 31.7539
2025-06-10 19:47:42,214 - mmdet - INFO - Epoch [6][1200/7033]	lr: 6.792e-06, eta: 1:41:24, time: 1.028, data_time: 0.030, memory: 17617, loss_cls: 0.0658, loss_bbox: 0.1933, d0.loss_cls: 0.1523, d0.loss_bbox: 0.2999, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1951, loss: 1.8700, grad_norm: 93.3004
2025-06-10 19:48:34,051 - mmdet - INFO - Epoch [6][1250/7033]	lr: 6.792e-06, eta: 1:40:32, time: 1.037, data_time: 0.034, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.1952, d0.loss_cls: 0.1479, d0.loss_bbox: 0.3067, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0791, d2.loss_bbox: 0.2131, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2098, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1989, loss: 1.8720, grad_norm: 22.0399
2025-06-10 19:49:25,719 - mmdet - INFO - Epoch [6][1300/7033]	lr: 6.792e-06, eta: 1:39:39, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0614, loss_bbox: 0.2035, d0.loss_cls: 0.1496, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2161, d4.loss_cls: 0.0629, d4.loss_bbox: 0.2054, loss: 1.9045, grad_norm: 35.2025
2025-06-10 19:50:17,279 - mmdet - INFO - Epoch [6][1350/7033]	lr: 6.792e-06, eta: 1:38:47, time: 1.031, data_time: 0.032, memory: 17617, loss_cls: 0.0619, loss_bbox: 0.1920, d0.loss_cls: 0.1470, d0.loss_bbox: 0.3027, d1.loss_cls: 0.0908, d1.loss_bbox: 0.2290, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2112, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1945, loss: 1.8421, grad_norm: 29.1304
2025-06-10 19:51:09,206 - mmdet - INFO - Epoch [6][1400/7033]	lr: 6.792e-06, eta: 1:37:55, time: 1.039, data_time: 0.037, memory: 17617, loss_cls: 0.0575, loss_bbox: 0.1986, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3026, d1.loss_cls: 0.0878, d1.loss_bbox: 0.2335, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2148, d3.loss_cls: 0.0635, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0610, d4.loss_bbox: 0.1999, loss: 1.8543, grad_norm: 22.6808
2025-06-10 19:52:00,714 - mmdet - INFO - Epoch [6][1450/7033]	lr: 6.792e-06, eta: 1:37:03, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0570, loss_bbox: 0.1878, d0.loss_cls: 0.1394, d0.loss_bbox: 0.3001, d1.loss_cls: 0.0839, d1.loss_bbox: 0.2215, d2.loss_cls: 0.0703, d2.loss_bbox: 0.2049, d3.loss_cls: 0.0627, d3.loss_bbox: 0.1996, d4.loss_cls: 0.0584, d4.loss_bbox: 0.1902, loss: 1.7757, grad_norm: 24.2652
2025-06-10 19:52:52,625 - mmdet - INFO - Epoch [6][1500/7033]	lr: 6.792e-06, eta: 1:36:11, time: 1.038, data_time: 0.034, memory: 17617, loss_cls: 0.0605, loss_bbox: 0.2020, d0.loss_cls: 0.1519, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0665, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0628, d4.loss_bbox: 0.2044, loss: 1.8889, grad_norm: 36.4406
2025-06-10 19:53:44,521 - mmdet - INFO - Epoch [6][1550/7033]	lr: 6.792e-06, eta: 1:35:18, time: 1.038, data_time: 0.036, memory: 17617, loss_cls: 0.0702, loss_bbox: 0.1946, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3038, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0830, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2086, d4.loss_cls: 0.0704, d4.loss_bbox: 0.1983, loss: 1.9036, grad_norm: 16.0003
2025-06-10 19:54:36,114 - mmdet - INFO - Epoch [6][1600/7033]	lr: 6.792e-06, eta: 1:34:26, time: 1.032, data_time: 0.034, memory: 17617, loss_cls: 0.0612, loss_bbox: 0.1976, d0.loss_cls: 0.1449, d0.loss_bbox: 0.3109, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0667, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0625, d4.loss_bbox: 0.2010, loss: 1.8763, grad_norm: 23.0119
2025-06-10 19:55:27,971 - mmdet - INFO - Epoch [6][1650/7033]	lr: 6.792e-06, eta: 1:33:34, time: 1.037, data_time: 0.033, memory: 17617, loss_cls: 0.0686, loss_bbox: 0.2034, d0.loss_cls: 0.1483, d0.loss_bbox: 0.3143, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0844, d2.loss_bbox: 0.2195, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2164, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2048, loss: 1.9411, grad_norm: 29.9360
2025-06-10 19:56:19,880 - mmdet - INFO - Epoch [6][1700/7033]	lr: 6.792e-06, eta: 1:32:42, time: 1.038, data_time: 0.034, memory: 17617, loss_cls: 0.0580, loss_bbox: 0.1995, d0.loss_cls: 0.1523, d0.loss_bbox: 0.3132, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0746, d2.loss_bbox: 0.2147, d3.loss_cls: 0.0650, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0602, d4.loss_bbox: 0.2019, loss: 1.8766, grad_norm: 17.6775
2025-06-10 19:57:11,291 - mmdet - INFO - Epoch [6][1750/7033]	lr: 6.792e-06, eta: 1:31:50, time: 1.028, data_time: 0.030, memory: 17617, loss_cls: 0.0615, loss_bbox: 0.1949, d0.loss_cls: 0.1493, d0.loss_bbox: 0.3071, d1.loss_cls: 0.0938, d1.loss_bbox: 0.2315, d2.loss_cls: 0.0755, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0665, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0635, d4.loss_bbox: 0.1969, loss: 1.8640, grad_norm: 19.2088
2025-06-10 19:59:08,603 - mmdet - INFO - Epoch [6][1800/7033]	lr: 6.792e-06, eta: 1:31:07, time: 2.346, data_time: 1.345, memory: 17617, loss_cls: 0.0659, loss_bbox: 0.1992, d0.loss_cls: 0.1514, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0928, d1.loss_bbox: 0.2332, d2.loss_cls: 0.0803, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0675, d4.loss_bbox: 0.2017, loss: 1.8975, grad_norm: 22.2911
2025-06-10 20:00:00,375 - mmdet - INFO - Epoch [6][1850/7033]	lr: 6.792e-06, eta: 1:30:14, time: 1.035, data_time: 0.033, memory: 17617, loss_cls: 0.0698, loss_bbox: 0.2024, d0.loss_cls: 0.1538, d0.loss_bbox: 0.3126, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2183, d3.loss_cls: 0.0750, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2039, loss: 1.9441, grad_norm: 56.3685
2025-06-10 20:00:53,289 - mmdet - INFO - Epoch [6][1900/7033]	lr: 6.792e-06, eta: 1:29:22, time: 1.058, data_time: 0.055, memory: 17617, loss_cls: 0.0627, loss_bbox: 0.1979, d0.loss_cls: 0.1447, d0.loss_bbox: 0.3041, d1.loss_cls: 0.0900, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0686, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0642, d4.loss_bbox: 0.2008, loss: 1.8725, grad_norm: 28.6702
2025-06-10 20:01:45,988 - mmdet - INFO - Epoch [6][1950/7033]	lr: 6.792e-06, eta: 1:28:30, time: 1.054, data_time: 0.054, memory: 17617, loss_cls: 0.0608, loss_bbox: 0.1929, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0855, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2084, d3.loss_cls: 0.0647, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0622, d4.loss_bbox: 0.1947, loss: 1.8253, grad_norm: 26.4182
2025-06-10 20:02:51,754 - mmdet - INFO - Epoch [6][2000/7033]	lr: 6.792e-06, eta: 1:27:40, time: 1.315, data_time: 0.310, memory: 17617, loss_cls: 0.0579, loss_bbox: 0.1932, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3006, d1.loss_cls: 0.0861, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2096, d3.loss_cls: 0.0643, d3.loss_bbox: 0.2047, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1949, loss: 1.8231, grad_norm: 17.3961
2025-06-10 20:03:43,409 - mmdet - INFO - Epoch [6][2050/7033]	lr: 6.792e-06, eta: 1:26:47, time: 1.033, data_time: 0.034, memory: 17617, loss_cls: 0.0591, loss_bbox: 0.1969, d0.loss_cls: 0.1419, d0.loss_bbox: 0.3018, d1.loss_cls: 0.0865, d1.loss_bbox: 0.2290, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0641, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0615, d4.loss_bbox: 0.1990, loss: 1.8330, grad_norm: 19.4150
2025-06-10 20:04:37,784 - mmdet - INFO - Epoch [6][2100/7033]	lr: 6.792e-06, eta: 1:25:55, time: 1.087, data_time: 0.034, memory: 17617, loss_cls: 0.0639, loss_bbox: 0.2029, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3095, d1.loss_cls: 0.0916, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0767, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2154, d4.loss_cls: 0.0663, d4.loss_bbox: 0.2046, loss: 1.9055, grad_norm: 30.1738
2025-06-10 20:05:30,577 - mmdet - INFO - Epoch [6][2150/7033]	lr: 6.792e-06, eta: 1:25:03, time: 1.056, data_time: 0.054, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.2011, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3173, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0784, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0651, d4.loss_bbox: 0.2031, loss: 1.9226, grad_norm: 18.9464
2025-06-10 20:06:22,664 - mmdet - INFO - Epoch [6][2200/7033]	lr: 6.792e-06, eta: 1:24:11, time: 1.042, data_time: 0.036, memory: 17617, loss_cls: 0.0687, loss_bbox: 0.1997, d0.loss_cls: 0.1491, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2373, d2.loss_cls: 0.0801, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2020, loss: 1.9154, grad_norm: 21.3760
2025-06-10 20:07:14,051 - mmdet - INFO - Epoch [6][2250/7033]	lr: 6.792e-06, eta: 1:23:18, time: 1.028, data_time: 0.032, memory: 17617, loss_cls: 0.0621, loss_bbox: 0.1982, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0638, d4.loss_bbox: 0.2000, loss: 1.8887, grad_norm: 26.1291
2025-06-10 20:08:05,755 - mmdet - INFO - Epoch [6][2300/7033]	lr: 6.792e-06, eta: 1:22:26, time: 1.034, data_time: 0.036, memory: 17617, loss_cls: 0.0643, loss_bbox: 0.1957, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0894, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0790, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0710, d3.loss_bbox: 0.2082, d4.loss_cls: 0.0667, d4.loss_bbox: 0.1977, loss: 1.8664, grad_norm: 21.8736
2025-06-10 20:08:57,602 - mmdet - INFO - Epoch [6][2350/7033]	lr: 6.792e-06, eta: 1:21:34, time: 1.037, data_time: 0.034, memory: 17617, loss_cls: 0.0602, loss_bbox: 0.1989, d0.loss_cls: 0.1458, d0.loss_bbox: 0.2987, d1.loss_cls: 0.0891, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0667, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0615, d4.loss_bbox: 0.2012, loss: 1.8518, grad_norm: 27.5860
2025-06-10 20:09:49,808 - mmdet - INFO - Epoch [6][2400/7033]	lr: 6.792e-06, eta: 1:20:42, time: 1.044, data_time: 0.037, memory: 17617, loss_cls: 0.0598, loss_bbox: 0.1939, d0.loss_cls: 0.1465, d0.loss_bbox: 0.2993, d1.loss_cls: 0.0862, d1.loss_bbox: 0.2280, d2.loss_cls: 0.0717, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0637, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1966, loss: 1.8271, grad_norm: 18.2407
2025-06-10 20:10:42,198 - mmdet - INFO - Epoch [6][2450/7033]	lr: 6.792e-06, eta: 1:19:49, time: 1.048, data_time: 0.034, memory: 17617, loss_cls: 0.0604, loss_bbox: 0.1945, d0.loss_cls: 0.1469, d0.loss_bbox: 0.2993, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0768, d2.loss_bbox: 0.2118, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0624, d4.loss_bbox: 0.1961, loss: 1.8442, grad_norm: 108.0285
2025-06-10 20:11:34,341 - mmdet - INFO - Epoch [6][2500/7033]	lr: 6.792e-06, eta: 1:18:57, time: 1.043, data_time: 0.032, memory: 17617, loss_cls: 0.0656, loss_bbox: 0.2079, d0.loss_cls: 0.1493, d0.loss_bbox: 0.3123, d1.loss_cls: 0.0925, d1.loss_bbox: 0.2406, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2095, loss: 1.9369, grad_norm: 16.5295
2025-06-10 20:12:26,574 - mmdet - INFO - Epoch [6][2550/7033]	lr: 6.792e-06, eta: 1:18:05, time: 1.045, data_time: 0.035, memory: 17617, loss_cls: 0.0625, loss_bbox: 0.1927, d0.loss_cls: 0.1480, d0.loss_bbox: 0.2938, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2231, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2074, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2030, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1941, loss: 1.8283, grad_norm: 19.4824
2025-06-10 20:13:30,056 - mmdet - INFO - Epoch [6][2600/7033]	lr: 6.792e-06, eta: 1:17:14, time: 1.270, data_time: 0.034, memory: 17617, loss_cls: 0.0629, loss_bbox: 0.1949, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0901, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0758, d2.loss_bbox: 0.2106, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0640, d4.loss_bbox: 0.1970, loss: 1.8517, grad_norm: 17.0864
2025-06-10 20:14:21,865 - mmdet - INFO - Epoch [6][2650/7033]	lr: 6.792e-06, eta: 1:16:21, time: 1.036, data_time: 0.033, memory: 17617, loss_cls: 0.0670, loss_bbox: 0.1984, d0.loss_cls: 0.1489, d0.loss_bbox: 0.3121, d1.loss_cls: 0.0927, d1.loss_bbox: 0.2368, d2.loss_cls: 0.0789, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0676, d4.loss_bbox: 0.2007, loss: 1.8995, grad_norm: 65.3038
2025-06-10 20:15:24,849 - mmdet - INFO - Epoch [6][2700/7033]	lr: 6.792e-06, eta: 1:15:30, time: 1.260, data_time: 0.044, memory: 17617, loss_cls: 0.0553, loss_bbox: 0.1953, d0.loss_cls: 0.1428, d0.loss_bbox: 0.3026, d1.loss_cls: 0.0876, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0705, d2.loss_bbox: 0.2116, d3.loss_cls: 0.0608, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0575, d4.loss_bbox: 0.1979, loss: 1.8178, grad_norm: 27.1810
2025-06-10 20:16:17,563 - mmdet - INFO - Epoch [6][2750/7033]	lr: 6.792e-06, eta: 1:14:38, time: 1.054, data_time: 0.049, memory: 17617, loss_cls: 0.0647, loss_bbox: 0.1963, d0.loss_cls: 0.1499, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0914, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0775, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1981, loss: 1.8791, grad_norm: 18.4623
2025-06-10 20:17:10,205 - mmdet - INFO - Epoch [6][2800/7033]	lr: 6.792e-06, eta: 1:13:46, time: 1.053, data_time: 0.028, memory: 17617, loss_cls: 0.0604, loss_bbox: 0.1933, d0.loss_cls: 0.1431, d0.loss_bbox: 0.3034, d1.loss_cls: 0.0864, d1.loss_bbox: 0.2285, d2.loss_cls: 0.0727, d2.loss_bbox: 0.2106, d3.loss_cls: 0.0658, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1958, loss: 1.8267, grad_norm: 16.6169
2025-06-10 20:18:01,921 - mmdet - INFO - Epoch [6][2850/7033]	lr: 6.792e-06, eta: 1:12:54, time: 1.034, data_time: 0.031, memory: 17617, loss_cls: 0.0599, loss_bbox: 0.1905, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3018, d1.loss_cls: 0.0887, d1.loss_bbox: 0.2266, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2059, d3.loss_cls: 0.0666, d3.loss_bbox: 0.2025, d4.loss_cls: 0.0625, d4.loss_bbox: 0.1922, loss: 1.8207, grad_norm: 16.4249
2025-06-10 20:18:54,524 - mmdet - INFO - Epoch [6][2900/7033]	lr: 6.792e-06, eta: 1:12:01, time: 1.052, data_time: 0.052, memory: 17617, loss_cls: 0.0704, loss_bbox: 0.2061, d0.loss_cls: 0.1538, d0.loss_bbox: 0.3155, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2090, loss: 1.9674, grad_norm: 17.5960
2025-06-10 20:19:47,560 - mmdet - INFO - Epoch [6][2950/7033]	lr: 6.792e-06, eta: 1:11:09, time: 1.061, data_time: 0.046, memory: 17617, loss_cls: 0.0570, loss_bbox: 0.1969, d0.loss_cls: 0.1426, d0.loss_bbox: 0.3084, d1.loss_cls: 0.0860, d1.loss_bbox: 0.2330, d2.loss_cls: 0.0730, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0633, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0579, d4.loss_bbox: 0.1993, loss: 1.8397, grad_norm: 20.7966
2025-06-10 20:20:40,257 - mmdet - INFO - Epoch [6][3000/7033]	lr: 6.792e-06, eta: 1:10:17, time: 1.054, data_time: 0.051, memory: 17617, loss_cls: 0.0622, loss_bbox: 0.1954, d0.loss_cls: 0.1394, d0.loss_bbox: 0.2967, d1.loss_cls: 0.0878, d1.loss_bbox: 0.2292, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2128, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0645, d4.loss_bbox: 0.1979, loss: 1.8387, grad_norm: 19.3369
2025-06-10 20:21:31,845 - mmdet - INFO - Epoch [6][3050/7033]	lr: 6.792e-06, eta: 1:09:25, time: 1.032, data_time: 0.031, memory: 17617, loss_cls: 0.0571, loss_bbox: 0.1901, d0.loss_cls: 0.1426, d0.loss_bbox: 0.2908, d1.loss_cls: 0.0865, d1.loss_bbox: 0.2216, d2.loss_cls: 0.0707, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0630, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0593, d4.loss_bbox: 0.1922, loss: 1.7834, grad_norm: 56.6660
2025-06-10 20:22:25,037 - mmdet - INFO - Epoch [6][3100/7033]	lr: 6.792e-06, eta: 1:08:32, time: 1.064, data_time: 0.049, memory: 17617, loss_cls: 0.0588, loss_bbox: 0.1911, d0.loss_cls: 0.1504, d0.loss_bbox: 0.2951, d1.loss_cls: 0.0823, d1.loss_bbox: 0.2266, d2.loss_cls: 0.0721, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0655, d3.loss_bbox: 0.2029, d4.loss_cls: 0.0609, d4.loss_bbox: 0.1922, loss: 1.8058, grad_norm: 19.8519
2025-06-10 20:23:16,678 - mmdet - INFO - Epoch [6][3150/7033]	lr: 6.792e-06, eta: 1:07:40, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.1978, d0.loss_cls: 0.1476, d0.loss_bbox: 0.3031, d1.loss_cls: 0.0898, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0654, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0618, d4.loss_bbox: 0.2009, loss: 1.8595, grad_norm: 3617.8763
2025-06-10 20:24:09,056 - mmdet - INFO - Epoch [6][3200/7033]	lr: 6.792e-06, eta: 1:06:48, time: 1.048, data_time: 0.034, memory: 17617, loss_cls: 0.0590, loss_bbox: 0.1921, d0.loss_cls: 0.1532, d0.loss_bbox: 0.2997, d1.loss_cls: 0.0892, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0749, d2.loss_bbox: 0.2086, d3.loss_cls: 0.0649, d3.loss_bbox: 0.2049, d4.loss_cls: 0.0602, d4.loss_bbox: 0.1951, loss: 1.8318, grad_norm: 406.7893
2025-06-10 20:25:00,941 - mmdet - INFO - Epoch [6][3250/7033]	lr: 6.792e-06, eta: 1:05:55, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0639, loss_bbox: 0.1961, d0.loss_cls: 0.1496, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0917, d1.loss_bbox: 0.2374, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0708, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0663, d4.loss_bbox: 0.1962, loss: 1.8822, grad_norm: 22.5878
2025-06-10 20:25:53,049 - mmdet - INFO - Epoch [6][3300/7033]	lr: 6.792e-06, eta: 1:05:03, time: 1.042, data_time: 0.033, memory: 17617, loss_cls: 0.0651, loss_bbox: 0.1958, d0.loss_cls: 0.1464, d0.loss_bbox: 0.3032, d1.loss_cls: 0.0936, d1.loss_bbox: 0.2297, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0664, d4.loss_bbox: 0.1981, loss: 1.8675, grad_norm: 17.8564
2025-06-10 20:26:44,692 - mmdet - INFO - Epoch [6][3350/7033]	lr: 6.792e-06, eta: 1:04:11, time: 1.033, data_time: 0.031, memory: 17617, loss_cls: 0.0668, loss_bbox: 0.1981, d0.loss_cls: 0.1502, d0.loss_bbox: 0.3021, d1.loss_cls: 0.0939, d1.loss_bbox: 0.2323, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2121, d4.loss_cls: 0.0693, d4.loss_bbox: 0.1996, loss: 1.8920, grad_norm: 993.2259
2025-06-10 20:27:36,249 - mmdet - INFO - Epoch [6][3400/7033]	lr: 6.792e-06, eta: 1:03:18, time: 1.031, data_time: 0.030, memory: 17617, loss_cls: 0.0649, loss_bbox: 0.1966, d0.loss_cls: 0.1472, d0.loss_bbox: 0.3103, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0694, d3.loss_bbox: 0.2111, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1999, loss: 1.8838, grad_norm: 45.7807
2025-06-10 20:28:28,236 - mmdet - INFO - Epoch [6][3450/7033]	lr: 6.792e-06, eta: 1:02:26, time: 1.040, data_time: 0.030, memory: 17617, loss_cls: 0.0688, loss_bbox: 0.2031, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3136, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2046, loss: 1.9406, grad_norm: 21.8448
2025-06-10 20:29:23,753 - mmdet - INFO - Epoch [6][3500/7033]	lr: 6.792e-06, eta: 1:01:34, time: 1.110, data_time: 0.100, memory: 17617, loss_cls: 0.0607, loss_bbox: 0.1909, d0.loss_cls: 0.1491, d0.loss_bbox: 0.2974, d1.loss_cls: 0.0913, d1.loss_bbox: 0.2255, d2.loss_cls: 0.0757, d2.loss_bbox: 0.2069, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2023, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1924, loss: 1.8219, grad_norm: 21.1049
2025-06-10 20:30:15,468 - mmdet - INFO - Epoch [6][3550/7033]	lr: 6.792e-06, eta: 1:00:42, time: 1.034, data_time: 0.032, memory: 17617, loss_cls: 0.0546, loss_bbox: 0.1909, d0.loss_cls: 0.1456, d0.loss_bbox: 0.2969, d1.loss_cls: 0.0817, d1.loss_bbox: 0.2243, d2.loss_cls: 0.0697, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0596, d3.loss_bbox: 0.2030, d4.loss_cls: 0.0569, d4.loss_bbox: 0.1934, loss: 1.7827, grad_norm: 197.0146
2025-06-10 20:31:07,541 - mmdet - INFO - Epoch [6][3600/7033]	lr: 6.792e-06, eta: 0:59:49, time: 1.041, data_time: 0.031, memory: 17617, loss_cls: 0.0580, loss_bbox: 0.1996, d0.loss_cls: 0.1452, d0.loss_bbox: 0.3075, d1.loss_cls: 0.0901, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0740, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0659, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0598, d4.loss_bbox: 0.2023, loss: 1.8631, grad_norm: 30.7034
2025-06-10 20:31:59,495 - mmdet - INFO - Epoch [6][3650/7033]	lr: 6.792e-06, eta: 0:58:57, time: 1.039, data_time: 0.034, memory: 17617, loss_cls: 0.0721, loss_bbox: 0.2015, d0.loss_cls: 0.1516, d0.loss_bbox: 0.3040, d1.loss_cls: 0.0977, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2037, loss: 1.9362, grad_norm: 17.1198
2025-06-10 20:32:50,744 - mmdet - INFO - Epoch [6][3700/7033]	lr: 6.792e-06, eta: 0:58:05, time: 1.025, data_time: 0.031, memory: 17617, loss_cls: 0.0629, loss_bbox: 0.1945, d0.loss_cls: 0.1445, d0.loss_bbox: 0.3016, d1.loss_cls: 0.0903, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0683, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0649, d4.loss_bbox: 0.1969, loss: 1.8530, grad_norm: 17.5550
2025-06-10 20:33:42,239 - mmdet - INFO - Epoch [6][3750/7033]	lr: 6.792e-06, eta: 0:57:12, time: 1.030, data_time: 0.030, memory: 17617, loss_cls: 0.0622, loss_bbox: 0.1904, d0.loss_cls: 0.1496, d0.loss_bbox: 0.2981, d1.loss_cls: 0.0934, d1.loss_bbox: 0.2261, d2.loss_cls: 0.0787, d2.loss_bbox: 0.2080, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2057, d4.loss_cls: 0.0642, d4.loss_bbox: 0.1924, loss: 1.8357, grad_norm: 40.1600
2025-06-10 20:34:34,519 - mmdet - INFO - Epoch [6][3800/7033]	lr: 6.792e-06, eta: 0:56:20, time: 1.046, data_time: 0.051, memory: 17617, loss_cls: 0.0659, loss_bbox: 0.1975, d0.loss_cls: 0.1482, d0.loss_bbox: 0.3048, d1.loss_cls: 0.0943, d1.loss_bbox: 0.2280, d2.loss_cls: 0.0788, d2.loss_bbox: 0.2140, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0683, d4.loss_bbox: 0.1997, loss: 1.8826, grad_norm: 21.9912
2025-06-10 20:35:26,154 - mmdet - INFO - Epoch [6][3850/7033]	lr: 6.792e-06, eta: 0:55:28, time: 1.033, data_time: 0.035, memory: 17617, loss_cls: 0.0543, loss_bbox: 0.1858, d0.loss_cls: 0.1434, d0.loss_bbox: 0.2919, d1.loss_cls: 0.0830, d1.loss_bbox: 0.2201, d2.loss_cls: 0.0679, d2.loss_bbox: 0.2028, d3.loss_cls: 0.0609, d3.loss_bbox: 0.1974, d4.loss_cls: 0.0559, d4.loss_bbox: 0.1877, loss: 1.7511, grad_norm: 26.2682
2025-06-10 20:36:17,394 - mmdet - INFO - Epoch [6][3900/7033]	lr: 6.792e-06, eta: 0:54:35, time: 1.025, data_time: 0.033, memory: 17617, loss_cls: 0.0586, loss_bbox: 0.1977, d0.loss_cls: 0.1427, d0.loss_bbox: 0.3082, d1.loss_cls: 0.0859, d1.loss_bbox: 0.2336, d2.loss_cls: 0.0719, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0646, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1993, loss: 1.8471, grad_norm: 21.5774
2025-06-10 20:37:08,875 - mmdet - INFO - Epoch [6][3950/7033]	lr: 6.792e-06, eta: 0:53:43, time: 1.030, data_time: 0.032, memory: 17617, loss_cls: 0.0635, loss_bbox: 0.2022, d0.loss_cls: 0.1473, d0.loss_bbox: 0.3075, d1.loss_cls: 0.0883, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0748, d2.loss_bbox: 0.2175, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2135, d4.loss_cls: 0.0650, d4.loss_bbox: 0.2030, loss: 1.8856, grad_norm: 26.2498
2025-06-10 20:38:00,008 - mmdet - INFO - Epoch [6][4000/7033]	lr: 6.792e-06, eta: 0:52:51, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.0588, loss_bbox: 0.1916, d0.loss_cls: 0.1454, d0.loss_bbox: 0.2944, d1.loss_cls: 0.0882, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0732, d2.loss_bbox: 0.2066, d3.loss_cls: 0.0640, d3.loss_bbox: 0.2017, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1936, loss: 1.8045, grad_norm: 53.5229
2025-06-10 20:38:51,014 - mmdet - INFO - Epoch [6][4050/7033]	lr: 6.792e-06, eta: 0:51:58, time: 1.020, data_time: 0.027, memory: 17617, loss_cls: 0.0574, loss_bbox: 0.1870, d0.loss_cls: 0.1477, d0.loss_bbox: 0.2933, d1.loss_cls: 0.0877, d1.loss_bbox: 0.2217, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2029, d3.loss_cls: 0.0638, d3.loss_bbox: 0.2008, d4.loss_cls: 0.0598, d4.loss_bbox: 0.1891, loss: 1.7857, grad_norm: 23.9753
2025-06-10 20:39:42,001 - mmdet - INFO - Epoch [6][4100/7033]	lr: 6.792e-06, eta: 0:51:06, time: 1.020, data_time: 0.028, memory: 17617, loss_cls: 0.0613, loss_bbox: 0.1907, d0.loss_cls: 0.1435, d0.loss_bbox: 0.2955, d1.loss_cls: 0.0893, d1.loss_bbox: 0.2240, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0663, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1926, loss: 1.8122, grad_norm: 40.9849
2025-06-10 20:40:33,349 - mmdet - INFO - Epoch [6][4150/7033]	lr: 6.792e-06, eta: 0:50:14, time: 1.027, data_time: 0.029, memory: 17617, loss_cls: 0.0590, loss_bbox: 0.1888, d0.loss_cls: 0.1446, d0.loss_bbox: 0.2966, d1.loss_cls: 0.0884, d1.loss_bbox: 0.2221, d2.loss_cls: 0.0723, d2.loss_bbox: 0.2072, d3.loss_cls: 0.0644, d3.loss_bbox: 0.2005, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1911, loss: 1.7954, grad_norm: 26.8660
2025-06-10 20:41:24,416 - mmdet - INFO - Epoch [6][4200/7033]	lr: 6.792e-06, eta: 0:49:21, time: 1.021, data_time: 0.030, memory: 17617, loss_cls: 0.0550, loss_bbox: 0.1879, d0.loss_cls: 0.1377, d0.loss_bbox: 0.2889, d1.loss_cls: 0.0843, d1.loss_bbox: 0.2154, d2.loss_cls: 0.0679, d2.loss_bbox: 0.2019, d3.loss_cls: 0.0610, d3.loss_bbox: 0.1978, d4.loss_cls: 0.0575, d4.loss_bbox: 0.1886, loss: 1.7440, grad_norm: 19.5223
2025-06-10 20:42:15,617 - mmdet - INFO - Epoch [6][4250/7033]	lr: 6.792e-06, eta: 0:48:29, time: 1.024, data_time: 0.028, memory: 17617, loss_cls: 0.0602, loss_bbox: 0.1928, d0.loss_cls: 0.1453, d0.loss_bbox: 0.2995, d1.loss_cls: 0.0846, d1.loss_bbox: 0.2259, d2.loss_cls: 0.0715, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0630, d3.loss_bbox: 0.2053, d4.loss_cls: 0.0605, d4.loss_bbox: 0.1947, loss: 1.8135, grad_norm: 35.0720
2025-06-10 20:43:06,970 - mmdet - INFO - Epoch [6][4300/7033]	lr: 6.792e-06, eta: 0:47:37, time: 1.027, data_time: 0.029, memory: 17617, loss_cls: 0.0617, loss_bbox: 0.1963, d0.loss_cls: 0.1441, d0.loss_bbox: 0.3072, d1.loss_cls: 0.0853, d1.loss_bbox: 0.2325, d2.loss_cls: 0.0739, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0628, d4.loss_bbox: 0.1991, loss: 1.8552, grad_norm: 28.4402
2025-06-10 20:43:58,023 - mmdet - INFO - Epoch [6][4350/7033]	lr: 6.792e-06, eta: 0:46:44, time: 1.021, data_time: 0.030, memory: 17617, loss_cls: 0.0615, loss_bbox: 0.2050, d0.loss_cls: 0.1472, d0.loss_bbox: 0.3080, d1.loss_cls: 0.0879, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0759, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0627, d4.loss_bbox: 0.2079, loss: 1.9066, grad_norm: 25.5127
2025-06-10 20:44:49,221 - mmdet - INFO - Epoch [6][4400/7033]	lr: 6.792e-06, eta: 0:45:52, time: 1.024, data_time: 0.028, memory: 17617, loss_cls: 0.0587, loss_bbox: 0.1955, d0.loss_cls: 0.1467, d0.loss_bbox: 0.3016, d1.loss_cls: 0.0881, d1.loss_bbox: 0.2303, d2.loss_cls: 0.0739, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1982, loss: 1.8402, grad_norm: 29.3162
2025-06-10 20:45:40,627 - mmdet - INFO - Epoch [6][4450/7033]	lr: 6.792e-06, eta: 0:45:00, time: 1.028, data_time: 0.028, memory: 17617, loss_cls: 0.0678, loss_bbox: 0.2018, d0.loss_cls: 0.1485, d0.loss_bbox: 0.3088, d1.loss_cls: 0.0947, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0812, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2048, loss: 1.9268, grad_norm: 26.5940
2025-06-10 20:46:32,560 - mmdet - INFO - Epoch [6][4500/7033]	lr: 6.792e-06, eta: 0:44:07, time: 1.039, data_time: 0.028, memory: 17617, loss_cls: 0.0609, loss_bbox: 0.2005, d0.loss_cls: 0.1454, d0.loss_bbox: 0.3081, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2311, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0674, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0643, d4.loss_bbox: 0.2011, loss: 1.8713, grad_norm: 21.2866
2025-06-10 20:47:24,535 - mmdet - INFO - Epoch [6][4550/7033]	lr: 6.792e-06, eta: 0:43:15, time: 1.040, data_time: 0.029, memory: 17617, loss_cls: 0.0633, loss_bbox: 0.1957, d0.loss_cls: 0.1487, d0.loss_bbox: 0.2971, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2263, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2103, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0659, d4.loss_bbox: 0.1969, loss: 1.8473, grad_norm: 26.6608
2025-06-10 20:48:19,060 - mmdet - INFO - Epoch [6][4600/7033]	lr: 6.792e-06, eta: 0:42:23, time: 1.090, data_time: 0.030, memory: 17617, loss_cls: 0.0662, loss_bbox: 0.1965, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3028, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2279, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2094, d4.loss_cls: 0.0685, d4.loss_bbox: 0.1977, loss: 1.8856, grad_norm: 20.8688
2025-06-10 20:49:10,468 - mmdet - INFO - Epoch [6][4650/7033]	lr: 6.792e-06, eta: 0:41:31, time: 1.028, data_time: 0.027, memory: 17617, loss_cls: 0.0610, loss_bbox: 0.2011, d0.loss_cls: 0.1491, d0.loss_bbox: 0.3076, d1.loss_cls: 0.0915, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0741, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0643, d4.loss_bbox: 0.2015, loss: 1.8880, grad_norm: 87.0535
2025-06-10 20:50:01,681 - mmdet - INFO - Epoch [6][4700/7033]	lr: 6.792e-06, eta: 0:40:38, time: 1.024, data_time: 0.027, memory: 17617, loss_cls: 0.0652, loss_bbox: 0.1967, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3054, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0674, d4.loss_bbox: 0.1992, loss: 1.8808, grad_norm: 38.1389
2025-06-10 20:50:53,207 - mmdet - INFO - Epoch [6][4750/7033]	lr: 6.792e-06, eta: 0:39:46, time: 1.031, data_time: 0.026, memory: 17617, loss_cls: 0.0593, loss_bbox: 0.1916, d0.loss_cls: 0.1491, d0.loss_bbox: 0.2992, d1.loss_cls: 0.0871, d1.loss_bbox: 0.2283, d2.loss_cls: 0.0738, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0648, d3.loss_bbox: 0.2058, d4.loss_cls: 0.0618, d4.loss_bbox: 0.1937, loss: 1.8241, grad_norm: 33.9495
2025-06-10 20:51:45,613 - mmdet - INFO - Epoch [6][4800/7033]	lr: 6.792e-06, eta: 0:38:54, time: 1.048, data_time: 0.040, memory: 17617, loss_cls: 0.0632, loss_bbox: 0.1965, d0.loss_cls: 0.1558, d0.loss_bbox: 0.3138, d1.loss_cls: 0.0945, d1.loss_bbox: 0.2337, d2.loss_cls: 0.0782, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0652, d4.loss_bbox: 0.1983, loss: 1.8929, grad_norm: 1060.1569
2025-06-10 20:52:37,535 - mmdet - INFO - Epoch [6][4850/7033]	lr: 6.792e-06, eta: 0:38:01, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0579, loss_bbox: 0.1935, d0.loss_cls: 0.1474, d0.loss_bbox: 0.2939, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2254, d2.loss_cls: 0.0712, d2.loss_bbox: 0.2077, d3.loss_cls: 0.0633, d3.loss_bbox: 0.2049, d4.loss_cls: 0.0603, d4.loss_bbox: 0.1943, loss: 1.8065, grad_norm: 24.0584
2025-06-10 20:53:29,544 - mmdet - INFO - Epoch [6][4900/7033]	lr: 6.792e-06, eta: 0:37:09, time: 1.040, data_time: 0.032, memory: 17617, loss_cls: 0.0639, loss_bbox: 0.2031, d0.loss_cls: 0.1535, d0.loss_bbox: 0.3218, d1.loss_cls: 0.0940, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2161, d4.loss_cls: 0.0673, d4.loss_bbox: 0.2047, loss: 1.9412, grad_norm: 22.4390
2025-06-10 20:54:21,501 - mmdet - INFO - Epoch [6][4950/7033]	lr: 6.792e-06, eta: 0:36:17, time: 1.039, data_time: 0.031, memory: 17617, loss_cls: 0.0593, loss_bbox: 0.1953, d0.loss_cls: 0.1412, d0.loss_bbox: 0.3051, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2306, d2.loss_cls: 0.0726, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0653, d3.loss_bbox: 0.2064, d4.loss_cls: 0.0606, d4.loss_bbox: 0.1973, loss: 1.8327, grad_norm: 24.3043
2025-06-10 20:55:13,295 - mmdet - INFO - Epoch [6][5000/7033]	lr: 6.792e-06, eta: 0:35:25, time: 1.036, data_time: 0.029, memory: 17617, loss_cls: 0.0658, loss_bbox: 0.2067, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3217, d1.loss_cls: 0.0962, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0813, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0682, d4.loss_bbox: 0.2100, loss: 1.9744, grad_norm: 44.7154
2025-06-10 20:56:05,350 - mmdet - INFO - Epoch [6][5050/7033]	lr: 6.792e-06, eta: 0:34:32, time: 1.041, data_time: 0.032, memory: 17617, loss_cls: 0.0593, loss_bbox: 0.1931, d0.loss_cls: 0.1457, d0.loss_bbox: 0.3002, d1.loss_cls: 0.0851, d1.loss_bbox: 0.2289, d2.loss_cls: 0.0712, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0628, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0612, d4.loss_bbox: 0.1955, loss: 1.8227, grad_norm: 22.4275
2025-06-10 20:56:57,337 - mmdet - INFO - Epoch [6][5100/7033]	lr: 6.792e-06, eta: 0:33:40, time: 1.040, data_time: 0.031, memory: 17617, loss_cls: 0.0620, loss_bbox: 0.1970, d0.loss_cls: 0.1549, d0.loss_bbox: 0.3071, d1.loss_cls: 0.0905, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2136, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2089, d4.loss_cls: 0.0646, d4.loss_bbox: 0.1991, loss: 1.8768, grad_norm: 18.6867
2025-06-10 20:57:55,247 - mmdet - INFO - Epoch [6][5150/7033]	lr: 6.792e-06, eta: 0:32:48, time: 1.158, data_time: 0.146, memory: 17617, loss_cls: 0.0608, loss_bbox: 0.1920, d0.loss_cls: 0.1510, d0.loss_bbox: 0.2955, d1.loss_cls: 0.0891, d1.loss_bbox: 0.2249, d2.loss_cls: 0.0751, d2.loss_bbox: 0.2092, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2044, d4.loss_cls: 0.0632, d4.loss_bbox: 0.1949, loss: 1.8273, grad_norm: 81.9443
2025-06-10 20:59:10,599 - mmdet - INFO - Epoch [6][5200/7033]	lr: 6.792e-06, eta: 0:31:57, time: 1.507, data_time: 0.482, memory: 17617, loss_cls: 0.0626, loss_bbox: 0.1941, d0.loss_cls: 0.1487, d0.loss_bbox: 0.3074, d1.loss_cls: 0.0903, d1.loss_bbox: 0.2293, d2.loss_cls: 0.0744, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0641, d4.loss_bbox: 0.1966, loss: 1.8502, grad_norm: 15.7409
2025-06-10 21:00:33,275 - mmdet - INFO - Epoch [6][5250/7033]	lr: 6.792e-06, eta: 0:31:06, time: 1.653, data_time: 0.585, memory: 17617, loss_cls: 0.0608, loss_bbox: 0.1927, d0.loss_cls: 0.1466, d0.loss_bbox: 0.3017, d1.loss_cls: 0.0880, d1.loss_bbox: 0.2263, d2.loss_cls: 0.0725, d2.loss_bbox: 0.2095, d3.loss_cls: 0.0666, d3.loss_bbox: 0.2061, d4.loss_cls: 0.0611, d4.loss_bbox: 0.1971, loss: 1.8291, grad_norm: 28.9485
2025-06-10 21:01:51,561 - mmdet - INFO - Epoch [6][5300/7033]	lr: 6.792e-06, eta: 0:30:15, time: 1.566, data_time: 0.512, memory: 17617, loss_cls: 0.0578, loss_bbox: 0.1919, d0.loss_cls: 0.1412, d0.loss_bbox: 0.2957, d1.loss_cls: 0.0879, d1.loss_bbox: 0.2231, d2.loss_cls: 0.0728, d2.loss_bbox: 0.2060, d3.loss_cls: 0.0636, d3.loss_bbox: 0.2019, d4.loss_cls: 0.0601, d4.loss_bbox: 0.1917, loss: 1.7937, grad_norm: 15.3816
2025-06-10 21:03:13,038 - mmdet - INFO - Epoch [6][5350/7033]	lr: 6.792e-06, eta: 0:29:23, time: 1.629, data_time: 0.493, memory: 17617, loss_cls: 0.0611, loss_bbox: 0.1955, d0.loss_cls: 0.1499, d0.loss_bbox: 0.3054, d1.loss_cls: 0.0903, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2083, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1976, loss: 1.8623, grad_norm: 36.9746
2025-06-10 21:04:33,986 - mmdet - INFO - Epoch [6][5400/7033]	lr: 6.792e-06, eta: 0:28:32, time: 1.619, data_time: 0.541, memory: 17617, loss_cls: 0.0690, loss_bbox: 0.1991, d0.loss_cls: 0.1526, d0.loss_bbox: 0.3053, d1.loss_cls: 0.0942, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2014, loss: 1.9112, grad_norm: 123.9444
2025-06-10 21:05:52,396 - mmdet - INFO - Epoch [6][5450/7033]	lr: 6.792e-06, eta: 0:27:41, time: 1.568, data_time: 0.481, memory: 17617, loss_cls: 0.0576, loss_bbox: 0.1916, d0.loss_cls: 0.1444, d0.loss_bbox: 0.2929, d1.loss_cls: 0.0855, d1.loss_bbox: 0.2231, d2.loss_cls: 0.0710, d2.loss_bbox: 0.2070, d3.loss_cls: 0.0613, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0591, d4.loss_bbox: 0.1942, loss: 1.7920, grad_norm: 19.0093
2025-06-10 21:07:10,246 - mmdet - INFO - Epoch [6][5500/7033]	lr: 6.792e-06, eta: 0:26:49, time: 1.557, data_time: 0.146, memory: 17617, loss_cls: 0.0625, loss_bbox: 0.2022, d0.loss_cls: 0.1497, d0.loss_bbox: 0.3124, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0785, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0648, d4.loss_bbox: 0.2028, loss: 1.9008, grad_norm: 33.8953
2025-06-10 21:08:35,988 - mmdet - INFO - Epoch [6][5550/7033]	lr: 6.792e-06, eta: 0:25:58, time: 1.715, data_time: 0.619, memory: 17617, loss_cls: 0.0696, loss_bbox: 0.2008, d0.loss_cls: 0.1528, d0.loss_bbox: 0.3107, d1.loss_cls: 0.0968, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2031, loss: 1.9315, grad_norm: 20.6988
2025-06-10 21:09:58,506 - mmdet - INFO - Epoch [6][5600/7033]	lr: 6.792e-06, eta: 0:25:06, time: 1.650, data_time: 0.525, memory: 17617, loss_cls: 0.0680, loss_bbox: 0.1993, d0.loss_cls: 0.1521, d0.loss_bbox: 0.3070, d1.loss_cls: 0.0946, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2011, loss: 1.9097, grad_norm: 24.2548
2025-06-10 21:11:21,184 - mmdet - INFO - Epoch [6][5650/7033]	lr: 6.792e-06, eta: 0:24:15, time: 1.654, data_time: 0.468, memory: 17617, loss_cls: 0.0630, loss_bbox: 0.1968, d0.loss_cls: 0.1450, d0.loss_bbox: 0.3070, d1.loss_cls: 0.0909, d1.loss_bbox: 0.2296, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2128, d3.loss_cls: 0.0651, d3.loss_bbox: 0.2109, d4.loss_cls: 0.0643, d4.loss_bbox: 0.1986, loss: 1.8603, grad_norm: 26.0033
2025-06-10 21:12:21,472 - mmdet - INFO - Epoch [6][5700/7033]	lr: 6.792e-06, eta: 0:23:23, time: 1.206, data_time: 0.150, memory: 17617, loss_cls: 0.0575, loss_bbox: 0.1977, d0.loss_cls: 0.1443, d0.loss_bbox: 0.3023, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0730, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0634, d3.loss_bbox: 0.2098, d4.loss_cls: 0.0604, d4.loss_bbox: 0.1990, loss: 1.8401, grad_norm: 19.9800
2025-06-10 21:13:13,217 - mmdet - INFO - Epoch [6][5750/7033]	lr: 6.792e-06, eta: 0:22:30, time: 1.035, data_time: 0.032, memory: 17617, loss_cls: 0.0638, loss_bbox: 0.1961, d0.loss_cls: 0.1454, d0.loss_bbox: 0.3077, d1.loss_cls: 0.0906, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0777, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0691, d3.loss_bbox: 0.2090, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1986, loss: 1.8679, grad_norm: 24.3587
2025-06-10 21:14:04,957 - mmdet - INFO - Epoch [6][5800/7033]	lr: 6.792e-06, eta: 0:21:37, time: 1.035, data_time: 0.033, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.2046, d0.loss_cls: 0.1452, d0.loss_bbox: 0.3100, d1.loss_cls: 0.0904, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0772, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0623, d4.loss_bbox: 0.2069, loss: 1.9000, grad_norm: 40.2036
2025-06-10 21:14:56,398 - mmdet - INFO - Epoch [6][5850/7033]	lr: 6.792e-06, eta: 0:20:45, time: 1.029, data_time: 0.029, memory: 17617, loss_cls: 0.0655, loss_bbox: 0.2011, d0.loss_cls: 0.1564, d0.loss_bbox: 0.3128, d1.loss_cls: 0.0978, d1.loss_bbox: 0.2376, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0714, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2030, loss: 1.9237, grad_norm: 41.8918
2025-06-10 21:15:47,961 - mmdet - INFO - Epoch [6][5900/7033]	lr: 6.792e-06, eta: 0:19:52, time: 1.031, data_time: 0.031, memory: 17617, loss_cls: 0.0615, loss_bbox: 0.1923, d0.loss_cls: 0.1431, d0.loss_bbox: 0.3029, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2270, d2.loss_cls: 0.0754, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0687, d3.loss_bbox: 0.2047, d4.loss_cls: 0.0638, d4.loss_bbox: 0.1947, loss: 1.8356, grad_norm: 26.2562
2025-06-10 21:16:40,038 - mmdet - INFO - Epoch [6][5950/7033]	lr: 6.792e-06, eta: 0:18:59, time: 1.042, data_time: 0.034, memory: 17617, loss_cls: 0.0621, loss_bbox: 0.1932, d0.loss_cls: 0.1482, d0.loss_bbox: 0.2984, d1.loss_cls: 0.0899, d1.loss_bbox: 0.2257, d2.loss_cls: 0.0769, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2060, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1947, loss: 1.8378, grad_norm: 60.8660
2025-06-10 21:17:31,901 - mmdet - INFO - Epoch [6][6000/7033]	lr: 6.792e-06, eta: 0:18:07, time: 1.037, data_time: 0.032, memory: 17617, loss_cls: 0.0617, loss_bbox: 0.1968, d0.loss_cls: 0.1532, d0.loss_bbox: 0.3015, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0774, d2.loss_bbox: 0.2124, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0636, d4.loss_bbox: 0.1988, loss: 1.8673, grad_norm: 49.1242
2025-06-10 21:18:23,981 - mmdet - INFO - Epoch [6][6050/7033]	lr: 6.792e-06, eta: 0:17:14, time: 1.042, data_time: 0.034, memory: 17617, loss_cls: 0.0607, loss_bbox: 0.1954, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3038, d1.loss_cls: 0.0921, d1.loss_bbox: 0.2308, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2119, d3.loss_cls: 0.0678, d3.loss_bbox: 0.2091, d4.loss_cls: 0.0620, d4.loss_bbox: 0.1975, loss: 1.8591, grad_norm: 24.4861
2025-06-10 21:19:15,711 - mmdet - INFO - Epoch [6][6100/7033]	lr: 6.792e-06, eta: 0:16:21, time: 1.035, data_time: 0.032, memory: 17617, loss_cls: 0.0597, loss_bbox: 0.1904, d0.loss_cls: 0.1429, d0.loss_bbox: 0.2997, d1.loss_cls: 0.0853, d1.loss_bbox: 0.2262, d2.loss_cls: 0.0731, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0656, d3.loss_bbox: 0.2035, d4.loss_cls: 0.0614, d4.loss_bbox: 0.1927, loss: 1.8093, grad_norm: 15.9189
2025-06-10 21:20:06,884 - mmdet - INFO - Epoch [6][6150/7033]	lr: 6.792e-06, eta: 0:15:29, time: 1.023, data_time: 0.030, memory: 17617, loss_cls: 0.0612, loss_bbox: 0.1931, d0.loss_cls: 0.1479, d0.loss_bbox: 0.3014, d1.loss_cls: 0.0912, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0776, d2.loss_bbox: 0.2099, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2063, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1955, loss: 1.8417, grad_norm: 24.2165
2025-06-10 21:21:03,055 - mmdet - INFO - Epoch [6][6200/7033]	lr: 6.792e-06, eta: 0:14:36, time: 1.123, data_time: 0.129, memory: 17617, loss_cls: 0.0621, loss_bbox: 0.1950, d0.loss_cls: 0.1422, d0.loss_bbox: 0.3065, d1.loss_cls: 0.0883, d1.loss_bbox: 0.2318, d2.loss_cls: 0.0747, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0672, d3.loss_bbox: 0.2077, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1974, loss: 1.8474, grad_norm: 23.2723
2025-06-10 21:21:54,274 - mmdet - INFO - Epoch [6][6250/7033]	lr: 6.792e-06, eta: 0:13:44, time: 1.024, data_time: 0.031, memory: 17617, loss_cls: 0.0631, loss_bbox: 0.1919, d0.loss_cls: 0.1452, d0.loss_bbox: 0.2949, d1.loss_cls: 0.0935, d1.loss_bbox: 0.2261, d2.loss_cls: 0.0770, d2.loss_bbox: 0.2097, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2048, d4.loss_cls: 0.0662, d4.loss_bbox: 0.1944, loss: 1.8375, grad_norm: 36.1316
2025-06-10 21:22:45,355 - mmdet - INFO - Epoch [6][6300/7033]	lr: 6.792e-06, eta: 0:12:51, time: 1.022, data_time: 0.029, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.1899, d0.loss_cls: 0.1481, d0.loss_bbox: 0.3003, d1.loss_cls: 0.0888, d1.loss_bbox: 0.2262, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2093, d3.loss_cls: 0.0660, d3.loss_bbox: 0.2027, d4.loss_cls: 0.0623, d4.loss_bbox: 0.1919, loss: 1.8221, grad_norm: 42.7189
2025-06-10 21:23:36,788 - mmdet - INFO - Epoch [6][6350/7033]	lr: 6.792e-06, eta: 0:11:58, time: 1.029, data_time: 0.031, memory: 17617, loss_cls: 0.0617, loss_bbox: 0.1980, d0.loss_cls: 0.1486, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0699, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2003, loss: 1.8816, grad_norm: 21.4239
2025-06-10 21:24:28,707 - mmdet - INFO - Epoch [6][6400/7033]	lr: 6.792e-06, eta: 0:11:06, time: 1.038, data_time: 0.031, memory: 17617, loss_cls: 0.0661, loss_bbox: 0.2058, d0.loss_cls: 0.1512, d0.loss_bbox: 0.3083, d1.loss_cls: 0.0911, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0793, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2064, loss: 1.9235, grad_norm: 27.2557
2025-06-10 21:25:20,347 - mmdet - INFO - Epoch [6][6450/7033]	lr: 6.792e-06, eta: 0:10:13, time: 1.033, data_time: 0.030, memory: 17617, loss_cls: 0.0606, loss_bbox: 0.2018, d0.loss_cls: 0.1478, d0.loss_bbox: 0.3150, d1.loss_cls: 0.0889, d1.loss_bbox: 0.2388, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0670, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0620, d4.loss_bbox: 0.2047, loss: 1.8947, grad_norm: 100.2617
2025-06-10 21:26:12,234 - mmdet - INFO - Epoch [6][6500/7033]	lr: 6.792e-06, eta: 0:09:20, time: 1.038, data_time: 0.030, memory: 17617, loss_cls: 0.0598, loss_bbox: 0.2004, d0.loss_cls: 0.1469, d0.loss_bbox: 0.2979, d1.loss_cls: 0.0888, d1.loss_bbox: 0.2300, d2.loss_cls: 0.0760, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0668, d3.loss_bbox: 0.2099, d4.loss_cls: 0.0618, d4.loss_bbox: 0.2013, loss: 1.8547, grad_norm: 20.0440
2025-06-10 21:27:04,573 - mmdet - INFO - Epoch [6][6550/7033]	lr: 6.792e-06, eta: 0:08:28, time: 1.047, data_time: 0.034, memory: 17617, loss_cls: 0.0670, loss_bbox: 0.1975, d0.loss_cls: 0.1504, d0.loss_bbox: 0.3110, d1.loss_cls: 0.0953, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0806, d2.loss_bbox: 0.2146, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0674, d4.loss_bbox: 0.2005, loss: 1.9019, grad_norm: 28.2930
2025-06-10 21:27:56,784 - mmdet - INFO - Epoch [6][6600/7033]	lr: 6.792e-06, eta: 0:07:35, time: 1.044, data_time: 0.031, memory: 17617, loss_cls: 0.0679, loss_bbox: 0.1954, d0.loss_cls: 0.1525, d0.loss_bbox: 0.3078, d1.loss_cls: 0.0944, d1.loss_bbox: 0.2334, d2.loss_cls: 0.0823, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2081, d4.loss_cls: 0.0703, d4.loss_bbox: 0.1968, loss: 1.8970, grad_norm: 17.8596
2025-06-10 21:28:48,895 - mmdet - INFO - Epoch [6][6650/7033]	lr: 6.792e-06, eta: 0:06:43, time: 1.042, data_time: 0.029, memory: 17617, loss_cls: 0.0649, loss_bbox: 0.1998, d0.loss_cls: 0.1462, d0.loss_bbox: 0.3008, d1.loss_cls: 0.0896, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0762, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0679, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0648, d4.loss_bbox: 0.2017, loss: 1.8710, grad_norm: 24.3066
2025-06-10 21:29:41,188 - mmdet - INFO - Epoch [6][6700/7033]	lr: 6.792e-06, eta: 0:05:50, time: 1.046, data_time: 0.035, memory: 17617, loss_cls: 0.0627, loss_bbox: 0.1996, d0.loss_cls: 0.1453, d0.loss_bbox: 0.3056, d1.loss_cls: 0.0886, d1.loss_bbox: 0.2319, d2.loss_cls: 0.0750, d2.loss_bbox: 0.2154, d3.loss_cls: 0.0681, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0637, d4.loss_bbox: 0.2018, loss: 1.8688, grad_norm: 25.3890
2025-06-10 21:30:33,315 - mmdet - INFO - Epoch [6][6750/7033]	lr: 6.792e-06, eta: 0:04:57, time: 1.043, data_time: 0.031, memory: 17617, loss_cls: 0.0654, loss_bbox: 0.2011, d0.loss_cls: 0.1490, d0.loss_bbox: 0.3055, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2332, d2.loss_cls: 0.0837, d2.loss_bbox: 0.2163, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0673, d4.loss_bbox: 0.2021, loss: 1.9066, grad_norm: 55.2579
2025-06-10 21:31:25,800 - mmdet - INFO - Epoch [6][6800/7033]	lr: 6.792e-06, eta: 0:04:05, time: 1.050, data_time: 0.037, memory: 17617, loss_cls: 0.0639, loss_bbox: 0.1937, d0.loss_cls: 0.1440, d0.loss_bbox: 0.2985, d1.loss_cls: 0.0867, d1.loss_bbox: 0.2278, d2.loss_cls: 0.0745, d2.loss_bbox: 0.2105, d3.loss_cls: 0.0671, d3.loss_bbox: 0.2062, d4.loss_cls: 0.0633, d4.loss_bbox: 0.1961, loss: 1.8324, grad_norm: 30.1655
2025-06-10 21:32:17,481 - mmdet - INFO - Epoch [6][6850/7033]	lr: 6.792e-06, eta: 0:03:12, time: 1.034, data_time: 0.028, memory: 17617, loss_cls: 0.0661, loss_bbox: 0.1991, d0.loss_cls: 0.1552, d0.loss_bbox: 0.3096, d1.loss_cls: 0.0961, d1.loss_bbox: 0.2317, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0716, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0684, d4.loss_bbox: 0.1991, loss: 1.9023, grad_norm: 27.1515
2025-06-10 21:33:08,562 - mmdet - INFO - Epoch [6][6900/7033]	lr: 6.792e-06, eta: 0:02:19, time: 1.022, data_time: 0.027, memory: 17617, loss_cls: 0.0610, loss_bbox: 0.1978, d0.loss_cls: 0.1447, d0.loss_bbox: 0.3018, d1.loss_cls: 0.0893, d1.loss_bbox: 0.2309, d2.loss_cls: 0.0742, d2.loss_bbox: 0.2133, d3.loss_cls: 0.0649, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0619, d4.loss_bbox: 0.1998, loss: 1.8499, grad_norm: 21.3136
2025-06-10 21:33:59,452 - mmdet - INFO - Epoch [6][6950/7033]	lr: 6.792e-06, eta: 0:01:27, time: 1.018, data_time: 0.029, memory: 17617, loss_cls: 0.0633, loss_bbox: 0.2000, d0.loss_cls: 0.1508, d0.loss_bbox: 0.3091, d1.loss_cls: 0.0919, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0798, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0698, d3.loss_bbox: 0.2113, d4.loss_cls: 0.0654, d4.loss_bbox: 0.2017, loss: 1.8900, grad_norm: 26.0540
2025-06-10 21:34:53,212 - mmdet - INFO - Epoch [6][7000/7033]	lr: 6.792e-06, eta: 0:00:34, time: 1.075, data_time: 0.026, memory: 17617, loss_cls: 0.0575, loss_bbox: 0.1931, d0.loss_cls: 0.1411, d0.loss_bbox: 0.3027, d1.loss_cls: 0.0915, d1.loss_bbox: 0.2263, d2.loss_cls: 0.0735, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0643, d3.loss_bbox: 0.2051, d4.loss_cls: 0.0601, d4.loss_bbox: 0.1952, loss: 1.8191, grad_norm: 32.8632
2025-06-10 21:35:26,919 - mmdet - INFO - Saving checkpoint at 6 epochs
2025-06-10 22:00:54,035 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-10 22:00:54,035 - mmdet - INFO - Epoch(val) [6][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7925, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8837, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9107, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9234, pts_bbox_NuScenes/car_trans_err: 0.1747, pts_bbox_NuScenes/car_scale_err: 0.1508, pts_bbox_NuScenes/car_orient_err: 0.0408, pts_bbox_NuScenes/car_vel_err: 0.3064, pts_bbox_NuScenes/car_attr_err: 0.1841, pts_bbox_NuScenes/mATE: 0.2849, pts_bbox_NuScenes/mASE: 0.2635, pts_bbox_NuScenes/mAOE: 0.2629, pts_bbox_NuScenes/mAVE: 0.2805, pts_bbox_NuScenes/mAAE: 0.1867, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4231, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6110, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7211, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7570, pts_bbox_NuScenes/truck_trans_err: 0.3443, pts_bbox_NuScenes/truck_scale_err: 0.2004, pts_bbox_NuScenes/truck_orient_err: 0.0452, pts_bbox_NuScenes/truck_vel_err: 0.2822, pts_bbox_NuScenes/truck_attr_err: 0.2017, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0571, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2037, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3984, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4698, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6558, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4363, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8234, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1185, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2989, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5212, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7555, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.9027, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9266, pts_bbox_NuScenes/bus_trans_err: 0.3288, pts_bbox_NuScenes/bus_scale_err: 0.1962, pts_bbox_NuScenes/bus_orient_err: 0.0518, pts_bbox_NuScenes/bus_vel_err: 0.4827, pts_bbox_NuScenes/bus_attr_err: 0.3026, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1925, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4366, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5972, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6732, pts_bbox_NuScenes/trailer_trans_err: 0.4661, pts_bbox_NuScenes/trailer_scale_err: 0.2347, pts_bbox_NuScenes/trailer_orient_err: 0.4993, pts_bbox_NuScenes/trailer_vel_err: 0.2382, pts_bbox_NuScenes/trailer_attr_err: 0.1686, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.6106, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7138, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7631, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7767, pts_bbox_NuScenes/barrier_trans_err: 0.2097, pts_bbox_NuScenes/barrier_scale_err: 0.2868, pts_bbox_NuScenes/barrier_orient_err: 0.0460, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6515, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7788, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8102, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8157, pts_bbox_NuScenes/motorcycle_trans_err: 0.2091, pts_bbox_NuScenes/motorcycle_scale_err: 0.2478, pts_bbox_NuScenes/motorcycle_orient_err: 0.2206, pts_bbox_NuScenes/motorcycle_vel_err: 0.3903, pts_bbox_NuScenes/motorcycle_attr_err: 0.2217, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5413, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6010, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6128, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6230, pts_bbox_NuScenes/bicycle_trans_err: 0.1784, pts_bbox_NuScenes/bicycle_scale_err: 0.2607, pts_bbox_NuScenes/bicycle_orient_err: 0.3167, pts_bbox_NuScenes/bicycle_vel_err: 0.2042, pts_bbox_NuScenes/bicycle_attr_err: 0.0046, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8192, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8618, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8818, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8930, pts_bbox_NuScenes/pedestrian_trans_err: 0.1467, pts_bbox_NuScenes/pedestrian_scale_err: 0.2935, pts_bbox_NuScenes/pedestrian_orient_err: 0.3222, pts_bbox_NuScenes/pedestrian_vel_err: 0.2214, pts_bbox_NuScenes/pedestrian_attr_err: 0.1115, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7320, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7711, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7960, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8166, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1350, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3281, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7100, pts_bbox_NuScenes/mAP: 0.6757
