2025-06-17 14:54:33,547 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+3056288
spconv2.0: True
------------------------------------------------------------

2025-06-17 14:54:35,739 - mmdet - INFO - 分布式训练: True
2025-06-17 14:54:37,524 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4092_LR2_SYNC_cudnn'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
cudnn_benchmark = True
gpu_ids = range(0, 2)

2025-06-17 14:54:37,525 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-17 14:54:39,626 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-17 14:54:40,219 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-17 14:54:40,433 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,434 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,435 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,436 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,437 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,437 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,438 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,440 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,446 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,452 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,458 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,464 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,470 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,476 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,482 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,488 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,493 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,500 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,505 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,512 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,517 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,524 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,530 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,536 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,542 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,548 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,554 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,560 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,566 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,572 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,598 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,621 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,642 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 14:54:40,706 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy - torch.Size([512, 64]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight - torch.Size([144, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([512, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight - torch.Size([512, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight - torch.Size([2048, 2048]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-17 14:54:40,747 - mmdet - INFO - 使用SyncBN
2025-06-17 14:54:40,784 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): ModuleList(
                (0): MultiheadAttention(
                  (attn): MultiheadAttention(
                    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                  )
                  (proj_drop): Dropout(p=0.0, inplace=False)
                  (dropout_layer): Dropout(p=0.1, inplace=False)
                )
                (1): RMSNorm()
                (2): DSS(
                  (layers): ModuleList(
                    (0): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): Identity()
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): RMSNorm()
                    )
                    (1): ModuleDict(
                      (norm_before): RMSNorm()
                      (mamba): DSSMamba(
                        (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                        (in_proj_xy): Linear(in_features=256, out_features=2048, bias=False)
                        (act): SiLU()
                        (x_proj_f): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b): Linear(in_features=16, out_features=512, bias=True)
                        (act_xy): SiLU()
                        (x_proj_f_xy): Linear(in_features=512, out_features=144, bias=False)
                        (x_proj_b_xy): Linear(in_features=512, out_features=144, bias=False)
                        (dt_proj_f_xy): Linear(in_features=16, out_features=512, bias=True)
                        (dt_proj_b_xy): Linear(in_features=16, out_features=512, bias=True)
                        (conv1d_x_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_f): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_x_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (conv1d_z_xy_b): Conv1d(512, 512, kernel_size=(4,), stride=(1,), groups=512)
                        (out_proj): Linear(in_features=2048, out_features=256, bias=False)
                        (global_proj): Linear(in_features=2048, out_features=2048, bias=True)
                      )
                      (dropout): DropPath(drop_prob=0.300)
                      (norm): RMSNorm()
                      (mlp): MLP(
                        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
                        (down_proj): Linear(in_features=1024, out_features=256, bias=False)
                        (act_fn): SiLU()
                      )
                      (mlp_norm): Identity()
                    )
                  )
                  (rope): RotaryEmbedding()
                )
                (3): DropPath(drop_prob=0.100)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-17 14:55:12,424 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-17 14:55:13,805 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias

2025-06-17 14:55:13,832 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4092_LR2_SYNC_cudnn
2025-06-17 14:55:13,833 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-17 14:55:13,834 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-17 14:55:13,836 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2/Port4092_LR2_SYNC_cudnn by HardDiskBackend.
2025-06-17 14:57:04,172 - mmdet - INFO - Epoch [1][50/7033]	lr: 7.973e-05, eta: 1 day, 1:43:05, time: 2.197, data_time: 0.179, memory: 17921, loss_cls: 1.0440, loss_bbox: 1.4655, d0.loss_cls: 1.0403, d0.loss_bbox: 1.5364, d1.loss_cls: 1.0689, d1.loss_bbox: 1.4929, d2.loss_cls: 0.9808, d2.loss_bbox: 1.4525, d3.loss_cls: 0.9494, d3.loss_bbox: 1.4638, d4.loss_cls: 0.9511, d4.loss_bbox: 1.4310, loss: 14.8766, grad_norm: 15.7044
2025-06-17 14:58:39,577 - mmdet - INFO - Epoch [1][100/7033]	lr: 9.307e-05, eta: 1 day, 0:00:01, time: 1.908, data_time: 0.059, memory: 17921, loss_cls: 0.7693, loss_bbox: 1.1548, d0.loss_cls: 0.8558, d0.loss_bbox: 1.2900, d1.loss_cls: 0.7942, d1.loss_bbox: 1.2141, d2.loss_cls: 0.7629, d2.loss_bbox: 1.1831, d3.loss_cls: 0.7596, d3.loss_bbox: 1.1773, d4.loss_cls: 0.7492, d4.loss_bbox: 1.1578, loss: 11.8684, grad_norm: 12.0240
2025-06-17 15:00:15,309 - mmdet - INFO - Epoch [1][150/7033]	lr: 1.064e-04, eta: 23:26:02, time: 1.914, data_time: 0.061, memory: 17921, loss_cls: 0.6576, loss_bbox: 1.0793, d0.loss_cls: 0.7794, d0.loss_bbox: 1.2452, d1.loss_cls: 0.6992, d1.loss_bbox: 1.1443, d2.loss_cls: 0.6825, d2.loss_bbox: 1.1028, d3.loss_cls: 0.6685, d3.loss_bbox: 1.1009, d4.loss_cls: 0.6485, d4.loss_bbox: 1.0853, loss: 10.8935, grad_norm: 10.5737
2025-06-17 15:01:52,671 - mmdet - INFO - Epoch [1][200/7033]	lr: 1.197e-04, eta: 23:14:03, time: 1.947, data_time: 0.079, memory: 17921, loss_cls: 0.6006, loss_bbox: 1.0446, d0.loss_cls: 0.7329, d0.loss_bbox: 1.2167, d1.loss_cls: 0.6595, d1.loss_bbox: 1.1155, d2.loss_cls: 0.6423, d2.loss_bbox: 1.0691, d3.loss_cls: 0.6305, d3.loss_bbox: 1.0660, d4.loss_cls: 0.6000, d4.loss_bbox: 1.0526, loss: 10.4301, grad_norm: 10.0779
2025-06-17 15:03:28,025 - mmdet - INFO - Epoch [1][250/7033]	lr: 1.331e-04, eta: 23:00:34, time: 1.907, data_time: 0.049, memory: 17921, loss_cls: 0.5627, loss_bbox: 1.0095, d0.loss_cls: 0.6863, d0.loss_bbox: 1.1840, d1.loss_cls: 0.6189, d1.loss_bbox: 1.0734, d2.loss_cls: 0.6116, d2.loss_bbox: 1.0356, d3.loss_cls: 0.6007, d3.loss_bbox: 1.0267, d4.loss_cls: 0.5542, d4.loss_bbox: 1.0213, loss: 9.9849, grad_norm: 9.7980
2025-06-17 15:05:05,653 - mmdet - INFO - Epoch [1][300/7033]	lr: 1.464e-04, eta: 22:56:17, time: 1.952, data_time: 0.056, memory: 17921, loss_cls: 0.5091, loss_bbox: 0.9598, d0.loss_cls: 0.6552, d0.loss_bbox: 1.1382, d1.loss_cls: 0.5739, d1.loss_bbox: 1.0273, d2.loss_cls: 0.5602, d2.loss_bbox: 0.9800, d3.loss_cls: 0.5472, d3.loss_bbox: 0.9735, d4.loss_cls: 0.5024, d4.loss_bbox: 0.9676, loss: 9.3944, grad_norm: 10.5171
2025-06-17 15:06:51,404 - mmdet - INFO - Epoch [1][350/7033]	lr: 1.597e-04, eta: 23:09:03, time: 2.116, data_time: 0.059, memory: 17921, loss_cls: 0.4763, loss_bbox: 0.9602, d0.loss_cls: 0.6208, d0.loss_bbox: 1.1523, d1.loss_cls: 0.5245, d1.loss_bbox: 1.0396, d2.loss_cls: 0.5217, d2.loss_bbox: 0.9847, d3.loss_cls: 0.5069, d3.loss_bbox: 0.9791, d4.loss_cls: 0.4688, d4.loss_bbox: 0.9726, loss: 9.2074, grad_norm: 10.9779
2025-06-17 15:08:27,227 - mmdet - INFO - Epoch [1][400/7033]	lr: 1.731e-04, eta: 23:00:49, time: 1.916, data_time: 0.074, memory: 17921, loss_cls: 0.4276, loss_bbox: 0.9298, d0.loss_cls: 0.5839, d0.loss_bbox: 1.1205, d1.loss_cls: 0.4843, d1.loss_bbox: 0.9959, d2.loss_cls: 0.4659, d2.loss_bbox: 0.9471, d3.loss_cls: 0.4528, d3.loss_bbox: 0.9413, d4.loss_cls: 0.4228, d4.loss_bbox: 0.9371, loss: 8.7089, grad_norm: 10.2602
2025-06-17 15:10:05,200 - mmdet - INFO - Epoch [1][450/7033]	lr: 1.864e-04, eta: 22:57:25, time: 1.959, data_time: 0.064, memory: 17921, loss_cls: 0.4210, loss_bbox: 0.9004, d0.loss_cls: 0.5496, d0.loss_bbox: 1.0910, d1.loss_cls: 0.4336, d1.loss_bbox: 0.9656, d2.loss_cls: 0.4308, d2.loss_bbox: 0.9204, d3.loss_cls: 0.4300, d3.loss_bbox: 0.9114, d4.loss_cls: 0.4117, d4.loss_bbox: 0.9103, loss: 8.3757, grad_norm: 11.2993
2025-06-17 15:11:43,663 - mmdet - INFO - Epoch [1][500/7033]	lr: 1.997e-04, eta: 22:54:57, time: 1.968, data_time: 0.065, memory: 17921, loss_cls: 0.3535, loss_bbox: 0.8643, d0.loss_cls: 0.5484, d0.loss_bbox: 1.0918, d1.loss_cls: 0.3598, d1.loss_bbox: 0.9371, d2.loss_cls: 0.3452, d2.loss_bbox: 0.8770, d3.loss_cls: 0.3409, d3.loss_bbox: 0.8678, d4.loss_cls: 0.3429, d4.loss_bbox: 0.8560, loss: 7.7846, grad_norm: 12.1949
2025-06-17 15:13:24,423 - mmdet - INFO - Epoch [1][550/7033]	lr: 2.000e-04, eta: 22:55:43, time: 2.017, data_time: 0.078, memory: 17921, loss_cls: 0.3103, loss_bbox: 0.8270, d0.loss_cls: 0.5287, d0.loss_bbox: 1.0695, d1.loss_cls: 0.3247, d1.loss_bbox: 0.9066, d2.loss_cls: 0.3106, d2.loss_bbox: 0.8445, d3.loss_cls: 0.2989, d3.loss_bbox: 0.8345, d4.loss_cls: 0.3045, d4.loss_bbox: 0.8202, loss: 7.3799, grad_norm: 14.6288
2025-06-17 15:15:06,276 - mmdet - INFO - Epoch [1][600/7033]	lr: 2.000e-04, eta: 22:57:14, time: 2.037, data_time: 0.058, memory: 17921, loss_cls: 0.2765, loss_bbox: 0.7915, d0.loss_cls: 0.4719, d0.loss_bbox: 1.0453, d1.loss_cls: 0.3144, d1.loss_bbox: 0.8651, d2.loss_cls: 0.2865, d2.loss_bbox: 0.8003, d3.loss_cls: 0.2728, d3.loss_bbox: 0.7911, d4.loss_cls: 0.2701, d4.loss_bbox: 0.7833, loss: 6.9687, grad_norm: 14.0492
2025-06-17 15:16:49,702 - mmdet - INFO - Epoch [1][650/7033]	lr: 2.000e-04, eta: 22:59:52, time: 2.067, data_time: 0.078, memory: 17921, loss_cls: 0.2609, loss_bbox: 0.7711, d0.loss_cls: 0.4393, d0.loss_bbox: 1.0224, d1.loss_cls: 0.2963, d1.loss_bbox: 0.8042, d2.loss_cls: 0.2699, d2.loss_bbox: 0.7439, d3.loss_cls: 0.2575, d3.loss_bbox: 0.7440, d4.loss_cls: 0.2517, d4.loss_bbox: 0.7541, loss: 6.6152, grad_norm: 15.6625
2025-06-17 15:18:26,366 - mmdet - INFO - Epoch [1][700/7033]	lr: 2.000e-04, eta: 22:55:21, time: 1.935, data_time: 0.070, memory: 17921, loss_cls: 0.2443, loss_bbox: 0.6547, d0.loss_cls: 0.4248, d0.loss_bbox: 0.9519, d1.loss_cls: 0.3104, d1.loss_bbox: 0.6663, d2.loss_cls: 0.2635, d2.loss_bbox: 0.6123, d3.loss_cls: 0.2441, d3.loss_bbox: 0.6212, d4.loss_cls: 0.2419, d4.loss_bbox: 0.6282, loss: 5.8635, grad_norm: 19.8853
2025-06-17 15:20:03,782 - mmdet - INFO - Epoch [1][750/7033]	lr: 2.000e-04, eta: 22:51:50, time: 1.948, data_time: 0.074, memory: 17921, loss_cls: 0.2301, loss_bbox: 0.6241, d0.loss_cls: 0.3852, d0.loss_bbox: 0.9033, d1.loss_cls: 0.2983, d1.loss_bbox: 0.5900, d2.loss_cls: 0.2471, d2.loss_bbox: 0.5643, d3.loss_cls: 0.2250, d3.loss_bbox: 0.5815, d4.loss_cls: 0.2277, d4.loss_bbox: 0.5897, loss: 5.4663, grad_norm: 21.5085
2025-06-17 15:21:37,023 - mmdet - INFO - Epoch [1][800/7033]	lr: 2.000e-04, eta: 22:44:57, time: 1.865, data_time: 0.060, memory: 17921, loss_cls: 0.1999, loss_bbox: 0.5570, d0.loss_cls: 0.3191, d0.loss_bbox: 0.7562, d1.loss_cls: 0.2998, d1.loss_bbox: 0.4978, d2.loss_cls: 0.2380, d2.loss_bbox: 0.4902, d3.loss_cls: 0.2081, d3.loss_bbox: 0.5052, d4.loss_cls: 0.2009, d4.loss_bbox: 0.5249, loss: 4.7971, grad_norm: 27.4805
2025-06-17 15:23:10,906 - mmdet - INFO - Epoch [1][850/7033]	lr: 2.000e-04, eta: 22:39:13, time: 1.877, data_time: 0.063, memory: 17921, loss_cls: 0.1994, loss_bbox: 0.5306, d0.loss_cls: 0.2835, d0.loss_bbox: 0.6980, d1.loss_cls: 0.2676, d1.loss_bbox: 0.4862, d2.loss_cls: 0.2208, d2.loss_bbox: 0.4801, d3.loss_cls: 0.1995, d3.loss_bbox: 0.4928, d4.loss_cls: 0.1966, d4.loss_bbox: 0.5064, loss: 4.5614, grad_norm: 27.4408
2025-06-17 15:24:45,201 - mmdet - INFO - Epoch [1][900/7033]	lr: 2.000e-04, eta: 22:34:16, time: 1.886, data_time: 0.067, memory: 17921, loss_cls: 0.1851, loss_bbox: 0.4903, d0.loss_cls: 0.2522, d0.loss_bbox: 0.6039, d1.loss_cls: 0.2431, d1.loss_bbox: 0.4391, d2.loss_cls: 0.2077, d2.loss_bbox: 0.4413, d3.loss_cls: 0.1822, d3.loss_bbox: 0.4561, d4.loss_cls: 0.1805, d4.loss_bbox: 0.4725, loss: 4.1542, grad_norm: 30.0333
2025-06-17 15:26:18,947 - mmdet - INFO - Epoch [1][950/7033]	lr: 2.000e-04, eta: 22:29:17, time: 1.875, data_time: 0.072, memory: 17921, loss_cls: 0.1623, loss_bbox: 0.4320, d0.loss_cls: 0.2421, d0.loss_bbox: 0.5391, d1.loss_cls: 0.2039, d1.loss_bbox: 0.4048, d2.loss_cls: 0.1745, d2.loss_bbox: 0.3939, d3.loss_cls: 0.1611, d3.loss_bbox: 0.4029, d4.loss_cls: 0.1591, d4.loss_bbox: 0.4144, loss: 3.6900, grad_norm: 25.0292
2025-06-17 15:27:52,828 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 15:27:52,828 - mmdet - INFO - Epoch [1][1000/7033]	lr: 2.000e-04, eta: 22:24:42, time: 1.877, data_time: 0.065, memory: 17921, loss_cls: 0.1509, loss_bbox: 0.3883, d0.loss_cls: 0.2356, d0.loss_bbox: 0.4768, d1.loss_cls: 0.1897, d1.loss_bbox: 0.3634, d2.loss_cls: 0.1642, d2.loss_bbox: 0.3587, d3.loss_cls: 0.1468, d3.loss_bbox: 0.3681, d4.loss_cls: 0.1466, d4.loss_bbox: 0.3745, loss: 3.3636, grad_norm: 32.2663
2025-06-17 15:29:30,504 - mmdet - INFO - Epoch [1][1050/7033]	lr: 2.000e-04, eta: 22:22:52, time: 1.952, data_time: 0.065, memory: 17921, loss_cls: 0.1573, loss_bbox: 0.4078, d0.loss_cls: 0.2346, d0.loss_bbox: 0.5019, d1.loss_cls: 0.1921, d1.loss_bbox: 0.3822, d2.loss_cls: 0.1657, d2.loss_bbox: 0.3706, d3.loss_cls: 0.1562, d3.loss_bbox: 0.3786, d4.loss_cls: 0.1554, d4.loss_bbox: 0.3933, loss: 3.4957, grad_norm: 25.8374
2025-06-17 15:31:07,530 - mmdet - INFO - Epoch [1][1100/7033]	lr: 2.000e-04, eta: 22:20:44, time: 1.942, data_time: 0.057, memory: 17921, loss_cls: 0.1509, loss_bbox: 0.3791, d0.loss_cls: 0.2295, d0.loss_bbox: 0.4667, d1.loss_cls: 0.1780, d1.loss_bbox: 0.3559, d2.loss_cls: 0.1581, d2.loss_bbox: 0.3440, d3.loss_cls: 0.1489, d3.loss_bbox: 0.3518, d4.loss_cls: 0.1478, d4.loss_bbox: 0.3616, loss: 3.2723, grad_norm: 31.0775
2025-06-17 15:32:41,820 - mmdet - INFO - Epoch [1][1150/7033]	lr: 2.000e-04, eta: 22:16:59, time: 1.886, data_time: 0.065, memory: 17921, loss_cls: 0.1518, loss_bbox: 0.3946, d0.loss_cls: 0.2257, d0.loss_bbox: 0.4711, d1.loss_cls: 0.1829, d1.loss_bbox: 0.3478, d2.loss_cls: 0.1560, d2.loss_bbox: 0.3433, d3.loss_cls: 0.1509, d3.loss_bbox: 0.3547, d4.loss_cls: 0.1482, d4.loss_bbox: 0.3696, loss: 3.2965, grad_norm: 59.8945
2025-06-17 15:34:17,799 - mmdet - INFO - Epoch [1][1200/7033]	lr: 2.000e-04, eta: 22:14:20, time: 1.918, data_time: 0.066, memory: 17921, loss_cls: 0.1518, loss_bbox: 0.3907, d0.loss_cls: 0.2195, d0.loss_bbox: 0.4756, d1.loss_cls: 0.1779, d1.loss_bbox: 0.3583, d2.loss_cls: 0.1548, d2.loss_bbox: 0.3520, d3.loss_cls: 0.1502, d3.loss_bbox: 0.3585, d4.loss_cls: 0.1475, d4.loss_bbox: 0.3716, loss: 3.3083, grad_norm: 24.6342
2025-06-17 15:35:51,257 - mmdet - INFO - Epoch [1][1250/7033]	lr: 2.000e-04, eta: 22:10:27, time: 1.870, data_time: 0.070, memory: 17921, loss_cls: 0.1398, loss_bbox: 0.3821, d0.loss_cls: 0.2217, d0.loss_bbox: 0.4601, d1.loss_cls: 0.1745, d1.loss_bbox: 0.3474, d2.loss_cls: 0.1496, d2.loss_bbox: 0.3422, d3.loss_cls: 0.1407, d3.loss_bbox: 0.3496, d4.loss_cls: 0.1381, d4.loss_bbox: 0.3616, loss: 3.2073, grad_norm: 478.8868
2025-06-17 15:37:25,216 - mmdet - INFO - Epoch [1][1300/7033]	lr: 2.000e-04, eta: 22:06:59, time: 1.879, data_time: 0.069, memory: 17921, loss_cls: 0.1343, loss_bbox: 0.3471, d0.loss_cls: 0.2137, d0.loss_bbox: 0.4415, d1.loss_cls: 0.1624, d1.loss_bbox: 0.3311, d2.loss_cls: 0.1458, d2.loss_bbox: 0.3177, d3.loss_cls: 0.1355, d3.loss_bbox: 0.3238, d4.loss_cls: 0.1351, d4.loss_bbox: 0.3304, loss: 3.0184, grad_norm: 37.3203
2025-06-17 15:38:57,137 - mmdet - INFO - Epoch [1][1350/7033]	lr: 2.000e-04, eta: 22:02:37, time: 1.838, data_time: 0.063, memory: 17921, loss_cls: 0.1333, loss_bbox: 0.3610, d0.loss_cls: 0.2105, d0.loss_bbox: 0.4491, d1.loss_cls: 0.1631, d1.loss_bbox: 0.3312, d2.loss_cls: 0.1427, d2.loss_bbox: 0.3245, d3.loss_cls: 0.1303, d3.loss_bbox: 0.3339, d4.loss_cls: 0.1287, d4.loss_bbox: 0.3442, loss: 3.0525, grad_norm: 28.8463
2025-06-17 15:40:34,099 - mmdet - INFO - Epoch [1][1400/7033]	lr: 2.000e-04, eta: 22:00:55, time: 1.939, data_time: 0.055, memory: 17921, loss_cls: 0.1304, loss_bbox: 0.3443, d0.loss_cls: 0.2074, d0.loss_bbox: 0.4312, d1.loss_cls: 0.1628, d1.loss_bbox: 0.3208, d2.loss_cls: 0.1362, d2.loss_bbox: 0.3115, d3.loss_cls: 0.1285, d3.loss_bbox: 0.3191, d4.loss_cls: 0.1278, d4.loss_bbox: 0.3282, loss: 2.9482, grad_norm: 37.2689
2025-06-17 15:42:13,150 - mmdet - INFO - Epoch [1][1450/7033]	lr: 2.000e-04, eta: 22:00:12, time: 1.981, data_time: 0.064, memory: 17921, loss_cls: 0.1360, loss_bbox: 0.3558, d0.loss_cls: 0.2066, d0.loss_bbox: 0.4352, d1.loss_cls: 0.1638, d1.loss_bbox: 0.3245, d2.loss_cls: 0.1447, d2.loss_bbox: 0.3194, d3.loss_cls: 0.1360, d3.loss_bbox: 0.3306, d4.loss_cls: 0.1352, d4.loss_bbox: 0.3398, loss: 3.0276, grad_norm: 25.3185
2025-06-17 15:43:48,715 - mmdet - INFO - Epoch [1][1500/7033]	lr: 2.000e-04, eta: 21:57:51, time: 1.911, data_time: 0.062, memory: 17921, loss_cls: 0.1266, loss_bbox: 0.3308, d0.loss_cls: 0.2040, d0.loss_bbox: 0.4282, d1.loss_cls: 0.1592, d1.loss_bbox: 0.3143, d2.loss_cls: 0.1321, d2.loss_bbox: 0.3050, d3.loss_cls: 0.1245, d3.loss_bbox: 0.3134, d4.loss_cls: 0.1241, d4.loss_bbox: 0.3182, loss: 2.8804, grad_norm: 77.6037
2025-06-17 15:45:25,374 - mmdet - INFO - Epoch [1][1550/7033]	lr: 2.000e-04, eta: 21:56:01, time: 1.933, data_time: 0.078, memory: 17921, loss_cls: 0.1275, loss_bbox: 0.3577, d0.loss_cls: 0.2143, d0.loss_bbox: 0.4246, d1.loss_cls: 0.1651, d1.loss_bbox: 0.3175, d2.loss_cls: 0.1332, d2.loss_bbox: 0.3141, d3.loss_cls: 0.1271, d3.loss_bbox: 0.3246, d4.loss_cls: 0.1263, d4.loss_bbox: 0.3373, loss: 2.9693, grad_norm: 35.0540
2025-06-17 15:47:02,502 - mmdet - INFO - Epoch [1][1600/7033]	lr: 2.000e-04, eta: 21:54:24, time: 1.943, data_time: 0.057, memory: 17921, loss_cls: 0.1349, loss_bbox: 0.3588, d0.loss_cls: 0.2094, d0.loss_bbox: 0.4387, d1.loss_cls: 0.1667, d1.loss_bbox: 0.3293, d2.loss_cls: 0.1429, d2.loss_bbox: 0.3183, d3.loss_cls: 0.1348, d3.loss_bbox: 0.3252, d4.loss_cls: 0.1323, d4.loss_bbox: 0.3389, loss: 3.0303, grad_norm: 30.1586
2025-06-17 15:48:48,786 - mmdet - INFO - Epoch [1][1650/7033]	lr: 2.000e-04, eta: 21:56:31, time: 2.125, data_time: 0.047, memory: 17921, loss_cls: 0.1172, loss_bbox: 0.3258, d0.loss_cls: 0.1964, d0.loss_bbox: 0.4130, d1.loss_cls: 0.1454, d1.loss_bbox: 0.3073, d2.loss_cls: 0.1276, d2.loss_bbox: 0.2975, d3.loss_cls: 0.1200, d3.loss_bbox: 0.3038, d4.loss_cls: 0.1177, d4.loss_bbox: 0.3109, loss: 2.7824, grad_norm: 35.4720
2025-06-17 15:50:26,305 - mmdet - INFO - Epoch [1][1700/7033]	lr: 2.000e-04, eta: 21:54:56, time: 1.950, data_time: 0.072, memory: 17921, loss_cls: 0.1255, loss_bbox: 0.3333, d0.loss_cls: 0.2048, d0.loss_bbox: 0.4263, d1.loss_cls: 0.1511, d1.loss_bbox: 0.3220, d2.loss_cls: 0.1299, d2.loss_bbox: 0.3104, d3.loss_cls: 0.1234, d3.loss_bbox: 0.3145, d4.loss_cls: 0.1221, d4.loss_bbox: 0.3218, loss: 2.8853, grad_norm: 28.4360
2025-06-17 15:52:02,107 - mmdet - INFO - Epoch [1][1750/7033]	lr: 2.000e-04, eta: 21:52:42, time: 1.917, data_time: 0.061, memory: 17921, loss_cls: 0.1209, loss_bbox: 0.3300, d0.loss_cls: 0.1992, d0.loss_bbox: 0.4070, d1.loss_cls: 0.1504, d1.loss_bbox: 0.3020, d2.loss_cls: 0.1256, d2.loss_bbox: 0.2952, d3.loss_cls: 0.1207, d3.loss_bbox: 0.3059, d4.loss_cls: 0.1205, d4.loss_bbox: 0.3159, loss: 2.7934, grad_norm: 55.0171
2025-06-17 15:53:37,417 - mmdet - INFO - Epoch [1][1800/7033]	lr: 2.000e-04, eta: 21:50:19, time: 1.906, data_time: 0.072, memory: 17921, loss_cls: 0.1191, loss_bbox: 0.3267, d0.loss_cls: 0.1989, d0.loss_bbox: 0.4134, d1.loss_cls: 0.1460, d1.loss_bbox: 0.3092, d2.loss_cls: 0.1244, d2.loss_bbox: 0.2989, d3.loss_cls: 0.1163, d3.loss_bbox: 0.3082, d4.loss_cls: 0.1176, d4.loss_bbox: 0.3143, loss: 2.7932, grad_norm: 31.0564
2025-06-17 15:55:10,720 - mmdet - INFO - Epoch [1][1850/7033]	lr: 2.000e-04, eta: 21:47:13, time: 1.865, data_time: 0.051, memory: 17921, loss_cls: 0.1181, loss_bbox: 0.3156, d0.loss_cls: 0.1996, d0.loss_bbox: 0.3976, d1.loss_cls: 0.1508, d1.loss_bbox: 0.2946, d2.loss_cls: 0.1313, d2.loss_bbox: 0.2839, d3.loss_cls: 0.1213, d3.loss_bbox: 0.2918, d4.loss_cls: 0.1166, d4.loss_bbox: 0.3030, loss: 2.7242, grad_norm: 49.0817
2025-06-17 15:56:47,519 - mmdet - INFO - Epoch [1][1900/7033]	lr: 2.000e-04, eta: 21:45:28, time: 1.937, data_time: 0.067, memory: 17921, loss_cls: 0.1183, loss_bbox: 0.3289, d0.loss_cls: 0.2004, d0.loss_bbox: 0.3920, d1.loss_cls: 0.1512, d1.loss_bbox: 0.2917, d2.loss_cls: 0.1273, d2.loss_bbox: 0.2901, d3.loss_cls: 0.1177, d3.loss_bbox: 0.2988, d4.loss_cls: 0.1188, d4.loss_bbox: 0.3112, loss: 2.7465, grad_norm: 32.8318
2025-06-17 15:58:23,334 - mmdet - INFO - Epoch [1][1950/7033]	lr: 2.000e-04, eta: 21:43:23, time: 1.916, data_time: 0.070, memory: 17921, loss_cls: 0.1169, loss_bbox: 0.3247, d0.loss_cls: 0.2016, d0.loss_bbox: 0.4063, d1.loss_cls: 0.1463, d1.loss_bbox: 0.3028, d2.loss_cls: 0.1254, d2.loss_bbox: 0.2959, d3.loss_cls: 0.1168, d3.loss_bbox: 0.3031, d4.loss_cls: 0.1165, d4.loss_bbox: 0.3097, loss: 2.7661, grad_norm: 44.6556
2025-06-17 15:59:56,080 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 15:59:56,081 - mmdet - INFO - Epoch [1][2000/7033]	lr: 2.000e-04, eta: 21:40:15, time: 1.853, data_time: 0.063, memory: 17921, loss_cls: 0.1291, loss_bbox: 0.3513, d0.loss_cls: 0.2135, d0.loss_bbox: 0.4309, d1.loss_cls: 0.1587, d1.loss_bbox: 0.3264, d2.loss_cls: 0.1380, d2.loss_bbox: 0.3219, d3.loss_cls: 0.1303, d3.loss_bbox: 0.3286, d4.loss_cls: 0.1289, d4.loss_bbox: 0.3379, loss: 2.9954, grad_norm: 22.2048
2025-06-17 16:01:28,188 - mmdet - INFO - Epoch [1][2050/7033]	lr: 2.000e-04, eta: 21:37:03, time: 1.844, data_time: 0.060, memory: 17921, loss_cls: 0.1162, loss_bbox: 0.3437, d0.loss_cls: 0.2023, d0.loss_bbox: 0.3953, d1.loss_cls: 0.1459, d1.loss_bbox: 0.3023, d2.loss_cls: 0.1266, d2.loss_bbox: 0.2993, d3.loss_cls: 0.1155, d3.loss_bbox: 0.3116, d4.loss_cls: 0.1171, d4.loss_bbox: 0.3237, loss: 2.7996, grad_norm: 35.6265
2025-06-17 16:03:00,483 - mmdet - INFO - Epoch [1][2100/7033]	lr: 2.000e-04, eta: 21:33:58, time: 1.846, data_time: 0.062, memory: 17921, loss_cls: 0.1192, loss_bbox: 0.3250, d0.loss_cls: 0.2086, d0.loss_bbox: 0.4061, d1.loss_cls: 0.1452, d1.loss_bbox: 0.3040, d2.loss_cls: 0.1240, d2.loss_bbox: 0.2969, d3.loss_cls: 0.1159, d3.loss_bbox: 0.3031, d4.loss_cls: 0.1170, d4.loss_bbox: 0.3115, loss: 2.7764, grad_norm: 29.1066
2025-06-17 16:04:38,135 - mmdet - INFO - Epoch [1][2150/7033]	lr: 2.000e-04, eta: 21:32:37, time: 1.953, data_time: 0.063, memory: 17921, loss_cls: 0.1230, loss_bbox: 0.3251, d0.loss_cls: 0.2051, d0.loss_bbox: 0.4107, d1.loss_cls: 0.1517, d1.loss_bbox: 0.3065, d2.loss_cls: 0.1305, d2.loss_bbox: 0.2942, d3.loss_cls: 0.1232, d3.loss_bbox: 0.3001, d4.loss_cls: 0.1214, d4.loss_bbox: 0.3091, loss: 2.8005, grad_norm: 34.1215
2025-06-17 16:06:11,399 - mmdet - INFO - Epoch [1][2200/7033]	lr: 2.000e-04, eta: 21:29:55, time: 1.865, data_time: 0.075, memory: 17921, loss_cls: 0.1133, loss_bbox: 0.3273, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3918, d1.loss_cls: 0.1421, d1.loss_bbox: 0.2962, d2.loss_cls: 0.1232, d2.loss_bbox: 0.2884, d3.loss_cls: 0.1140, d3.loss_bbox: 0.2980, d4.loss_cls: 0.1130, d4.loss_bbox: 0.3099, loss: 2.7132, grad_norm: 29.0253
2025-06-17 16:07:45,583 - mmdet - INFO - Epoch [1][2250/7033]	lr: 2.000e-04, eta: 21:27:32, time: 1.884, data_time: 0.062, memory: 17921, loss_cls: 0.1117, loss_bbox: 0.3138, d0.loss_cls: 0.1971, d0.loss_bbox: 0.3930, d1.loss_cls: 0.1430, d1.loss_bbox: 0.2968, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2891, d3.loss_cls: 0.1130, d3.loss_bbox: 0.2953, d4.loss_cls: 0.1121, d4.loss_bbox: 0.3032, loss: 2.6901, grad_norm: 661.1996
2025-06-17 16:09:18,897 - mmdet - INFO - Epoch [1][2300/7033]	lr: 2.000e-04, eta: 21:24:56, time: 1.865, data_time: 0.058, memory: 17921, loss_cls: 0.1155, loss_bbox: 0.3155, d0.loss_cls: 0.2019, d0.loss_bbox: 0.3882, d1.loss_cls: 0.1506, d1.loss_bbox: 0.2927, d2.loss_cls: 0.1277, d2.loss_bbox: 0.2832, d3.loss_cls: 0.1179, d3.loss_bbox: 0.2933, d4.loss_cls: 0.1132, d4.loss_bbox: 0.3036, loss: 2.7031, grad_norm: 28.7437
2025-06-17 16:10:54,669 - mmdet - INFO - Epoch [1][2350/7033]	lr: 2.000e-04, eta: 21:23:07, time: 1.917, data_time: 0.063, memory: 17921, loss_cls: 0.1180, loss_bbox: 0.3264, d0.loss_cls: 0.1993, d0.loss_bbox: 0.4004, d1.loss_cls: 0.1464, d1.loss_bbox: 0.3026, d2.loss_cls: 0.1248, d2.loss_bbox: 0.2944, d3.loss_cls: 0.1170, d3.loss_bbox: 0.3036, d4.loss_cls: 0.1165, d4.loss_bbox: 0.3136, loss: 2.7631, grad_norm: 33.5423
2025-06-17 16:12:31,769 - mmdet - INFO - Epoch [1][2400/7033]	lr: 2.000e-04, eta: 21:21:38, time: 1.942, data_time: 0.076, memory: 17921, loss_cls: 0.1120, loss_bbox: 0.3310, d0.loss_cls: 0.1989, d0.loss_bbox: 0.4137, d1.loss_cls: 0.1515, d1.loss_bbox: 0.3035, d2.loss_cls: 0.1225, d2.loss_bbox: 0.2947, d3.loss_cls: 0.1144, d3.loss_bbox: 0.3014, d4.loss_cls: 0.1129, d4.loss_bbox: 0.3100, loss: 2.7665, grad_norm: 30.2432
2025-06-17 16:14:08,476 - mmdet - INFO - Epoch [1][2450/7033]	lr: 2.000e-04, eta: 21:20:02, time: 1.933, data_time: 0.069, memory: 17921, loss_cls: 0.1187, loss_bbox: 0.3147, d0.loss_cls: 0.2064, d0.loss_bbox: 0.3937, d1.loss_cls: 0.1498, d1.loss_bbox: 0.2941, d2.loss_cls: 0.1269, d2.loss_bbox: 0.2836, d3.loss_cls: 0.1180, d3.loss_bbox: 0.2928, d4.loss_cls: 0.1157, d4.loss_bbox: 0.3021, loss: 2.7165, grad_norm: 30.8454
2025-06-17 16:15:40,771 - mmdet - INFO - Epoch [1][2500/7033]	lr: 2.000e-04, eta: 21:17:18, time: 1.847, data_time: 0.071, memory: 17921, loss_cls: 0.1138, loss_bbox: 0.3217, d0.loss_cls: 0.2038, d0.loss_bbox: 0.4004, d1.loss_cls: 0.1457, d1.loss_bbox: 0.2928, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2848, d3.loss_cls: 0.1145, d3.loss_bbox: 0.2920, d4.loss_cls: 0.1130, d4.loss_bbox: 0.3055, loss: 2.7076, grad_norm: 29.8732
2025-06-17 16:17:18,150 - mmdet - INFO - Epoch [1][2550/7033]	lr: 2.000e-04, eta: 21:15:55, time: 1.948, data_time: 0.060, memory: 17921, loss_cls: 0.1109, loss_bbox: 0.3072, d0.loss_cls: 0.1922, d0.loss_bbox: 0.3869, d1.loss_cls: 0.1375, d1.loss_bbox: 0.2878, d2.loss_cls: 0.1162, d2.loss_bbox: 0.2794, d3.loss_cls: 0.1125, d3.loss_bbox: 0.2824, d4.loss_cls: 0.1117, d4.loss_bbox: 0.2928, loss: 2.6174, grad_norm: 39.5027
2025-06-17 16:18:51,143 - mmdet - INFO - Epoch [1][2600/7033]	lr: 2.000e-04, eta: 21:13:24, time: 1.860, data_time: 0.061, memory: 17921, loss_cls: 0.1138, loss_bbox: 0.3140, d0.loss_cls: 0.2046, d0.loss_bbox: 0.3975, d1.loss_cls: 0.1469, d1.loss_bbox: 0.2958, d2.loss_cls: 0.1251, d2.loss_bbox: 0.2883, d3.loss_cls: 0.1158, d3.loss_bbox: 0.2940, d4.loss_cls: 0.1143, d4.loss_bbox: 0.3002, loss: 2.7104, grad_norm: 32.2468
2025-06-17 16:20:24,314 - mmdet - INFO - Epoch [1][2650/7033]	lr: 2.000e-04, eta: 21:10:57, time: 1.862, data_time: 0.054, memory: 17921, loss_cls: 0.1149, loss_bbox: 0.3199, d0.loss_cls: 0.1909, d0.loss_bbox: 0.4032, d1.loss_cls: 0.1421, d1.loss_bbox: 0.3087, d2.loss_cls: 0.1207, d2.loss_bbox: 0.2991, d3.loss_cls: 0.1168, d3.loss_bbox: 0.3032, d4.loss_cls: 0.1144, d4.loss_bbox: 0.3093, loss: 2.7431, grad_norm: 26.5202
2025-06-17 16:22:00,090 - mmdet - INFO - Epoch [1][2700/7033]	lr: 2.000e-04, eta: 21:09:12, time: 1.916, data_time: 0.052, memory: 17921, loss_cls: 0.1136, loss_bbox: 0.3055, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3788, d1.loss_cls: 0.1367, d1.loss_bbox: 0.2876, d2.loss_cls: 0.1198, d2.loss_bbox: 0.2787, d3.loss_cls: 0.1111, d3.loss_bbox: 0.2866, d4.loss_cls: 0.1118, d4.loss_bbox: 0.2925, loss: 2.6119, grad_norm: 50.2580
2025-06-17 16:23:32,175 - mmdet - INFO - Epoch [1][2750/7033]	lr: 2.000e-04, eta: 21:06:34, time: 1.842, data_time: 0.053, memory: 17921, loss_cls: 0.1115, loss_bbox: 0.3164, d0.loss_cls: 0.2038, d0.loss_bbox: 0.3997, d1.loss_cls: 0.1430, d1.loss_bbox: 0.3040, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2938, d3.loss_cls: 0.1125, d3.loss_bbox: 0.2995, d4.loss_cls: 0.1111, d4.loss_bbox: 0.3034, loss: 2.7196, grad_norm: 25.8549
2025-06-17 16:25:08,883 - mmdet - INFO - Epoch [1][2800/7033]	lr: 2.000e-04, eta: 21:05:03, time: 1.934, data_time: 0.062, memory: 17921, loss_cls: 0.1038, loss_bbox: 0.3030, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3811, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2831, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2733, d3.loss_cls: 0.1027, d3.loss_bbox: 0.2806, d4.loss_cls: 0.1037, d4.loss_bbox: 0.2866, loss: 2.5527, grad_norm: 25.8730
2025-06-17 16:26:41,681 - mmdet - INFO - Epoch [1][2850/7033]	lr: 2.000e-04, eta: 21:02:38, time: 1.856, data_time: 0.061, memory: 17921, loss_cls: 0.1022, loss_bbox: 0.3065, d0.loss_cls: 0.1928, d0.loss_bbox: 0.3831, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2875, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2771, d3.loss_cls: 0.1023, d3.loss_bbox: 0.2836, d4.loss_cls: 0.1006, d4.loss_bbox: 0.2920, loss: 2.5708, grad_norm: 67.2408
2025-06-17 16:28:16,147 - mmdet - INFO - Epoch [1][2900/7033]	lr: 2.000e-04, eta: 21:00:38, time: 1.889, data_time: 0.067, memory: 17921, loss_cls: 0.0961, loss_bbox: 0.2894, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1350, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1064, d2.loss_bbox: 0.2641, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2706, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2772, loss: 2.4628, grad_norm: 38.0406
2025-06-17 16:30:02,010 - mmdet - INFO - Epoch [1][2950/7033]	lr: 2.000e-04, eta: 21:01:08, time: 2.115, data_time: 0.063, memory: 17921, loss_cls: 0.1124, loss_bbox: 0.3091, d0.loss_cls: 0.2104, d0.loss_bbox: 0.3888, d1.loss_cls: 0.1500, d1.loss_bbox: 0.2907, d2.loss_cls: 0.1179, d2.loss_bbox: 0.2846, d3.loss_cls: 0.1084, d3.loss_bbox: 0.2901, d4.loss_cls: 0.1097, d4.loss_bbox: 0.2970, loss: 2.6691, grad_norm: 26.4711
2025-06-17 16:31:35,768 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 16:31:35,769 - mmdet - INFO - Epoch [1][3000/7033]	lr: 2.000e-04, eta: 20:58:58, time: 1.877, data_time: 0.064, memory: 17921, loss_cls: 0.1193, loss_bbox: 0.3100, d0.loss_cls: 0.2083, d0.loss_bbox: 0.3947, d1.loss_cls: 0.1542, d1.loss_bbox: 0.2980, d2.loss_cls: 0.1288, d2.loss_bbox: 0.2847, d3.loss_cls: 0.1186, d3.loss_bbox: 0.2892, d4.loss_cls: 0.1189, d4.loss_bbox: 0.2978, loss: 2.7224, grad_norm: 39.1116
2025-06-17 16:33:10,312 - mmdet - INFO - Epoch [1][3050/7033]	lr: 2.000e-04, eta: 20:56:59, time: 1.891, data_time: 0.071, memory: 17921, loss_cls: 0.1106, loss_bbox: 0.3232, d0.loss_cls: 0.1983, d0.loss_bbox: 0.4008, d1.loss_cls: 0.1453, d1.loss_bbox: 0.3057, d2.loss_cls: 0.1263, d2.loss_bbox: 0.2962, d3.loss_cls: 0.1140, d3.loss_bbox: 0.3011, d4.loss_cls: 0.1120, d4.loss_bbox: 0.3083, loss: 2.7418, grad_norm: 41.7629
2025-06-17 16:34:42,454 - mmdet - INFO - Epoch [1][3100/7033]	lr: 2.000e-04, eta: 20:54:29, time: 1.842, data_time: 0.066, memory: 17921, loss_cls: 0.1060, loss_bbox: 0.3056, d0.loss_cls: 0.1945, d0.loss_bbox: 0.3922, d1.loss_cls: 0.1339, d1.loss_bbox: 0.2937, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2808, d3.loss_cls: 0.1050, d3.loss_bbox: 0.2836, d4.loss_cls: 0.1037, d4.loss_bbox: 0.2930, loss: 2.6056, grad_norm: 40.4022
2025-06-17 16:36:16,784 - mmdet - INFO - Epoch [1][3150/7033]	lr: 2.000e-04, eta: 20:52:29, time: 1.887, data_time: 0.057, memory: 17921, loss_cls: 0.1170, loss_bbox: 0.3085, d0.loss_cls: 0.2085, d0.loss_bbox: 0.4126, d1.loss_cls: 0.1498, d1.loss_bbox: 0.3114, d2.loss_cls: 0.1293, d2.loss_bbox: 0.2947, d3.loss_cls: 0.1206, d3.loss_bbox: 0.2964, d4.loss_cls: 0.1163, d4.loss_bbox: 0.3013, loss: 2.7664, grad_norm: 32.2847
2025-06-17 16:37:50,858 - mmdet - INFO - Epoch [1][3200/7033]	lr: 2.000e-04, eta: 20:50:27, time: 1.881, data_time: 0.066, memory: 17921, loss_cls: 0.1033, loss_bbox: 0.3108, d0.loss_cls: 0.2024, d0.loss_bbox: 0.3895, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2927, d2.loss_cls: 0.1177, d2.loss_bbox: 0.2833, d3.loss_cls: 0.1071, d3.loss_bbox: 0.2878, d4.loss_cls: 0.1029, d4.loss_bbox: 0.2970, loss: 2.6350, grad_norm: 29.1946
2025-06-17 16:39:24,755 - mmdet - INFO - Epoch [1][3250/7033]	lr: 2.000e-04, eta: 20:48:23, time: 1.878, data_time: 0.059, memory: 17921, loss_cls: 0.1016, loss_bbox: 0.3075, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3885, d1.loss_cls: 0.1343, d1.loss_bbox: 0.2914, d2.loss_cls: 0.1132, d2.loss_bbox: 0.2796, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2845, d4.loss_cls: 0.1015, d4.loss_bbox: 0.2943, loss: 2.5886, grad_norm: 41.9689
2025-06-17 16:40:58,507 - mmdet - INFO - Epoch [1][3300/7033]	lr: 2.000e-04, eta: 20:46:18, time: 1.873, data_time: 0.055, memory: 17921, loss_cls: 0.1073, loss_bbox: 0.3134, d0.loss_cls: 0.1967, d0.loss_bbox: 0.3966, d1.loss_cls: 0.1342, d1.loss_bbox: 0.2993, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2874, d3.loss_cls: 0.1068, d3.loss_bbox: 0.2934, d4.loss_cls: 0.1069, d4.loss_bbox: 0.3001, loss: 2.6571, grad_norm: 38.4534
2025-06-17 16:42:36,057 - mmdet - INFO - Epoch [1][3350/7033]	lr: 2.000e-04, eta: 20:44:59, time: 1.953, data_time: 0.074, memory: 17921, loss_cls: 0.1022, loss_bbox: 0.3253, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3938, d1.loss_cls: 0.1308, d1.loss_bbox: 0.2960, d2.loss_cls: 0.1118, d2.loss_bbox: 0.2870, d3.loss_cls: 0.1037, d3.loss_bbox: 0.2959, d4.loss_cls: 0.1027, d4.loss_bbox: 0.3063, loss: 2.6479, grad_norm: 24.8773
2025-06-17 16:44:11,171 - mmdet - INFO - Epoch [1][3400/7033]	lr: 2.000e-04, eta: 20:43:11, time: 1.902, data_time: 0.074, memory: 17921, loss_cls: 0.1028, loss_bbox: 0.3088, d0.loss_cls: 0.1990, d0.loss_bbox: 0.3889, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2924, d2.loss_cls: 0.1157, d2.loss_bbox: 0.2805, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2890, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2949, loss: 2.6137, grad_norm: 29.9764
2025-06-17 16:45:43,954 - mmdet - INFO - Epoch [1][3450/7033]	lr: 2.000e-04, eta: 20:40:58, time: 1.856, data_time: 0.071, memory: 17921, loss_cls: 0.0971, loss_bbox: 0.2925, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3775, d1.loss_cls: 0.1308, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2693, d3.loss_cls: 0.1003, d3.loss_bbox: 0.2743, d4.loss_cls: 0.0971, d4.loss_bbox: 0.2806, loss: 2.4996, grad_norm: 26.2762
2025-06-17 16:47:16,169 - mmdet - INFO - Epoch [1][3500/7033]	lr: 2.000e-04, eta: 20:38:39, time: 1.844, data_time: 0.057, memory: 17921, loss_cls: 0.1001, loss_bbox: 0.3006, d0.loss_cls: 0.1971, d0.loss_bbox: 0.3753, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2812, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2742, d3.loss_cls: 0.1028, d3.loss_bbox: 0.2805, d4.loss_cls: 0.0992, d4.loss_bbox: 0.2875, loss: 2.5389, grad_norm: 51.0586
2025-06-17 16:48:49,254 - mmdet - INFO - Epoch [1][3550/7033]	lr: 2.000e-04, eta: 20:36:30, time: 1.860, data_time: 0.071, memory: 17921, loss_cls: 0.1016, loss_bbox: 0.2888, d0.loss_cls: 0.1963, d0.loss_bbox: 0.3842, d1.loss_cls: 0.1326, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1110, d2.loss_bbox: 0.2695, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2729, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2772, loss: 2.5170, grad_norm: 27.2087
2025-06-17 16:50:21,344 - mmdet - INFO - Epoch [1][3600/7033]	lr: 2.000e-04, eta: 20:34:12, time: 1.842, data_time: 0.052, memory: 17921, loss_cls: 0.0989, loss_bbox: 0.2951, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3702, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2805, d2.loss_cls: 0.1090, d2.loss_bbox: 0.2689, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2779, d4.loss_cls: 0.0975, d4.loss_bbox: 0.2828, loss: 2.4952, grad_norm: 28.5826
2025-06-17 16:51:57,058 - mmdet - INFO - Epoch [1][3650/7033]	lr: 2.000e-04, eta: 20:32:35, time: 1.916, data_time: 0.060, memory: 17921, loss_cls: 0.1072, loss_bbox: 0.2995, d0.loss_cls: 0.2014, d0.loss_bbox: 0.3932, d1.loss_cls: 0.1383, d1.loss_bbox: 0.2925, d2.loss_cls: 0.1176, d2.loss_bbox: 0.2782, d3.loss_cls: 0.1073, d3.loss_bbox: 0.2823, d4.loss_cls: 0.1084, d4.loss_bbox: 0.2869, loss: 2.6127, grad_norm: 35.7281
2025-06-17 16:53:31,212 - mmdet - INFO - Epoch [1][3700/7033]	lr: 2.000e-04, eta: 20:30:40, time: 1.883, data_time: 0.070, memory: 17921, loss_cls: 0.1055, loss_bbox: 0.3005, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3921, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2947, d2.loss_cls: 0.1136, d2.loss_bbox: 0.2771, d3.loss_cls: 0.1092, d3.loss_bbox: 0.2809, d4.loss_cls: 0.1057, d4.loss_bbox: 0.2865, loss: 2.5854, grad_norm: 53.4210
2025-06-17 16:55:03,712 - mmdet - INFO - Epoch [1][3750/7033]	lr: 2.000e-04, eta: 20:28:30, time: 1.850, data_time: 0.065, memory: 17921, loss_cls: 0.1007, loss_bbox: 0.2955, d0.loss_cls: 0.1937, d0.loss_bbox: 0.3711, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2795, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2697, d3.loss_cls: 0.1042, d3.loss_bbox: 0.2765, d4.loss_cls: 0.1000, d4.loss_bbox: 0.2830, loss: 2.5172, grad_norm: 36.5085
2025-06-17 16:56:36,480 - mmdet - INFO - Epoch [1][3800/7033]	lr: 2.000e-04, eta: 20:26:23, time: 1.855, data_time: 0.050, memory: 17921, loss_cls: 0.1073, loss_bbox: 0.2913, d0.loss_cls: 0.2022, d0.loss_bbox: 0.3801, d1.loss_cls: 0.1427, d1.loss_bbox: 0.2834, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2724, d3.loss_cls: 0.1067, d3.loss_bbox: 0.2800, d4.loss_cls: 0.1060, d4.loss_bbox: 0.2827, loss: 2.5762, grad_norm: 86.7252
2025-06-17 16:58:10,198 - mmdet - INFO - Epoch [1][3850/7033]	lr: 2.000e-04, eta: 20:24:26, time: 1.874, data_time: 0.062, memory: 17921, loss_cls: 0.0995, loss_bbox: 0.2913, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3828, d1.loss_cls: 0.1317, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1108, d2.loss_bbox: 0.2723, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2767, d4.loss_cls: 0.0995, d4.loss_bbox: 0.2810, loss: 2.5218, grad_norm: 91.4810
2025-06-17 16:59:42,099 - mmdet - INFO - Epoch [1][3900/7033]	lr: 2.000e-04, eta: 20:22:12, time: 1.838, data_time: 0.065, memory: 17921, loss_cls: 0.1010, loss_bbox: 0.2950, d0.loss_cls: 0.2005, d0.loss_bbox: 0.3739, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2711, d3.loss_cls: 0.1032, d3.loss_bbox: 0.2764, d4.loss_cls: 0.1036, d4.loss_bbox: 0.2821, loss: 2.5403, grad_norm: 39.4344
2025-06-17 17:01:18,464 - mmdet - INFO - Epoch [1][3950/7033]	lr: 2.000e-04, eta: 20:20:42, time: 1.927, data_time: 0.061, memory: 17921, loss_cls: 0.0989, loss_bbox: 0.3058, d0.loss_cls: 0.2026, d0.loss_bbox: 0.3957, d1.loss_cls: 0.1371, d1.loss_bbox: 0.2972, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2783, d3.loss_cls: 0.1013, d3.loss_bbox: 0.2831, d4.loss_cls: 0.0993, d4.loss_bbox: 0.2919, loss: 2.6052, grad_norm: 26.5301
2025-06-17 17:02:52,859 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 17:02:52,859 - mmdet - INFO - Epoch [1][4000/7033]	lr: 2.000e-04, eta: 20:18:53, time: 1.888, data_time: 0.054, memory: 17921, loss_cls: 0.1060, loss_bbox: 0.2956, d0.loss_cls: 0.2018, d0.loss_bbox: 0.3784, d1.loss_cls: 0.1407, d1.loss_bbox: 0.2833, d2.loss_cls: 0.1214, d2.loss_bbox: 0.2700, d3.loss_cls: 0.1087, d3.loss_bbox: 0.2764, d4.loss_cls: 0.1056, d4.loss_bbox: 0.2841, loss: 2.5720, grad_norm: 34.6939
2025-06-17 17:04:27,986 - mmdet - INFO - Epoch [1][4050/7033]	lr: 2.000e-04, eta: 20:17:12, time: 1.903, data_time: 0.054, memory: 17921, loss_cls: 0.1081, loss_bbox: 0.2930, d0.loss_cls: 0.2020, d0.loss_bbox: 0.3857, d1.loss_cls: 0.1435, d1.loss_bbox: 0.2886, d2.loss_cls: 0.1254, d2.loss_bbox: 0.2723, d3.loss_cls: 0.1135, d3.loss_bbox: 0.2774, d4.loss_cls: 0.1103, d4.loss_bbox: 0.2822, loss: 2.6021, grad_norm: 42.3688
2025-06-17 17:06:04,334 - mmdet - INFO - Epoch [1][4100/7033]	lr: 2.000e-04, eta: 20:15:42, time: 1.927, data_time: 0.062, memory: 17921, loss_cls: 0.1024, loss_bbox: 0.2925, d0.loss_cls: 0.2003, d0.loss_bbox: 0.3790, d1.loss_cls: 0.1355, d1.loss_bbox: 0.2942, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2774, d3.loss_cls: 0.1078, d3.loss_bbox: 0.2813, d4.loss_cls: 0.1025, d4.loss_bbox: 0.2849, loss: 2.5741, grad_norm: 40.0487
2025-06-17 17:07:38,892 - mmdet - INFO - Epoch [1][4150/7033]	lr: 2.000e-04, eta: 20:13:56, time: 1.891, data_time: 0.054, memory: 17921, loss_cls: 0.0919, loss_bbox: 0.2936, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3662, d1.loss_cls: 0.1265, d1.loss_bbox: 0.2803, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2688, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2751, d4.loss_cls: 0.0937, d4.loss_bbox: 0.2802, loss: 2.4659, grad_norm: 28.1370
2025-06-17 17:09:24,912 - mmdet - INFO - Epoch [1][4200/7033]	lr: 2.000e-04, eta: 20:13:53, time: 2.121, data_time: 0.089, memory: 17921, loss_cls: 0.0980, loss_bbox: 0.2905, d0.loss_cls: 0.1977, d0.loss_bbox: 0.3838, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2976, d2.loss_cls: 0.1131, d2.loss_bbox: 0.2783, d3.loss_cls: 0.1026, d3.loss_bbox: 0.2817, d4.loss_cls: 0.0985, d4.loss_bbox: 0.2822, loss: 2.5552, grad_norm: 31.5652
2025-06-17 17:10:58,156 - mmdet - INFO - Epoch [1][4250/7033]	lr: 2.000e-04, eta: 20:11:54, time: 1.864, data_time: 0.056, memory: 17921, loss_cls: 0.0968, loss_bbox: 0.2776, d0.loss_cls: 0.1914, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1296, d1.loss_bbox: 0.2727, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2591, d3.loss_cls: 0.0977, d3.loss_bbox: 0.2636, d4.loss_cls: 0.0976, d4.loss_bbox: 0.2662, loss: 2.4230, grad_norm: 32.6801
2025-06-17 17:12:32,040 - mmdet - INFO - Epoch [1][4300/7033]	lr: 2.000e-04, eta: 20:10:01, time: 1.878, data_time: 0.068, memory: 17921, loss_cls: 0.1036, loss_bbox: 0.2910, d0.loss_cls: 0.1998, d0.loss_bbox: 0.3927, d1.loss_cls: 0.1361, d1.loss_bbox: 0.2942, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2824, d3.loss_cls: 0.1030, d3.loss_bbox: 0.2845, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2835, loss: 2.5854, grad_norm: 42.3493
2025-06-17 17:14:07,839 - mmdet - INFO - Epoch [1][4350/7033]	lr: 2.000e-04, eta: 20:08:26, time: 1.916, data_time: 0.069, memory: 17921, loss_cls: 0.0930, loss_bbox: 0.2873, d0.loss_cls: 0.1899, d0.loss_bbox: 0.3692, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2802, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2698, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2770, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2756, loss: 2.4600, grad_norm: 37.0825
2025-06-17 17:15:42,713 - mmdet - INFO - Epoch [1][4400/7033]	lr: 2.000e-04, eta: 20:06:42, time: 1.898, data_time: 0.058, memory: 17921, loss_cls: 0.0931, loss_bbox: 0.2804, d0.loss_cls: 0.1955, d0.loss_bbox: 0.3864, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2841, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2703, d3.loss_cls: 0.0966, d3.loss_bbox: 0.2751, d4.loss_cls: 0.0953, d4.loss_bbox: 0.2718, loss: 2.4854, grad_norm: 28.3004
2025-06-17 17:17:18,305 - mmdet - INFO - Epoch [1][4450/7033]	lr: 2.000e-04, eta: 20:05:04, time: 1.910, data_time: 0.060, memory: 17921, loss_cls: 0.0961, loss_bbox: 0.2791, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1283, d1.loss_bbox: 0.2811, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2660, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2714, d4.loss_cls: 0.0986, d4.loss_bbox: 0.2696, loss: 2.4575, grad_norm: 27.5623
2025-06-17 17:18:53,370 - mmdet - INFO - Epoch [1][4500/7033]	lr: 2.000e-04, eta: 20:03:23, time: 1.903, data_time: 0.066, memory: 17921, loss_cls: 0.0945, loss_bbox: 0.2840, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2739, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2621, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2684, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2701, loss: 2.4309, grad_norm: 25.6715
2025-06-17 17:20:24,386 - mmdet - INFO - Epoch [1][4550/7033]	lr: 2.000e-04, eta: 20:01:08, time: 1.820, data_time: 0.057, memory: 17921, loss_cls: 0.1009, loss_bbox: 0.2833, d0.loss_cls: 0.1985, d0.loss_bbox: 0.3839, d1.loss_cls: 0.1393, d1.loss_bbox: 0.2816, d2.loss_cls: 0.1168, d2.loss_bbox: 0.2699, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2737, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2731, loss: 2.5284, grad_norm: 101.6807
2025-06-17 17:22:01,601 - mmdet - INFO - Epoch [1][4600/7033]	lr: 2.000e-04, eta: 19:59:44, time: 1.944, data_time: 0.053, memory: 17921, loss_cls: 0.1092, loss_bbox: 0.2882, d0.loss_cls: 0.1997, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1434, d1.loss_bbox: 0.2865, d2.loss_cls: 0.1219, d2.loss_bbox: 0.2728, d3.loss_cls: 0.1108, d3.loss_bbox: 0.2788, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2774, loss: 2.5769, grad_norm: 33.6142
2025-06-17 17:23:39,612 - mmdet - INFO - Epoch [1][4650/7033]	lr: 2.000e-04, eta: 19:58:27, time: 1.960, data_time: 0.062, memory: 17921, loss_cls: 0.1062, loss_bbox: 0.2766, d0.loss_cls: 0.1961, d0.loss_bbox: 0.3777, d1.loss_cls: 0.1374, d1.loss_bbox: 0.2853, d2.loss_cls: 0.1229, d2.loss_bbox: 0.2691, d3.loss_cls: 0.1065, d3.loss_bbox: 0.2767, d4.loss_cls: 0.1059, d4.loss_bbox: 0.2710, loss: 2.5315, grad_norm: 90.2443
2025-06-17 17:25:16,439 - mmdet - INFO - Epoch [1][4700/7033]	lr: 2.000e-04, eta: 19:57:00, time: 1.937, data_time: 0.067, memory: 17921, loss_cls: 0.1038, loss_bbox: 0.2847, d0.loss_cls: 0.1986, d0.loss_bbox: 0.3767, d1.loss_cls: 0.1364, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2700, d3.loss_cls: 0.1033, d3.loss_bbox: 0.2762, d4.loss_cls: 0.1026, d4.loss_bbox: 0.2744, loss: 2.5280, grad_norm: 41.7397
2025-06-17 17:26:52,602 - mmdet - INFO - Epoch [1][4750/7033]	lr: 2.000e-04, eta: 19:55:27, time: 1.923, data_time: 0.071, memory: 17921, loss_cls: 0.0950, loss_bbox: 0.2614, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3595, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2666, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2539, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2584, d4.loss_cls: 0.0952, d4.loss_bbox: 0.2536, loss: 2.3601, grad_norm: 44.5571
2025-06-17 17:28:29,599 - mmdet - INFO - Epoch [1][4800/7033]	lr: 2.000e-04, eta: 19:54:01, time: 1.940, data_time: 0.060, memory: 17921, loss_cls: 0.0944, loss_bbox: 0.2717, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3600, d1.loss_cls: 0.1289, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1121, d2.loss_bbox: 0.2571, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2635, d4.loss_cls: 0.0975, d4.loss_bbox: 0.2620, loss: 2.4097, grad_norm: 39.5520
2025-06-17 17:30:05,967 - mmdet - INFO - Epoch [1][4850/7033]	lr: 2.000e-04, eta: 19:52:30, time: 1.928, data_time: 0.058, memory: 17921, loss_cls: 0.0999, loss_bbox: 0.2923, d0.loss_cls: 0.1997, d0.loss_bbox: 0.3759, d1.loss_cls: 0.1388, d1.loss_bbox: 0.2902, d2.loss_cls: 0.1231, d2.loss_bbox: 0.2750, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2823, d4.loss_cls: 0.1030, d4.loss_bbox: 0.2830, loss: 2.5705, grad_norm: 29.9680
2025-06-17 17:31:43,887 - mmdet - INFO - Epoch [1][4900/7033]	lr: 2.000e-04, eta: 19:51:10, time: 1.958, data_time: 0.057, memory: 17921, loss_cls: 0.0994, loss_bbox: 0.2783, d0.loss_cls: 0.1951, d0.loss_bbox: 0.3866, d1.loss_cls: 0.1388, d1.loss_bbox: 0.2907, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2748, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2781, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2704, loss: 2.5305, grad_norm: 40.9678
2025-06-17 17:33:20,939 - mmdet - INFO - Epoch [1][4950/7033]	lr: 2.000e-04, eta: 19:49:44, time: 1.942, data_time: 0.064, memory: 17921, loss_cls: 0.1052, loss_bbox: 0.2886, d0.loss_cls: 0.1979, d0.loss_bbox: 0.3911, d1.loss_cls: 0.1404, d1.loss_bbox: 0.2975, d2.loss_cls: 0.1211, d2.loss_bbox: 0.2786, d3.loss_cls: 0.1069, d3.loss_bbox: 0.2814, d4.loss_cls: 0.1063, d4.loss_bbox: 0.2788, loss: 2.5937, grad_norm: 35.7958
2025-06-17 17:35:01,148 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 17:35:01,149 - mmdet - INFO - Epoch [1][5000/7033]	lr: 2.000e-04, eta: 19:48:41, time: 2.004, data_time: 0.081, memory: 17921, loss_cls: 0.0871, loss_bbox: 0.2714, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3609, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2786, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2659, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2688, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2651, loss: 2.3695, grad_norm: 23.9820
2025-06-17 17:36:38,029 - mmdet - INFO - Epoch [1][5050/7033]	lr: 2.000e-04, eta: 19:47:12, time: 1.938, data_time: 0.055, memory: 17921, loss_cls: 0.1024, loss_bbox: 0.2670, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3724, d1.loss_cls: 0.1332, d1.loss_bbox: 0.2823, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2642, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2648, d4.loss_cls: 0.1038, d4.loss_bbox: 0.2595, loss: 2.4644, grad_norm: 42.5170
2025-06-17 17:38:15,366 - mmdet - INFO - Epoch [1][5100/7033]	lr: 2.000e-04, eta: 19:45:47, time: 1.947, data_time: 0.069, memory: 17921, loss_cls: 0.0949, loss_bbox: 0.2646, d0.loss_cls: 0.1905, d0.loss_bbox: 0.3805, d1.loss_cls: 0.1347, d1.loss_bbox: 0.2844, d2.loss_cls: 0.1147, d2.loss_bbox: 0.2610, d3.loss_cls: 0.0999, d3.loss_bbox: 0.2633, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2620, loss: 2.4456, grad_norm: 54.3129
2025-06-17 17:39:49,523 - mmdet - INFO - Epoch [1][5150/7033]	lr: 2.000e-04, eta: 19:43:59, time: 1.883, data_time: 0.059, memory: 17921, loss_cls: 0.1039, loss_bbox: 0.2706, d0.loss_cls: 0.1949, d0.loss_bbox: 0.3722, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1171, d2.loss_bbox: 0.2667, d3.loss_cls: 0.1062, d3.loss_bbox: 0.2725, d4.loss_cls: 0.1067, d4.loss_bbox: 0.2636, loss: 2.4930, grad_norm: 36.9943
2025-06-17 17:41:25,346 - mmdet - INFO - Epoch [1][5200/7033]	lr: 2.000e-04, eta: 19:42:22, time: 1.916, data_time: 0.055, memory: 17921, loss_cls: 0.0993, loss_bbox: 0.2739, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3721, d1.loss_cls: 0.1302, d1.loss_bbox: 0.2862, d2.loss_cls: 0.1135, d2.loss_bbox: 0.2681, d3.loss_cls: 0.1012, d3.loss_bbox: 0.2708, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2653, loss: 2.4642, grad_norm: 62.9556
2025-06-17 17:42:59,128 - mmdet - INFO - Epoch [1][5250/7033]	lr: 2.000e-04, eta: 19:40:32, time: 1.876, data_time: 0.055, memory: 17921, loss_cls: 0.0949, loss_bbox: 0.2651, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3712, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2810, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2643, d3.loss_cls: 0.0995, d3.loss_bbox: 0.2666, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2598, loss: 2.4244, grad_norm: 31.0163
2025-06-17 17:44:34,569 - mmdet - INFO - Epoch [1][5300/7033]	lr: 2.000e-04, eta: 19:38:53, time: 1.909, data_time: 0.055, memory: 17921, loss_cls: 0.0944, loss_bbox: 0.2649, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3589, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2688, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2553, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2597, d4.loss_cls: 0.0952, d4.loss_bbox: 0.2559, loss: 2.3704, grad_norm: 52.5077
2025-06-17 17:46:11,471 - mmdet - INFO - Epoch [1][5350/7033]	lr: 2.000e-04, eta: 19:37:24, time: 1.938, data_time: 0.054, memory: 17921, loss_cls: 0.1054, loss_bbox: 0.2802, d0.loss_cls: 0.1989, d0.loss_bbox: 0.3862, d1.loss_cls: 0.1387, d1.loss_bbox: 0.2965, d2.loss_cls: 0.1194, d2.loss_bbox: 0.2755, d3.loss_cls: 0.1080, d3.loss_bbox: 0.2790, d4.loss_cls: 0.1067, d4.loss_bbox: 0.2734, loss: 2.5680, grad_norm: 24.5151
2025-06-17 17:47:49,414 - mmdet - INFO - Epoch [1][5400/7033]	lr: 2.000e-04, eta: 19:36:03, time: 1.959, data_time: 0.063, memory: 17921, loss_cls: 0.0978, loss_bbox: 0.2705, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3791, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2664, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2706, d4.loss_cls: 0.0989, d4.loss_bbox: 0.2648, loss: 2.4696, grad_norm: 144.7568
2025-06-17 17:49:24,482 - mmdet - INFO - Epoch [1][5450/7033]	lr: 2.000e-04, eta: 19:34:21, time: 1.901, data_time: 0.060, memory: 17921, loss_cls: 0.0999, loss_bbox: 0.2786, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3799, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2885, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2721, d3.loss_cls: 0.1030, d3.loss_bbox: 0.2760, d4.loss_cls: 0.1015, d4.loss_bbox: 0.2708, loss: 2.5148, grad_norm: 28.6537
2025-06-17 17:51:06,266 - mmdet - INFO - Epoch [1][5500/7033]	lr: 2.000e-04, eta: 19:33:25, time: 2.036, data_time: 0.062, memory: 17921, loss_cls: 0.1036, loss_bbox: 0.2708, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1381, d1.loss_bbox: 0.2809, d2.loss_cls: 0.1160, d2.loss_bbox: 0.2648, d3.loss_cls: 0.1064, d3.loss_bbox: 0.2673, d4.loss_cls: 0.1039, d4.loss_bbox: 0.2634, loss: 2.4664, grad_norm: 33.1415
2025-06-17 17:52:43,500 - mmdet - INFO - Epoch [1][5550/7033]	lr: 2.000e-04, eta: 19:31:58, time: 1.945, data_time: 0.060, memory: 17921, loss_cls: 0.1004, loss_bbox: 0.2749, d0.loss_cls: 0.1874, d0.loss_bbox: 0.3749, d1.loss_cls: 0.1305, d1.loss_bbox: 0.2838, d2.loss_cls: 0.1142, d2.loss_bbox: 0.2696, d3.loss_cls: 0.1007, d3.loss_bbox: 0.2736, d4.loss_cls: 0.0990, d4.loss_bbox: 0.2671, loss: 2.4763, grad_norm: 31.7005
2025-06-17 17:54:17,377 - mmdet - INFO - Epoch [1][5600/7033]	lr: 2.000e-04, eta: 19:30:08, time: 1.876, data_time: 0.055, memory: 17921, loss_cls: 0.1008, loss_bbox: 0.2846, d0.loss_cls: 0.1935, d0.loss_bbox: 0.3744, d1.loss_cls: 0.1326, d1.loss_bbox: 0.2821, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2691, d3.loss_cls: 0.1038, d3.loss_bbox: 0.2752, d4.loss_cls: 0.1014, d4.loss_bbox: 0.2728, loss: 2.5056, grad_norm: 45.0433
2025-06-17 17:55:55,413 - mmdet - INFO - Epoch [1][5650/7033]	lr: 2.000e-04, eta: 19:28:46, time: 1.962, data_time: 0.070, memory: 17921, loss_cls: 0.1010, loss_bbox: 0.2798, d0.loss_cls: 0.1957, d0.loss_bbox: 0.3786, d1.loss_cls: 0.1322, d1.loss_bbox: 0.2879, d2.loss_cls: 0.1118, d2.loss_bbox: 0.2718, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2759, d4.loss_cls: 0.1051, d4.loss_bbox: 0.2693, loss: 2.5126, grad_norm: 34.3884
2025-06-17 17:57:27,865 - mmdet - INFO - Epoch [1][5700/7033]	lr: 2.000e-04, eta: 19:26:48, time: 1.849, data_time: 0.057, memory: 17921, loss_cls: 0.0953, loss_bbox: 0.2602, d0.loss_cls: 0.1848, d0.loss_bbox: 0.3616, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2749, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2613, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2638, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2559, loss: 2.3851, grad_norm: 30.2271
2025-06-17 17:59:02,699 - mmdet - INFO - Epoch [1][5750/7033]	lr: 2.000e-04, eta: 19:25:05, time: 1.897, data_time: 0.062, memory: 17921, loss_cls: 0.0871, loss_bbox: 0.2590, d0.loss_cls: 0.1890, d0.loss_bbox: 0.3648, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2703, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2529, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2570, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2513, loss: 2.3383, grad_norm: 42.3667
2025-06-17 18:00:36,367 - mmdet - INFO - Epoch [1][5800/7033]	lr: 2.000e-04, eta: 19:23:14, time: 1.871, data_time: 0.064, memory: 17921, loss_cls: 0.0938, loss_bbox: 0.2641, d0.loss_cls: 0.1901, d0.loss_bbox: 0.3740, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2771, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2629, d3.loss_cls: 0.0966, d3.loss_bbox: 0.2658, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2569, loss: 2.4027, grad_norm: 21.7675
2025-06-17 18:02:15,648 - mmdet - INFO - Epoch [1][5850/7033]	lr: 2.000e-04, eta: 19:22:00, time: 1.987, data_time: 0.067, memory: 17921, loss_cls: 0.0954, loss_bbox: 0.2623, d0.loss_cls: 0.1987, d0.loss_bbox: 0.3651, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2774, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2621, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2636, d4.loss_cls: 0.0954, d4.loss_bbox: 0.2574, loss: 2.4114, grad_norm: 31.7702
2025-06-17 18:03:53,421 - mmdet - INFO - Epoch [1][5900/7033]	lr: 2.000e-04, eta: 19:20:36, time: 1.955, data_time: 0.062, memory: 17921, loss_cls: 0.0887, loss_bbox: 0.2642, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2791, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2633, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2686, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2579, loss: 2.3831, grad_norm: 381.7455
2025-06-17 18:05:32,646 - mmdet - INFO - Epoch [1][5950/7033]	lr: 2.000e-04, eta: 19:19:20, time: 1.985, data_time: 0.067, memory: 17921, loss_cls: 0.0949, loss_bbox: 0.2656, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3655, d1.loss_cls: 0.1289, d1.loss_bbox: 0.2756, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2618, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2630, d4.loss_cls: 0.0975, d4.loss_bbox: 0.2573, loss: 2.3985, grad_norm: 32.1264
2025-06-17 18:07:13,305 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 18:07:13,305 - mmdet - INFO - Epoch [1][6000/7033]	lr: 2.000e-04, eta: 19:18:12, time: 2.011, data_time: 0.063, memory: 17921, loss_cls: 0.0934, loss_bbox: 0.2601, d0.loss_cls: 0.1916, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2765, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2628, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2679, d4.loss_cls: 0.0950, d4.loss_bbox: 0.2546, loss: 2.3913, grad_norm: 38.0969
2025-06-17 18:08:57,067 - mmdet - INFO - Epoch [1][6050/7033]	lr: 2.000e-04, eta: 19:17:23, time: 2.076, data_time: 0.086, memory: 17921, loss_cls: 0.0914, loss_bbox: 0.2731, d0.loss_cls: 0.1877, d0.loss_bbox: 0.3701, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2813, d2.loss_cls: 0.1053, d2.loss_bbox: 0.2675, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2735, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2658, loss: 2.4300, grad_norm: 27.4229
2025-06-17 18:10:39,037 - mmdet - INFO - Epoch [1][6100/7033]	lr: 2.000e-04, eta: 19:16:22, time: 2.040, data_time: 0.059, memory: 17921, loss_cls: 0.1086, loss_bbox: 0.2909, d0.loss_cls: 0.1987, d0.loss_bbox: 0.3848, d1.loss_cls: 0.1417, d1.loss_bbox: 0.3012, d2.loss_cls: 0.1263, d2.loss_bbox: 0.2887, d3.loss_cls: 0.1128, d3.loss_bbox: 0.2924, d4.loss_cls: 0.1116, d4.loss_bbox: 0.2842, loss: 2.6418, grad_norm: 62.5452
2025-06-17 18:12:19,043 - mmdet - INFO - Epoch [1][6150/7033]	lr: 2.000e-04, eta: 19:15:09, time: 2.000, data_time: 0.060, memory: 17921, loss_cls: 0.0967, loss_bbox: 0.2824, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1304, d1.loss_bbox: 0.2939, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2829, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2839, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2766, loss: 2.5130, grad_norm: 36.1621
2025-06-17 18:13:57,875 - mmdet - INFO - Epoch [1][6200/7033]	lr: 2.000e-04, eta: 19:13:48, time: 1.977, data_time: 0.068, memory: 17921, loss_cls: 0.0871, loss_bbox: 0.2612, d0.loss_cls: 0.1924, d0.loss_bbox: 0.3602, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2593, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2539, loss: 2.3487, grad_norm: 31.5282
2025-06-17 18:15:37,578 - mmdet - INFO - Epoch [1][6250/7033]	lr: 2.000e-04, eta: 19:12:32, time: 1.994, data_time: 0.050, memory: 17921, loss_cls: 0.0950, loss_bbox: 0.2597, d0.loss_cls: 0.1860, d0.loss_bbox: 0.3702, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2742, d2.loss_cls: 0.1091, d2.loss_bbox: 0.2603, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2645, d4.loss_cls: 0.0979, d4.loss_bbox: 0.2538, loss: 2.3968, grad_norm: 35.5601
2025-06-17 18:17:12,944 - mmdet - INFO - Epoch [1][6300/7033]	lr: 2.000e-04, eta: 19:10:51, time: 1.907, data_time: 0.057, memory: 17921, loss_cls: 0.0924, loss_bbox: 0.2716, d0.loss_cls: 0.1850, d0.loss_bbox: 0.3798, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2887, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2705, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2745, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2636, loss: 2.4441, grad_norm: 22.3806
2025-06-17 18:18:48,002 - mmdet - INFO - Epoch [1][6350/7033]	lr: 2.000e-04, eta: 19:09:09, time: 1.902, data_time: 0.055, memory: 17921, loss_cls: 0.0975, loss_bbox: 0.2621, d0.loss_cls: 0.1919, d0.loss_bbox: 0.3695, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2799, d2.loss_cls: 0.1095, d2.loss_bbox: 0.2642, d3.loss_cls: 0.0999, d3.loss_bbox: 0.2648, d4.loss_cls: 0.0988, d4.loss_bbox: 0.2566, loss: 2.4260, grad_norm: 24.4031
2025-06-17 18:20:25,146 - mmdet - INFO - Epoch [1][6400/7033]	lr: 2.000e-04, eta: 19:07:38, time: 1.941, data_time: 0.056, memory: 17921, loss_cls: 0.0995, loss_bbox: 0.2694, d0.loss_cls: 0.1941, d0.loss_bbox: 0.3793, d1.loss_cls: 0.1337, d1.loss_bbox: 0.2848, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2692, d3.loss_cls: 0.1014, d3.loss_bbox: 0.2730, d4.loss_cls: 0.1001, d4.loss_bbox: 0.2646, loss: 2.4832, grad_norm: 32.9161
2025-06-17 18:22:01,738 - mmdet - INFO - Epoch [1][6450/7033]	lr: 2.000e-04, eta: 19:06:04, time: 1.933, data_time: 0.055, memory: 17921, loss_cls: 0.0916, loss_bbox: 0.2523, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3582, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2682, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2566, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2580, d4.loss_cls: 0.0937, d4.loss_bbox: 0.2490, loss: 2.3306, grad_norm: 25.9824
2025-06-17 18:23:42,500 - mmdet - INFO - Epoch [1][6500/7033]	lr: 2.000e-04, eta: 19:04:53, time: 2.014, data_time: 0.055, memory: 17921, loss_cls: 0.1031, loss_bbox: 0.2676, d0.loss_cls: 0.1940, d0.loss_bbox: 0.3822, d1.loss_cls: 0.1379, d1.loss_bbox: 0.2887, d2.loss_cls: 0.1163, d2.loss_bbox: 0.2712, d3.loss_cls: 0.1062, d3.loss_bbox: 0.2749, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2632, loss: 2.5097, grad_norm: 32.5140
2025-06-17 18:25:20,833 - mmdet - INFO - Epoch [1][6550/7033]	lr: 2.000e-04, eta: 19:03:28, time: 1.967, data_time: 0.072, memory: 17921, loss_cls: 0.1000, loss_bbox: 0.2588, d0.loss_cls: 0.1947, d0.loss_bbox: 0.3722, d1.loss_cls: 0.1371, d1.loss_bbox: 0.2804, d2.loss_cls: 0.1150, d2.loss_bbox: 0.2645, d3.loss_cls: 0.1073, d3.loss_bbox: 0.2651, d4.loss_cls: 0.1034, d4.loss_bbox: 0.2539, loss: 2.4525, grad_norm: 42.7954
2025-06-17 18:27:00,485 - mmdet - INFO - Epoch [1][6600/7033]	lr: 2.000e-04, eta: 19:02:11, time: 1.995, data_time: 0.059, memory: 17921, loss_cls: 0.0899, loss_bbox: 0.2462, d0.loss_cls: 0.1871, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2637, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2513, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2430, loss: 2.3075, grad_norm: 31.7685
2025-06-17 18:28:36,934 - mmdet - INFO - Epoch [1][6650/7033]	lr: 2.000e-04, eta: 19:00:36, time: 1.929, data_time: 0.065, memory: 17921, loss_cls: 0.0921, loss_bbox: 0.2538, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3606, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2698, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2583, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2630, d4.loss_cls: 0.0957, d4.loss_bbox: 0.2480, loss: 2.3611, grad_norm: 22.3930
2025-06-17 18:30:16,235 - mmdet - INFO - Epoch [1][6700/7033]	lr: 2.000e-04, eta: 18:59:15, time: 1.985, data_time: 0.080, memory: 17921, loss_cls: 0.0846, loss_bbox: 0.2536, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3573, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2537, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2575, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2477, loss: 2.3147, grad_norm: 136.6035
2025-06-17 18:31:57,315 - mmdet - INFO - Epoch [1][6750/7033]	lr: 2.000e-04, eta: 18:58:04, time: 2.023, data_time: 0.056, memory: 17921, loss_cls: 0.0907, loss_bbox: 0.2551, d0.loss_cls: 0.1930, d0.loss_bbox: 0.3673, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2753, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2575, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2586, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2477, loss: 2.3722, grad_norm: 68.0852
2025-06-17 18:33:43,792 - mmdet - INFO - Epoch [1][6800/7033]	lr: 2.000e-04, eta: 18:57:21, time: 2.129, data_time: 0.054, memory: 17921, loss_cls: 0.1040, loss_bbox: 0.2506, d0.loss_cls: 0.1971, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1388, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1215, d2.loss_bbox: 0.2545, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2593, d4.loss_cls: 0.1063, d4.loss_bbox: 0.2465, loss: 2.4223, grad_norm: 51.6476
2025-06-17 18:35:20,083 - mmdet - INFO - Epoch [1][6850/7033]	lr: 2.000e-04, eta: 18:55:44, time: 1.927, data_time: 0.057, memory: 17921, loss_cls: 0.0867, loss_bbox: 0.2522, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3592, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2735, d2.loss_cls: 0.1055, d2.loss_bbox: 0.2581, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2633, d4.loss_cls: 0.0893, d4.loss_bbox: 0.2469, loss: 2.3280, grad_norm: 29.9918
2025-06-17 18:36:54,674 - mmdet - INFO - Epoch [1][6900/7033]	lr: 2.000e-04, eta: 18:53:59, time: 1.892, data_time: 0.057, memory: 17921, loss_cls: 0.0900, loss_bbox: 0.2656, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3693, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2802, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2636, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2667, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2606, loss: 2.3924, grad_norm: 30.9333
2025-06-17 18:38:32,610 - mmdet - INFO - Epoch [1][6950/7033]	lr: 2.000e-04, eta: 18:52:30, time: 1.957, data_time: 0.071, memory: 17921, loss_cls: 0.0888, loss_bbox: 0.2467, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3639, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2549, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2560, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2426, loss: 2.3097, grad_norm: 38.8082
2025-06-17 18:40:06,974 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 18:40:06,974 - mmdet - INFO - Epoch [1][7000/7033]	lr: 2.000e-04, eta: 18:50:43, time: 1.888, data_time: 0.056, memory: 17921, loss_cls: 0.0935, loss_bbox: 0.2504, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3658, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2752, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2588, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2636, d4.loss_cls: 0.0941, d4.loss_bbox: 0.2473, loss: 2.3669, grad_norm: 25.7495
2025-06-17 18:41:10,929 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-17 19:17:30,534 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 19:17:30,534 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7580, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8709, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9043, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9212, pts_bbox_NuScenes/car_trans_err: 0.2223, pts_bbox_NuScenes/car_scale_err: 0.1581, pts_bbox_NuScenes/car_orient_err: 0.0565, pts_bbox_NuScenes/car_vel_err: 0.3977, pts_bbox_NuScenes/car_attr_err: 0.1818, pts_bbox_NuScenes/mATE: 0.3297, pts_bbox_NuScenes/mASE: 0.2707, pts_bbox_NuScenes/mAOE: 0.2929, pts_bbox_NuScenes/mAVE: 0.3388, pts_bbox_NuScenes/mAAE: 0.1789, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.3799, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6038, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7226, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7621, pts_bbox_NuScenes/truck_trans_err: 0.3887, pts_bbox_NuScenes/truck_scale_err: 0.2103, pts_bbox_NuScenes/truck_orient_err: 0.0873, pts_bbox_NuScenes/truck_vel_err: 0.3453, pts_bbox_NuScenes/truck_attr_err: 0.2135, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0578, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1932, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4158, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4850, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6667, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4687, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.9134, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1094, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2911, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.3571, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7075, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8846, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9156, pts_bbox_NuScenes/bus_trans_err: 0.4469, pts_bbox_NuScenes/bus_scale_err: 0.1976, pts_bbox_NuScenes/bus_orient_err: 0.0585, pts_bbox_NuScenes/bus_vel_err: 0.5933, pts_bbox_NuScenes/bus_attr_err: 0.2255, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1364, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3957, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5679, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6672, pts_bbox_NuScenes/trailer_trans_err: 0.5318, pts_bbox_NuScenes/trailer_scale_err: 0.2283, pts_bbox_NuScenes/trailer_orient_err: 0.4857, pts_bbox_NuScenes/trailer_vel_err: 0.2604, pts_bbox_NuScenes/trailer_attr_err: 0.1780, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5559, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6559, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7108, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7314, pts_bbox_NuScenes/barrier_trans_err: 0.2346, pts_bbox_NuScenes/barrier_scale_err: 0.2954, pts_bbox_NuScenes/barrier_orient_err: 0.0799, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5907, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7598, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7965, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8041, pts_bbox_NuScenes/motorcycle_trans_err: 0.2484, pts_bbox_NuScenes/motorcycle_scale_err: 0.2532, pts_bbox_NuScenes/motorcycle_orient_err: 0.2366, pts_bbox_NuScenes/motorcycle_vel_err: 0.5819, pts_bbox_NuScenes/motorcycle_attr_err: 0.2182, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5411, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6104, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6246, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6338, pts_bbox_NuScenes/bicycle_trans_err: 0.2036, pts_bbox_NuScenes/bicycle_scale_err: 0.2728, pts_bbox_NuScenes/bicycle_orient_err: 0.3756, pts_bbox_NuScenes/bicycle_vel_err: 0.1725, pts_bbox_NuScenes/bicycle_attr_err: 0.0049, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7790, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8422, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8727, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8898, pts_bbox_NuScenes/pedestrian_trans_err: 0.1865, pts_bbox_NuScenes/pedestrian_scale_err: 0.3000, pts_bbox_NuScenes/pedestrian_orient_err: 0.3424, pts_bbox_NuScenes/pedestrian_vel_err: 0.2500, pts_bbox_NuScenes/pedestrian_attr_err: 0.1182, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6872, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7411, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7735, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8048, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1672, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3230, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6853, pts_bbox_NuScenes/mAP: 0.6528
2025-06-17 19:19:22,422 - mmdet - INFO - Epoch [2][50/7033]	lr: 1.866e-04, eta: 18:43:42, time: 2.147, data_time: 0.365, memory: 17921, loss_cls: 0.0918, loss_bbox: 0.2638, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2864, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2745, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2770, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2617, loss: 2.4268, grad_norm: 31.9542
2025-06-17 19:20:53,506 - mmdet - INFO - Epoch [2][100/7033]	lr: 1.866e-04, eta: 18:41:42, time: 1.822, data_time: 0.060, memory: 17921, loss_cls: 0.1026, loss_bbox: 0.2737, d0.loss_cls: 0.1959, d0.loss_bbox: 0.3887, d1.loss_cls: 0.1400, d1.loss_bbox: 0.3031, d2.loss_cls: 0.1191, d2.loss_bbox: 0.2856, d3.loss_cls: 0.1064, d3.loss_bbox: 0.2872, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2716, loss: 2.5785, grad_norm: 45.6255
2025-06-17 19:22:30,933 - mmdet - INFO - Epoch [2][150/7033]	lr: 1.866e-04, eta: 18:40:13, time: 1.948, data_time: 0.056, memory: 17921, loss_cls: 0.0833, loss_bbox: 0.2547, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2762, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2623, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2634, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2510, loss: 2.3151, grad_norm: 92.7776
2025-06-17 19:24:05,346 - mmdet - INFO - Epoch [2][200/7033]	lr: 1.866e-04, eta: 18:38:30, time: 1.888, data_time: 0.064, memory: 17921, loss_cls: 0.0940, loss_bbox: 0.2506, d0.loss_cls: 0.1873, d0.loss_bbox: 0.3767, d1.loss_cls: 0.1296, d1.loss_bbox: 0.2816, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2643, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2655, d4.loss_cls: 0.0960, d4.loss_bbox: 0.2493, loss: 2.4009, grad_norm: 37.7483
2025-06-17 19:25:39,653 - mmdet - INFO - Epoch [2][250/7033]	lr: 1.866e-04, eta: 18:36:46, time: 1.886, data_time: 0.064, memory: 17921, loss_cls: 0.0900, loss_bbox: 0.2464, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2683, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2538, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2566, d4.loss_cls: 0.0918, d4.loss_bbox: 0.2429, loss: 2.3029, grad_norm: 28.5976
2025-06-17 19:27:16,328 - mmdet - INFO - Epoch [2][300/7033]	lr: 1.866e-04, eta: 18:35:13, time: 1.933, data_time: 0.058, memory: 17921, loss_cls: 0.0849, loss_bbox: 0.2485, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3539, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2699, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2559, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2597, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2449, loss: 2.2919, grad_norm: 43.3960
2025-06-17 19:28:49,767 - mmdet - INFO - Epoch [2][350/7033]	lr: 1.866e-04, eta: 18:33:25, time: 1.869, data_time: 0.064, memory: 17921, loss_cls: 0.0969, loss_bbox: 0.2628, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3852, d1.loss_cls: 0.1365, d1.loss_bbox: 0.2901, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2720, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2756, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2604, loss: 2.4903, grad_norm: 54.4886
2025-06-17 19:30:24,283 - mmdet - INFO - Epoch [2][400/7033]	lr: 1.866e-04, eta: 18:31:43, time: 1.890, data_time: 0.066, memory: 17921, loss_cls: 0.0853, loss_bbox: 0.2414, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3522, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2509, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2533, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2398, loss: 2.2666, grad_norm: 38.8610
2025-06-17 19:31:58,945 - mmdet - INFO - Epoch [2][450/7033]	lr: 1.866e-04, eta: 18:30:01, time: 1.893, data_time: 0.059, memory: 17921, loss_cls: 0.0872, loss_bbox: 0.2539, d0.loss_cls: 0.1862, d0.loss_bbox: 0.3766, d1.loss_cls: 0.1301, d1.loss_bbox: 0.2796, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2631, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2653, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2498, loss: 2.3814, grad_norm: 30.2932
2025-06-17 19:33:32,379 - mmdet - INFO - Epoch [2][500/7033]	lr: 1.866e-04, eta: 18:28:13, time: 1.868, data_time: 0.060, memory: 17921, loss_cls: 0.0845, loss_bbox: 0.2433, d0.loss_cls: 0.1821, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2664, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2507, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2533, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2392, loss: 2.2698, grad_norm: 35.2187
2025-06-17 19:35:09,316 - mmdet - INFO - Epoch [2][550/7033]	lr: 1.866e-04, eta: 18:26:42, time: 1.939, data_time: 0.063, memory: 17921, loss_cls: 0.0905, loss_bbox: 0.2708, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3883, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2956, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2808, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2843, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2684, loss: 2.4876, grad_norm: 23.5453
2025-06-17 19:36:43,287 - mmdet - INFO - Epoch [2][600/7033]	lr: 1.866e-04, eta: 18:24:57, time: 1.879, data_time: 0.066, memory: 17921, loss_cls: 0.0884, loss_bbox: 0.2598, d0.loss_cls: 0.1896, d0.loss_bbox: 0.3738, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2813, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2687, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2734, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2568, loss: 2.4038, grad_norm: 39.8771
2025-06-17 19:38:15,586 - mmdet - INFO - Epoch [2][650/7033]	lr: 1.866e-04, eta: 18:23:05, time: 1.846, data_time: 0.062, memory: 17921, loss_cls: 0.0913, loss_bbox: 0.2667, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3805, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2910, d2.loss_cls: 0.1052, d2.loss_bbox: 0.2777, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2792, d4.loss_cls: 0.0931, d4.loss_bbox: 0.2656, loss: 2.4467, grad_norm: 32.8519
2025-06-17 19:39:51,833 - mmdet - INFO - Epoch [2][700/7033]	lr: 1.866e-04, eta: 18:21:31, time: 1.925, data_time: 0.065, memory: 17921, loss_cls: 0.0953, loss_bbox: 0.2655, d0.loss_cls: 0.1960, d0.loss_bbox: 0.3823, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2959, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2782, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2773, d4.loss_cls: 0.0980, d4.loss_bbox: 0.2632, loss: 2.5003, grad_norm: 29.8507
2025-06-17 19:41:25,892 - mmdet - INFO - Epoch [2][750/7033]	lr: 1.866e-04, eta: 18:19:47, time: 1.881, data_time: 0.058, memory: 17921, loss_cls: 0.0908, loss_bbox: 0.2493, d0.loss_cls: 0.1907, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2567, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2563, d4.loss_cls: 0.0918, d4.loss_bbox: 0.2450, loss: 2.3557, grad_norm: 23.1910
2025-06-17 19:42:58,841 - mmdet - INFO - Epoch [2][800/7033]	lr: 1.866e-04, eta: 18:17:58, time: 1.859, data_time: 0.068, memory: 17921, loss_cls: 0.0942, loss_bbox: 0.2404, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3645, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2681, d2.loss_cls: 0.1055, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2549, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2399, loss: 2.3146, grad_norm: 63.9676
2025-06-17 19:44:34,478 - mmdet - INFO - Epoch [2][850/7033]	lr: 1.866e-04, eta: 18:16:22, time: 1.913, data_time: 0.056, memory: 17921, loss_cls: 0.0837, loss_bbox: 0.2387, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2637, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2488, d3.loss_cls: 0.0840, d3.loss_bbox: 0.2528, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2381, loss: 2.2362, grad_norm: 64.0803
2025-06-17 19:46:08,754 - mmdet - INFO - Epoch [2][900/7033]	lr: 1.866e-04, eta: 18:14:39, time: 1.885, data_time: 0.059, memory: 17921, loss_cls: 0.0861, loss_bbox: 0.2513, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1234, d1.loss_bbox: 0.2775, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2619, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2627, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2491, loss: 2.3317, grad_norm: 23.3555
2025-06-17 19:47:48,800 - mmdet - INFO - Epoch [2][950/7033]	lr: 1.866e-04, eta: 18:13:21, time: 2.001, data_time: 0.066, memory: 17921, loss_cls: 0.0835, loss_bbox: 0.2346, d0.loss_cls: 0.1845, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2592, d2.loss_cls: 0.1031, d2.loss_bbox: 0.2438, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2310, loss: 2.2317, grad_norm: 32.7909
2025-06-17 19:49:24,417 - mmdet - INFO - Epoch [2][1000/7033]	lr: 1.866e-04, eta: 18:11:44, time: 1.912, data_time: 0.069, memory: 17921, loss_cls: 0.0808, loss_bbox: 0.2398, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3583, d1.loss_cls: 0.1172, d1.loss_bbox: 0.2662, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2522, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2539, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2387, loss: 2.2469, grad_norm: 23.2004
2025-06-17 19:50:58,337 - mmdet - INFO - Epoch [2][1050/7033]	lr: 1.866e-04, eta: 18:10:00, time: 1.878, data_time: 0.062, memory: 17921, loss_cls: 0.0863, loss_bbox: 0.2393, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1163, d1.loss_bbox: 0.2760, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2545, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2567, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2399, loss: 2.2800, grad_norm: 35.6872
2025-06-17 19:52:30,472 - mmdet - INFO - Epoch [2][1100/7033]	lr: 1.866e-04, eta: 18:08:09, time: 1.843, data_time: 0.058, memory: 17921, loss_cls: 0.0869, loss_bbox: 0.2418, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2653, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2518, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2554, d4.loss_cls: 0.0880, d4.loss_bbox: 0.2408, loss: 2.2851, grad_norm: 29.7148
2025-06-17 19:54:06,380 - mmdet - INFO - Epoch [2][1150/7033]	lr: 1.866e-04, eta: 18:06:33, time: 1.918, data_time: 0.058, memory: 17921, loss_cls: 0.0808, loss_bbox: 0.2354, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3419, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2607, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2330, loss: 2.2023, grad_norm: 39.8275
2025-06-17 19:55:40,654 - mmdet - INFO - Epoch [2][1200/7033]	lr: 1.866e-04, eta: 18:04:51, time: 1.886, data_time: 0.069, memory: 17921, loss_cls: 0.0892, loss_bbox: 0.2334, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3518, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2623, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2464, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2496, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2315, loss: 2.2461, grad_norm: 27.0871
2025-06-17 19:57:15,517 - mmdet - INFO - Epoch [2][1250/7033]	lr: 1.866e-04, eta: 18:03:11, time: 1.897, data_time: 0.060, memory: 17921, loss_cls: 0.0808, loss_bbox: 0.2423, d0.loss_cls: 0.1767, d0.loss_bbox: 0.3636, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2736, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2546, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2416, loss: 2.2616, grad_norm: 256.8709
2025-06-17 19:58:48,709 - mmdet - INFO - Epoch [2][1300/7033]	lr: 1.866e-04, eta: 18:01:25, time: 1.863, data_time: 0.058, memory: 17921, loss_cls: 0.0825, loss_bbox: 0.2271, d0.loss_cls: 0.1780, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2558, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2417, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2246, loss: 2.1651, grad_norm: 20.8432
2025-06-17 20:00:22,986 - mmdet - INFO - Epoch [2][1350/7033]	lr: 1.866e-04, eta: 17:59:42, time: 1.884, data_time: 0.058, memory: 17921, loss_cls: 0.0818, loss_bbox: 0.2331, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3421, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2582, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2459, d3.loss_cls: 0.0850, d3.loss_bbox: 0.2482, d4.loss_cls: 0.0857, d4.loss_bbox: 0.2305, loss: 2.1976, grad_norm: 25.0934
2025-06-17 20:01:53,766 - mmdet - INFO - Epoch [2][1400/7033]	lr: 1.866e-04, eta: 17:57:47, time: 1.817, data_time: 0.053, memory: 17921, loss_cls: 0.0895, loss_bbox: 0.2356, d0.loss_cls: 0.1915, d0.loss_bbox: 0.3522, d1.loss_cls: 0.1265, d1.loss_bbox: 0.2651, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2509, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2488, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2334, loss: 2.2881, grad_norm: 26.6159
2025-06-17 20:03:28,621 - mmdet - INFO - Epoch [2][1450/7033]	lr: 1.866e-04, eta: 17:56:07, time: 1.897, data_time: 0.046, memory: 17921, loss_cls: 0.0916, loss_bbox: 0.2252, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2569, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0939, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2239, loss: 2.2306, grad_norm: 36.0759
2025-06-17 20:05:00,710 - mmdet - INFO - Epoch [2][1500/7033]	lr: 1.866e-04, eta: 17:54:17, time: 1.842, data_time: 0.054, memory: 17921, loss_cls: 0.0859, loss_bbox: 0.2361, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3549, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2654, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2493, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2491, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2338, loss: 2.2513, grad_norm: 42.9644
2025-06-17 20:06:33,133 - mmdet - INFO - Epoch [2][1550/7033]	lr: 1.866e-04, eta: 17:52:29, time: 1.848, data_time: 0.059, memory: 17921, loss_cls: 0.0898, loss_bbox: 0.2328, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3481, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2623, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2457, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2309, loss: 2.2359, grad_norm: 22.5199
2025-06-17 20:08:05,579 - mmdet - INFO - Epoch [2][1600/7033]	lr: 1.866e-04, eta: 17:50:40, time: 1.848, data_time: 0.055, memory: 17921, loss_cls: 0.0849, loss_bbox: 0.2269, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2548, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2402, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2260, loss: 2.1829, grad_norm: 27.6376
2025-06-17 20:09:41,778 - mmdet - INFO - Epoch [2][1650/7033]	lr: 1.866e-04, eta: 17:49:06, time: 1.925, data_time: 0.081, memory: 17921, loss_cls: 0.0854, loss_bbox: 0.2377, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2490, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2490, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2354, loss: 2.2580, grad_norm: 32.5250
2025-06-17 20:11:13,668 - mmdet - INFO - Epoch [2][1700/7033]	lr: 1.866e-04, eta: 17:47:16, time: 1.837, data_time: 0.061, memory: 17921, loss_cls: 0.0828, loss_bbox: 0.2322, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3561, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2642, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2505, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2314, loss: 2.2251, grad_norm: 22.3145
2025-06-17 20:12:45,403 - mmdet - INFO - Epoch [2][1750/7033]	lr: 1.866e-04, eta: 17:45:25, time: 1.835, data_time: 0.060, memory: 17921, loss_cls: 0.0866, loss_bbox: 0.2324, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1213, d1.loss_bbox: 0.2617, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2436, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2449, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2301, loss: 2.2362, grad_norm: 76.9859
2025-06-17 20:14:44,718 - mmdet - INFO - Epoch [2][1800/7033]	lr: 1.866e-04, eta: 17:45:19, time: 2.387, data_time: 0.073, memory: 17921, loss_cls: 0.0880, loss_bbox: 0.2305, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3474, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2579, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2421, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2443, d4.loss_cls: 0.0919, d4.loss_bbox: 0.2282, loss: 2.2294, grad_norm: 28.5893
2025-06-17 20:16:20,340 - mmdet - INFO - Epoch [2][1850/7033]	lr: 1.866e-04, eta: 17:43:43, time: 1.913, data_time: 0.065, memory: 17921, loss_cls: 0.0840, loss_bbox: 0.2298, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2606, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2457, d3.loss_cls: 0.0901, d3.loss_bbox: 0.2499, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2290, loss: 2.2321, grad_norm: 29.1183
2025-06-17 20:17:54,146 - mmdet - INFO - Epoch [2][1900/7033]	lr: 1.866e-04, eta: 17:42:00, time: 1.876, data_time: 0.063, memory: 17921, loss_cls: 0.0832, loss_bbox: 0.2271, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3442, d1.loss_cls: 0.1197, d1.loss_bbox: 0.2514, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2372, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2241, loss: 2.1875, grad_norm: 27.5568
2025-06-17 20:19:29,700 - mmdet - INFO - Epoch [2][1950/7033]	lr: 1.866e-04, eta: 17:40:23, time: 1.911, data_time: 0.081, memory: 17921, loss_cls: 0.0830, loss_bbox: 0.2283, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2552, d2.loss_cls: 0.1015, d2.loss_bbox: 0.2376, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2390, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2265, loss: 2.1904, grad_norm: 23.4883
2025-06-17 20:21:05,110 - mmdet - INFO - Epoch [2][2000/7033]	lr: 1.866e-04, eta: 17:38:46, time: 1.908, data_time: 0.058, memory: 17921, loss_cls: 0.0813, loss_bbox: 0.2269, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2566, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2392, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2255, loss: 2.1889, grad_norm: 102.0186
2025-06-17 20:22:39,184 - mmdet - INFO - Epoch [2][2050/7033]	lr: 1.866e-04, eta: 17:37:04, time: 1.881, data_time: 0.074, memory: 17921, loss_cls: 0.0837, loss_bbox: 0.2333, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3596, d1.loss_cls: 0.1195, d1.loss_bbox: 0.2603, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2447, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2310, loss: 2.2324, grad_norm: 25.7334
2025-06-17 20:24:14,728 - mmdet - INFO - Epoch [2][2100/7033]	lr: 1.866e-04, eta: 17:35:27, time: 1.910, data_time: 0.048, memory: 17921, loss_cls: 0.0845, loss_bbox: 0.2238, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1150, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2344, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2214, loss: 2.1553, grad_norm: 24.6104
2025-06-17 20:25:45,046 - mmdet - INFO - Epoch [2][2150/7033]	lr: 1.866e-04, eta: 17:33:32, time: 1.807, data_time: 0.051, memory: 17921, loss_cls: 0.0862, loss_bbox: 0.2274, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3475, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2423, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2274, loss: 2.2047, grad_norm: 26.7441
2025-06-17 20:27:18,482 - mmdet - INFO - Epoch [2][2200/7033]	lr: 1.866e-04, eta: 17:31:48, time: 1.869, data_time: 0.058, memory: 17921, loss_cls: 0.0833, loss_bbox: 0.2340, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2633, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2437, d3.loss_cls: 0.0872, d3.loss_bbox: 0.2436, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2321, loss: 2.2141, grad_norm: 38.4244
2025-06-17 20:28:54,529 - mmdet - INFO - Epoch [2][2250/7033]	lr: 1.866e-04, eta: 17:30:13, time: 1.921, data_time: 0.063, memory: 17921, loss_cls: 0.0870, loss_bbox: 0.2376, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3584, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2711, d2.loss_cls: 0.0987, d2.loss_bbox: 0.2526, d3.loss_cls: 0.0907, d3.loss_bbox: 0.2524, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2384, loss: 2.2617, grad_norm: 23.8986
2025-06-17 20:30:27,582 - mmdet - INFO - Epoch [2][2300/7033]	lr: 1.866e-04, eta: 17:28:28, time: 1.861, data_time: 0.062, memory: 17921, loss_cls: 0.0922, loss_bbox: 0.2358, d0.loss_cls: 0.1754, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2691, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2519, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2530, d4.loss_cls: 0.0938, d4.loss_bbox: 0.2357, loss: 2.2951, grad_norm: 25.8174
2025-06-17 20:31:54,156 - mmdet - INFO - Epoch [2][2350/7033]	lr: 1.866e-04, eta: 17:26:21, time: 1.731, data_time: 0.060, memory: 17921, loss_cls: 0.0935, loss_bbox: 0.2260, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1305, d1.loss_bbox: 0.2574, d2.loss_cls: 0.1109, d2.loss_bbox: 0.2390, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2422, d4.loss_cls: 0.0984, d4.loss_bbox: 0.2257, loss: 2.2502, grad_norm: 45.3008
2025-06-17 20:33:27,975 - mmdet - INFO - Epoch [2][2400/7033]	lr: 1.866e-04, eta: 17:24:39, time: 1.876, data_time: 0.056, memory: 17921, loss_cls: 0.0985, loss_bbox: 0.2327, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3606, d1.loss_cls: 0.1394, d1.loss_bbox: 0.2733, d2.loss_cls: 0.1222, d2.loss_bbox: 0.2537, d3.loss_cls: 0.1035, d3.loss_bbox: 0.2538, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2323, loss: 2.3591, grad_norm: 79.1252
2025-06-17 20:35:03,084 - mmdet - INFO - Epoch [2][2450/7033]	lr: 1.866e-04, eta: 17:23:01, time: 1.902, data_time: 0.060, memory: 17921, loss_cls: 0.0905, loss_bbox: 0.2321, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3547, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2516, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2530, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2331, loss: 2.2809, grad_norm: 46.5832
2025-06-17 20:36:40,012 - mmdet - INFO - Epoch [2][2500/7033]	lr: 1.866e-04, eta: 17:21:30, time: 1.938, data_time: 0.057, memory: 17921, loss_cls: 0.0887, loss_bbox: 0.2335, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3548, d1.loss_cls: 0.1241, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2474, d3.loss_cls: 0.0945, d3.loss_bbox: 0.2477, d4.loss_cls: 0.0911, d4.loss_bbox: 0.2330, loss: 2.2642, grad_norm: 88.7215
2025-06-17 20:38:13,696 - mmdet - INFO - Epoch [2][2550/7033]	lr: 1.866e-04, eta: 17:19:47, time: 1.873, data_time: 0.063, memory: 17921, loss_cls: 0.0861, loss_bbox: 0.2426, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3654, d1.loss_cls: 0.1254, d1.loss_bbox: 0.2709, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2543, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2550, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2400, loss: 2.3127, grad_norm: 23.2442
2025-06-17 20:39:47,719 - mmdet - INFO - Epoch [2][2600/7033]	lr: 1.866e-04, eta: 17:18:06, time: 1.882, data_time: 0.062, memory: 17921, loss_cls: 0.0956, loss_bbox: 0.2445, d0.loss_cls: 0.1908, d0.loss_bbox: 0.3667, d1.loss_cls: 0.1320, d1.loss_bbox: 0.2765, d2.loss_cls: 0.1127, d2.loss_bbox: 0.2588, d3.loss_cls: 0.0995, d3.loss_bbox: 0.2622, d4.loss_cls: 0.0982, d4.loss_bbox: 0.2435, loss: 2.3811, grad_norm: 23.3566
2025-06-17 20:41:22,045 - mmdet - INFO - Epoch [2][2650/7033]	lr: 1.866e-04, eta: 17:16:26, time: 1.885, data_time: 0.064, memory: 17921, loss_cls: 0.0881, loss_bbox: 0.2329, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3578, d1.loss_cls: 0.1302, d1.loss_bbox: 0.2632, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2446, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2447, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2290, loss: 2.2673, grad_norm: 31.2541
2025-06-17 20:42:56,467 - mmdet - INFO - Epoch [2][2700/7033]	lr: 1.866e-04, eta: 17:14:47, time: 1.889, data_time: 0.067, memory: 17921, loss_cls: 0.0920, loss_bbox: 0.2359, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3566, d1.loss_cls: 0.1287, d1.loss_bbox: 0.2661, d2.loss_cls: 0.1081, d2.loss_bbox: 0.2493, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2505, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2345, loss: 2.3002, grad_norm: 24.8558
2025-06-17 20:44:27,030 - mmdet - INFO - Epoch [2][2750/7033]	lr: 1.866e-04, eta: 17:12:54, time: 1.812, data_time: 0.056, memory: 17921, loss_cls: 0.0915, loss_bbox: 0.2317, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3577, d1.loss_cls: 0.1234, d1.loss_bbox: 0.2660, d2.loss_cls: 0.1068, d2.loss_bbox: 0.2446, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2453, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2302, loss: 2.2670, grad_norm: 27.9064
2025-06-17 20:45:53,783 - mmdet - INFO - Epoch [2][2800/7033]	lr: 1.866e-04, eta: 17:10:50, time: 1.735, data_time: 0.062, memory: 17921, loss_cls: 0.0882, loss_bbox: 0.2459, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3702, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2757, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2551, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2565, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2405, loss: 2.3361, grad_norm: 42.9009
2025-06-17 20:47:25,971 - mmdet - INFO - Epoch [2][2850/7033]	lr: 1.866e-04, eta: 17:09:03, time: 1.844, data_time: 0.057, memory: 17921, loss_cls: 0.0921, loss_bbox: 0.2350, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1318, d1.loss_bbox: 0.2680, d2.loss_cls: 0.1115, d2.loss_bbox: 0.2500, d3.loss_cls: 0.0996, d3.loss_bbox: 0.2509, d4.loss_cls: 0.0966, d4.loss_bbox: 0.2339, loss: 2.3139, grad_norm: 38.4949
2025-06-17 20:48:59,810 - mmdet - INFO - Epoch [2][2900/7033]	lr: 1.866e-04, eta: 17:07:22, time: 1.877, data_time: 0.073, memory: 17921, loss_cls: 0.0868, loss_bbox: 0.2304, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3574, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2619, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2466, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2299, loss: 2.2513, grad_norm: 24.7576
2025-06-17 20:50:33,016 - mmdet - INFO - Epoch [2][2950/7033]	lr: 1.866e-04, eta: 17:05:39, time: 1.864, data_time: 0.062, memory: 17921, loss_cls: 0.0854, loss_bbox: 0.2190, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3413, d1.loss_cls: 0.1193, d1.loss_bbox: 0.2513, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0872, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2184, loss: 2.1532, grad_norm: 31.5654
2025-06-17 20:52:06,812 - mmdet - INFO - Epoch [2][3000/7033]	lr: 1.866e-04, eta: 17:03:58, time: 1.876, data_time: 0.052, memory: 17921, loss_cls: 0.0755, loss_bbox: 0.2241, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2586, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2402, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2399, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2250, loss: 2.1330, grad_norm: 29.0461
2025-06-17 20:53:41,255 - mmdet - INFO - Epoch [2][3050/7033]	lr: 1.866e-04, eta: 17:02:19, time: 1.889, data_time: 0.055, memory: 17921, loss_cls: 0.0899, loss_bbox: 0.2378, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3536, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2680, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2501, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2487, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2365, loss: 2.2875, grad_norm: 31.7089
2025-06-17 20:55:16,633 - mmdet - INFO - Epoch [2][3100/7033]	lr: 1.866e-04, eta: 17:00:43, time: 1.908, data_time: 0.056, memory: 17921, loss_cls: 0.0880, loss_bbox: 0.2427, d0.loss_cls: 0.1931, d0.loss_bbox: 0.3663, d1.loss_cls: 0.1288, d1.loss_bbox: 0.2746, d2.loss_cls: 0.1089, d2.loss_bbox: 0.2563, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2600, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2414, loss: 2.3481, grad_norm: 26.4793
2025-06-17 20:56:50,233 - mmdet - INFO - Epoch [2][3150/7033]	lr: 1.866e-04, eta: 16:59:02, time: 1.872, data_time: 0.067, memory: 17921, loss_cls: 0.0877, loss_bbox: 0.2347, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1036, d2.loss_bbox: 0.2498, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2521, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2335, loss: 2.2770, grad_norm: 31.1642
2025-06-17 20:58:16,321 - mmdet - INFO - Epoch [2][3200/7033]	lr: 1.866e-04, eta: 16:56:57, time: 1.722, data_time: 0.050, memory: 17921, loss_cls: 0.0948, loss_bbox: 0.2415, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3575, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2523, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2516, d4.loss_cls: 0.0960, d4.loss_bbox: 0.2393, loss: 2.3297, grad_norm: 27.2496
2025-06-17 20:59:50,246 - mmdet - INFO - Epoch [2][3250/7033]	lr: 1.866e-04, eta: 16:55:17, time: 1.878, data_time: 0.055, memory: 17921, loss_cls: 0.0885, loss_bbox: 0.2280, d0.loss_cls: 0.1823, d0.loss_bbox: 0.3555, d1.loss_cls: 0.1228, d1.loss_bbox: 0.2616, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2438, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2455, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2293, loss: 2.2412, grad_norm: 63.8013
2025-06-17 21:01:21,986 - mmdet - INFO - Epoch [2][3300/7033]	lr: 1.866e-04, eta: 16:53:30, time: 1.833, data_time: 0.058, memory: 17921, loss_cls: 0.0844, loss_bbox: 0.2272, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3463, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2573, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2412, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2283, loss: 2.1882, grad_norm: 25.0258
2025-06-17 21:02:57,376 - mmdet - INFO - Epoch [2][3350/7033]	lr: 1.866e-04, eta: 16:51:54, time: 1.909, data_time: 0.067, memory: 17921, loss_cls: 0.0790, loss_bbox: 0.2225, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2541, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2346, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2214, loss: 2.1650, grad_norm: 24.3988
2025-06-17 21:04:34,434 - mmdet - INFO - Epoch [2][3400/7033]	lr: 1.866e-04, eta: 16:50:24, time: 1.939, data_time: 0.058, memory: 17921, loss_cls: 0.0871, loss_bbox: 0.2370, d0.loss_cls: 0.1858, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1252, d1.loss_bbox: 0.2757, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2534, d3.loss_cls: 0.0885, d3.loss_bbox: 0.2550, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2396, loss: 2.3110, grad_norm: 25.6859
2025-06-17 21:06:07,658 - mmdet - INFO - Epoch [2][3450/7033]	lr: 1.866e-04, eta: 16:48:42, time: 1.866, data_time: 0.065, memory: 17921, loss_cls: 0.0932, loss_bbox: 0.2288, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3607, d1.loss_cls: 0.1268, d1.loss_bbox: 0.2640, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2469, d3.loss_cls: 0.0982, d3.loss_bbox: 0.2470, d4.loss_cls: 0.0949, d4.loss_bbox: 0.2310, loss: 2.2814, grad_norm: 25.5603
2025-06-17 21:07:48,001 - mmdet - INFO - Epoch [2][3500/7033]	lr: 1.866e-04, eta: 16:47:21, time: 2.007, data_time: 0.081, memory: 17921, loss_cls: 0.0930, loss_bbox: 0.2350, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3629, d1.loss_cls: 0.1323, d1.loss_bbox: 0.2616, d2.loss_cls: 0.1109, d2.loss_bbox: 0.2497, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2503, d4.loss_cls: 0.0941, d4.loss_bbox: 0.2344, loss: 2.3103, grad_norm: 49.8820
2025-06-17 21:09:19,199 - mmdet - INFO - Epoch [2][3550/7033]	lr: 1.866e-04, eta: 16:45:33, time: 1.823, data_time: 0.060, memory: 17921, loss_cls: 0.0829, loss_bbox: 0.2390, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3570, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2603, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2483, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2521, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2360, loss: 2.2561, grad_norm: 37.5262
2025-06-17 21:10:52,245 - mmdet - INFO - Epoch [2][3600/7033]	lr: 1.866e-04, eta: 16:43:51, time: 1.862, data_time: 0.052, memory: 17921, loss_cls: 0.0895, loss_bbox: 0.2464, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3530, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2621, d2.loss_cls: 0.1058, d2.loss_bbox: 0.2504, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2566, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2402, loss: 2.2978, grad_norm: 28.6717
2025-06-17 21:12:29,447 - mmdet - INFO - Epoch [2][3650/7033]	lr: 1.866e-04, eta: 16:42:20, time: 1.944, data_time: 0.061, memory: 17921, loss_cls: 0.0818, loss_bbox: 0.2282, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2629, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2453, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2448, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2280, loss: 2.2133, grad_norm: 59.9698
2025-06-17 21:14:02,646 - mmdet - INFO - Epoch [2][3700/7033]	lr: 1.866e-04, eta: 16:40:39, time: 1.864, data_time: 0.060, memory: 17921, loss_cls: 0.0920, loss_bbox: 0.2292, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3587, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2629, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2417, d3.loss_cls: 0.0981, d3.loss_bbox: 0.2444, d4.loss_cls: 0.0956, d4.loss_bbox: 0.2279, loss: 2.2745, grad_norm: 60.9130
2025-06-17 21:15:29,760 - mmdet - INFO - Epoch [2][3750/7033]	lr: 1.866e-04, eta: 16:38:39, time: 1.742, data_time: 0.079, memory: 17921, loss_cls: 0.0820, loss_bbox: 0.2237, d0.loss_cls: 0.1843, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2562, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2234, loss: 2.1852, grad_norm: 41.5044
2025-06-17 21:17:05,522 - mmdet - INFO - Epoch [2][3800/7033]	lr: 1.866e-04, eta: 16:37:05, time: 1.915, data_time: 0.069, memory: 17921, loss_cls: 0.0799, loss_bbox: 0.2300, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3627, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2663, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2473, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2440, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2310, loss: 2.2296, grad_norm: 37.5407
2025-06-17 21:18:38,740 - mmdet - INFO - Epoch [2][3850/7033]	lr: 1.866e-04, eta: 16:35:23, time: 1.864, data_time: 0.059, memory: 17921, loss_cls: 0.0861, loss_bbox: 0.2329, d0.loss_cls: 0.1866, d0.loss_bbox: 0.3629, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2674, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2486, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2500, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2355, loss: 2.2773, grad_norm: 32.5462
2025-06-17 21:20:12,760 - mmdet - INFO - Epoch [2][3900/7033]	lr: 1.866e-04, eta: 16:33:44, time: 1.880, data_time: 0.065, memory: 17921, loss_cls: 0.0853, loss_bbox: 0.2352, d0.loss_cls: 0.1911, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2454, d3.loss_cls: 0.0920, d3.loss_bbox: 0.2478, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2342, loss: 2.2677, grad_norm: 27.6503
2025-06-17 21:21:47,868 - mmdet - INFO - Epoch [2][3950/7033]	lr: 1.866e-04, eta: 16:32:08, time: 1.903, data_time: 0.059, memory: 17921, loss_cls: 0.0835, loss_bbox: 0.2308, d0.loss_cls: 0.1923, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2616, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2445, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2416, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2312, loss: 2.2405, grad_norm: 40.5020
2025-06-17 21:23:21,265 - mmdet - INFO - Epoch [2][4000/7033]	lr: 1.866e-04, eta: 16:30:27, time: 1.868, data_time: 0.056, memory: 17921, loss_cls: 0.0864, loss_bbox: 0.2369, d0.loss_cls: 0.1922, d0.loss_bbox: 0.3690, d1.loss_cls: 0.1237, d1.loss_bbox: 0.2715, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2525, d3.loss_cls: 0.0910, d3.loss_bbox: 0.2542, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2377, loss: 2.3053, grad_norm: 39.3514
2025-06-17 21:24:54,129 - mmdet - INFO - Epoch [2][4050/7033]	lr: 1.866e-04, eta: 16:28:45, time: 1.856, data_time: 0.055, memory: 17921, loss_cls: 0.0858, loss_bbox: 0.2406, d0.loss_cls: 0.1877, d0.loss_bbox: 0.3706, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2528, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2529, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2406, loss: 2.3066, grad_norm: 35.6604
2025-06-17 21:26:29,250 - mmdet - INFO - Epoch [2][4100/7033]	lr: 1.866e-04, eta: 16:27:09, time: 1.904, data_time: 0.059, memory: 17921, loss_cls: 0.0873, loss_bbox: 0.2407, d0.loss_cls: 0.1854, d0.loss_bbox: 0.3716, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2772, d2.loss_cls: 0.1055, d2.loss_bbox: 0.2567, d3.loss_cls: 0.0941, d3.loss_bbox: 0.2605, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2424, loss: 2.3368, grad_norm: 41.0376
2025-06-17 21:28:03,157 - mmdet - INFO - Epoch [2][4150/7033]	lr: 1.866e-04, eta: 16:25:30, time: 1.878, data_time: 0.072, memory: 17921, loss_cls: 0.0847, loss_bbox: 0.2318, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2669, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2434, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2435, d4.loss_cls: 0.0888, d4.loss_bbox: 0.2316, loss: 2.2729, grad_norm: 20.9135
2025-06-17 21:29:36,756 - mmdet - INFO - Epoch [2][4200/7033]	lr: 1.866e-04, eta: 16:23:49, time: 1.872, data_time: 0.064, memory: 17921, loss_cls: 0.0837, loss_bbox: 0.2346, d0.loss_cls: 0.1973, d0.loss_bbox: 0.3701, d1.loss_cls: 0.1274, d1.loss_bbox: 0.2685, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2496, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2489, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2333, loss: 2.2926, grad_norm: 50.9834
2025-06-17 21:31:07,580 - mmdet - INFO - Epoch [2][4250/7033]	lr: 1.866e-04, eta: 16:22:02, time: 1.816, data_time: 0.058, memory: 17921, loss_cls: 0.0870, loss_bbox: 0.2325, d0.loss_cls: 0.1947, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2723, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2506, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2500, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2325, loss: 2.3049, grad_norm: 28.6691
2025-06-17 21:32:37,681 - mmdet - INFO - Epoch [2][4300/7033]	lr: 1.866e-04, eta: 16:20:12, time: 1.802, data_time: 0.053, memory: 17921, loss_cls: 0.0876, loss_bbox: 0.2245, d0.loss_cls: 0.2005, d0.loss_bbox: 0.3758, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2390, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2258, loss: 2.2851, grad_norm: 39.0949
2025-06-17 21:34:09,598 - mmdet - INFO - Epoch [2][4350/7033]	lr: 1.866e-04, eta: 16:18:28, time: 1.838, data_time: 0.078, memory: 17921, loss_cls: 0.0874, loss_bbox: 0.2302, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2627, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0937, d3.loss_bbox: 0.2468, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2288, loss: 2.2682, grad_norm: 28.0639
2025-06-17 21:35:43,033 - mmdet - INFO - Epoch [2][4400/7033]	lr: 1.866e-04, eta: 16:16:48, time: 1.869, data_time: 0.060, memory: 17921, loss_cls: 0.0779, loss_bbox: 0.2207, d0.loss_cls: 0.1830, d0.loss_bbox: 0.3578, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2374, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2203, loss: 2.1668, grad_norm: 33.7534
2025-06-17 21:37:15,107 - mmdet - INFO - Epoch [2][4450/7033]	lr: 1.866e-04, eta: 16:15:04, time: 1.841, data_time: 0.066, memory: 17921, loss_cls: 0.0906, loss_bbox: 0.2308, d0.loss_cls: 0.1944, d0.loss_bbox: 0.3665, d1.loss_cls: 0.1294, d1.loss_bbox: 0.2621, d2.loss_cls: 0.1082, d2.loss_bbox: 0.2440, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2458, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2304, loss: 2.2911, grad_norm: 22.5580
2025-06-17 21:38:51,698 - mmdet - INFO - Epoch [2][4500/7033]	lr: 1.866e-04, eta: 16:13:32, time: 1.932, data_time: 0.064, memory: 17921, loss_cls: 0.0874, loss_bbox: 0.2326, d0.loss_cls: 0.1936, d0.loss_bbox: 0.3673, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1036, d2.loss_bbox: 0.2477, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0905, d4.loss_bbox: 0.2311, loss: 2.2884, grad_norm: 90.3594
2025-06-17 21:40:27,241 - mmdet - INFO - Epoch [2][4550/7033]	lr: 1.866e-04, eta: 16:11:58, time: 1.911, data_time: 0.059, memory: 17921, loss_cls: 0.0855, loss_bbox: 0.2261, d0.loss_cls: 0.1897, d0.loss_bbox: 0.3650, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2625, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2451, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2467, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2292, loss: 2.2532, grad_norm: 30.3945
2025-06-17 21:41:59,879 - mmdet - INFO - Epoch [2][4600/7033]	lr: 1.866e-04, eta: 16:10:16, time: 1.851, data_time: 0.064, memory: 17921, loss_cls: 0.0877, loss_bbox: 0.2274, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2571, d2.loss_cls: 0.1050, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2414, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2278, loss: 2.2301, grad_norm: 29.9742
2025-06-17 21:43:34,430 - mmdet - INFO - Epoch [2][4650/7033]	lr: 1.866e-04, eta: 16:08:39, time: 1.892, data_time: 0.059, memory: 17921, loss_cls: 0.0886, loss_bbox: 0.2355, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3598, d1.loss_cls: 0.1200, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2539, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2534, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2373, loss: 2.2847, grad_norm: 37.0170
2025-06-17 21:45:09,024 - mmdet - INFO - Epoch [2][4700/7033]	lr: 1.866e-04, eta: 16:07:02, time: 1.890, data_time: 0.060, memory: 17921, loss_cls: 0.0844, loss_bbox: 0.2341, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1231, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2475, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2471, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2346, loss: 2.2585, grad_norm: 39.8028
2025-06-17 21:46:42,610 - mmdet - INFO - Epoch [2][4750/7033]	lr: 1.866e-04, eta: 16:05:23, time: 1.874, data_time: 0.072, memory: 17921, loss_cls: 0.0871, loss_bbox: 0.2374, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2764, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2564, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2567, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2411, loss: 2.3066, grad_norm: 62.9949
2025-06-17 21:48:19,267 - mmdet - INFO - Epoch [2][4800/7033]	lr: 1.866e-04, eta: 16:03:51, time: 1.933, data_time: 0.058, memory: 17921, loss_cls: 0.0879, loss_bbox: 0.2411, d0.loss_cls: 0.1910, d0.loss_bbox: 0.3618, d1.loss_cls: 0.1254, d1.loss_bbox: 0.2761, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2554, d3.loss_cls: 0.0934, d3.loss_bbox: 0.2566, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2432, loss: 2.3285, grad_norm: 33.3377
2025-06-17 21:49:56,714 - mmdet - INFO - Epoch [2][4850/7033]	lr: 1.866e-04, eta: 16:02:21, time: 1.949, data_time: 0.063, memory: 17921, loss_cls: 0.0778, loss_bbox: 0.2202, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3445, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2198, loss: 2.1333, grad_norm: 26.8092
2025-06-17 21:51:35,083 - mmdet - INFO - Epoch [2][4900/7033]	lr: 1.866e-04, eta: 16:00:54, time: 1.966, data_time: 0.060, memory: 17921, loss_cls: 0.0901, loss_bbox: 0.2424, d0.loss_cls: 0.1861, d0.loss_bbox: 0.3546, d1.loss_cls: 0.1243, d1.loss_bbox: 0.2762, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2588, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2602, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2427, loss: 2.3257, grad_norm: 27.0448
2025-06-17 21:53:11,216 - mmdet - INFO - Epoch [2][4950/7033]	lr: 1.866e-04, eta: 15:59:21, time: 1.924, data_time: 0.057, memory: 17921, loss_cls: 0.0886, loss_bbox: 0.2385, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3579, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2740, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2529, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2551, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2385, loss: 2.2985, grad_norm: 33.1190
2025-06-17 21:54:45,365 - mmdet - INFO - Epoch [2][5000/7033]	lr: 1.866e-04, eta: 15:57:43, time: 1.883, data_time: 0.056, memory: 17921, loss_cls: 0.0818, loss_bbox: 0.2256, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3427, d1.loss_cls: 0.1181, d1.loss_bbox: 0.2552, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2413, d4.loss_cls: 0.0841, d4.loss_bbox: 0.2255, loss: 2.1772, grad_norm: 19.1046
2025-06-17 21:56:25,545 - mmdet - INFO - Epoch [2][5050/7033]	lr: 1.866e-04, eta: 15:56:20, time: 2.002, data_time: 0.061, memory: 17921, loss_cls: 0.0883, loss_bbox: 0.2282, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3538, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2629, d2.loss_cls: 0.1026, d2.loss_bbox: 0.2476, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2483, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2294, loss: 2.2529, grad_norm: 22.6064
2025-06-17 21:57:59,877 - mmdet - INFO - Epoch [2][5100/7033]	lr: 1.866e-04, eta: 15:54:42, time: 1.887, data_time: 0.067, memory: 17921, loss_cls: 0.0808, loss_bbox: 0.2281, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2618, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2413, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2446, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2275, loss: 2.1792, grad_norm: 29.8471
2025-06-17 21:59:31,150 - mmdet - INFO - Epoch [2][5150/7033]	lr: 1.866e-04, eta: 15:52:57, time: 1.827, data_time: 0.058, memory: 17921, loss_cls: 0.0869, loss_bbox: 0.2338, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2719, d2.loss_cls: 0.1055, d2.loss_bbox: 0.2489, d3.loss_cls: 0.0944, d3.loss_bbox: 0.2498, d4.loss_cls: 0.0905, d4.loss_bbox: 0.2342, loss: 2.2896, grad_norm: 29.8744
2025-06-17 22:01:03,111 - mmdet - INFO - Epoch [2][5200/7033]	lr: 1.866e-04, eta: 15:51:14, time: 1.839, data_time: 0.058, memory: 17921, loss_cls: 0.0884, loss_bbox: 0.2212, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3439, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2531, d2.loss_cls: 0.1046, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0912, d4.loss_bbox: 0.2227, loss: 2.1996, grad_norm: 52.0308
2025-06-17 22:02:32,908 - mmdet - INFO - Epoch [2][5250/7033]	lr: 1.866e-04, eta: 15:49:25, time: 1.795, data_time: 0.062, memory: 17921, loss_cls: 0.0890, loss_bbox: 0.2279, d0.loss_cls: 0.1937, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2620, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2425, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2429, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2282, loss: 2.2580, grad_norm: 26.6269
2025-06-17 22:04:11,297 - mmdet - INFO - Epoch [2][5300/7033]	lr: 1.866e-04, eta: 15:47:58, time: 1.969, data_time: 0.055, memory: 17921, loss_cls: 0.0818, loss_bbox: 0.2313, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1185, d1.loss_bbox: 0.2665, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2467, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2472, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2328, loss: 2.2235, grad_norm: 20.4918
2025-06-17 22:05:49,313 - mmdet - INFO - Epoch [2][5350/7033]	lr: 1.866e-04, eta: 15:46:29, time: 1.959, data_time: 0.061, memory: 17921, loss_cls: 0.0809, loss_bbox: 0.2224, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2523, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2379, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2247, loss: 2.1767, grad_norm: 20.4816
2025-06-17 22:07:25,803 - mmdet - INFO - Epoch [2][5400/7033]	lr: 1.866e-04, eta: 15:44:57, time: 1.931, data_time: 0.068, memory: 17921, loss_cls: 0.0791, loss_bbox: 0.2173, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2544, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2351, d4.loss_cls: 0.0813, d4.loss_bbox: 0.2172, loss: 2.1415, grad_norm: 24.9593
2025-06-17 22:09:00,408 - mmdet - INFO - Epoch [2][5450/7033]	lr: 1.866e-04, eta: 15:43:21, time: 1.892, data_time: 0.062, memory: 17921, loss_cls: 0.0770, loss_bbox: 0.2293, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3639, d1.loss_cls: 0.1191, d1.loss_bbox: 0.2656, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2445, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2474, d4.loss_cls: 0.0797, d4.loss_bbox: 0.2299, loss: 2.2130, grad_norm: 62.1876
2025-06-17 22:11:01,745 - mmdet - INFO - Epoch [2][5500/7033]	lr: 1.866e-04, eta: 15:42:47, time: 2.425, data_time: 0.464, memory: 17921, loss_cls: 0.0841, loss_bbox: 0.2369, d0.loss_cls: 0.1880, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2694, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2482, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2520, d4.loss_cls: 0.0870, d4.loss_bbox: 0.2347, loss: 2.2635, grad_norm: 23.1052
2025-06-17 22:12:49,903 - mmdet - INFO - Epoch [2][5550/7033]	lr: 1.866e-04, eta: 15:41:42, time: 2.165, data_time: 0.254, memory: 17921, loss_cls: 0.0850, loss_bbox: 0.2316, d0.loss_cls: 0.1847, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1036, d2.loss_bbox: 0.2489, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2481, d4.loss_cls: 0.0878, d4.loss_bbox: 0.2327, loss: 2.2674, grad_norm: 28.9674
2025-06-17 22:14:27,015 - mmdet - INFO - Epoch [2][5600/7033]	lr: 1.866e-04, eta: 15:40:11, time: 1.942, data_time: 0.068, memory: 17921, loss_cls: 0.0837, loss_bbox: 0.2254, d0.loss_cls: 0.1825, d0.loss_bbox: 0.3489, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2403, d4.loss_cls: 0.0857, d4.loss_bbox: 0.2259, loss: 2.1959, grad_norm: 29.0859
2025-06-17 22:15:58,706 - mmdet - INFO - Epoch [2][5650/7033]	lr: 1.866e-04, eta: 15:38:26, time: 1.834, data_time: 0.059, memory: 17921, loss_cls: 0.0822, loss_bbox: 0.2369, d0.loss_cls: 0.1898, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2716, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2511, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2509, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2391, loss: 2.2890, grad_norm: 79.4130
2025-06-17 22:17:39,387 - mmdet - INFO - Epoch [2][5700/7033]	lr: 1.866e-04, eta: 15:37:03, time: 2.012, data_time: 0.054, memory: 17921, loss_cls: 0.0732, loss_bbox: 0.2162, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3411, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2166, loss: 2.0822, grad_norm: 27.1770
2025-06-17 22:19:18,889 - mmdet - INFO - Epoch [2][5750/7033]	lr: 1.866e-04, eta: 15:35:37, time: 1.992, data_time: 0.085, memory: 17921, loss_cls: 0.0897, loss_bbox: 0.2259, d0.loss_cls: 0.1933, d0.loss_bbox: 0.3540, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2618, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2437, d3.loss_cls: 0.0931, d3.loss_bbox: 0.2457, d4.loss_cls: 0.0903, d4.loss_bbox: 0.2293, loss: 2.2557, grad_norm: 39.1395
2025-06-17 22:20:55,149 - mmdet - INFO - Epoch [2][5800/7033]	lr: 1.866e-04, eta: 15:34:04, time: 1.925, data_time: 0.086, memory: 17921, loss_cls: 0.0779, loss_bbox: 0.2201, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3466, d1.loss_cls: 0.1106, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2344, d3.loss_cls: 0.0801, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2221, loss: 2.1309, grad_norm: 50.9349
2025-06-17 22:22:35,093 - mmdet - INFO - Epoch [2][5850/7033]	lr: 1.866e-04, eta: 15:32:39, time: 1.999, data_time: 0.080, memory: 17921, loss_cls: 0.0837, loss_bbox: 0.2299, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2579, d2.loss_cls: 0.1008, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2404, d4.loss_cls: 0.0868, d4.loss_bbox: 0.2315, loss: 2.2109, grad_norm: 21.4653
2025-06-17 22:24:11,644 - mmdet - INFO - Epoch [2][5900/7033]	lr: 1.866e-04, eta: 15:31:06, time: 1.931, data_time: 0.064, memory: 17921, loss_cls: 0.0824, loss_bbox: 0.2219, d0.loss_cls: 0.1798, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2535, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2380, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2233, loss: 2.1733, grad_norm: 24.8776
2025-06-17 22:25:45,439 - mmdet - INFO - Epoch [2][5950/7033]	lr: 1.866e-04, eta: 15:29:27, time: 1.876, data_time: 0.062, memory: 17921, loss_cls: 0.0840, loss_bbox: 0.2285, d0.loss_cls: 0.1828, d0.loss_bbox: 0.3611, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2686, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2477, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2456, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2311, loss: 2.2396, grad_norm: 41.5071
2025-06-17 22:27:18,660 - mmdet - INFO - Epoch [2][6000/7033]	lr: 1.866e-04, eta: 15:27:46, time: 1.864, data_time: 0.064, memory: 17921, loss_cls: 0.0723, loss_bbox: 0.2153, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3428, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2163, loss: 2.0696, grad_norm: 66.0616
2025-06-17 22:28:51,989 - mmdet - INFO - Epoch [2][6050/7033]	lr: 1.866e-04, eta: 15:26:06, time: 1.867, data_time: 0.051, memory: 17921, loss_cls: 0.0855, loss_bbox: 0.2352, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2673, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2513, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2535, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2379, loss: 2.2673, grad_norm: 23.6352
2025-06-17 22:30:41,215 - mmdet - INFO - Epoch [2][6100/7033]	lr: 1.866e-04, eta: 15:25:01, time: 2.184, data_time: 0.075, memory: 17921, loss_cls: 0.0764, loss_bbox: 0.2173, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3375, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2496, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2322, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2177, loss: 2.0958, grad_norm: 38.0487
2025-06-17 22:32:17,956 - mmdet - INFO - Epoch [2][6150/7033]	lr: 1.866e-04, eta: 15:23:28, time: 1.935, data_time: 0.066, memory: 17921, loss_cls: 0.0780, loss_bbox: 0.2226, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3467, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2591, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2375, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2237, loss: 2.1502, grad_norm: 21.5358
2025-06-17 22:33:55,896 - mmdet - INFO - Epoch [2][6200/7033]	lr: 1.866e-04, eta: 15:21:58, time: 1.959, data_time: 0.070, memory: 17921, loss_cls: 0.0743, loss_bbox: 0.2238, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3536, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2654, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2447, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2263, loss: 2.1668, grad_norm: 32.3077
2025-06-17 22:35:29,595 - mmdet - INFO - Epoch [2][6250/7033]	lr: 1.866e-04, eta: 15:20:19, time: 1.873, data_time: 0.067, memory: 17921, loss_cls: 0.0868, loss_bbox: 0.2249, d0.loss_cls: 0.1772, d0.loss_bbox: 0.3561, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2637, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2449, d3.loss_cls: 0.0887, d3.loss_bbox: 0.2443, d4.loss_cls: 0.0893, d4.loss_bbox: 0.2267, loss: 2.2179, grad_norm: 34.1720
2025-06-17 22:37:01,506 - mmdet - INFO - Epoch [2][6300/7033]	lr: 1.866e-04, eta: 15:18:36, time: 1.838, data_time: 0.063, memory: 17921, loss_cls: 0.0845, loss_bbox: 0.2168, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3588, d1.loss_cls: 0.1168, d1.loss_bbox: 0.2593, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0870, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2191, loss: 2.1793, grad_norm: 41.6918
2025-06-17 22:38:38,414 - mmdet - INFO - Epoch [2][6350/7033]	lr: 1.866e-04, eta: 15:17:03, time: 1.939, data_time: 0.060, memory: 17921, loss_cls: 0.0877, loss_bbox: 0.2240, d0.loss_cls: 0.1868, d0.loss_bbox: 0.3604, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2652, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0921, d3.loss_bbox: 0.2419, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2267, loss: 2.2474, grad_norm: 21.0430
2025-06-17 22:40:17,696 - mmdet - INFO - Epoch [2][6400/7033]	lr: 1.866e-04, eta: 15:15:36, time: 1.985, data_time: 0.079, memory: 17921, loss_cls: 0.0844, loss_bbox: 0.2232, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1167, d1.loss_bbox: 0.2550, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2413, d3.loss_cls: 0.0864, d3.loss_bbox: 0.2448, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2268, loss: 2.1879, grad_norm: 59.4113
2025-06-17 22:41:54,523 - mmdet - INFO - Epoch [2][6450/7033]	lr: 1.866e-04, eta: 15:14:03, time: 1.937, data_time: 0.065, memory: 17921, loss_cls: 0.0811, loss_bbox: 0.2226, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3517, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2563, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0870, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2229, loss: 2.1611, grad_norm: 31.4202
2025-06-17 22:43:32,066 - mmdet - INFO - Epoch [2][6500/7033]	lr: 1.866e-04, eta: 15:12:32, time: 1.949, data_time: 0.060, memory: 17921, loss_cls: 0.0810, loss_bbox: 0.2186, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3450, d1.loss_cls: 0.1190, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2331, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2184, loss: 2.1457, grad_norm: 64.9732
2025-06-17 22:45:15,124 - mmdet - INFO - Epoch [2][6550/7033]	lr: 1.866e-04, eta: 15:11:13, time: 2.063, data_time: 0.061, memory: 17921, loss_cls: 0.0774, loss_bbox: 0.2136, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3405, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0789, d4.loss_bbox: 0.2154, loss: 2.0929, grad_norm: 29.8854
2025-06-17 22:46:53,350 - mmdet - INFO - Epoch [2][6600/7033]	lr: 1.866e-04, eta: 15:09:43, time: 1.965, data_time: 0.067, memory: 17921, loss_cls: 0.0792, loss_bbox: 0.2186, d0.loss_cls: 0.1764, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2546, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2197, loss: 2.1352, grad_norm: 27.5185
2025-06-17 22:48:32,881 - mmdet - INFO - Epoch [2][6650/7033]	lr: 1.866e-04, eta: 15:08:15, time: 1.991, data_time: 0.057, memory: 17921, loss_cls: 0.0875, loss_bbox: 0.2221, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3519, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2583, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2390, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2246, loss: 2.1934, grad_norm: 65.1523
2025-06-17 22:50:12,220 - mmdet - INFO - Epoch [2][6700/7033]	lr: 1.866e-04, eta: 15:06:48, time: 1.985, data_time: 0.057, memory: 17921, loss_cls: 0.0762, loss_bbox: 0.2212, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2547, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0830, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2228, loss: 2.1393, grad_norm: 34.6705
2025-06-17 22:52:14,207 - mmdet - INFO - Epoch [2][6750/7033]	lr: 1.866e-04, eta: 15:06:07, time: 2.441, data_time: 0.098, memory: 17921, loss_cls: 0.0798, loss_bbox: 0.2279, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2583, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2435, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2292, loss: 2.1652, grad_norm: 24.6512
2025-06-17 22:53:50,380 - mmdet - INFO - Epoch [2][6800/7033]	lr: 1.866e-04, eta: 15:04:32, time: 1.923, data_time: 0.071, memory: 17921, loss_cls: 0.0784, loss_bbox: 0.2140, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3441, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2505, d2.loss_cls: 0.0936, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2329, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2162, loss: 2.1154, grad_norm: 19.6350
2025-06-17 22:55:31,854 - mmdet - INFO - Epoch [2][6850/7033]	lr: 1.866e-04, eta: 15:03:08, time: 2.030, data_time: 0.063, memory: 17921, loss_cls: 0.0866, loss_bbox: 0.2220, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3522, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2587, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2418, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2425, d4.loss_cls: 0.0889, d4.loss_bbox: 0.2244, loss: 2.2133, grad_norm: 21.7644
2025-06-17 22:57:10,765 - mmdet - INFO - Epoch [2][6900/7033]	lr: 1.866e-04, eta: 15:01:39, time: 1.978, data_time: 0.057, memory: 17921, loss_cls: 0.0851, loss_bbox: 0.2249, d0.loss_cls: 0.1883, d0.loss_bbox: 0.3508, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2630, d2.loss_cls: 0.1038, d2.loss_bbox: 0.2430, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2411, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2264, loss: 2.2264, grad_norm: 29.4684
2025-06-17 22:58:58,709 - mmdet - INFO - Epoch [2][6950/7033]	lr: 1.866e-04, eta: 15:00:28, time: 2.159, data_time: 0.061, memory: 17921, loss_cls: 0.0775, loss_bbox: 0.2192, d0.loss_cls: 0.1842, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2182, loss: 2.1327, grad_norm: 21.9559
2025-06-17 23:00:43,927 - mmdet - INFO - Epoch [2][7000/7033]	lr: 1.866e-04, eta: 14:59:11, time: 2.102, data_time: 0.220, memory: 17921, loss_cls: 0.0780, loss_bbox: 0.2211, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3424, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2543, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0855, d3.loss_bbox: 0.2384, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2220, loss: 2.1579, grad_norm: 27.7626
2025-06-17 23:02:44,745 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-17 23:40:53,800 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-17 23:40:53,801 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7934, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8835, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9096, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9221, pts_bbox_NuScenes/car_trans_err: 0.1792, pts_bbox_NuScenes/car_scale_err: 0.1493, pts_bbox_NuScenes/car_orient_err: 0.0485, pts_bbox_NuScenes/car_vel_err: 0.3059, pts_bbox_NuScenes/car_attr_err: 0.1766, pts_bbox_NuScenes/mATE: 0.2951, pts_bbox_NuScenes/mASE: 0.2612, pts_bbox_NuScenes/mAOE: 0.2524, pts_bbox_NuScenes/mAVE: 0.2982, pts_bbox_NuScenes/mAAE: 0.1899, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4279, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6202, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7256, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7628, pts_bbox_NuScenes/truck_trans_err: 0.3419, pts_bbox_NuScenes/truck_scale_err: 0.1982, pts_bbox_NuScenes/truck_orient_err: 0.0518, pts_bbox_NuScenes/truck_vel_err: 0.2681, pts_bbox_NuScenes/truck_attr_err: 0.2095, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0583, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1981, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4051, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4694, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6574, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4373, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7328, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1102, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.2998, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5005, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7342, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8917, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9211, pts_bbox_NuScenes/bus_trans_err: 0.3398, pts_bbox_NuScenes/bus_scale_err: 0.1859, pts_bbox_NuScenes/bus_orient_err: 0.0433, pts_bbox_NuScenes/bus_vel_err: 0.5462, pts_bbox_NuScenes/bus_attr_err: 0.2633, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1752, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4176, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5896, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6730, pts_bbox_NuScenes/trailer_trans_err: 0.4913, pts_bbox_NuScenes/trailer_scale_err: 0.2200, pts_bbox_NuScenes/trailer_orient_err: 0.4316, pts_bbox_NuScenes/trailer_vel_err: 0.2452, pts_bbox_NuScenes/trailer_attr_err: 0.1800, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5881, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6850, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7353, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7511, pts_bbox_NuScenes/barrier_trans_err: 0.2144, pts_bbox_NuScenes/barrier_scale_err: 0.2849, pts_bbox_NuScenes/barrier_orient_err: 0.0523, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6280, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7506, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7934, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8000, pts_bbox_NuScenes/motorcycle_trans_err: 0.2248, pts_bbox_NuScenes/motorcycle_scale_err: 0.2440, pts_bbox_NuScenes/motorcycle_orient_err: 0.2175, pts_bbox_NuScenes/motorcycle_vel_err: 0.4516, pts_bbox_NuScenes/motorcycle_attr_err: 0.2651, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5211, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5835, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5926, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5993, pts_bbox_NuScenes/bicycle_trans_err: 0.1865, pts_bbox_NuScenes/bicycle_scale_err: 0.2617, pts_bbox_NuScenes/bicycle_orient_err: 0.3334, pts_bbox_NuScenes/bicycle_vel_err: 0.2390, pts_bbox_NuScenes/bicycle_attr_err: 0.0079, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7969, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8530, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8763, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8884, pts_bbox_NuScenes/pedestrian_trans_err: 0.1619, pts_bbox_NuScenes/pedestrian_scale_err: 0.2908, pts_bbox_NuScenes/pedestrian_orient_err: 0.3607, pts_bbox_NuScenes/pedestrian_vel_err: 0.2194, pts_bbox_NuScenes/pedestrian_attr_err: 0.1170, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7045, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7526, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7824, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8052, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1537, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3401, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7024, pts_bbox_NuScenes/mAP: 0.6642
2025-06-17 23:42:57,193 - mmdet - INFO - Epoch [3][50/7033]	lr: 1.501e-04, eta: 14:55:13, time: 2.386, data_time: 0.338, memory: 17921, loss_cls: 0.0791, loss_bbox: 0.2179, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3404, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2534, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2322, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2191, loss: 2.1262, grad_norm: 39.9869
2025-06-17 23:44:38,093 - mmdet - INFO - Epoch [3][100/7033]	lr: 1.501e-04, eta: 14:53:48, time: 2.018, data_time: 0.086, memory: 17921, loss_cls: 0.0795, loss_bbox: 0.2215, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2587, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2386, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2228, loss: 2.1663, grad_norm: 27.8456
2025-06-17 23:46:08,680 - mmdet - INFO - Epoch [3][150/7033]	lr: 1.501e-04, eta: 14:52:02, time: 1.811, data_time: 0.068, memory: 17921, loss_cls: 0.0693, loss_bbox: 0.2129, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3330, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0747, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2151, loss: 2.0321, grad_norm: 25.7497
2025-06-17 23:47:43,862 - mmdet - INFO - Epoch [3][200/7033]	lr: 1.501e-04, eta: 14:50:26, time: 1.903, data_time: 0.068, memory: 17921, loss_cls: 0.0757, loss_bbox: 0.2210, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3511, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2571, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2401, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2408, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2239, loss: 2.1509, grad_norm: 23.5137
2025-06-17 23:49:18,853 - mmdet - INFO - Epoch [3][250/7033]	lr: 1.501e-04, eta: 14:48:49, time: 1.901, data_time: 0.065, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2093, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0779, d4.loss_bbox: 0.2112, loss: 2.0428, grad_norm: 2095.8241
2025-06-17 23:50:51,971 - mmdet - INFO - Epoch [3][300/7033]	lr: 1.501e-04, eta: 14:47:08, time: 1.862, data_time: 0.066, memory: 17921, loss_cls: 0.0774, loss_bbox: 0.2142, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2165, loss: 2.1156, grad_norm: 61.3017
2025-06-17 23:52:24,283 - mmdet - INFO - Epoch [3][350/7033]	lr: 1.501e-04, eta: 14:45:26, time: 1.846, data_time: 0.062, memory: 17921, loss_cls: 0.0767, loss_bbox: 0.2235, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3466, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2555, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2395, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2263, loss: 2.1510, grad_norm: 22.0758
2025-06-17 23:53:57,178 - mmdet - INFO - Epoch [3][400/7033]	lr: 1.501e-04, eta: 14:43:45, time: 1.858, data_time: 0.069, memory: 17921, loss_cls: 0.0776, loss_bbox: 0.2117, d0.loss_cls: 0.1826, d0.loss_bbox: 0.3321, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2135, loss: 2.0985, grad_norm: 47.6348
2025-06-17 23:55:30,875 - mmdet - INFO - Epoch [3][450/7033]	lr: 1.501e-04, eta: 14:42:06, time: 1.874, data_time: 0.073, memory: 17921, loss_cls: 0.0801, loss_bbox: 0.2137, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3309, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2267, d4.loss_cls: 0.0794, d4.loss_bbox: 0.2162, loss: 2.0873, grad_norm: 21.3496
2025-06-17 23:57:04,709 - mmdet - INFO - Epoch [3][500/7033]	lr: 1.501e-04, eta: 14:40:27, time: 1.876, data_time: 0.061, memory: 17921, loss_cls: 0.0784, loss_bbox: 0.2114, d0.loss_cls: 0.1870, d0.loss_bbox: 0.3320, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2263, d3.loss_cls: 0.0836, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2129, loss: 2.0930, grad_norm: 30.4570
2025-06-17 23:58:36,674 - mmdet - INFO - Epoch [3][550/7033]	lr: 1.501e-04, eta: 14:38:45, time: 1.839, data_time: 0.060, memory: 17921, loss_cls: 0.0699, loss_bbox: 0.2113, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2162, loss: 2.0547, grad_norm: 52.1983
2025-06-18 00:00:08,695 - mmdet - INFO - Epoch [3][600/7033]	lr: 1.501e-04, eta: 14:37:02, time: 1.840, data_time: 0.060, memory: 17921, loss_cls: 0.0804, loss_bbox: 0.2215, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3378, d1.loss_cls: 0.1135, d1.loss_bbox: 0.2555, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2365, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2370, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2248, loss: 2.1450, grad_norm: 117.9895
2025-06-18 00:01:42,966 - mmdet - INFO - Epoch [3][650/7033]	lr: 1.501e-04, eta: 14:35:25, time: 1.886, data_time: 0.063, memory: 17921, loss_cls: 0.0746, loss_bbox: 0.2077, d0.loss_cls: 0.1717, d0.loss_bbox: 0.3309, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2240, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2103, loss: 2.0443, grad_norm: 37.5697
2025-06-18 00:03:21,928 - mmdet - INFO - Epoch [3][700/7033]	lr: 1.501e-04, eta: 14:33:55, time: 1.979, data_time: 0.066, memory: 17921, loss_cls: 0.0829, loss_bbox: 0.2196, d0.loss_cls: 0.1842, d0.loss_bbox: 0.3441, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2342, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2213, loss: 2.1636, grad_norm: 24.9436
2025-06-18 00:04:58,590 - mmdet - INFO - Epoch [3][750/7033]	lr: 1.501e-04, eta: 14:32:22, time: 1.932, data_time: 0.063, memory: 17921, loss_cls: 0.0783, loss_bbox: 0.2167, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1096, d1.loss_bbox: 0.2544, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2207, loss: 2.1153, grad_norm: 97.2873
2025-06-18 00:06:33,080 - mmdet - INFO - Epoch [3][800/7033]	lr: 1.501e-04, eta: 14:30:44, time: 1.891, data_time: 0.059, memory: 17921, loss_cls: 0.0827, loss_bbox: 0.2147, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3370, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2495, d2.loss_cls: 0.0996, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2307, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2169, loss: 2.1214, grad_norm: 51.5061
2025-06-18 00:08:11,957 - mmdet - INFO - Epoch [3][850/7033]	lr: 1.501e-04, eta: 14:29:15, time: 1.977, data_time: 0.057, memory: 17921, loss_cls: 0.0726, loss_bbox: 0.2141, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3316, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2169, loss: 2.0490, grad_norm: 25.2874
2025-06-18 00:09:49,635 - mmdet - INFO - Epoch [3][900/7033]	lr: 1.501e-04, eta: 14:27:43, time: 1.954, data_time: 0.061, memory: 17921, loss_cls: 0.0774, loss_bbox: 0.2199, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2568, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2368, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2206, loss: 2.1503, grad_norm: 82.8800
2025-06-18 00:11:23,516 - mmdet - INFO - Epoch [3][950/7033]	lr: 1.501e-04, eta: 14:26:04, time: 1.877, data_time: 0.071, memory: 17921, loss_cls: 0.0736, loss_bbox: 0.2089, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3382, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2108, loss: 2.0685, grad_norm: 19.4387
2025-06-18 00:12:56,477 - mmdet - INFO - Epoch [3][1000/7033]	lr: 1.501e-04, eta: 14:24:24, time: 1.859, data_time: 0.058, memory: 17921, loss_cls: 0.0741, loss_bbox: 0.2058, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3316, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2221, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2082, loss: 2.0375, grad_norm: 50.8731
2025-06-18 00:14:29,563 - mmdet - INFO - Epoch [3][1050/7033]	lr: 1.501e-04, eta: 14:22:44, time: 1.862, data_time: 0.065, memory: 17921, loss_cls: 0.0683, loss_bbox: 0.2130, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0759, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2135, loss: 2.0430, grad_norm: 311.5648
2025-06-18 00:16:02,682 - mmdet - INFO - Epoch [3][1100/7033]	lr: 1.501e-04, eta: 14:21:04, time: 1.862, data_time: 0.058, memory: 17921, loss_cls: 0.0782, loss_bbox: 0.2107, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3362, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2466, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2138, loss: 2.0746, grad_norm: 85.3891
2025-06-18 00:17:35,995 - mmdet - INFO - Epoch [3][1150/7033]	lr: 1.501e-04, eta: 14:19:24, time: 1.866, data_time: 0.057, memory: 17921, loss_cls: 0.0818, loss_bbox: 0.2245, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3542, d1.loss_cls: 0.1177, d1.loss_bbox: 0.2585, d2.loss_cls: 0.1006, d2.loss_bbox: 0.2409, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2270, loss: 2.1975, grad_norm: 82.1260
2025-06-18 00:19:13,506 - mmdet - INFO - Epoch [3][1200/7033]	lr: 1.501e-04, eta: 14:17:52, time: 1.950, data_time: 0.063, memory: 17921, loss_cls: 0.0725, loss_bbox: 0.2169, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3437, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2515, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0751, d4.loss_bbox: 0.2181, loss: 2.0933, grad_norm: 166.1902
2025-06-18 00:20:48,817 - mmdet - INFO - Epoch [3][1250/7033]	lr: 1.501e-04, eta: 14:16:16, time: 1.905, data_time: 0.068, memory: 17921, loss_cls: 0.0811, loss_bbox: 0.2254, d0.loss_cls: 0.1834, d0.loss_bbox: 0.3531, d1.loss_cls: 0.1173, d1.loss_bbox: 0.2574, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2441, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2454, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2276, loss: 2.2024, grad_norm: 48.3970
2025-06-18 00:22:23,206 - mmdet - INFO - Epoch [3][1300/7033]	lr: 1.501e-04, eta: 14:14:39, time: 1.889, data_time: 0.068, memory: 17921, loss_cls: 0.0778, loss_bbox: 0.2149, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1105, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2164, loss: 2.1078, grad_norm: 32.8322
2025-06-18 00:23:58,774 - mmdet - INFO - Epoch [3][1350/7033]	lr: 1.501e-04, eta: 14:13:03, time: 1.911, data_time: 0.057, memory: 17921, loss_cls: 0.0793, loss_bbox: 0.2152, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3319, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0847, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2163, loss: 2.1051, grad_norm: 28.0521
2025-06-18 00:25:33,673 - mmdet - INFO - Epoch [3][1400/7033]	lr: 1.501e-04, eta: 14:11:26, time: 1.898, data_time: 0.062, memory: 17921, loss_cls: 0.0778, loss_bbox: 0.2130, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0796, d4.loss_bbox: 0.2142, loss: 2.0914, grad_norm: 28.8217
2025-06-18 00:27:12,208 - mmdet - INFO - Epoch [3][1450/7033]	lr: 1.501e-04, eta: 14:09:56, time: 1.971, data_time: 0.067, memory: 17921, loss_cls: 0.0850, loss_bbox: 0.2172, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2527, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2334, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2202, loss: 2.1683, grad_norm: 37.8674
2025-06-18 00:28:46,140 - mmdet - INFO - Epoch [3][1500/7033]	lr: 1.501e-04, eta: 14:08:18, time: 1.879, data_time: 0.074, memory: 17921, loss_cls: 0.0803, loss_bbox: 0.2188, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3373, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0988, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2201, loss: 2.1351, grad_norm: 51.1970
2025-06-18 00:30:25,434 - mmdet - INFO - Epoch [3][1550/7033]	lr: 1.501e-04, eta: 14:06:48, time: 1.986, data_time: 0.062, memory: 17921, loss_cls: 0.0757, loss_bbox: 0.2111, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3400, d1.loss_cls: 0.1134, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2281, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2133, loss: 2.0858, grad_norm: 48.9628
2025-06-18 00:31:58,137 - mmdet - INFO - Epoch [3][1600/7033]	lr: 1.501e-04, eta: 14:05:08, time: 1.854, data_time: 0.055, memory: 17921, loss_cls: 0.0783, loss_bbox: 0.2188, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3328, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2342, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2209, loss: 2.1166, grad_norm: 28.1881
2025-06-18 00:33:35,920 - mmdet - INFO - Epoch [3][1650/7033]	lr: 1.501e-04, eta: 14:03:36, time: 1.952, data_time: 0.068, memory: 17921, loss_cls: 0.0782, loss_bbox: 0.2185, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3439, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2569, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2211, loss: 2.1354, grad_norm: 25.8875
2025-06-18 00:35:13,856 - mmdet - INFO - Epoch [3][1700/7033]	lr: 1.501e-04, eta: 14:02:04, time: 1.962, data_time: 0.070, memory: 17921, loss_cls: 0.0685, loss_bbox: 0.2044, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2402, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2069, loss: 1.9989, grad_norm: 42.7379
2025-06-18 00:36:49,329 - mmdet - INFO - Epoch [3][1750/7033]	lr: 1.501e-04, eta: 14:00:29, time: 1.909, data_time: 0.077, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2125, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3371, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2295, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2157, loss: 2.0672, grad_norm: 23.8769
2025-06-18 00:38:49,766 - mmdet - INFO - Epoch [3][1800/7033]	lr: 1.501e-04, eta: 13:59:34, time: 2.408, data_time: 0.074, memory: 17921, loss_cls: 0.0767, loss_bbox: 0.2146, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1107, d1.loss_bbox: 0.2491, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2307, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2182, loss: 2.1015, grad_norm: 24.9360
2025-06-18 00:40:23,712 - mmdet - INFO - Epoch [3][1850/7033]	lr: 1.501e-04, eta: 13:57:56, time: 1.879, data_time: 0.056, memory: 17921, loss_cls: 0.0762, loss_bbox: 0.2122, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3341, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2139, loss: 2.0714, grad_norm: 31.9763
2025-06-18 00:41:55,523 - mmdet - INFO - Epoch [3][1900/7033]	lr: 1.501e-04, eta: 13:56:14, time: 1.835, data_time: 0.056, memory: 17921, loss_cls: 0.0765, loss_bbox: 0.2197, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3465, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2363, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2217, loss: 2.1279, grad_norm: 101.5755
2025-06-18 00:43:30,308 - mmdet - INFO - Epoch [3][1950/7033]	lr: 1.501e-04, eta: 13:54:37, time: 1.897, data_time: 0.058, memory: 17921, loss_cls: 0.0727, loss_bbox: 0.2188, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2497, d2.loss_cls: 0.0892, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2218, loss: 2.0959, grad_norm: 47.3456
2025-06-18 00:45:04,856 - mmdet - INFO - Epoch [3][2000/7033]	lr: 1.501e-04, eta: 13:53:00, time: 1.891, data_time: 0.061, memory: 17921, loss_cls: 0.0758, loss_bbox: 0.2113, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2119, loss: 2.0481, grad_norm: 33.7728
2025-06-18 00:46:45,238 - mmdet - INFO - Epoch [3][2050/7033]	lr: 1.501e-04, eta: 13:51:32, time: 2.008, data_time: 0.075, memory: 17921, loss_cls: 0.0683, loss_bbox: 0.2082, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1063, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2092, loss: 2.0291, grad_norm: 39.5369
2025-06-18 00:48:22,173 - mmdet - INFO - Epoch [3][2100/7033]	lr: 1.501e-04, eta: 13:49:58, time: 1.939, data_time: 0.069, memory: 17921, loss_cls: 0.0794, loss_bbox: 0.2206, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3530, d1.loss_cls: 0.1124, d1.loss_bbox: 0.2624, d2.loss_cls: 0.0946, d2.loss_bbox: 0.2399, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2395, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2231, loss: 2.1695, grad_norm: 25.9108
2025-06-18 00:49:56,637 - mmdet - INFO - Epoch [3][2150/7033]	lr: 1.501e-04, eta: 13:48:21, time: 1.889, data_time: 0.061, memory: 17921, loss_cls: 0.0728, loss_bbox: 0.2068, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0793, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2073, loss: 2.0373, grad_norm: 45.8683
2025-06-18 00:51:30,767 - mmdet - INFO - Epoch [3][2200/7033]	lr: 1.501e-04, eta: 13:46:42, time: 1.882, data_time: 0.063, memory: 17921, loss_cls: 0.0812, loss_bbox: 0.2119, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1154, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0976, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2266, d4.loss_cls: 0.0843, d4.loss_bbox: 0.2133, loss: 2.1139, grad_norm: 74.8949
2025-06-18 00:53:06,246 - mmdet - INFO - Epoch [3][2250/7033]	lr: 1.501e-04, eta: 13:45:07, time: 1.910, data_time: 0.060, memory: 17921, loss_cls: 0.0809, loss_bbox: 0.2188, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2559, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2366, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2214, loss: 2.1564, grad_norm: 51.9202
2025-06-18 00:54:40,355 - mmdet - INFO - Epoch [3][2300/7033]	lr: 1.501e-04, eta: 13:43:28, time: 1.881, data_time: 0.059, memory: 17921, loss_cls: 0.0850, loss_bbox: 0.2221, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3402, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2532, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2236, loss: 2.1645, grad_norm: 26.3359
2025-06-18 00:56:15,912 - mmdet - INFO - Epoch [3][2350/7033]	lr: 1.501e-04, eta: 13:41:53, time: 1.913, data_time: 0.056, memory: 17921, loss_cls: 0.0834, loss_bbox: 0.2253, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3481, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2621, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2411, d4.loss_cls: 0.0863, d4.loss_bbox: 0.2269, loss: 2.2052, grad_norm: 27.2511
2025-06-18 00:57:49,591 - mmdet - INFO - Epoch [3][2400/7033]	lr: 1.501e-04, eta: 13:40:14, time: 1.873, data_time: 0.060, memory: 17921, loss_cls: 0.0794, loss_bbox: 0.2196, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1143, d1.loss_bbox: 0.2573, d2.loss_cls: 0.0973, d2.loss_bbox: 0.2359, d3.loss_cls: 0.0851, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2210, loss: 2.1423, grad_norm: 88.7826
2025-06-18 00:59:20,081 - mmdet - INFO - Epoch [3][2450/7033]	lr: 1.501e-04, eta: 13:38:30, time: 1.810, data_time: 0.064, memory: 17921, loss_cls: 0.0718, loss_bbox: 0.2128, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2469, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2247, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2138, loss: 2.0408, grad_norm: 109.1295
2025-06-18 01:00:53,030 - mmdet - INFO - Epoch [3][2500/7033]	lr: 1.501e-04, eta: 13:36:51, time: 1.859, data_time: 0.064, memory: 17921, loss_cls: 0.0771, loss_bbox: 0.2141, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2162, loss: 2.1146, grad_norm: 20.6722
2025-06-18 01:02:25,759 - mmdet - INFO - Epoch [3][2550/7033]	lr: 1.501e-04, eta: 13:35:11, time: 1.855, data_time: 0.056, memory: 17921, loss_cls: 0.0750, loss_bbox: 0.2114, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2291, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2160, loss: 2.0830, grad_norm: 37.4432
2025-06-18 01:03:58,902 - mmdet - INFO - Epoch [3][2600/7033]	lr: 1.501e-04, eta: 13:33:31, time: 1.863, data_time: 0.065, memory: 17921, loss_cls: 0.0786, loss_bbox: 0.2195, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3364, d1.loss_cls: 0.1130, d1.loss_bbox: 0.2511, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2355, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2217, loss: 2.1258, grad_norm: 26.1997
2025-06-18 01:05:35,398 - mmdet - INFO - Epoch [3][2650/7033]	lr: 1.501e-04, eta: 13:31:57, time: 1.930, data_time: 0.064, memory: 17921, loss_cls: 0.0716, loss_bbox: 0.2099, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3276, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2120, loss: 2.0321, grad_norm: 32.8801
2025-06-18 01:07:08,336 - mmdet - INFO - Epoch [3][2700/7033]	lr: 1.501e-04, eta: 13:30:17, time: 1.858, data_time: 0.053, memory: 17921, loss_cls: 0.0807, loss_bbox: 0.2167, d0.loss_cls: 0.1791, d0.loss_bbox: 0.3364, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2518, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2352, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2339, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2198, loss: 2.1331, grad_norm: 32.1883
2025-06-18 01:08:41,318 - mmdet - INFO - Epoch [3][2750/7033]	lr: 1.501e-04, eta: 13:28:38, time: 1.860, data_time: 0.063, memory: 17921, loss_cls: 0.0781, loss_bbox: 0.2166, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1095, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0948, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0831, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2171, loss: 2.0926, grad_norm: 32.8277
2025-06-18 01:10:21,248 - mmdet - INFO - Epoch [3][2800/7033]	lr: 1.501e-04, eta: 13:27:09, time: 1.998, data_time: 0.055, memory: 17921, loss_cls: 0.0827, loss_bbox: 0.2186, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3442, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2553, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2198, loss: 2.1637, grad_norm: 41.4451
2025-06-18 01:11:56,869 - mmdet - INFO - Epoch [3][2850/7033]	lr: 1.501e-04, eta: 13:25:33, time: 1.912, data_time: 0.056, memory: 17921, loss_cls: 0.0797, loss_bbox: 0.2181, d0.loss_cls: 0.1857, d0.loss_bbox: 0.3394, d1.loss_cls: 0.1196, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2334, d4.loss_cls: 0.0828, d4.loss_bbox: 0.2190, loss: 2.1443, grad_norm: 59.7666
2025-06-18 01:13:30,469 - mmdet - INFO - Epoch [3][2900/7033]	lr: 1.501e-04, eta: 13:23:55, time: 1.872, data_time: 0.059, memory: 17921, loss_cls: 0.0740, loss_bbox: 0.2113, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2405, d2.loss_cls: 0.0937, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2112, loss: 2.0523, grad_norm: 26.6590
2025-06-18 01:15:05,941 - mmdet - INFO - Epoch [3][2950/7033]	lr: 1.501e-04, eta: 13:22:19, time: 1.909, data_time: 0.059, memory: 17921, loss_cls: 0.0690, loss_bbox: 0.2105, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3279, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2242, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0712, d4.loss_bbox: 0.2126, loss: 2.0140, grad_norm: 65.2376
2025-06-18 01:16:42,337 - mmdet - INFO - Epoch [3][3000/7033]	lr: 1.501e-04, eta: 13:20:45, time: 1.928, data_time: 0.067, memory: 17921, loss_cls: 0.0724, loss_bbox: 0.2089, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3354, d1.loss_cls: 0.1123, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2109, loss: 2.0602, grad_norm: 73.7964
2025-06-18 01:18:19,052 - mmdet - INFO - Epoch [3][3050/7033]	lr: 1.501e-04, eta: 13:19:11, time: 1.933, data_time: 0.060, memory: 17921, loss_cls: 0.0767, loss_bbox: 0.2193, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2530, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0801, d4.loss_bbox: 0.2215, loss: 2.1367, grad_norm: 24.3204
2025-06-18 01:19:57,539 - mmdet - INFO - Epoch [3][3100/7033]	lr: 1.501e-04, eta: 13:17:39, time: 1.972, data_time: 0.071, memory: 17921, loss_cls: 0.0803, loss_bbox: 0.2149, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2152, loss: 2.1192, grad_norm: 24.6014
2025-06-18 01:21:30,329 - mmdet - INFO - Epoch [3][3150/7033]	lr: 1.501e-04, eta: 13:16:00, time: 1.854, data_time: 0.053, memory: 17921, loss_cls: 0.0736, loss_bbox: 0.2066, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3305, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0920, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2088, loss: 2.0402, grad_norm: 35.6071
2025-06-18 01:23:03,730 - mmdet - INFO - Epoch [3][3200/7033]	lr: 1.501e-04, eta: 13:14:21, time: 1.869, data_time: 0.065, memory: 17921, loss_cls: 0.0841, loss_bbox: 0.2092, d0.loss_cls: 0.1809, d0.loss_bbox: 0.3397, d1.loss_cls: 0.1221, d1.loss_bbox: 0.2459, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0857, d4.loss_bbox: 0.2116, loss: 2.1264, grad_norm: 23.5693
2025-06-18 01:24:40,022 - mmdet - INFO - Epoch [3][3250/7033]	lr: 1.501e-04, eta: 13:12:46, time: 1.927, data_time: 0.075, memory: 17921, loss_cls: 0.0750, loss_bbox: 0.2137, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2291, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2155, loss: 2.0850, grad_norm: 91.4708
2025-06-18 01:26:14,152 - mmdet - INFO - Epoch [3][3300/7033]	lr: 1.501e-04, eta: 13:11:09, time: 1.882, data_time: 0.060, memory: 17921, loss_cls: 0.0769, loss_bbox: 0.2132, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1141, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0952, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2307, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2167, loss: 2.1111, grad_norm: 21.5599
2025-06-18 01:27:49,345 - mmdet - INFO - Epoch [3][3350/7033]	lr: 1.501e-04, eta: 13:09:33, time: 1.905, data_time: 0.071, memory: 17921, loss_cls: 0.0712, loss_bbox: 0.2161, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3434, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2517, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0752, d4.loss_bbox: 0.2162, loss: 2.0930, grad_norm: 36.3065
2025-06-18 01:29:24,873 - mmdet - INFO - Epoch [3][3400/7033]	lr: 1.501e-04, eta: 13:07:57, time: 1.911, data_time: 0.073, memory: 17921, loss_cls: 0.0712, loss_bbox: 0.2080, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3374, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2103, loss: 2.0425, grad_norm: 26.8161
2025-06-18 01:30:57,001 - mmdet - INFO - Epoch [3][3450/7033]	lr: 1.501e-04, eta: 13:06:16, time: 1.842, data_time: 0.060, memory: 17921, loss_cls: 0.0785, loss_bbox: 0.2166, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3390, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2559, d2.loss_cls: 0.0935, d2.loss_bbox: 0.2381, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2351, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2178, loss: 2.1286, grad_norm: 27.0804
2025-06-18 01:32:29,044 - mmdet - INFO - Epoch [3][3500/7033]	lr: 1.501e-04, eta: 13:04:36, time: 1.841, data_time: 0.064, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2109, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3288, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0926, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0808, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0760, d4.loss_bbox: 0.2141, loss: 2.0586, grad_norm: 30.4374
2025-06-18 01:34:01,966 - mmdet - INFO - Epoch [3][3550/7033]	lr: 1.501e-04, eta: 13:02:57, time: 1.858, data_time: 0.061, memory: 17921, loss_cls: 0.0698, loss_bbox: 0.2076, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1057, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0877, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2084, loss: 2.0148, grad_norm: 22.6672
2025-06-18 01:35:32,593 - mmdet - INFO - Epoch [3][3600/7033]	lr: 1.501e-04, eta: 13:01:14, time: 1.813, data_time: 0.055, memory: 17921, loss_cls: 0.0756, loss_bbox: 0.2076, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3319, d1.loss_cls: 0.1112, d1.loss_bbox: 0.2432, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2241, d4.loss_cls: 0.0781, d4.loss_bbox: 0.2100, loss: 2.0547, grad_norm: 33.2865
2025-06-18 01:37:06,050 - mmdet - INFO - Epoch [3][3650/7033]	lr: 1.501e-04, eta: 12:59:36, time: 1.869, data_time: 0.059, memory: 17921, loss_cls: 0.0775, loss_bbox: 0.2144, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2520, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2171, loss: 2.0962, grad_norm: 21.7370
2025-06-18 01:38:42,616 - mmdet - INFO - Epoch [3][3700/7033]	lr: 1.501e-04, eta: 12:58:02, time: 1.932, data_time: 0.055, memory: 17921, loss_cls: 0.0910, loss_bbox: 0.2175, d0.loss_cls: 0.1915, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1205, d1.loss_bbox: 0.2529, d2.loss_cls: 0.1041, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0939, d3.loss_bbox: 0.2382, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2193, loss: 2.1900, grad_norm: 65.0888
2025-06-18 01:40:18,688 - mmdet - INFO - Epoch [3][3750/7033]	lr: 1.501e-04, eta: 12:56:27, time: 1.921, data_time: 0.071, memory: 17921, loss_cls: 0.0828, loss_bbox: 0.2183, d0.loss_cls: 0.1780, d0.loss_bbox: 0.3453, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2550, d2.loss_cls: 0.1005, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0857, d4.loss_bbox: 0.2224, loss: 2.1718, grad_norm: 25.1749
2025-06-18 01:41:55,807 - mmdet - INFO - Epoch [3][3800/7033]	lr: 1.501e-04, eta: 12:54:54, time: 1.943, data_time: 0.072, memory: 17921, loss_cls: 0.0747, loss_bbox: 0.2127, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2516, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2144, loss: 2.0898, grad_norm: 22.1315
2025-06-18 01:43:31,645 - mmdet - INFO - Epoch [3][3850/7033]	lr: 1.501e-04, eta: 12:53:18, time: 1.917, data_time: 0.057, memory: 17921, loss_cls: 0.0865, loss_bbox: 0.2179, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3433, d1.loss_cls: 0.1198, d1.loss_bbox: 0.2564, d2.loss_cls: 0.1036, d2.loss_bbox: 0.2362, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0907, d4.loss_bbox: 0.2197, loss: 2.1801, grad_norm: 24.7867
2025-06-18 01:45:10,265 - mmdet - INFO - Epoch [3][3900/7033]	lr: 1.501e-04, eta: 12:51:47, time: 1.972, data_time: 0.061, memory: 17921, loss_cls: 0.0778, loss_bbox: 0.2138, d0.loss_cls: 0.1858, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0810, d3.loss_bbox: 0.2328, d4.loss_cls: 0.0793, d4.loss_bbox: 0.2150, loss: 2.1020, grad_norm: 28.8722
2025-06-18 01:46:44,060 - mmdet - INFO - Epoch [3][3950/7033]	lr: 1.501e-04, eta: 12:50:09, time: 1.876, data_time: 0.060, memory: 17921, loss_cls: 0.0719, loss_bbox: 0.2121, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0734, d4.loss_bbox: 0.2135, loss: 2.0506, grad_norm: 38.0851
2025-06-18 01:48:16,047 - mmdet - INFO - Epoch [3][4000/7033]	lr: 1.501e-04, eta: 12:48:29, time: 1.840, data_time: 0.054, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2055, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3315, d1.loss_cls: 0.1131, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0802, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2105, loss: 2.0423, grad_norm: 50.7886
2025-06-18 01:49:47,807 - mmdet - INFO - Epoch [3][4050/7033]	lr: 1.501e-04, eta: 12:46:48, time: 1.834, data_time: 0.059, memory: 17921, loss_cls: 0.0676, loss_bbox: 0.2058, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3137, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0701, d3.loss_bbox: 0.2176, d4.loss_cls: 0.0684, d4.loss_bbox: 0.2076, loss: 1.9537, grad_norm: 17.5296
2025-06-18 01:51:27,855 - mmdet - INFO - Epoch [3][4100/7033]	lr: 1.501e-04, eta: 12:45:19, time: 2.002, data_time: 0.063, memory: 17921, loss_cls: 0.0754, loss_bbox: 0.2169, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2535, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0778, d4.loss_bbox: 0.2179, loss: 2.0862, grad_norm: 17.2265
2025-06-18 01:53:04,423 - mmdet - INFO - Epoch [3][4150/7033]	lr: 1.501e-04, eta: 12:43:45, time: 1.931, data_time: 0.058, memory: 17921, loss_cls: 0.0696, loss_bbox: 0.2085, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3256, d1.loss_cls: 0.0996, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0730, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2099, loss: 2.0038, grad_norm: 29.9431
2025-06-18 01:54:40,896 - mmdet - INFO - Epoch [3][4200/7033]	lr: 1.501e-04, eta: 12:42:10, time: 1.930, data_time: 0.063, memory: 17921, loss_cls: 0.0745, loss_bbox: 0.2115, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1080, d1.loss_bbox: 0.2428, d2.loss_cls: 0.0891, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2103, loss: 2.0475, grad_norm: 21.1041
2025-06-18 01:56:17,195 - mmdet - INFO - Epoch [3][4250/7033]	lr: 1.501e-04, eta: 12:40:36, time: 1.925, data_time: 0.060, memory: 17921, loss_cls: 0.0856, loss_bbox: 0.2209, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3472, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2582, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2378, d3.loss_cls: 0.0916, d3.loss_bbox: 0.2355, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2221, loss: 2.1842, grad_norm: 42.5078
2025-06-18 01:57:49,324 - mmdet - INFO - Epoch [3][4300/7033]	lr: 1.501e-04, eta: 12:38:56, time: 1.843, data_time: 0.074, memory: 17921, loss_cls: 0.0766, loss_bbox: 0.2225, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2631, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2386, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2243, loss: 2.1650, grad_norm: 33.9041
2025-06-18 01:59:25,518 - mmdet - INFO - Epoch [3][4350/7033]	lr: 1.501e-04, eta: 12:37:21, time: 1.923, data_time: 0.065, memory: 17921, loss_cls: 0.0832, loss_bbox: 0.2174, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1179, d1.loss_bbox: 0.2560, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2366, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2194, loss: 2.1708, grad_norm: 38.2300
2025-06-18 02:01:02,591 - mmdet - INFO - Epoch [3][4400/7033]	lr: 1.501e-04, eta: 12:35:47, time: 1.942, data_time: 0.053, memory: 17921, loss_cls: 0.0767, loss_bbox: 0.2171, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2541, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2348, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2183, loss: 2.1273, grad_norm: 55.0213
2025-06-18 02:02:36,561 - mmdet - INFO - Epoch [3][4450/7033]	lr: 1.501e-04, eta: 12:34:10, time: 1.880, data_time: 0.055, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2085, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3273, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2262, d3.loss_cls: 0.0766, d3.loss_bbox: 0.2259, d4.loss_cls: 0.0744, d4.loss_bbox: 0.2111, loss: 2.0248, grad_norm: 38.5914
2025-06-18 02:04:07,670 - mmdet - INFO - Epoch [3][4500/7033]	lr: 1.501e-04, eta: 12:32:29, time: 1.822, data_time: 0.059, memory: 17921, loss_cls: 0.0746, loss_bbox: 0.2112, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2506, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0779, d4.loss_bbox: 0.2130, loss: 2.0744, grad_norm: 29.9736
2025-06-18 02:05:42,432 - mmdet - INFO - Epoch [3][4550/7033]	lr: 1.501e-04, eta: 12:30:52, time: 1.895, data_time: 0.050, memory: 17921, loss_cls: 0.0786, loss_bbox: 0.2148, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3346, d1.loss_cls: 0.1136, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0950, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2180, loss: 2.1095, grad_norm: 54.6292
2025-06-18 02:07:19,028 - mmdet - INFO - Epoch [3][4600/7033]	lr: 1.501e-04, eta: 12:29:18, time: 1.931, data_time: 0.072, memory: 17921, loss_cls: 0.0690, loss_bbox: 0.2154, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3389, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2175, loss: 2.0556, grad_norm: 29.5968
2025-06-18 02:08:54,464 - mmdet - INFO - Epoch [3][4650/7033]	lr: 1.501e-04, eta: 12:27:42, time: 1.910, data_time: 0.072, memory: 17921, loss_cls: 0.0805, loss_bbox: 0.2124, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3345, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2138, loss: 2.1044, grad_norm: 187.1483
2025-06-18 02:10:30,322 - mmdet - INFO - Epoch [3][4700/7033]	lr: 1.501e-04, eta: 12:26:07, time: 1.917, data_time: 0.060, memory: 17921, loss_cls: 0.0797, loss_bbox: 0.2186, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3372, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2566, d2.loss_cls: 0.0940, d2.loss_bbox: 0.2362, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2345, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2219, loss: 2.1259, grad_norm: 47.4011
2025-06-18 02:12:02,986 - mmdet - INFO - Epoch [3][4750/7033]	lr: 1.501e-04, eta: 12:24:28, time: 1.854, data_time: 0.056, memory: 17921, loss_cls: 0.0727, loss_bbox: 0.2073, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3302, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2223, d3.loss_cls: 0.0783, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2105, loss: 2.0371, grad_norm: 41.1636
2025-06-18 02:13:33,704 - mmdet - INFO - Epoch [3][4800/7033]	lr: 1.501e-04, eta: 12:22:47, time: 1.814, data_time: 0.060, memory: 17921, loss_cls: 0.0791, loss_bbox: 0.2168, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3344, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2179, loss: 2.1020, grad_norm: 24.6894
2025-06-18 02:15:02,396 - mmdet - INFO - Epoch [3][4850/7033]	lr: 1.501e-04, eta: 12:21:03, time: 1.774, data_time: 0.064, memory: 17921, loss_cls: 0.0673, loss_bbox: 0.2020, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3185, d1.loss_cls: 0.1050, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2035, loss: 1.9668, grad_norm: 33.7668
2025-06-18 02:16:37,600 - mmdet - INFO - Epoch [3][4900/7033]	lr: 1.501e-04, eta: 12:19:27, time: 1.904, data_time: 0.068, memory: 17921, loss_cls: 0.0693, loss_bbox: 0.2017, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3277, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0848, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0692, d4.loss_bbox: 0.2059, loss: 1.9870, grad_norm: 23.9109
2025-06-18 02:18:11,138 - mmdet - INFO - Epoch [3][4950/7033]	lr: 1.501e-04, eta: 12:17:49, time: 1.871, data_time: 0.055, memory: 17921, loss_cls: 0.0644, loss_bbox: 0.2019, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3250, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2340, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0705, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0679, d4.loss_bbox: 0.2023, loss: 1.9547, grad_norm: 31.4211
2025-06-18 02:19:45,053 - mmdet - INFO - Epoch [3][5000/7033]	lr: 1.501e-04, eta: 12:16:12, time: 1.878, data_time: 0.053, memory: 17921, loss_cls: 0.0745, loss_bbox: 0.2106, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2281, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2278, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2134, loss: 2.0658, grad_norm: 22.3597
2025-06-18 02:21:20,909 - mmdet - INFO - Epoch [3][5050/7033]	lr: 1.501e-04, eta: 12:14:37, time: 1.917, data_time: 0.052, memory: 17921, loss_cls: 0.0823, loss_bbox: 0.2132, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2463, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2159, loss: 2.1163, grad_norm: 31.6083
2025-06-18 02:22:55,342 - mmdet - INFO - Epoch [3][5100/7033]	lr: 1.501e-04, eta: 12:13:00, time: 1.889, data_time: 0.065, memory: 17921, loss_cls: 0.0660, loss_bbox: 0.2007, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3273, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2343, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2172, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2021, loss: 1.9539, grad_norm: 46.0003
2025-06-18 02:24:24,073 - mmdet - INFO - Epoch [3][5150/7033]	lr: 1.501e-04, eta: 12:11:16, time: 1.775, data_time: 0.061, memory: 17921, loss_cls: 0.0702, loss_bbox: 0.2056, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3256, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2085, loss: 2.0016, grad_norm: 26.0319
2025-06-18 02:25:54,305 - mmdet - INFO - Epoch [3][5200/7033]	lr: 1.501e-04, eta: 12:09:35, time: 1.805, data_time: 0.072, memory: 17921, loss_cls: 0.0712, loss_bbox: 0.2083, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3300, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0886, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2260, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2111, loss: 2.0308, grad_norm: 27.2627
2025-06-18 02:27:26,326 - mmdet - INFO - Epoch [3][5250/7033]	lr: 1.501e-04, eta: 12:07:55, time: 1.839, data_time: 0.051, memory: 17921, loss_cls: 0.0754, loss_bbox: 0.2110, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0809, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2120, loss: 2.0490, grad_norm: 44.8884
2025-06-18 02:29:00,316 - mmdet - INFO - Epoch [3][5300/7033]	lr: 1.501e-04, eta: 12:06:18, time: 1.881, data_time: 0.071, memory: 17921, loss_cls: 0.0678, loss_bbox: 0.2192, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3329, d1.loss_cls: 0.1093, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0700, d4.loss_bbox: 0.2183, loss: 2.0685, grad_norm: 32.4458
2025-06-18 02:30:42,452 - mmdet - INFO - Epoch [3][5350/7033]	lr: 1.501e-04, eta: 12:04:51, time: 2.042, data_time: 0.063, memory: 17921, loss_cls: 0.0697, loss_bbox: 0.2042, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0873, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0716, d4.loss_bbox: 0.2024, loss: 1.9735, grad_norm: 50.8117
2025-06-18 02:32:15,508 - mmdet - INFO - Epoch [3][5400/7033]	lr: 1.501e-04, eta: 12:03:12, time: 1.862, data_time: 0.062, memory: 17921, loss_cls: 0.0766, loss_bbox: 0.2133, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3323, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2476, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0815, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2144, loss: 2.0812, grad_norm: 27.5496
2025-06-18 02:33:44,873 - mmdet - INFO - Epoch [3][5450/7033]	lr: 1.501e-04, eta: 12:01:30, time: 1.787, data_time: 0.075, memory: 17921, loss_cls: 0.0844, loss_bbox: 0.2102, d0.loss_cls: 0.1811, d0.loss_bbox: 0.3305, d1.loss_cls: 0.1156, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2269, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2128, loss: 2.1150, grad_norm: 19.4405
2025-06-18 02:35:16,286 - mmdet - INFO - Epoch [3][5500/7033]	lr: 1.501e-04, eta: 11:59:50, time: 1.828, data_time: 0.059, memory: 17921, loss_cls: 0.0691, loss_bbox: 0.2096, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2121, loss: 2.0273, grad_norm: 25.4301
2025-06-18 02:36:51,167 - mmdet - INFO - Epoch [3][5550/7033]	lr: 1.501e-04, eta: 11:58:14, time: 1.898, data_time: 0.057, memory: 17921, loss_cls: 0.0661, loss_bbox: 0.2048, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3186, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2370, d2.loss_cls: 0.0820, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0717, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2079, loss: 1.9605, grad_norm: 29.7021
2025-06-18 02:38:21,741 - mmdet - INFO - Epoch [3][5600/7033]	lr: 1.501e-04, eta: 11:56:33, time: 1.811, data_time: 0.047, memory: 17921, loss_cls: 0.0718, loss_bbox: 0.2038, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3331, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0900, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2192, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2059, loss: 2.0165, grad_norm: 19.2615
2025-06-18 02:39:51,646 - mmdet - INFO - Epoch [3][5650/7033]	lr: 1.501e-04, eta: 11:54:51, time: 1.798, data_time: 0.061, memory: 17921, loss_cls: 0.0680, loss_bbox: 0.2026, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3313, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2398, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2205, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2053, loss: 1.9799, grad_norm: 76.7539
2025-06-18 02:41:31,139 - mmdet - INFO - Epoch [3][5700/7033]	lr: 1.501e-04, eta: 11:53:20, time: 1.990, data_time: 0.065, memory: 17921, loss_cls: 0.0718, loss_bbox: 0.2087, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2257, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2267, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2089, loss: 2.0309, grad_norm: 27.3687
2025-06-18 02:43:07,785 - mmdet - INFO - Epoch [3][5750/7033]	lr: 1.501e-04, eta: 11:51:46, time: 1.933, data_time: 0.075, memory: 17921, loss_cls: 0.0764, loss_bbox: 0.2075, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3381, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2478, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2251, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2087, loss: 2.0569, grad_norm: 27.4265
2025-06-18 02:44:40,609 - mmdet - INFO - Epoch [3][5800/7033]	lr: 1.501e-04, eta: 11:50:08, time: 1.856, data_time: 0.061, memory: 17921, loss_cls: 0.0758, loss_bbox: 0.2100, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2489, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2273, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0772, d4.loss_bbox: 0.2133, loss: 2.0721, grad_norm: 20.3418
2025-06-18 02:46:15,177 - mmdet - INFO - Epoch [3][5850/7033]	lr: 1.501e-04, eta: 11:48:32, time: 1.889, data_time: 0.055, memory: 17921, loss_cls: 0.0739, loss_bbox: 0.2161, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3419, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2582, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2339, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2194, loss: 2.1041, grad_norm: 45.5028
2025-06-18 02:47:52,237 - mmdet - INFO - Epoch [3][5900/7033]	lr: 1.501e-04, eta: 11:46:58, time: 1.943, data_time: 0.062, memory: 17921, loss_cls: 0.0767, loss_bbox: 0.2160, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2543, d2.loss_cls: 0.0939, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2192, loss: 2.1130, grad_norm: 22.3911
2025-06-18 02:49:24,290 - mmdet - INFO - Epoch [3][5950/7033]	lr: 1.501e-04, eta: 11:45:19, time: 1.840, data_time: 0.062, memory: 17921, loss_cls: 0.0805, loss_bbox: 0.2091, d0.loss_cls: 0.1767, d0.loss_bbox: 0.3414, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2483, d2.loss_cls: 0.0929, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2281, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2135, loss: 2.0950, grad_norm: 94.1244
2025-06-18 02:50:57,410 - mmdet - INFO - Epoch [3][6000/7033]	lr: 1.501e-04, eta: 11:43:41, time: 1.863, data_time: 0.050, memory: 17921, loss_cls: 0.0795, loss_bbox: 0.2112, d0.loss_cls: 0.1772, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2502, d2.loss_cls: 0.0922, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2138, loss: 2.0921, grad_norm: 26.4677
2025-06-18 02:52:30,729 - mmdet - INFO - Epoch [3][6050/7033]	lr: 1.501e-04, eta: 11:42:03, time: 1.866, data_time: 0.054, memory: 17921, loss_cls: 0.0775, loss_bbox: 0.2028, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3232, d1.loss_cls: 0.1092, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0934, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0795, d4.loss_bbox: 0.2062, loss: 2.0203, grad_norm: 19.6088
2025-06-18 02:54:05,631 - mmdet - INFO - Epoch [3][6100/7033]	lr: 1.501e-04, eta: 11:40:28, time: 1.899, data_time: 0.077, memory: 17921, loss_cls: 0.0747, loss_bbox: 0.2057, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0947, d2.loss_bbox: 0.2220, d3.loss_cls: 0.0811, d3.loss_bbox: 0.2222, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2082, loss: 2.0347, grad_norm: 33.5235
2025-06-18 02:55:41,130 - mmdet - INFO - Epoch [3][6150/7033]	lr: 1.501e-04, eta: 11:38:52, time: 1.910, data_time: 0.074, memory: 17921, loss_cls: 0.0800, loss_bbox: 0.2195, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3405, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2570, d2.loss_cls: 0.0982, d2.loss_bbox: 0.2362, d3.loss_cls: 0.0867, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2224, loss: 2.1471, grad_norm: 24.2698
2025-06-18 02:57:17,696 - mmdet - INFO - Epoch [3][6200/7033]	lr: 1.501e-04, eta: 11:37:18, time: 1.930, data_time: 0.060, memory: 17921, loss_cls: 0.0786, loss_bbox: 0.2068, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3265, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2477, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0815, d4.loss_bbox: 0.2108, loss: 2.0749, grad_norm: 29.5007
2025-06-18 02:58:52,679 - mmdet - INFO - Epoch [3][6250/7033]	lr: 1.501e-04, eta: 11:35:42, time: 1.901, data_time: 0.057, memory: 17921, loss_cls: 0.0766, loss_bbox: 0.2190, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3363, d1.loss_cls: 0.1094, d1.loss_bbox: 0.2557, d2.loss_cls: 0.0925, d2.loss_bbox: 0.2357, d3.loss_cls: 0.0823, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2215, loss: 2.1164, grad_norm: 43.6696
2025-06-18 03:00:28,472 - mmdet - INFO - Epoch [3][6300/7033]	lr: 1.501e-04, eta: 11:34:07, time: 1.914, data_time: 0.078, memory: 17921, loss_cls: 0.0714, loss_bbox: 0.2109, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3268, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2514, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0763, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0723, d4.loss_bbox: 0.2138, loss: 2.0405, grad_norm: 33.3417
2025-06-18 03:02:00,731 - mmdet - INFO - Epoch [3][6350/7033]	lr: 1.501e-04, eta: 11:32:29, time: 1.847, data_time: 0.064, memory: 17921, loss_cls: 0.0729, loss_bbox: 0.2213, d0.loss_cls: 0.1772, d0.loss_bbox: 0.3398, d1.loss_cls: 0.1119, d1.loss_bbox: 0.2532, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2356, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2226, loss: 2.1145, grad_norm: 22.6411
2025-06-18 03:03:36,631 - mmdet - INFO - Epoch [3][6400/7033]	lr: 1.501e-04, eta: 11:30:54, time: 1.918, data_time: 0.062, memory: 17921, loss_cls: 0.0673, loss_bbox: 0.2104, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3217, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2426, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2126, loss: 2.0062, grad_norm: 35.7250
2025-06-18 03:05:13,345 - mmdet - INFO - Epoch [3][6450/7033]	lr: 1.501e-04, eta: 11:29:20, time: 1.934, data_time: 0.060, memory: 17921, loss_cls: 0.0764, loss_bbox: 0.2184, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1090, d1.loss_bbox: 0.2524, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0784, d4.loss_bbox: 0.2188, loss: 2.1005, grad_norm: 19.5141
2025-06-18 03:06:45,184 - mmdet - INFO - Epoch [3][6500/7033]	lr: 1.501e-04, eta: 11:27:41, time: 1.837, data_time: 0.058, memory: 17921, loss_cls: 0.0776, loss_bbox: 0.2167, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3424, d1.loss_cls: 0.1121, d1.loss_bbox: 0.2551, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0843, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2194, loss: 2.1189, grad_norm: 59.7065
2025-06-18 03:08:17,953 - mmdet - INFO - Epoch [3][6550/7033]	lr: 1.501e-04, eta: 11:26:03, time: 1.855, data_time: 0.067, memory: 17921, loss_cls: 0.0752, loss_bbox: 0.2199, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3411, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2572, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2377, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2232, loss: 2.1212, grad_norm: 21.0953
2025-06-18 03:09:53,087 - mmdet - INFO - Epoch [3][6600/7033]	lr: 1.501e-04, eta: 11:24:27, time: 1.901, data_time: 0.059, memory: 17921, loss_cls: 0.0715, loss_bbox: 0.2164, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3356, d1.loss_cls: 0.1023, d1.loss_bbox: 0.2522, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2328, d4.loss_cls: 0.0727, d4.loss_bbox: 0.2200, loss: 2.0626, grad_norm: 24.6978
2025-06-18 03:11:34,571 - mmdet - INFO - Epoch [3][6650/7033]	lr: 1.501e-04, eta: 11:22:58, time: 2.030, data_time: 0.056, memory: 17921, loss_cls: 0.0662, loss_bbox: 0.2030, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1027, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0870, d2.loss_bbox: 0.2186, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0668, d4.loss_bbox: 0.2069, loss: 1.9788, grad_norm: 25.9018
2025-06-18 03:13:07,347 - mmdet - INFO - Epoch [3][6700/7033]	lr: 1.501e-04, eta: 11:21:20, time: 1.857, data_time: 0.058, memory: 17921, loss_cls: 0.0712, loss_bbox: 0.2051, d0.loss_cls: 0.1613, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0787, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0735, d4.loss_bbox: 0.2080, loss: 1.9877, grad_norm: 22.8440
2025-06-18 03:14:40,359 - mmdet - INFO - Epoch [3][6750/7033]	lr: 1.501e-04, eta: 11:19:42, time: 1.860, data_time: 0.065, memory: 17921, loss_cls: 0.0788, loss_bbox: 0.2106, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3352, d1.loss_cls: 0.1126, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2256, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2130, loss: 2.0857, grad_norm: 32.5092
2025-06-18 03:16:14,143 - mmdet - INFO - Epoch [3][6800/7033]	lr: 1.501e-04, eta: 11:18:05, time: 1.876, data_time: 0.053, memory: 17921, loss_cls: 0.0756, loss_bbox: 0.2070, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3242, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2405, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2094, loss: 2.0231, grad_norm: 37.9395
2025-06-18 03:17:49,423 - mmdet - INFO - Epoch [3][6850/7033]	lr: 1.501e-04, eta: 11:16:30, time: 1.906, data_time: 0.061, memory: 17921, loss_cls: 0.0769, loss_bbox: 0.2206, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1152, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2383, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0785, d4.loss_bbox: 0.2237, loss: 2.1372, grad_norm: 23.1787
2025-06-18 03:19:24,093 - mmdet - INFO - Epoch [3][6900/7033]	lr: 1.501e-04, eta: 11:14:54, time: 1.893, data_time: 0.074, memory: 17921, loss_cls: 0.0788, loss_bbox: 0.2139, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3259, d1.loss_cls: 0.1120, d1.loss_bbox: 0.2485, d2.loss_cls: 0.0944, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0833, d3.loss_bbox: 0.2293, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2165, loss: 2.0948, grad_norm: 19.6773
2025-06-18 03:20:58,037 - mmdet - INFO - Epoch [3][6950/7033]	lr: 1.501e-04, eta: 11:13:17, time: 1.879, data_time: 0.056, memory: 17921, loss_cls: 0.0700, loss_bbox: 0.2157, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3300, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2492, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2324, d4.loss_cls: 0.0707, d4.loss_bbox: 0.2196, loss: 2.0585, grad_norm: 68.8032
2025-06-18 03:22:36,577 - mmdet - INFO - Epoch [3][7000/7033]	lr: 1.501e-04, eta: 11:11:45, time: 1.971, data_time: 0.058, memory: 17921, loss_cls: 0.0692, loss_bbox: 0.2070, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3202, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2090, loss: 1.9991, grad_norm: 19.7938
2025-06-18 03:23:38,051 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-18 04:01:03,948 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2.py
2025-06-18 04:01:03,948 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7903, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8811, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9075, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9201, pts_bbox_NuScenes/car_trans_err: 0.1821, pts_bbox_NuScenes/car_scale_err: 0.1523, pts_bbox_NuScenes/car_orient_err: 0.0460, pts_bbox_NuScenes/car_vel_err: 0.3399, pts_bbox_NuScenes/car_attr_err: 0.1691, pts_bbox_NuScenes/mATE: 0.2936, pts_bbox_NuScenes/mASE: 0.2640, pts_bbox_NuScenes/mAOE: 0.2621, pts_bbox_NuScenes/mAVE: 0.2907, pts_bbox_NuScenes/mAAE: 0.1864, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4304, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6151, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7204, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7592, pts_bbox_NuScenes/truck_trans_err: 0.3394, pts_bbox_NuScenes/truck_scale_err: 0.2008, pts_bbox_NuScenes/truck_orient_err: 0.0506, pts_bbox_NuScenes/truck_vel_err: 0.2611, pts_bbox_NuScenes/truck_attr_err: 0.1881, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0498, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1961, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3843, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4564, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6706, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4311, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8112, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1136, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3004, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5149, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7383, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8891, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9144, pts_bbox_NuScenes/bus_trans_err: 0.3525, pts_bbox_NuScenes/bus_scale_err: 0.2032, pts_bbox_NuScenes/bus_orient_err: 0.0376, pts_bbox_NuScenes/bus_vel_err: 0.5282, pts_bbox_NuScenes/bus_attr_err: 0.3100, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1863, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4254, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5844, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6682, pts_bbox_NuScenes/trailer_trans_err: 0.4881, pts_bbox_NuScenes/trailer_scale_err: 0.2288, pts_bbox_NuScenes/trailer_orient_err: 0.4968, pts_bbox_NuScenes/trailer_vel_err: 0.2580, pts_bbox_NuScenes/trailer_attr_err: 0.1543, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5997, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.7031, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7529, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7677, pts_bbox_NuScenes/barrier_trans_err: 0.2238, pts_bbox_NuScenes/barrier_scale_err: 0.2890, pts_bbox_NuScenes/barrier_orient_err: 0.0497, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6424, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7769, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8095, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8141, pts_bbox_NuScenes/motorcycle_trans_err: 0.2153, pts_bbox_NuScenes/motorcycle_scale_err: 0.2476, pts_bbox_NuScenes/motorcycle_orient_err: 0.2197, pts_bbox_NuScenes/motorcycle_vel_err: 0.3677, pts_bbox_NuScenes/motorcycle_attr_err: 0.2474, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5556, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6099, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6196, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6274, pts_bbox_NuScenes/bicycle_trans_err: 0.1779, pts_bbox_NuScenes/bicycle_scale_err: 0.2692, pts_bbox_NuScenes/bicycle_orient_err: 0.3293, pts_bbox_NuScenes/bicycle_vel_err: 0.2296, pts_bbox_NuScenes/bicycle_attr_err: 0.0089, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.8101, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8516, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8731, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8850, pts_bbox_NuScenes/pedestrian_trans_err: 0.1503, pts_bbox_NuScenes/pedestrian_scale_err: 0.2949, pts_bbox_NuScenes/pedestrian_orient_err: 0.3178, pts_bbox_NuScenes/pedestrian_vel_err: 0.2273, pts_bbox_NuScenes/pedestrian_attr_err: 0.1128, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.7393, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7749, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7989, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8206, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1355, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3228, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7061, pts_bbox_NuScenes/mAP: 0.6716
2025-06-18 04:02:59,655 - mmdet - INFO - Epoch [4][50/7033]	lr: 1.001e-04, eta: 11:08:20, time: 2.226, data_time: 0.365, memory: 17921, loss_cls: 0.0742, loss_bbox: 0.2077, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2465, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2276, d3.loss_cls: 0.0814, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0765, d4.loss_bbox: 0.2111, loss: 2.0493, grad_norm: 28.4256
2025-06-18 04:04:37,339 - mmdet - INFO - Epoch [4][100/7033]	lr: 1.001e-04, eta: 11:06:47, time: 1.954, data_time: 0.071, memory: 17921, loss_cls: 0.0677, loss_bbox: 0.2014, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3133, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2345, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2157, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2036, loss: 1.9480, grad_norm: 19.0310
2025-06-18 04:06:10,653 - mmdet - INFO - Epoch [4][150/7033]	lr: 1.001e-04, eta: 11:05:10, time: 1.866, data_time: 0.078, memory: 17921, loss_cls: 0.0641, loss_bbox: 0.2032, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3206, d1.loss_cls: 0.1043, d1.loss_bbox: 0.2361, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2195, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0672, d4.loss_bbox: 0.2063, loss: 1.9616, grad_norm: 33.0773
2025-06-18 04:07:45,183 - mmdet - INFO - Epoch [4][200/7033]	lr: 1.001e-04, eta: 11:03:34, time: 1.889, data_time: 0.060, memory: 17921, loss_cls: 0.0655, loss_bbox: 0.2052, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3236, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2437, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0720, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2092, loss: 1.9899, grad_norm: 35.7887
2025-06-18 04:09:20,801 - mmdet - INFO - Epoch [4][250/7033]	lr: 1.001e-04, eta: 11:01:59, time: 1.914, data_time: 0.069, memory: 17921, loss_cls: 0.0687, loss_bbox: 0.1963, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3185, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2321, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2146, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2096, d4.loss_cls: 0.0714, d4.loss_bbox: 0.1991, loss: 1.9482, grad_norm: 24.7300
2025-06-18 04:10:54,507 - mmdet - INFO - Epoch [4][300/7033]	lr: 1.001e-04, eta: 11:00:22, time: 1.874, data_time: 0.077, memory: 17921, loss_cls: 0.0658, loss_bbox: 0.2020, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3298, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2210, d3.loss_cls: 0.0702, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0667, d4.loss_bbox: 0.2055, loss: 1.9662, grad_norm: 43.4566
2025-06-18 04:12:23,420 - mmdet - INFO - Epoch [4][350/7033]	lr: 1.001e-04, eta: 10:58:41, time: 1.778, data_time: 0.071, memory: 17921, loss_cls: 0.0698, loss_bbox: 0.2065, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3231, d1.loss_cls: 0.1071, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0909, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0761, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2081, loss: 2.0007, grad_norm: 18.6100
2025-06-18 04:13:56,465 - mmdet - INFO - Epoch [4][400/7033]	lr: 1.001e-04, eta: 10:57:03, time: 1.861, data_time: 0.055, memory: 17921, loss_cls: 0.0760, loss_bbox: 0.2039, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2431, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2208, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2071, loss: 2.0294, grad_norm: 321.2037
2025-06-18 04:15:31,167 - mmdet - INFO - Epoch [4][450/7033]	lr: 1.001e-04, eta: 10:55:28, time: 1.894, data_time: 0.051, memory: 17921, loss_cls: 0.0645, loss_bbox: 0.2024, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3161, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0711, d3.loss_bbox: 0.2145, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2055, loss: 1.9439, grad_norm: 22.7013
2025-06-18 04:17:05,046 - mmdet - INFO - Epoch [4][500/7033]	lr: 1.001e-04, eta: 10:53:51, time: 1.878, data_time: 0.070, memory: 17921, loss_cls: 0.0736, loss_bbox: 0.2015, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3219, d1.loss_cls: 0.1075, d1.loss_bbox: 0.2418, d2.loss_cls: 0.0901, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0745, d4.loss_bbox: 0.2056, loss: 2.0006, grad_norm: 19.9969
2025-06-18 04:18:38,823 - mmdet - INFO - Epoch [4][550/7033]	lr: 1.001e-04, eta: 10:52:14, time: 1.875, data_time: 0.054, memory: 17921, loss_cls: 0.0705, loss_bbox: 0.2091, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3296, d1.loss_cls: 0.1065, d1.loss_bbox: 0.2435, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2244, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2118, loss: 2.0193, grad_norm: 50.9425
2025-06-18 04:20:13,693 - mmdet - INFO - Epoch [4][600/7033]	lr: 1.001e-04, eta: 10:50:39, time: 1.898, data_time: 0.069, memory: 17921, loss_cls: 0.0733, loss_bbox: 0.1959, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3089, d1.loss_cls: 0.1056, d1.loss_bbox: 0.2294, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2117, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2084, d4.loss_cls: 0.0739, d4.loss_bbox: 0.1985, loss: 1.9386, grad_norm: 38.3437
2025-06-18 04:21:49,329 - mmdet - INFO - Epoch [4][650/7033]	lr: 1.001e-04, eta: 10:49:04, time: 1.913, data_time: 0.060, memory: 17921, loss_cls: 0.0682, loss_bbox: 0.2065, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1040, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2088, loss: 1.9977, grad_norm: 20.4044
2025-06-18 04:23:25,321 - mmdet - INFO - Epoch [4][700/7033]	lr: 1.001e-04, eta: 10:47:30, time: 1.920, data_time: 0.069, memory: 17921, loss_cls: 0.0748, loss_bbox: 0.2066, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3276, d1.loss_cls: 0.1058, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0917, d2.loss_bbox: 0.2232, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2108, loss: 2.0293, grad_norm: 24.2099
2025-06-18 04:24:56,843 - mmdet - INFO - Epoch [4][750/7033]	lr: 1.001e-04, eta: 10:45:51, time: 1.830, data_time: 0.064, memory: 17921, loss_cls: 0.0697, loss_bbox: 0.2002, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3154, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2142, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2022, loss: 1.9518, grad_norm: 20.5977
2025-06-18 04:26:36,656 - mmdet - INFO - Epoch [4][800/7033]	lr: 1.001e-04, eta: 10:44:20, time: 1.996, data_time: 0.064, memory: 17921, loss_cls: 0.0705, loss_bbox: 0.2014, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0868, d2.loss_bbox: 0.2192, d3.loss_cls: 0.0753, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2043, loss: 1.9690, grad_norm: 18.7300
2025-06-18 04:28:07,627 - mmdet - INFO - Epoch [4][850/7033]	lr: 1.001e-04, eta: 10:42:41, time: 1.819, data_time: 0.051, memory: 17921, loss_cls: 0.0644, loss_bbox: 0.2090, d0.loss_cls: 0.1599, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2442, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2234, d3.loss_cls: 0.0713, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2125, loss: 1.9825, grad_norm: 34.6071
2025-06-18 04:29:35,452 - mmdet - INFO - Epoch [4][900/7033]	lr: 1.001e-04, eta: 10:40:59, time: 1.756, data_time: 0.061, memory: 17921, loss_cls: 0.0642, loss_bbox: 0.2012, d0.loss_cls: 0.1573, d0.loss_bbox: 0.3173, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2165, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0651, d4.loss_bbox: 0.2044, loss: 1.9259, grad_norm: 113.4311
2025-06-18 04:31:02,765 - mmdet - INFO - Epoch [4][950/7033]	lr: 1.001e-04, eta: 10:39:16, time: 1.746, data_time: 0.061, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2127, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3344, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0775, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2159, loss: 2.0600, grad_norm: 25.0873
2025-06-18 04:32:29,863 - mmdet - INFO - Epoch [4][1000/7033]	lr: 1.001e-04, eta: 10:37:34, time: 1.742, data_time: 0.059, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2107, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3248, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0912, d2.loss_bbox: 0.2297, d3.loss_cls: 0.0797, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2138, loss: 2.0464, grad_norm: 26.9455
2025-06-18 04:34:01,794 - mmdet - INFO - Epoch [4][1050/7033]	lr: 1.001e-04, eta: 10:35:56, time: 1.838, data_time: 0.064, memory: 17921, loss_cls: 0.0587, loss_bbox: 0.1985, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3119, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2286, d2.loss_cls: 0.0792, d2.loss_bbox: 0.2104, d3.loss_cls: 0.0649, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0613, d4.loss_bbox: 0.2005, loss: 1.8814, grad_norm: 35.2550
2025-06-18 04:35:36,816 - mmdet - INFO - Epoch [4][1100/7033]	lr: 1.001e-04, eta: 10:34:20, time: 1.901, data_time: 0.059, memory: 17921, loss_cls: 0.0684, loss_bbox: 0.2061, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3193, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2218, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2102, loss: 1.9813, grad_norm: 27.6568
2025-06-18 04:37:08,743 - mmdet - INFO - Epoch [4][1150/7033]	lr: 1.001e-04, eta: 10:32:42, time: 1.838, data_time: 0.061, memory: 17921, loss_cls: 0.0684, loss_bbox: 0.1981, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3215, d1.loss_cls: 0.1038, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2008, loss: 1.9539, grad_norm: 23.2612
2025-06-18 04:38:41,330 - mmdet - INFO - Epoch [4][1200/7033]	lr: 1.001e-04, eta: 10:31:05, time: 1.852, data_time: 0.053, memory: 17921, loss_cls: 0.0661, loss_bbox: 0.1965, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3188, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2350, d2.loss_cls: 0.0833, d2.loss_bbox: 0.2144, d3.loss_cls: 0.0715, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0690, d4.loss_bbox: 0.1997, loss: 1.9297, grad_norm: 18.5586
2025-06-18 04:40:14,705 - mmdet - INFO - Epoch [4][1250/7033]	lr: 1.001e-04, eta: 10:29:28, time: 1.868, data_time: 0.057, memory: 17921, loss_cls: 0.0759, loss_bbox: 0.2095, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3300, d1.loss_cls: 0.1144, d1.loss_bbox: 0.2455, d2.loss_cls: 0.0970, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2116, loss: 2.0617, grad_norm: 31.7804
2025-06-18 04:41:47,940 - mmdet - INFO - Epoch [4][1300/7033]	lr: 1.001e-04, eta: 10:27:51, time: 1.865, data_time: 0.055, memory: 17921, loss_cls: 0.0742, loss_bbox: 0.2074, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3286, d1.loss_cls: 0.1083, d1.loss_bbox: 0.2431, d2.loss_cls: 0.0902, d2.loss_bbox: 0.2243, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2212, d4.loss_cls: 0.0762, d4.loss_bbox: 0.2101, loss: 2.0290, grad_norm: 40.1608
2025-06-18 04:43:23,337 - mmdet - INFO - Epoch [4][1350/7033]	lr: 1.001e-04, eta: 10:26:17, time: 1.908, data_time: 0.068, memory: 17921, loss_cls: 0.0672, loss_bbox: 0.2017, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1030, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2150, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2044, loss: 1.9614, grad_norm: 23.0231
2025-06-18 04:44:56,650 - mmdet - INFO - Epoch [4][1400/7033]	lr: 1.001e-04, eta: 10:24:40, time: 1.866, data_time: 0.066, memory: 17921, loss_cls: 0.0688, loss_bbox: 0.2023, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2352, d2.loss_cls: 0.0881, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2138, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2036, loss: 1.9608, grad_norm: 15.9517
2025-06-18 04:46:29,453 - mmdet - INFO - Epoch [4][1450/7033]	lr: 1.001e-04, eta: 10:23:03, time: 1.856, data_time: 0.065, memory: 17921, loss_cls: 0.0693, loss_bbox: 0.1976, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2320, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2150, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0746, d4.loss_bbox: 0.2001, loss: 1.9539, grad_norm: 19.9350
2025-06-18 04:48:08,889 - mmdet - INFO - Epoch [4][1500/7033]	lr: 1.001e-04, eta: 10:21:31, time: 1.987, data_time: 0.054, memory: 17921, loss_cls: 0.0679, loss_bbox: 0.2057, d0.loss_cls: 0.1592, d0.loss_bbox: 0.3304, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0827, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2089, loss: 1.9811, grad_norm: 18.9906
2025-06-18 04:49:46,933 - mmdet - INFO - Epoch [4][1550/7033]	lr: 1.001e-04, eta: 10:19:59, time: 1.961, data_time: 0.062, memory: 17921, loss_cls: 0.0680, loss_bbox: 0.2057, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3147, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2080, loss: 1.9642, grad_norm: 43.8228
2025-06-18 04:51:18,448 - mmdet - INFO - Epoch [4][1600/7033]	lr: 1.001e-04, eta: 10:18:20, time: 1.832, data_time: 0.069, memory: 17921, loss_cls: 0.0650, loss_bbox: 0.2050, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3267, d1.loss_cls: 0.1015, d1.loss_bbox: 0.2403, d2.loss_cls: 0.0836, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0669, d4.loss_bbox: 0.2076, loss: 1.9649, grad_norm: 25.1651
2025-06-18 04:52:53,275 - mmdet - INFO - Epoch [4][1650/7033]	lr: 1.001e-04, eta: 10:16:45, time: 1.897, data_time: 0.067, memory: 17921, loss_cls: 0.0654, loss_bbox: 0.2003, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3274, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0818, d2.loss_bbox: 0.2194, d3.loss_cls: 0.0725, d3.loss_bbox: 0.2135, d4.loss_cls: 0.0665, d4.loss_bbox: 0.2043, loss: 1.9518, grad_norm: 24.1455
2025-06-18 04:54:26,977 - mmdet - INFO - Epoch [4][1700/7033]	lr: 1.001e-04, eta: 10:15:09, time: 1.874, data_time: 0.062, memory: 17921, loss_cls: 0.0715, loss_bbox: 0.2051, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3298, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2447, d2.loss_cls: 0.0876, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0769, d3.loss_bbox: 0.2205, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2091, loss: 2.0163, grad_norm: 19.7834
2025-06-18 04:56:03,913 - mmdet - INFO - Epoch [4][1750/7033]	lr: 1.001e-04, eta: 10:13:35, time: 1.939, data_time: 0.081, memory: 17921, loss_cls: 0.0715, loss_bbox: 0.2014, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3234, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0841, d2.loss_bbox: 0.2203, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0726, d4.loss_bbox: 0.2050, loss: 1.9692, grad_norm: 34.4521
2025-06-18 04:57:37,698 - mmdet - INFO - Epoch [4][1800/7033]	lr: 1.001e-04, eta: 10:11:59, time: 1.876, data_time: 0.065, memory: 17921, loss_cls: 0.0627, loss_bbox: 0.1987, d0.loss_cls: 0.1625, d0.loss_bbox: 0.3212, d1.loss_cls: 0.0965, d1.loss_bbox: 0.2373, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0669, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0640, d4.loss_bbox: 0.2023, loss: 1.9193, grad_norm: 27.5024
2025-06-18 04:59:12,219 - mmdet - INFO - Epoch [4][1850/7033]	lr: 1.001e-04, eta: 10:10:23, time: 1.890, data_time: 0.059, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2008, d0.loss_cls: 0.1660, d0.loss_bbox: 0.3144, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0882, d2.loss_bbox: 0.2176, d3.loss_cls: 0.0774, d3.loss_bbox: 0.2138, d4.loss_cls: 0.0748, d4.loss_bbox: 0.2024, loss: 1.9700, grad_norm: 75.6798
2025-06-18 05:00:45,414 - mmdet - INFO - Epoch [4][1900/7033]	lr: 1.001e-04, eta: 10:08:46, time: 1.864, data_time: 0.061, memory: 17921, loss_cls: 0.0707, loss_bbox: 0.2074, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3192, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0887, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0762, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2098, loss: 1.9903, grad_norm: 16.0539
2025-06-18 05:02:20,330 - mmdet - INFO - Epoch [4][1950/7033]	lr: 1.001e-04, eta: 10:07:11, time: 1.898, data_time: 0.069, memory: 17921, loss_cls: 0.0716, loss_bbox: 0.2097, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3297, d1.loss_cls: 0.1067, d1.loss_bbox: 0.2459, d2.loss_cls: 0.0896, d2.loss_bbox: 0.2256, d3.loss_cls: 0.0778, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0738, d4.loss_bbox: 0.2116, loss: 2.0346, grad_norm: 20.2499
2025-06-18 05:03:59,479 - mmdet - INFO - Epoch [4][2000/7033]	lr: 1.001e-04, eta: 10:05:39, time: 1.983, data_time: 0.075, memory: 17921, loss_cls: 0.0679, loss_bbox: 0.2069, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3295, d1.loss_cls: 0.0979, d1.loss_bbox: 0.2449, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0702, d4.loss_bbox: 0.2099, loss: 1.9859, grad_norm: 20.1361
2025-06-18 05:05:40,250 - mmdet - INFO - Epoch [4][2050/7033]	lr: 1.001e-04, eta: 10:04:09, time: 2.015, data_time: 0.085, memory: 17921, loss_cls: 0.0692, loss_bbox: 0.2051, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3254, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0808, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0693, d4.loss_bbox: 0.2083, loss: 1.9654, grad_norm: 22.7691
2025-06-18 05:07:21,752 - mmdet - INFO - Epoch [4][2100/7033]	lr: 1.001e-04, eta: 10:02:39, time: 2.030, data_time: 0.080, memory: 17921, loss_cls: 0.0652, loss_bbox: 0.1983, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3124, d1.loss_cls: 0.1001, d1.loss_bbox: 0.2360, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2146, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2119, d4.loss_cls: 0.0681, d4.loss_bbox: 0.1996, loss: 1.9227, grad_norm: 27.2943
2025-06-18 05:08:54,996 - mmdet - INFO - Epoch [4][2150/7033]	lr: 1.001e-04, eta: 10:01:02, time: 1.863, data_time: 0.067, memory: 17921, loss_cls: 0.0770, loss_bbox: 0.2071, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3225, d1.loss_cls: 0.1063, d1.loss_bbox: 0.2421, d2.loss_cls: 0.0893, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2099, loss: 2.0295, grad_norm: 21.4975
2025-06-18 05:10:27,171 - mmdet - INFO - Epoch [4][2200/7033]	lr: 1.001e-04, eta: 9:59:24, time: 1.845, data_time: 0.061, memory: 17921, loss_cls: 0.0666, loss_bbox: 0.2062, d0.loss_cls: 0.1656, d0.loss_bbox: 0.3233, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0828, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0755, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2099, loss: 1.9830, grad_norm: 41.0055
2025-06-18 05:12:01,064 - mmdet - INFO - Epoch [4][2250/7033]	lr: 1.001e-04, eta: 9:57:48, time: 1.878, data_time: 0.059, memory: 17921, loss_cls: 0.0646, loss_bbox: 0.1970, d0.loss_cls: 0.1616, d0.loss_bbox: 0.3057, d1.loss_cls: 0.0959, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0779, d2.loss_bbox: 0.2110, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2076, d4.loss_cls: 0.0650, d4.loss_bbox: 0.1987, loss: 1.8806, grad_norm: 18.6128
2025-06-18 05:13:33,842 - mmdet - INFO - Epoch [4][2300/7033]	lr: 1.001e-04, eta: 9:56:11, time: 1.855, data_time: 0.056, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2098, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3193, d1.loss_cls: 0.1028, d1.loss_bbox: 0.2439, d2.loss_cls: 0.0865, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0772, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0730, d4.loss_bbox: 0.2136, loss: 2.0124, grad_norm: 72.4069
2025-06-18 05:15:08,661 - mmdet - INFO - Epoch [4][2350/7033]	lr: 1.001e-04, eta: 9:54:36, time: 1.896, data_time: 0.056, memory: 17921, loss_cls: 0.0635, loss_bbox: 0.1984, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3149, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2331, d2.loss_cls: 0.0816, d2.loss_bbox: 0.2130, d3.loss_cls: 0.0689, d3.loss_bbox: 0.2102, d4.loss_cls: 0.0659, d4.loss_bbox: 0.2008, loss: 1.9063, grad_norm: 22.5544
2025-06-18 05:16:43,443 - mmdet - INFO - Epoch [4][2400/7033]	lr: 1.001e-04, eta: 9:53:00, time: 1.896, data_time: 0.063, memory: 17921, loss_cls: 0.0682, loss_bbox: 0.2105, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3196, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0754, d3.loss_bbox: 0.2223, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2128, loss: 2.0092, grad_norm: 17.0746
2025-06-18 05:18:19,306 - mmdet - INFO - Epoch [4][2450/7033]	lr: 1.001e-04, eta: 9:51:26, time: 1.917, data_time: 0.075, memory: 17921, loss_cls: 0.0683, loss_bbox: 0.2099, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3207, d1.loss_cls: 0.1016, d1.loss_bbox: 0.2408, d2.loss_cls: 0.0852, d2.loss_bbox: 0.2247, d3.loss_cls: 0.0743, d3.loss_bbox: 0.2212, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2107, loss: 1.9916, grad_norm: 23.0227
2025-06-18 05:19:55,323 - mmdet - INFO - Epoch [4][2500/7033]	lr: 1.001e-04, eta: 9:49:51, time: 1.920, data_time: 0.071, memory: 17921, loss_cls: 0.0709, loss_bbox: 0.2072, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3256, d1.loss_cls: 0.1068, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2096, loss: 2.0081, grad_norm: 18.2351
2025-06-18 05:21:26,536 - mmdet - INFO - Epoch [4][2550/7033]	lr: 1.001e-04, eta: 9:48:13, time: 1.824, data_time: 0.063, memory: 17921, loss_cls: 0.0721, loss_bbox: 0.2102, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1073, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0907, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0784, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2140, loss: 2.0337, grad_norm: 32.4326
2025-06-18 05:22:59,677 - mmdet - INFO - Epoch [4][2600/7033]	lr: 1.001e-04, eta: 9:46:36, time: 1.863, data_time: 0.063, memory: 17921, loss_cls: 0.0708, loss_bbox: 0.2124, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3246, d1.loss_cls: 0.1057, d1.loss_bbox: 0.2471, d2.loss_cls: 0.0889, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0768, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0737, d4.loss_bbox: 0.2150, loss: 2.0385, grad_norm: 28.1790
2025-06-18 05:24:32,599 - mmdet - INFO - Epoch [4][2650/7033]	lr: 1.001e-04, eta: 9:45:00, time: 1.858, data_time: 0.054, memory: 17921, loss_cls: 0.0668, loss_bbox: 0.2074, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3223, d1.loss_cls: 0.1042, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0726, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0684, d4.loss_bbox: 0.2103, loss: 1.9845, grad_norm: 36.1685
2025-06-18 05:26:05,413 - mmdet - INFO - Epoch [4][2700/7033]	lr: 1.001e-04, eta: 9:43:23, time: 1.856, data_time: 0.060, memory: 17921, loss_cls: 0.0716, loss_bbox: 0.2034, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3167, d1.loss_cls: 0.1041, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0869, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0779, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0736, d4.loss_bbox: 0.2054, loss: 1.9717, grad_norm: 24.2986
2025-06-18 05:27:49,346 - mmdet - INFO - Epoch [4][2750/7033]	lr: 1.001e-04, eta: 9:41:54, time: 2.079, data_time: 0.077, memory: 17921, loss_cls: 0.0700, loss_bbox: 0.2038, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0880, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0738, d3.loss_bbox: 0.2192, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2064, loss: 1.9799, grad_norm: 17.5499
2025-06-18 05:29:22,452 - mmdet - INFO - Epoch [4][2800/7033]	lr: 1.001e-04, eta: 9:40:18, time: 1.862, data_time: 0.062, memory: 17921, loss_cls: 0.0660, loss_bbox: 0.2014, d0.loss_cls: 0.1576, d0.loss_bbox: 0.3223, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0851, d2.loss_bbox: 0.2186, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2040, loss: 1.9527, grad_norm: 22.3615
2025-06-18 05:30:57,616 - mmdet - INFO - Epoch [4][2850/7033]	lr: 1.001e-04, eta: 9:38:43, time: 1.904, data_time: 0.052, memory: 17921, loss_cls: 0.0695, loss_bbox: 0.2096, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3349, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2498, d2.loss_cls: 0.0857, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2232, d4.loss_cls: 0.0724, d4.loss_bbox: 0.2117, loss: 2.0331, grad_norm: 37.2029
2025-06-18 05:32:30,295 - mmdet - INFO - Epoch [4][2900/7033]	lr: 1.001e-04, eta: 9:37:06, time: 1.854, data_time: 0.062, memory: 17921, loss_cls: 0.0637, loss_bbox: 0.2071, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3259, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0799, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2205, d4.loss_cls: 0.0664, d4.loss_bbox: 0.2097, loss: 1.9661, grad_norm: 16.3297
2025-06-18 05:33:59,157 - mmdet - INFO - Epoch [4][2950/7033]	lr: 1.001e-04, eta: 9:35:26, time: 1.777, data_time: 0.054, memory: 17921, loss_cls: 0.0626, loss_bbox: 0.2028, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3158, d1.loss_cls: 0.0988, d1.loss_bbox: 0.2379, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2170, d3.loss_cls: 0.0692, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0656, d4.loss_bbox: 0.2045, loss: 1.9389, grad_norm: 28.8358
2025-06-18 05:35:33,272 - mmdet - INFO - Epoch [4][3000/7033]	lr: 1.001e-04, eta: 9:33:50, time: 1.882, data_time: 0.059, memory: 17921, loss_cls: 0.0716, loss_bbox: 0.2070, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3282, d1.loss_cls: 0.1072, d1.loss_bbox: 0.2460, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2263, d3.loss_cls: 0.0777, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0728, d4.loss_bbox: 0.2116, loss: 2.0283, grad_norm: 39.0162
2025-06-18 05:37:05,045 - mmdet - INFO - Epoch [4][3050/7033]	lr: 1.001e-04, eta: 9:32:12, time: 1.835, data_time: 0.056, memory: 17921, loss_cls: 0.0758, loss_bbox: 0.2062, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3240, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2210, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2106, loss: 2.0378, grad_norm: 18.8424
2025-06-18 05:38:43,417 - mmdet - INFO - Epoch [4][3100/7033]	lr: 1.001e-04, eta: 9:30:39, time: 1.967, data_time: 0.054, memory: 17921, loss_cls: 0.0690, loss_bbox: 0.2088, d0.loss_cls: 0.1698, d0.loss_bbox: 0.3242, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2433, d2.loss_cls: 0.0855, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2196, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2117, loss: 2.0011, grad_norm: 65.7193
2025-06-18 05:40:20,651 - mmdet - INFO - Epoch [4][3150/7033]	lr: 1.001e-04, eta: 9:29:06, time: 1.944, data_time: 0.069, memory: 17921, loss_cls: 0.0697, loss_bbox: 0.2066, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3300, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0838, d2.loss_bbox: 0.2233, d3.loss_cls: 0.0734, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0725, d4.loss_bbox: 0.2084, loss: 1.9935, grad_norm: 46.0973
2025-06-18 05:41:54,815 - mmdet - INFO - Epoch [4][3200/7033]	lr: 1.001e-04, eta: 9:27:30, time: 1.884, data_time: 0.054, memory: 17921, loss_cls: 0.0674, loss_bbox: 0.2044, d0.loss_cls: 0.1630, d0.loss_bbox: 0.3172, d1.loss_cls: 0.0989, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0831, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2073, loss: 1.9617, grad_norm: 89.4754
2025-06-18 05:43:30,076 - mmdet - INFO - Epoch [4][3250/7033]	lr: 1.001e-04, eta: 9:25:55, time: 1.905, data_time: 0.067, memory: 17921, loss_cls: 0.0720, loss_bbox: 0.2131, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3455, d1.loss_cls: 0.1017, d1.loss_bbox: 0.2594, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2319, d3.loss_cls: 0.0748, d3.loss_bbox: 0.2287, d4.loss_cls: 0.0713, d4.loss_bbox: 0.2168, loss: 2.0664, grad_norm: 38.2344
2025-06-18 05:45:05,695 - mmdet - INFO - Epoch [4][3300/7033]	lr: 1.001e-04, eta: 9:24:20, time: 1.910, data_time: 0.060, memory: 17921, loss_cls: 0.0693, loss_bbox: 0.2029, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3163, d1.loss_cls: 0.1011, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2177, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2154, d4.loss_cls: 0.0721, d4.loss_bbox: 0.2051, loss: 1.9609, grad_norm: 31.2856
2025-06-18 05:46:38,701 - mmdet - INFO - Epoch [4][3350/7033]	lr: 1.001e-04, eta: 9:22:44, time: 1.862, data_time: 0.061, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2086, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3220, d1.loss_cls: 0.1082, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2120, loss: 2.0282, grad_norm: 25.4293
2025-06-18 05:48:17,056 - mmdet - INFO - Epoch [4][3400/7033]	lr: 1.001e-04, eta: 9:21:11, time: 1.966, data_time: 0.056, memory: 17921, loss_cls: 0.0646, loss_bbox: 0.2133, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2496, d2.loss_cls: 0.0850, d2.loss_bbox: 0.2267, d3.loss_cls: 0.0706, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0669, d4.loss_bbox: 0.2149, loss: 2.0104, grad_norm: 29.1649
2025-06-18 05:49:49,936 - mmdet - INFO - Epoch [4][3450/7033]	lr: 1.001e-04, eta: 9:19:34, time: 1.858, data_time: 0.068, memory: 17921, loss_cls: 0.0720, loss_bbox: 0.2205, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2538, d2.loss_cls: 0.0908, d2.loss_bbox: 0.2342, d3.loss_cls: 0.0799, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2209, loss: 2.0854, grad_norm: 34.5725
2025-06-18 05:51:24,759 - mmdet - INFO - Epoch [4][3500/7033]	lr: 1.001e-04, eta: 9:17:59, time: 1.896, data_time: 0.052, memory: 17921, loss_cls: 0.0737, loss_bbox: 0.2020, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3261, d1.loss_cls: 0.1066, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2178, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2163, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2053, loss: 2.0109, grad_norm: 30.3922
2025-06-18 05:53:02,614 - mmdet - INFO - Epoch [4][3550/7033]	lr: 1.001e-04, eta: 9:16:26, time: 1.958, data_time: 0.074, memory: 17921, loss_cls: 0.0784, loss_bbox: 0.2125, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1098, d1.loss_bbox: 0.2549, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0792, d4.loss_bbox: 0.2149, loss: 2.0964, grad_norm: 20.6066
2025-06-18 05:54:35,886 - mmdet - INFO - Epoch [4][3600/7033]	lr: 1.001e-04, eta: 9:14:49, time: 1.865, data_time: 0.062, memory: 17921, loss_cls: 0.0815, loss_bbox: 0.2110, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3278, d1.loss_cls: 0.1187, d1.loss_bbox: 0.2484, d2.loss_cls: 0.0985, d2.loss_bbox: 0.2292, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2268, d4.loss_cls: 0.0834, d4.loss_bbox: 0.2130, loss: 2.0957, grad_norm: 25.9121
2025-06-18 05:56:10,413 - mmdet - INFO - Epoch [4][3650/7033]	lr: 1.001e-04, eta: 9:13:14, time: 1.891, data_time: 0.078, memory: 17921, loss_cls: 0.0729, loss_bbox: 0.1984, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1060, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2141, d3.loss_cls: 0.0786, d3.loss_bbox: 0.2126, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2019, loss: 1.9760, grad_norm: 218.5394
2025-06-18 05:57:44,288 - mmdet - INFO - Epoch [4][3700/7033]	lr: 1.001e-04, eta: 9:11:38, time: 1.877, data_time: 0.060, memory: 17921, loss_cls: 0.0684, loss_bbox: 0.2066, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3230, d1.loss_cls: 0.1049, d1.loss_bbox: 0.2430, d2.loss_cls: 0.0863, d2.loss_bbox: 0.2221, d3.loss_cls: 0.0740, d3.loss_bbox: 0.2198, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2085, loss: 1.9993, grad_norm: 28.9588
2025-06-18 05:59:18,158 - mmdet - INFO - Epoch [4][3750/7033]	lr: 1.001e-04, eta: 9:10:02, time: 1.877, data_time: 0.052, memory: 17921, loss_cls: 0.0699, loss_bbox: 0.1988, d0.loss_cls: 0.1658, d0.loss_bbox: 0.3170, d1.loss_cls: 0.0995, d1.loss_bbox: 0.2353, d2.loss_cls: 0.0853, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2123, d4.loss_cls: 0.0703, d4.loss_bbox: 0.2015, loss: 1.9449, grad_norm: 40.7748
2025-06-18 06:00:59,856 - mmdet - INFO - Epoch [4][3800/7033]	lr: 1.001e-04, eta: 9:08:31, time: 2.034, data_time: 0.083, memory: 17921, loss_cls: 0.0744, loss_bbox: 0.2055, d0.loss_cls: 0.1644, d0.loss_bbox: 0.3158, d1.loss_cls: 0.1031, d1.loss_bbox: 0.2416, d2.loss_cls: 0.0884, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2096, loss: 1.9976, grad_norm: 19.0602
2025-06-18 06:02:32,821 - mmdet - INFO - Epoch [4][3850/7033]	lr: 1.001e-04, eta: 9:06:55, time: 1.859, data_time: 0.064, memory: 17921, loss_cls: 0.0633, loss_bbox: 0.1978, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3014, d1.loss_cls: 0.0941, d1.loss_bbox: 0.2307, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0662, d3.loss_bbox: 0.2093, d4.loss_cls: 0.0636, d4.loss_bbox: 0.2002, loss: 1.8726, grad_norm: 24.4103
2025-06-18 06:04:03,614 - mmdet - INFO - Epoch [4][3900/7033]	lr: 1.001e-04, eta: 9:05:16, time: 1.816, data_time: 0.052, memory: 17921, loss_cls: 0.0637, loss_bbox: 0.1967, d0.loss_cls: 0.1633, d0.loss_bbox: 0.3142, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0825, d2.loss_bbox: 0.2135, d3.loss_cls: 0.0685, d3.loss_bbox: 0.2108, d4.loss_cls: 0.0653, d4.loss_bbox: 0.1990, loss: 1.9073, grad_norm: 21.7465
2025-06-18 06:05:36,166 - mmdet - INFO - Epoch [4][3950/7033]	lr: 1.001e-04, eta: 9:03:40, time: 1.851, data_time: 0.054, memory: 17921, loss_cls: 0.0699, loss_bbox: 0.2048, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3195, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0872, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0731, d4.loss_bbox: 0.2071, loss: 1.9898, grad_norm: 33.8788
2025-06-18 06:07:13,614 - mmdet - INFO - Epoch [4][4000/7033]	lr: 1.001e-04, eta: 9:02:06, time: 1.949, data_time: 0.064, memory: 17921, loss_cls: 0.0740, loss_bbox: 0.2095, d0.loss_cls: 0.1668, d0.loss_bbox: 0.3354, d1.loss_cls: 0.1076, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0894, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0776, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2120, loss: 2.0415, grad_norm: 30.4338
2025-06-18 06:08:46,828 - mmdet - INFO - Epoch [4][4050/7033]	lr: 1.001e-04, eta: 9:00:30, time: 1.863, data_time: 0.063, memory: 17921, loss_cls: 0.0689, loss_bbox: 0.2064, d0.loss_cls: 0.1648, d0.loss_bbox: 0.3270, d1.loss_cls: 0.0984, d1.loss_bbox: 0.2444, d2.loss_cls: 0.0832, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0728, d3.loss_bbox: 0.2175, d4.loss_cls: 0.0689, d4.loss_bbox: 0.2093, loss: 1.9837, grad_norm: 38.2911
2025-06-18 06:10:24,158 - mmdet - INFO - Epoch [4][4100/7033]	lr: 1.001e-04, eta: 8:58:56, time: 1.948, data_time: 0.058, memory: 17921, loss_cls: 0.0689, loss_bbox: 0.2076, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3226, d1.loss_cls: 0.1048, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2107, loss: 1.9945, grad_norm: 17.9829
2025-06-18 06:12:04,421 - mmdet - INFO - Epoch [4][4150/7033]	lr: 1.001e-04, eta: 8:57:24, time: 2.005, data_time: 0.058, memory: 17921, loss_cls: 0.0693, loss_bbox: 0.2045, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3124, d1.loss_cls: 0.1044, d1.loss_bbox: 0.2342, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0727, d3.loss_bbox: 0.2155, d4.loss_cls: 0.0697, d4.loss_bbox: 0.2072, loss: 1.9598, grad_norm: 24.4261
2025-06-18 06:13:42,935 - mmdet - INFO - Epoch [4][4200/7033]	lr: 1.001e-04, eta: 8:55:52, time: 1.970, data_time: 0.087, memory: 17921, loss_cls: 0.0639, loss_bbox: 0.2063, d0.loss_cls: 0.1586, d0.loss_bbox: 0.3266, d1.loss_cls: 0.0964, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0696, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0661, d4.loss_bbox: 0.2091, loss: 1.9594, grad_norm: 22.5781
2025-06-18 06:15:21,178 - mmdet - INFO - Epoch [4][4250/7033]	lr: 1.001e-04, eta: 8:54:19, time: 1.965, data_time: 0.072, memory: 17921, loss_cls: 0.0732, loss_bbox: 0.2041, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3212, d1.loss_cls: 0.1055, d1.loss_bbox: 0.2378, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2166, d3.loss_cls: 0.0798, d3.loss_bbox: 0.2146, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2055, loss: 1.9936, grad_norm: 34.5184
2025-06-18 06:16:57,688 - mmdet - INFO - Epoch [4][4300/7033]	lr: 1.001e-04, eta: 8:52:44, time: 1.930, data_time: 0.066, memory: 17921, loss_cls: 0.0791, loss_bbox: 0.2042, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3252, d1.loss_cls: 0.1111, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0945, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2167, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2068, loss: 2.0452, grad_norm: 16.8408
2025-06-18 06:18:38,283 - mmdet - INFO - Epoch [4][4350/7033]	lr: 1.001e-04, eta: 8:51:13, time: 2.012, data_time: 0.070, memory: 17921, loss_cls: 0.0694, loss_bbox: 0.2034, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3251, d1.loss_cls: 0.1018, d1.loss_bbox: 0.2391, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2208, d3.loss_cls: 0.0746, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0718, d4.loss_bbox: 0.2075, loss: 1.9865, grad_norm: 32.4982
2025-06-18 06:20:11,717 - mmdet - INFO - Epoch [4][4400/7033]	lr: 1.001e-04, eta: 8:49:36, time: 1.868, data_time: 0.069, memory: 17921, loss_cls: 0.0753, loss_bbox: 0.2085, d0.loss_cls: 0.1664, d0.loss_bbox: 0.3332, d1.loss_cls: 0.1114, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0911, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0788, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2128, loss: 2.0487, grad_norm: 20.9468
2025-06-18 06:21:44,858 - mmdet - INFO - Epoch [4][4450/7033]	lr: 1.001e-04, eta: 8:48:00, time: 1.863, data_time: 0.057, memory: 17921, loss_cls: 0.0641, loss_bbox: 0.1940, d0.loss_cls: 0.1603, d0.loss_bbox: 0.3109, d1.loss_cls: 0.0974, d1.loss_bbox: 0.2298, d2.loss_cls: 0.0795, d2.loss_bbox: 0.2087, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0647, d4.loss_bbox: 0.1967, loss: 1.8821, grad_norm: 42.0066
2025-06-18 06:23:20,145 - mmdet - INFO - Epoch [4][4500/7033]	lr: 1.001e-04, eta: 8:46:25, time: 1.905, data_time: 0.057, memory: 17921, loss_cls: 0.0659, loss_bbox: 0.2012, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3132, d1.loss_cls: 0.0975, d1.loss_bbox: 0.2355, d2.loss_cls: 0.0804, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2147, d4.loss_cls: 0.0677, d4.loss_bbox: 0.2020, loss: 1.9201, grad_norm: 19.9255
2025-06-18 06:24:55,722 - mmdet - INFO - Epoch [4][4550/7033]	lr: 1.001e-04, eta: 8:44:50, time: 1.912, data_time: 0.061, memory: 17921, loss_cls: 0.0669, loss_bbox: 0.2036, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3221, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2414, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0729, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0690, d4.loss_bbox: 0.2070, loss: 1.9679, grad_norm: 23.6856
2025-06-18 06:26:36,163 - mmdet - INFO - Epoch [4][4600/7033]	lr: 1.001e-04, eta: 8:43:18, time: 2.007, data_time: 0.067, memory: 17921, loss_cls: 0.0741, loss_bbox: 0.2108, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1069, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2264, d3.loss_cls: 0.0795, d3.loss_bbox: 0.2239, d4.loss_cls: 0.0767, d4.loss_bbox: 0.2137, loss: 2.0428, grad_norm: 21.8033
2025-06-18 06:28:15,577 - mmdet - INFO - Epoch [4][4650/7033]	lr: 1.001e-04, eta: 8:41:46, time: 1.990, data_time: 0.071, memory: 17921, loss_cls: 0.0704, loss_bbox: 0.2027, d0.loss_cls: 0.1631, d0.loss_bbox: 0.3175, d1.loss_cls: 0.1004, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0834, d2.loss_bbox: 0.2188, d3.loss_cls: 0.0756, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2054, loss: 1.9650, grad_norm: 39.0658
2025-06-18 06:29:52,281 - mmdet - INFO - Epoch [4][4700/7033]	lr: 1.001e-04, eta: 8:40:12, time: 1.932, data_time: 0.057, memory: 17921, loss_cls: 0.0703, loss_bbox: 0.2067, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3243, d1.loss_cls: 0.1019, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0878, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0760, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0717, d4.loss_bbox: 0.2101, loss: 2.0128, grad_norm: 25.9505
2025-06-18 06:31:25,209 - mmdet - INFO - Epoch [4][4750/7033]	lr: 1.001e-04, eta: 8:38:35, time: 1.860, data_time: 0.062, memory: 17921, loss_cls: 0.0723, loss_bbox: 0.2076, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3185, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2417, d2.loss_cls: 0.0867, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0758, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2115, loss: 2.0042, grad_norm: 20.3952
2025-06-18 06:33:01,372 - mmdet - INFO - Epoch [4][4800/7033]	lr: 1.001e-04, eta: 8:37:01, time: 1.923, data_time: 0.057, memory: 17921, loss_cls: 0.0735, loss_bbox: 0.2063, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3197, d1.loss_cls: 0.1101, d1.loss_bbox: 0.2420, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0791, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0754, d4.loss_bbox: 0.2096, loss: 2.0203, grad_norm: 55.4894
2025-06-18 06:34:34,891 - mmdet - INFO - Epoch [4][4850/7033]	lr: 1.001e-04, eta: 8:35:24, time: 1.871, data_time: 0.058, memory: 17921, loss_cls: 0.0669, loss_bbox: 0.2009, d0.loss_cls: 0.1565, d0.loss_bbox: 0.3159, d1.loss_cls: 0.1008, d1.loss_bbox: 0.2344, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2146, d3.loss_cls: 0.0731, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0695, d4.loss_bbox: 0.2029, loss: 1.9317, grad_norm: 16.5607
2025-06-18 06:36:13,899 - mmdet - INFO - Epoch [4][4900/7033]	lr: 1.001e-04, eta: 8:33:52, time: 1.980, data_time: 0.066, memory: 17921, loss_cls: 0.0766, loss_bbox: 0.2082, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3254, d1.loss_cls: 0.1087, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2245, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2218, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2109, loss: 2.0430, grad_norm: 36.5110
2025-06-18 06:38:19,052 - mmdet - INFO - Epoch [4][4950/7033]	lr: 1.001e-04, eta: 8:32:35, time: 2.503, data_time: 0.082, memory: 17921, loss_cls: 0.0622, loss_bbox: 0.2041, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3267, d1.loss_cls: 0.0960, d1.loss_bbox: 0.2393, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0637, d4.loss_bbox: 0.2069, loss: 1.9455, grad_norm: 29.7998
2025-06-18 06:39:52,388 - mmdet - INFO - Epoch [4][5000/7033]	lr: 1.001e-04, eta: 8:30:59, time: 1.867, data_time: 0.062, memory: 17921, loss_cls: 0.0751, loss_bbox: 0.2107, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3205, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0761, d4.loss_bbox: 0.2138, loss: 2.0392, grad_norm: 24.5511
2025-06-18 06:41:28,874 - mmdet - INFO - Epoch [4][5050/7033]	lr: 1.001e-04, eta: 8:29:24, time: 1.928, data_time: 0.055, memory: 17921, loss_cls: 0.0710, loss_bbox: 0.2048, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3130, d1.loss_cls: 0.1054, d1.loss_bbox: 0.2382, d2.loss_cls: 0.0895, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2164, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2082, loss: 1.9714, grad_norm: 22.6951
2025-06-18 06:42:59,982 - mmdet - INFO - Epoch [4][5100/7033]	lr: 1.001e-04, eta: 8:27:47, time: 1.824, data_time: 0.055, memory: 17921, loss_cls: 0.0803, loss_bbox: 0.2169, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1187, d1.loss_bbox: 0.2523, d2.loss_cls: 0.0991, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0871, d3.loss_bbox: 0.2282, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2192, loss: 2.1333, grad_norm: 24.7456
2025-06-18 06:44:34,643 - mmdet - INFO - Epoch [4][5150/7033]	lr: 1.001e-04, eta: 8:26:11, time: 1.892, data_time: 0.052, memory: 17921, loss_cls: 0.0687, loss_bbox: 0.2025, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3194, d1.loss_cls: 0.1091, d1.loss_bbox: 0.2358, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2161, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0715, d4.loss_bbox: 0.2043, loss: 1.9823, grad_norm: 33.6143
2025-06-18 06:46:03,584 - mmdet - INFO - Epoch [4][5200/7033]	lr: 1.001e-04, eta: 8:24:32, time: 1.780, data_time: 0.059, memory: 17921, loss_cls: 0.0639, loss_bbox: 0.2013, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3172, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2383, d2.loss_cls: 0.0810, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0654, d4.loss_bbox: 0.2043, loss: 1.9454, grad_norm: 21.4218
2025-06-18 06:47:35,848 - mmdet - INFO - Epoch [4][5250/7033]	lr: 1.001e-04, eta: 8:22:55, time: 1.845, data_time: 0.057, memory: 17921, loss_cls: 0.0684, loss_bbox: 0.1975, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3018, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0824, d2.loss_bbox: 0.2106, d3.loss_cls: 0.0736, d3.loss_bbox: 0.2071, d4.loss_cls: 0.0699, d4.loss_bbox: 0.1991, loss: 1.9025, grad_norm: 26.0063
2025-06-18 06:49:14,596 - mmdet - INFO - Epoch [4][5300/7033]	lr: 1.001e-04, eta: 8:21:22, time: 1.974, data_time: 0.082, memory: 17921, loss_cls: 0.0732, loss_bbox: 0.2026, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3155, d1.loss_cls: 0.1064, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0897, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0780, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2057, loss: 1.9840, grad_norm: 41.8100
2025-06-18 06:50:52,613 - mmdet - INFO - Epoch [4][5350/7033]	lr: 1.001e-04, eta: 8:19:48, time: 1.961, data_time: 0.074, memory: 17921, loss_cls: 0.0636, loss_bbox: 0.1994, d0.loss_cls: 0.1573, d0.loss_bbox: 0.3176, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2383, d2.loss_cls: 0.0819, d2.loss_bbox: 0.2127, d3.loss_cls: 0.0697, d3.loss_bbox: 0.2106, d4.loss_cls: 0.0648, d4.loss_bbox: 0.2010, loss: 1.9161, grad_norm: 89.7708
2025-06-18 06:52:29,526 - mmdet - INFO - Epoch [4][5400/7033]	lr: 1.001e-04, eta: 8:18:14, time: 1.938, data_time: 0.069, memory: 17921, loss_cls: 0.0717, loss_bbox: 0.2044, d0.loss_cls: 0.1607, d0.loss_bbox: 0.3201, d1.loss_cls: 0.1024, d1.loss_bbox: 0.2404, d2.loss_cls: 0.0874, d2.loss_bbox: 0.2198, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0733, d4.loss_bbox: 0.2076, loss: 1.9835, grad_norm: 20.9844
2025-06-18 06:54:03,238 - mmdet - INFO - Epoch [4][5450/7033]	lr: 1.001e-04, eta: 8:16:38, time: 1.874, data_time: 0.062, memory: 17921, loss_cls: 0.0634, loss_bbox: 0.1913, d0.loss_cls: 0.1549, d0.loss_bbox: 0.3070, d1.loss_cls: 0.0950, d1.loss_bbox: 0.2275, d2.loss_cls: 0.0781, d2.loss_bbox: 0.2056, d3.loss_cls: 0.0688, d3.loss_bbox: 0.2030, d4.loss_cls: 0.0656, d4.loss_bbox: 0.1929, loss: 1.8529, grad_norm: 21.9969
2025-06-18 06:55:35,002 - mmdet - INFO - Epoch [4][5500/7033]	lr: 1.001e-04, eta: 8:15:01, time: 1.835, data_time: 0.066, memory: 17921, loss_cls: 0.0660, loss_bbox: 0.1986, d0.loss_cls: 0.1609, d0.loss_bbox: 0.3183, d1.loss_cls: 0.1022, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0829, d2.loss_bbox: 0.2145, d3.loss_cls: 0.0709, d3.loss_bbox: 0.2112, d4.loss_cls: 0.0678, d4.loss_bbox: 0.2016, loss: 1.9295, grad_norm: 28.2879
2025-06-18 06:57:08,750 - mmdet - INFO - Epoch [4][5550/7033]	lr: 1.001e-04, eta: 8:13:25, time: 1.875, data_time: 0.054, memory: 17921, loss_cls: 0.0689, loss_bbox: 0.1977, d0.loss_cls: 0.1619, d0.loss_bbox: 0.3194, d1.loss_cls: 0.1036, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2117, d3.loss_cls: 0.0751, d3.loss_bbox: 0.2087, d4.loss_cls: 0.0708, d4.loss_bbox: 0.2000, loss: 1.9366, grad_norm: 49.6905
2025-06-18 06:58:45,483 - mmdet - INFO - Epoch [4][5600/7033]	lr: 1.001e-04, eta: 8:11:50, time: 1.935, data_time: 0.058, memory: 17921, loss_cls: 0.0632, loss_bbox: 0.1902, d0.loss_cls: 0.1572, d0.loss_bbox: 0.3074, d1.loss_cls: 0.0969, d1.loss_bbox: 0.2273, d2.loss_cls: 0.0796, d2.loss_bbox: 0.2076, d3.loss_cls: 0.0673, d3.loss_bbox: 0.2043, d4.loss_cls: 0.0634, d4.loss_bbox: 0.1936, loss: 1.8580, grad_norm: 24.6654
2025-06-18 07:00:18,807 - mmdet - INFO - Epoch [4][5650/7033]	lr: 1.001e-04, eta: 8:10:14, time: 1.866, data_time: 0.064, memory: 17921, loss_cls: 0.0721, loss_bbox: 0.2091, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3276, d1.loss_cls: 0.1062, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0888, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2187, d4.loss_cls: 0.0739, d4.loss_bbox: 0.2111, loss: 2.0200, grad_norm: 21.9111
2025-06-18 07:01:51,991 - mmdet - INFO - Epoch [4][5700/7033]	lr: 1.001e-04, eta: 8:08:38, time: 1.864, data_time: 0.058, memory: 17921, loss_cls: 0.0770, loss_bbox: 0.2034, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3275, d1.loss_cls: 0.1079, d1.loss_bbox: 0.2459, d2.loss_cls: 0.0890, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2061, loss: 2.0229, grad_norm: 28.5482
2025-06-18 07:03:22,261 - mmdet - INFO - Epoch [4][5750/7033]	lr: 1.001e-04, eta: 8:07:00, time: 1.805, data_time: 0.057, memory: 17921, loss_cls: 0.0688, loss_bbox: 0.2087, d0.loss_cls: 0.1618, d0.loss_bbox: 0.3306, d1.loss_cls: 0.1010, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0839, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0737, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0709, d4.loss_bbox: 0.2118, loss: 2.0051, grad_norm: 16.5503
2025-06-18 07:04:49,944 - mmdet - INFO - Epoch [4][5800/7033]	lr: 1.001e-04, eta: 8:05:20, time: 1.754, data_time: 0.060, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2005, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3174, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2371, d2.loss_cls: 0.0883, d2.loss_bbox: 0.2143, d3.loss_cls: 0.0773, d3.loss_bbox: 0.2120, d4.loss_cls: 0.0732, d4.loss_bbox: 0.2040, loss: 1.9631, grad_norm: 21.3542
2025-06-18 07:06:23,450 - mmdet - INFO - Epoch [4][5850/7033]	lr: 1.001e-04, eta: 8:03:44, time: 1.870, data_time: 0.068, memory: 17921, loss_cls: 0.0732, loss_bbox: 0.2131, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3279, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2464, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0757, d4.loss_bbox: 0.2161, loss: 2.0538, grad_norm: 31.0622
2025-06-18 07:07:54,708 - mmdet - INFO - Epoch [4][5900/7033]	lr: 1.001e-04, eta: 8:02:07, time: 1.825, data_time: 0.076, memory: 17921, loss_cls: 0.0687, loss_bbox: 0.2021, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3174, d1.loss_cls: 0.1039, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0856, d2.loss_bbox: 0.2156, d3.loss_cls: 0.0745, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0706, d4.loss_bbox: 0.2051, loss: 1.9565, grad_norm: 27.5859
2025-06-18 07:09:35,801 - mmdet - INFO - Epoch [4][5950/7033]	lr: 1.001e-04, eta: 8:00:35, time: 2.022, data_time: 0.055, memory: 17921, loss_cls: 0.0674, loss_bbox: 0.1939, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3215, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0849, d2.loss_bbox: 0.2138, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0702, d4.loss_bbox: 0.1982, loss: 1.9297, grad_norm: 22.0149
2025-06-18 07:11:11,424 - mmdet - INFO - Epoch [4][6000/7033]	lr: 1.001e-04, eta: 7:59:00, time: 1.912, data_time: 0.058, memory: 17921, loss_cls: 0.0689, loss_bbox: 0.1990, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3173, d1.loss_cls: 0.0999, d1.loss_bbox: 0.2349, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2149, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2129, d4.loss_cls: 0.0686, d4.loss_bbox: 0.2039, loss: 1.9465, grad_norm: 18.5473
2025-06-18 07:12:46,849 - mmdet - INFO - Epoch [4][6050/7033]	lr: 1.001e-04, eta: 7:57:25, time: 1.907, data_time: 0.078, memory: 17921, loss_cls: 0.0656, loss_bbox: 0.1982, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3210, d1.loss_cls: 0.1045, d1.loss_bbox: 0.2380, d2.loss_cls: 0.0846, d2.loss_bbox: 0.2158, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0681, d4.loss_bbox: 0.2011, loss: 1.9482, grad_norm: 21.9152
2025-06-18 07:14:25,421 - mmdet - INFO - Epoch [4][6100/7033]	lr: 1.001e-04, eta: 7:55:52, time: 1.972, data_time: 0.057, memory: 17921, loss_cls: 0.0642, loss_bbox: 0.1990, d0.loss_cls: 0.1602, d0.loss_bbox: 0.3185, d1.loss_cls: 0.0966, d1.loss_bbox: 0.2381, d2.loss_cls: 0.0800, d2.loss_bbox: 0.2159, d3.loss_cls: 0.0682, d3.loss_bbox: 0.2127, d4.loss_cls: 0.0649, d4.loss_bbox: 0.2024, loss: 1.9205, grad_norm: 18.0180
2025-06-18 07:16:01,697 - mmdet - INFO - Epoch [4][6150/7033]	lr: 1.001e-04, eta: 7:54:17, time: 1.926, data_time: 0.070, memory: 17921, loss_cls: 0.0742, loss_bbox: 0.2108, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3255, d1.loss_cls: 0.1070, d1.loss_bbox: 0.2473, d2.loss_cls: 0.0906, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2247, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2138, loss: 2.0426, grad_norm: 41.0931
2025-06-18 07:17:39,530 - mmdet - INFO - Epoch [4][6200/7033]	lr: 1.001e-04, eta: 7:52:43, time: 1.958, data_time: 0.068, memory: 17921, loss_cls: 0.0670, loss_bbox: 0.2029, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3178, d1.loss_cls: 0.1006, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0843, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0722, d3.loss_bbox: 0.2144, d4.loss_cls: 0.0688, d4.loss_bbox: 0.2055, loss: 1.9474, grad_norm: 34.2894
2025-06-18 07:19:18,414 - mmdet - INFO - Epoch [4][6250/7033]	lr: 1.001e-04, eta: 7:51:10, time: 1.978, data_time: 0.055, memory: 17921, loss_cls: 0.0686, loss_bbox: 0.2002, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3201, d1.loss_cls: 0.1034, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0862, d2.loss_bbox: 0.2179, d3.loss_cls: 0.0757, d3.loss_bbox: 0.2131, d4.loss_cls: 0.0705, d4.loss_bbox: 0.2025, loss: 1.9636, grad_norm: 30.0177
2025-06-18 07:20:56,201 - mmdet - INFO - Epoch [4][6300/7033]	lr: 1.001e-04, eta: 7:49:37, time: 1.956, data_time: 0.072, memory: 17921, loss_cls: 0.0703, loss_bbox: 0.2064, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3299, d1.loss_cls: 0.1037, d1.loss_bbox: 0.2480, d2.loss_cls: 0.0866, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0742, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0729, d4.loss_bbox: 0.2090, loss: 2.0099, grad_norm: 39.2271
2025-06-18 07:22:26,537 - mmdet - INFO - Epoch [4][6350/7033]	lr: 1.001e-04, eta: 7:47:59, time: 1.807, data_time: 0.065, memory: 17921, loss_cls: 0.0680, loss_bbox: 0.2040, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3224, d1.loss_cls: 0.1059, d1.loss_bbox: 0.2411, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0741, d3.loss_bbox: 0.2166, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2067, loss: 1.9791, grad_norm: 44.8508
2025-06-18 07:23:59,615 - mmdet - INFO - Epoch [4][6400/7033]	lr: 1.001e-04, eta: 7:46:22, time: 1.862, data_time: 0.080, memory: 17921, loss_cls: 0.0701, loss_bbox: 0.2004, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3126, d1.loss_cls: 0.1057, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0871, d2.loss_bbox: 0.2142, d3.loss_cls: 0.0749, d3.loss_bbox: 0.2116, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2035, loss: 1.9480, grad_norm: 48.0252
2025-06-18 07:25:35,129 - mmdet - INFO - Epoch [4][6450/7033]	lr: 1.001e-04, eta: 7:44:47, time: 1.910, data_time: 0.062, memory: 17921, loss_cls: 0.0677, loss_bbox: 0.2008, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3101, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2153, d3.loss_cls: 0.0721, d3.loss_bbox: 0.2124, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2031, loss: 1.9326, grad_norm: 183.2657
2025-06-18 07:27:09,946 - mmdet - INFO - Epoch [4][6500/7033]	lr: 1.001e-04, eta: 7:43:12, time: 1.896, data_time: 0.063, memory: 17921, loss_cls: 0.0674, loss_bbox: 0.2013, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3208, d1.loss_cls: 0.1032, d1.loss_bbox: 0.2402, d2.loss_cls: 0.0858, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0723, d3.loss_bbox: 0.2143, d4.loss_cls: 0.0694, d4.loss_bbox: 0.2041, loss: 1.9642, grad_norm: 51.8286
2025-06-18 07:28:43,496 - mmdet - INFO - Epoch [4][6550/7033]	lr: 1.001e-04, eta: 7:41:36, time: 1.871, data_time: 0.071, memory: 17921, loss_cls: 0.0696, loss_bbox: 0.1993, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3131, d1.loss_cls: 0.0994, d1.loss_bbox: 0.2357, d2.loss_cls: 0.0826, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0752, d3.loss_bbox: 0.2114, d4.loss_cls: 0.0710, d4.loss_bbox: 0.2035, loss: 1.9332, grad_norm: 74.9865
2025-06-18 07:30:19,062 - mmdet - INFO - Epoch [4][6600/7033]	lr: 1.001e-04, eta: 7:40:01, time: 1.911, data_time: 0.070, memory: 17921, loss_cls: 0.0722, loss_bbox: 0.2015, d0.loss_cls: 0.1557, d0.loss_bbox: 0.3120, d1.loss_cls: 0.0983, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0875, d2.loss_bbox: 0.2151, d3.loss_cls: 0.0770, d3.loss_bbox: 0.2137, d4.loss_cls: 0.0742, d4.loss_bbox: 0.2031, loss: 1.9444, grad_norm: 44.6616
2025-06-18 07:31:51,315 - mmdet - INFO - Epoch [4][6650/7033]	lr: 1.001e-04, eta: 7:38:24, time: 1.845, data_time: 0.066, memory: 17921, loss_cls: 0.0679, loss_bbox: 0.1981, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3098, d1.loss_cls: 0.0986, d1.loss_bbox: 0.2329, d2.loss_cls: 0.0805, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0724, d3.loss_bbox: 0.2105, d4.loss_cls: 0.0704, d4.loss_bbox: 0.2003, loss: 1.9190, grad_norm: 24.5038
2025-06-18 07:33:23,579 - mmdet - INFO - Epoch [4][6700/7033]	lr: 1.001e-04, eta: 7:36:48, time: 1.846, data_time: 0.062, memory: 17921, loss_cls: 0.0805, loss_bbox: 0.2013, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3212, d1.loss_cls: 0.1108, d1.loss_bbox: 0.2375, d2.loss_cls: 0.0951, d2.loss_bbox: 0.2185, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2159, d4.loss_cls: 0.0839, d4.loss_bbox: 0.2038, loss: 2.0301, grad_norm: 31.1326
2025-06-18 07:34:53,998 - mmdet - INFO - Epoch [4][6750/7033]	lr: 1.001e-04, eta: 7:35:10, time: 1.808, data_time: 0.066, memory: 17921, loss_cls: 0.0770, loss_bbox: 0.2105, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2452, d2.loss_cls: 0.0930, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2199, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2113, loss: 2.0565, grad_norm: 29.9655
2025-06-18 07:36:24,733 - mmdet - INFO - Epoch [4][6800/7033]	lr: 1.001e-04, eta: 7:33:32, time: 1.814, data_time: 0.062, memory: 17921, loss_cls: 0.0626, loss_bbox: 0.1964, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3135, d1.loss_cls: 0.0990, d1.loss_bbox: 0.2338, d2.loss_cls: 0.0821, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0680, d3.loss_bbox: 0.2110, d4.loss_cls: 0.0641, d4.loss_bbox: 0.2003, loss: 1.9054, grad_norm: 39.4832
2025-06-18 07:37:53,210 - mmdet - INFO - Epoch [4][6850/7033]	lr: 1.001e-04, eta: 7:31:54, time: 1.770, data_time: 0.053, memory: 17921, loss_cls: 0.0595, loss_bbox: 0.1970, d0.loss_cls: 0.1567, d0.loss_bbox: 0.3115, d1.loss_cls: 0.0915, d1.loss_bbox: 0.2327, d2.loss_cls: 0.0765, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0658, d3.loss_bbox: 0.2075, d4.loss_cls: 0.0628, d4.loss_bbox: 0.1977, loss: 1.8696, grad_norm: 26.0454
2025-06-18 07:39:26,032 - mmdet - INFO - Epoch [4][6900/7033]	lr: 1.001e-04, eta: 7:30:18, time: 1.855, data_time: 0.059, memory: 17921, loss_cls: 0.0706, loss_bbox: 0.2040, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1026, d1.loss_bbox: 0.2451, d2.loss_cls: 0.0861, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0765, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0722, d4.loss_bbox: 0.2062, loss: 2.0036, grad_norm: 486.1757
2025-06-18 07:40:57,682 - mmdet - INFO - Epoch [4][6950/7033]	lr: 1.001e-04, eta: 7:28:41, time: 1.834, data_time: 0.060, memory: 17921, loss_cls: 0.0670, loss_bbox: 0.2066, d0.loss_cls: 0.1559, d0.loss_bbox: 0.3183, d1.loss_cls: 0.1002, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0847, d2.loss_bbox: 0.2215, d3.loss_cls: 0.0719, d3.loss_bbox: 0.2190, d4.loss_cls: 0.0696, d4.loss_bbox: 0.2088, loss: 1.9633, grad_norm: 32.1255
2025-06-18 07:42:31,605 - mmdet - INFO - Epoch [4][7000/7033]	lr: 1.001e-04, eta: 7:27:05, time: 1.879, data_time: 0.069, memory: 17921, loss_cls: 0.0698, loss_bbox: 0.2021, d0.loss_cls: 0.1571, d0.loss_bbox: 0.3144, d1.loss_cls: 0.1013, d1.loss_bbox: 0.2364, d2.loss_cls: 0.0835, d2.loss_bbox: 0.2164, d3.loss_cls: 0.0732, d3.loss_bbox: 0.2134, d4.loss_cls: 0.0714, d4.loss_bbox: 0.2035, loss: 1.9424, grad_norm: 41.8452
2025-06-18 07:43:34,472 - mmdet - INFO - Saving checkpoint at 4 epochs
