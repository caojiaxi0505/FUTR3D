2025-04-05 20:21:18,639 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 | packaged by conda-forge | (default, Sep 30 2024, 17:52:49) [GCC 13.3.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.6, V11.6.55
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.1-Product Build 20220311 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+9130d71
spconv2.0: False
------------------------------------------------------------

2025-04-05 20:21:19,420 - mmdet - INFO - Distributed training: False
2025-04-05 20:21:20,184 - mmdet - INFO - Config:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=False,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='ObjectSample',
        db_sampler=dict(
            data_root='data/nuscenes/',
            info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
            rate=1.0,
            prepare=dict(
                filter_by_difficulty=[-1],
                filter_by_min_points=dict(
                    car=5,
                    truck=5,
                    bus=5,
                    trailer=5,
                    construction_vehicle=5,
                    traffic_cone=5,
                    barrier=5,
                    motorcycle=5,
                    bicycle=5,
                    pedestrian=5)),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            sample_groups=dict(
                car=2,
                truck=3,
                construction_vehicle=7,
                bus=4,
                trailer=6,
                barrier=2,
                motorcycle=6,
                bicycle=6,
                pedestrian=2,
                traffic_cone=2),
            points_loader=dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk')))),
    dict(
        type='GlobalRotScaleTrans',
        rot_range=[-0.785, 0.785],
        scale_ratio_range=[0.9, 1.1],
        translation_std=[0.5, 0.5, 0.5]),
    dict(
        type='RandomFlip3D',
        sync_2d=False,
        flip_ratio_bev_horizontal=0.5,
        flip_ratio_bev_vertical=0.5),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(type='Collect3D', keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='CBGSDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=10,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[-0.3925, 0.3925],
                scale_ratio_range=[0.95, 1.05],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-50, -50, -5, 50, 50, 3]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-50, -50, -5, 50, 50, 3]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                    'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                    'barrier'
                ]),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                    'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                    'barrier'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        dataset=dict(
            type='NuScenesDataset',
            data_root='data/nuscenes/',
            ann_file='data/nuscenes/nuscenes_infos_train.pkl',
            pipeline=[
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=5,
                    use_dim=5,
                    file_client_args=dict(backend='disk')),
                dict(
                    type='LoadPointsFromMultiSweeps',
                    sweeps_num=9,
                    use_dim=[0, 1, 2, 3, 4],
                    file_client_args=dict(backend='disk'),
                    pad_empty_sweeps=True,
                    remove_close=True),
                dict(
                    type='LoadAnnotations3D',
                    with_bbox_3d=True,
                    with_label_3d=True),
                dict(
                    type='ObjectSample',
                    db_sampler=dict(
                        data_root='data/nuscenes/',
                        info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
                        rate=1.0,
                        prepare=dict(
                            filter_by_difficulty=[-1],
                            filter_by_min_points=dict(
                                car=5,
                                truck=5,
                                bus=5,
                                trailer=5,
                                construction_vehicle=5,
                                traffic_cone=5,
                                barrier=5,
                                motorcycle=5,
                                bicycle=5,
                                pedestrian=5)),
                        classes=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        sample_groups=dict(
                            car=2,
                            truck=3,
                            construction_vehicle=7,
                            bus=4,
                            trailer=6,
                            barrier=2,
                            motorcycle=6,
                            bicycle=6,
                            pedestrian=2,
                            traffic_cone=2),
                        points_loader=dict(
                            type='LoadPointsFromFile',
                            coord_type='LIDAR',
                            load_dim=5,
                            use_dim=[0, 1, 2, 3, 4],
                            file_client_args=dict(backend='disk')))),
                dict(
                    type='GlobalRotScaleTrans',
                    rot_range=[-0.785, 0.785],
                    scale_ratio_range=[0.9, 1.1],
                    translation_std=[0.5, 0.5, 0.5]),
                dict(
                    type='RandomFlip3D',
                    sync_2d=False,
                    flip_ratio_bev_horizontal=0.5,
                    flip_ratio_bev_vertical=0.5),
                dict(
                    type='PointsRangeFilter',
                    point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                dict(
                    type='ObjectRangeFilter',
                    point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                dict(
                    type='ObjectNameFilter',
                    classes=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ]),
                dict(type='PointShuffle'),
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ]),
                dict(
                    type='Collect3D',
                    keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
            ],
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            test_mode=False,
            use_valid_flag=True,
            box_type_3d='LiDAR')),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_test.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
optimizer = dict(type='AdamW', lr=6.25e-06, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='cyclic',
    target_ratio=(10, 0.0001),
    cyclic_times=1,
    step_ratio_up=0.4)
momentum_config = dict(
    policy='cyclic',
    target_ratio=(0.8947368421052632, 1),
    cyclic_times=1,
    step_ratio_up=0.4)
runner = dict(type='EpochBasedRunner', max_epochs=20)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/dss3_small_morton_conv'
load_from = None
resume_from = 'work_dirs/dss3_small_morton_conv/epoch_3.pth'
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    aux_weight=0.5,
    pts_voxel_layer=dict(
        max_num_points=10,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(120000, 160000),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(type='HardSimpleVFE', num_features=5),
    pts_middle_encoder=dict(
        type='SparseEncoder',
        in_channels=5,
        sparse_shape=[41, 1440, 1440],
        output_channels=128,
        order=('conv', 'norm', 'act'),
        encoder_channels=((16, 16, 32), (32, 32, 64), (64, 64, 128), (128,
                                                                      128)),
        encoder_paddings=((0, 0, 1), (0, 0, 1), (0, 0, [0, 1, 1]), (0, 0)),
        block_type='basicblock'),
    pts_backbone=dict(
        type='SECOND',
        in_channels=256,
        out_channels=[128, 256],
        layer_nums=[5, 5],
        layer_strides=[1, 2],
        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
        conv_cfg=dict(type='Conv2d', bias=False)),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[128, 256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        dss_batch_first=False,
        dss_drop_prob=0.1,
        dss_mamba_prenorm=False,
        dss_mamba_cfg=dict(),
        dss_mamba_version='dssmamba_s_morton_conv',
        dss_num_layers=3,
        anchor_size=3,
        use_aux=True,
        aux_head=dict(
            type='CenterHead',
            in_channels=512,
            tasks=[
                dict(num_class=1, class_names=['car']),
                dict(
                    num_class=2, class_names=['truck',
                                              'construction_vehicle']),
                dict(num_class=2, class_names=['bus', 'trailer']),
                dict(num_class=1, class_names=['barrier']),
                dict(num_class=2, class_names=['motorcycle', 'bicycle']),
                dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
            ],
            common_heads=dict(
                reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
            share_conv_channel=64,
            bbox_coder=dict(
                type='CenterPointBBoxCoder',
                pc_range=[-54, -54],
                post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
                max_num=500,
                score_threshold=0.1,
                out_size_factor=8,
                voxel_size=[0.075, 0.075],
                code_size=9),
            separate_head=dict(
                type='SeparateHead', init_bias=-2.19, final_kernel=3),
            loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
            loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
            norm_bbox=True),
        mix_selection=False,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(type='FUTR3DAttention', embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
custom_hooks = [dict(type='FadeOjectSampleHook', num_last_epochs=5)]
gpu_ids = [0]

2025-04-05 20:21:20,185 - mmdet - INFO - Set random seed to 0, deterministic: False
2025-04-05 20:21:20,374 - mmdet - INFO - initialize SECOND with init_cfg {'type': 'Kaiming', 'layer': 'Conv2d'}
2025-04-05 20:21:20,385 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_middle_encoder.conv_input.0.weight - torch.Size([3, 3, 3, 5, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_input.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_input.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.conv1.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.conv2.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.conv1.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.conv2.weight - torch.Size([3, 3, 3, 16, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.0.weight - torch.Size([3, 3, 3, 16, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.conv1.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.conv2.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.conv1.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.conv2.weight - torch.Size([3, 3, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.0.weight - torch.Size([3, 3, 3, 32, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.conv1.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.conv2.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.conv1.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.conv2.weight - torch.Size([3, 3, 3, 64, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.0.weight - torch.Size([3, 3, 3, 64, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.conv1.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.conv2.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.conv1.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.conv2.weight - torch.Size([3, 3, 3, 128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([3, 1, 1, 128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.0.weight - torch.Size([128, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.3.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.6.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.7.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.7.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.9.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.10.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.10.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.12.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.13.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.13.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.15.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.16.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.16.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.0.weight - torch.Size([256, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.3.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.6.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.9.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.10.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.10.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.12.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.13.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.13.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.15.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.16.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.16.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 128, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.1.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.A_log_f - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.A_log_b - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.A_log_f_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.A_log_b_down - torch.Size([256, 128]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.D_f_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.D_b_down - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.in_proj.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.x_proj_f.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.x_proj_b.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.conv_f_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.conv_b_down.weight - torch.Size([256, 1, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.x_proj_f_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.x_proj_b_down.weight - torch.Size([272, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_f_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_f_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_b_down.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.dt_proj_b_down.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.2.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.conv.weight - torch.Size([64, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-04-05 20:21:20,469 - mmdet - INFO - Model:
FUTR3D(
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=10, max_voxels=(120000, 160000), deterministic=True)
  (pts_voxel_encoder): HardSimpleVFE()
  (pts_middle_encoder): SparseEncoder(
    (conv_input): SparseSequential(
      (0): SubMConv3d()
      (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (encoder_layers): SparseSequential(
      (encoder_layer1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer2): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer3): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d()
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer4): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d()
          (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d()
          (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d()
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (pts_backbone): SECOND(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (11): ReLU(inplace=True)
        (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (14): ReLU(inplace=True)
        (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (17): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (11): ReLU(inplace=True)
        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (14): ReLU(inplace=True)
        (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (17): ReLU(inplace=True)
      )
    )
  )
  init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.050)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (2): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=2048, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (conv_f_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (conv_b_down): Conv1d(256, 256, kernel_size=(4,), stride=(1,), groups=256, bias=False)
                      (act_f_down): SiLU()
                      (act_b_down): SiLU()
                      (x_proj_f_down): Linear(in_features=256, out_features=272, bias=False)
                      (x_proj_b_down): Linear(in_features=256, out_features=272, bias=False)
                      (dt_proj_f_down): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_down): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=10, bias=True)
      (1): Linear(in_features=256, out_features=10, bias=True)
      (2): Linear(in_features=256, out_features=10, bias=True)
      (3): Linear(in_features=256, out_features=10, bias=True)
      (4): Linear(in_features=256, out_features=10, bias=True)
      (5): Linear(in_features=256, out_features=10, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
    (aux_head): CenterHead(
      (loss_cls): GaussianFocalLoss()
      (loss_bbox): L1Loss()
      (shared_conv): ConvModule(
        (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (task_heads): ModuleList(
        (0): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (1): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (2): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (3): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (4): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (5): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
      )
    )
  )
)
2025-04-05 20:21:46,964 - mmdet - INFO - load checkpoint from local path: work_dirs/dss3_small_morton_conv/epoch_3.pth
2025-04-05 20:21:55,151 - mmdet - INFO - resumed epoch 3, iter 185370
2025-04-05 20:21:55,154 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /home/ubuntu/jxcao/hdd/jxc/FUTR3D/work_dirs/dss3_small_morton_conv
2025-04-05 20:21:55,154 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) FadeOjectSampleHook                
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-05 20:21:55,154 - mmdet - INFO - workflow: [('train', 1)], max: 20 epochs
2025-04-05 20:21:55,154 - mmdet - INFO - Checkpoints will be saved to /home/ubuntu/jxcao/hdd/jxc/FUTR3D/work_dirs/dss3_small_morton_conv by HardDiskBackend.
2025-04-05 20:22:29,702 - mmdet - INFO - Epoch [4][50/61790]	lr: 2.362e-05, eta: 8 days, 8:47:50, time: 0.688, data_time: 0.086, memory: 7525, loss_cls: 0.3599, loss_bbox: 0.6472, d0.loss_cls: 0.4415, d0.loss_bbox: 0.8031, d1.loss_cls: 0.4055, d1.loss_bbox: 0.6940, d2.loss_cls: 0.3768, d2.loss_bbox: 0.6747, d3.loss_cls: 0.3664, d3.loss_bbox: 0.6619, d4.loss_cls: 0.3609, d4.loss_bbox: 0.6524, aux_task0.loss_heatmap: 0.5297, aux_task0.loss_bbox: 0.2028, aux_task1.loss_heatmap: 0.6976, aux_task1.loss_bbox: 0.2760, aux_task2.loss_heatmap: 0.7766, aux_task2.loss_bbox: 0.2908, aux_task3.loss_heatmap: 0.5025, aux_task3.loss_bbox: 0.2497, aux_task4.loss_heatmap: 0.4025, aux_task4.loss_bbox: 0.2278, aux_task5.loss_heatmap: 0.3961, aux_task5.loss_bbox: 0.2759, loss: 11.2722, grad_norm: 86.4616
2025-04-05 20:22:58,238 - mmdet - INFO - Epoch [4][100/61790]	lr: 2.363e-05, eta: 7 days, 15:39:04, time: 0.571, data_time: 0.007, memory: 7525, loss_cls: 0.3562, loss_bbox: 0.6443, d0.loss_cls: 0.4437, d0.loss_bbox: 0.7907, d1.loss_cls: 0.4005, d1.loss_bbox: 0.6879, d2.loss_cls: 0.3739, d2.loss_bbox: 0.6654, d3.loss_cls: 0.3630, d3.loss_bbox: 0.6556, d4.loss_cls: 0.3578, d4.loss_bbox: 0.6479, aux_task0.loss_heatmap: 0.5215, aux_task0.loss_bbox: 0.2032, aux_task1.loss_heatmap: 0.6943, aux_task1.loss_bbox: 0.2549, aux_task2.loss_heatmap: 0.6799, aux_task2.loss_bbox: 0.2979, aux_task3.loss_heatmap: 0.5231, aux_task3.loss_bbox: 0.2752, aux_task4.loss_heatmap: 0.3849, aux_task4.loss_bbox: 0.2302, aux_task5.loss_heatmap: 0.4378, aux_task5.loss_bbox: 0.2781, loss: 11.1679, grad_norm: 81.6937
2025-04-05 20:23:27,007 - mmdet - INFO - Epoch [4][150/61790]	lr: 2.364e-05, eta: 7 days, 10:22:48, time: 0.575, data_time: 0.007, memory: 7525, loss_cls: 0.3545, loss_bbox: 0.6543, d0.loss_cls: 0.4441, d0.loss_bbox: 0.7970, d1.loss_cls: 0.4009, d1.loss_bbox: 0.6977, d2.loss_cls: 0.3761, d2.loss_bbox: 0.6792, d3.loss_cls: 0.3636, d3.loss_bbox: 0.6675, d4.loss_cls: 0.3546, d4.loss_bbox: 0.6598, aux_task0.loss_heatmap: 0.5427, aux_task0.loss_bbox: 0.2072, aux_task1.loss_heatmap: 0.7120, aux_task1.loss_bbox: 0.2723, aux_task2.loss_heatmap: 0.6666, aux_task2.loss_bbox: 0.2766, aux_task3.loss_heatmap: 0.5546, aux_task3.loss_bbox: 0.2631, aux_task4.loss_heatmap: 0.3927, aux_task4.loss_bbox: 0.2336, aux_task5.loss_heatmap: 0.4318, aux_task5.loss_bbox: 0.2800, loss: 11.2824, grad_norm: 90.8098
2025-04-05 20:23:55,243 - mmdet - INFO - Epoch [4][200/61790]	lr: 2.364e-05, eta: 7 days, 6:57:57, time: 0.565, data_time: 0.007, memory: 7525, loss_cls: 0.3458, loss_bbox: 0.6370, d0.loss_cls: 0.4419, d0.loss_bbox: 0.7783, d1.loss_cls: 0.3956, d1.loss_bbox: 0.6799, d2.loss_cls: 0.3660, d2.loss_bbox: 0.6606, d3.loss_cls: 0.3547, d3.loss_bbox: 0.6525, d4.loss_cls: 0.3453, d4.loss_bbox: 0.6431, aux_task0.loss_heatmap: 0.4980, aux_task0.loss_bbox: 0.2052, aux_task1.loss_heatmap: 0.7510, aux_task1.loss_bbox: 0.2871, aux_task2.loss_heatmap: 0.6918, aux_task2.loss_bbox: 0.2736, aux_task3.loss_heatmap: 0.5562, aux_task3.loss_bbox: 0.2640, aux_task4.loss_heatmap: 0.3961, aux_task4.loss_bbox: 0.2350, aux_task5.loss_heatmap: 0.4032, aux_task5.loss_bbox: 0.2720, loss: 11.1339, grad_norm: 81.8600
2025-04-05 20:24:23,646 - mmdet - INFO - Epoch [4][250/61790]	lr: 2.365e-05, eta: 7 days, 5:06:24, time: 0.568, data_time: 0.006, memory: 7525, loss_cls: 0.3608, loss_bbox: 0.6488, d0.loss_cls: 0.4469, d0.loss_bbox: 0.7980, d1.loss_cls: 0.4058, d1.loss_bbox: 0.6967, d2.loss_cls: 0.3791, d2.loss_bbox: 0.6755, d3.loss_cls: 0.3673, d3.loss_bbox: 0.6629, d4.loss_cls: 0.3605, d4.loss_bbox: 0.6559, aux_task0.loss_heatmap: 0.5428, aux_task0.loss_bbox: 0.2071, aux_task1.loss_heatmap: 0.7123, aux_task1.loss_bbox: 0.2745, aux_task2.loss_heatmap: 0.6949, aux_task2.loss_bbox: 0.2960, aux_task3.loss_heatmap: 0.4941, aux_task3.loss_bbox: 0.2688, aux_task4.loss_heatmap: 0.3919, aux_task4.loss_bbox: 0.2264, aux_task5.loss_heatmap: 0.4007, aux_task5.loss_bbox: 0.2805, loss: 11.2481, grad_norm: 81.6366
2025-04-05 20:24:51,808 - mmdet - INFO - Epoch [4][300/61790]	lr: 2.366e-05, eta: 7 days, 3:37:55, time: 0.563, data_time: 0.006, memory: 7525, loss_cls: 0.3564, loss_bbox: 0.6537, d0.loss_cls: 0.4414, d0.loss_bbox: 0.7974, d1.loss_cls: 0.4059, d1.loss_bbox: 0.6960, d2.loss_cls: 0.3786, d2.loss_bbox: 0.6792, d3.loss_cls: 0.3682, d3.loss_bbox: 0.6658, d4.loss_cls: 0.3599, d4.loss_bbox: 0.6576, aux_task0.loss_heatmap: 0.5598, aux_task0.loss_bbox: 0.2085, aux_task1.loss_heatmap: 0.7282, aux_task1.loss_bbox: 0.2730, aux_task2.loss_heatmap: 0.6713, aux_task2.loss_bbox: 0.2823, aux_task3.loss_heatmap: 0.5640, aux_task3.loss_bbox: 0.2810, aux_task4.loss_heatmap: 0.3647, aux_task4.loss_bbox: 0.2262, aux_task5.loss_heatmap: 0.4194, aux_task5.loss_bbox: 0.2849, loss: 11.3235, grad_norm: 94.8086
2025-04-05 20:25:20,510 - mmdet - INFO - Epoch [4][350/61790]	lr: 2.367e-05, eta: 7 days, 3:01:33, time: 0.574, data_time: 0.008, memory: 7525, loss_cls: 0.3389, loss_bbox: 0.6291, d0.loss_cls: 0.4293, d0.loss_bbox: 0.7832, d1.loss_cls: 0.3869, d1.loss_bbox: 0.6789, d2.loss_cls: 0.3583, d2.loss_bbox: 0.6572, d3.loss_cls: 0.3481, d3.loss_bbox: 0.6435, d4.loss_cls: 0.3426, d4.loss_bbox: 0.6341, aux_task0.loss_heatmap: 0.5378, aux_task0.loss_bbox: 0.2189, aux_task1.loss_heatmap: 0.6504, aux_task1.loss_bbox: 0.2650, aux_task2.loss_heatmap: 0.6886, aux_task2.loss_bbox: 0.2786, aux_task3.loss_heatmap: 0.4922, aux_task3.loss_bbox: 0.2530, aux_task4.loss_heatmap: 0.3850, aux_task4.loss_bbox: 0.2217, aux_task5.loss_heatmap: 0.3972, aux_task5.loss_bbox: 0.2739, loss: 10.8925, grad_norm: 79.8186
2025-04-05 20:25:48,992 - mmdet - INFO - Epoch [4][400/61790]	lr: 2.368e-05, eta: 7 days, 2:24:32, time: 0.570, data_time: 0.006, memory: 7525, loss_cls: 0.3405, loss_bbox: 0.6280, d0.loss_cls: 0.4223, d0.loss_bbox: 0.7764, d1.loss_cls: 0.3817, d1.loss_bbox: 0.6786, d2.loss_cls: 0.3581, d2.loss_bbox: 0.6584, d3.loss_cls: 0.3461, d3.loss_bbox: 0.6429, d4.loss_cls: 0.3415, d4.loss_bbox: 0.6326, aux_task0.loss_heatmap: 0.5131, aux_task0.loss_bbox: 0.2086, aux_task1.loss_heatmap: 0.6927, aux_task1.loss_bbox: 0.2632, aux_task2.loss_heatmap: 0.6700, aux_task2.loss_bbox: 0.2950, aux_task3.loss_heatmap: 0.4765, aux_task3.loss_bbox: 0.2525, aux_task4.loss_heatmap: 0.3614, aux_task4.loss_bbox: 0.2202, aux_task5.loss_heatmap: 0.4278, aux_task5.loss_bbox: 0.2803, loss: 10.8682, grad_norm: 78.0930
2025-04-05 20:26:17,615 - mmdet - INFO - Epoch [4][450/61790]	lr: 2.369e-05, eta: 7 days, 2:01:09, time: 0.572, data_time: 0.007, memory: 7525, loss_cls: 0.3405, loss_bbox: 0.6241, d0.loss_cls: 0.4250, d0.loss_bbox: 0.7690, d1.loss_cls: 0.3884, d1.loss_bbox: 0.6705, d2.loss_cls: 0.3602, d2.loss_bbox: 0.6500, d3.loss_cls: 0.3505, d3.loss_bbox: 0.6379, d4.loss_cls: 0.3430, d4.loss_bbox: 0.6289, aux_task0.loss_heatmap: 0.5151, aux_task0.loss_bbox: 0.2029, aux_task1.loss_heatmap: 0.6651, aux_task1.loss_bbox: 0.2637, aux_task2.loss_heatmap: 0.6451, aux_task2.loss_bbox: 0.2763, aux_task3.loss_heatmap: 0.5100, aux_task3.loss_bbox: 0.2653, aux_task4.loss_heatmap: 0.4021, aux_task4.loss_bbox: 0.2225, aux_task5.loss_heatmap: 0.3767, aux_task5.loss_bbox: 0.2711, loss: 10.8041, grad_norm: 81.0553
2025-04-05 20:26:45,735 - mmdet - INFO - Epoch [4][500/61790]	lr: 2.369e-05, eta: 7 days, 1:24:43, time: 0.562, data_time: 0.007, memory: 7525, loss_cls: 0.3647, loss_bbox: 0.6440, d0.loss_cls: 0.4548, d0.loss_bbox: 0.7862, d1.loss_cls: 0.4115, d1.loss_bbox: 0.6889, d2.loss_cls: 0.3852, d2.loss_bbox: 0.6701, d3.loss_cls: 0.3718, d3.loss_bbox: 0.6579, d4.loss_cls: 0.3674, d4.loss_bbox: 0.6495, aux_task0.loss_heatmap: 0.5042, aux_task0.loss_bbox: 0.2045, aux_task1.loss_heatmap: 0.7172, aux_task1.loss_bbox: 0.2759, aux_task2.loss_heatmap: 0.7612, aux_task2.loss_bbox: 0.3022, aux_task3.loss_heatmap: 0.5513, aux_task3.loss_bbox: 0.2770, aux_task4.loss_heatmap: 0.4137, aux_task4.loss_bbox: 0.2332, aux_task5.loss_heatmap: 0.4082, aux_task5.loss_bbox: 0.2796, loss: 11.3801, grad_norm: 93.3382
2025-04-05 20:27:14,210 - mmdet - INFO - Epoch [4][550/61790]	lr: 2.370e-05, eta: 7 days, 1:06:08, time: 0.570, data_time: 0.007, memory: 7525, loss_cls: 0.3680, loss_bbox: 0.6549, d0.loss_cls: 0.4575, d0.loss_bbox: 0.8012, d1.loss_cls: 0.4170, d1.loss_bbox: 0.7003, d2.loss_cls: 0.3883, d2.loss_bbox: 0.6791, d3.loss_cls: 0.3752, d3.loss_bbox: 0.6678, d4.loss_cls: 0.3689, d4.loss_bbox: 0.6613, aux_task0.loss_heatmap: 0.5302, aux_task0.loss_bbox: 0.2108, aux_task1.loss_heatmap: 0.6979, aux_task1.loss_bbox: 0.2741, aux_task2.loss_heatmap: 0.7269, aux_task2.loss_bbox: 0.3025, aux_task3.loss_heatmap: 0.5327, aux_task3.loss_bbox: 0.2686, aux_task4.loss_heatmap: 0.4213, aux_task4.loss_bbox: 0.2351, aux_task5.loss_heatmap: 0.4357, aux_task5.loss_bbox: 0.2781, loss: 11.4536, grad_norm: 82.9805
2025-04-05 20:27:42,599 - mmdet - INFO - Epoch [4][600/61790]	lr: 2.371e-05, eta: 7 days, 0:48:02, time: 0.568, data_time: 0.007, memory: 7525, loss_cls: 0.3332, loss_bbox: 0.6241, d0.loss_cls: 0.4226, d0.loss_bbox: 0.7731, d1.loss_cls: 0.3768, d1.loss_bbox: 0.6732, d2.loss_cls: 0.3502, d2.loss_bbox: 0.6524, d3.loss_cls: 0.3392, d3.loss_bbox: 0.6378, d4.loss_cls: 0.3340, d4.loss_bbox: 0.6290, aux_task0.loss_heatmap: 0.4812, aux_task0.loss_bbox: 0.2040, aux_task1.loss_heatmap: 0.7356, aux_task1.loss_bbox: 0.2690, aux_task2.loss_heatmap: 0.6151, aux_task2.loss_bbox: 0.2823, aux_task3.loss_heatmap: 0.5717, aux_task3.loss_bbox: 0.2551, aux_task4.loss_heatmap: 0.3867, aux_task4.loss_bbox: 0.2242, aux_task5.loss_heatmap: 0.3820, aux_task5.loss_bbox: 0.2719, loss: 10.8244, grad_norm: 111.5706
2025-04-05 20:28:11,156 - mmdet - INFO - Epoch [4][650/61790]	lr: 2.372e-05, eta: 7 days, 0:37:11, time: 0.571, data_time: 0.007, memory: 7525, loss_cls: 0.3755, loss_bbox: 0.6611, d0.loss_cls: 0.4690, d0.loss_bbox: 0.8195, d1.loss_cls: 0.4239, d1.loss_bbox: 0.7116, d2.loss_cls: 0.3971, d2.loss_bbox: 0.6869, d3.loss_cls: 0.3826, d3.loss_bbox: 0.6760, d4.loss_cls: 0.3768, d4.loss_bbox: 0.6667, aux_task0.loss_heatmap: 0.5689, aux_task0.loss_bbox: 0.2116, aux_task1.loss_heatmap: 0.7947, aux_task1.loss_bbox: 0.2843, aux_task2.loss_heatmap: 0.7452, aux_task2.loss_bbox: 0.3086, aux_task3.loss_heatmap: 0.5670, aux_task3.loss_bbox: 0.2664, aux_task4.loss_heatmap: 0.3938, aux_task4.loss_bbox: 0.2265, aux_task5.loss_heatmap: 0.4104, aux_task5.loss_bbox: 0.2782, loss: 11.7023, grad_norm: 85.8570
2025-04-05 20:28:39,650 - mmdet - INFO - Epoch [4][700/61790]	lr: 2.373e-05, eta: 7 days, 0:26:15, time: 0.570, data_time: 0.007, memory: 7525, loss_cls: 0.3717, loss_bbox: 0.6503, d0.loss_cls: 0.4608, d0.loss_bbox: 0.7992, d1.loss_cls: 0.4199, d1.loss_bbox: 0.6924, d2.loss_cls: 0.3903, d2.loss_bbox: 0.6745, d3.loss_cls: 0.3794, d3.loss_bbox: 0.6634, d4.loss_cls: 0.3729, d4.loss_bbox: 0.6549, aux_task0.loss_heatmap: 0.4864, aux_task0.loss_bbox: 0.2003, aux_task1.loss_heatmap: 0.7347, aux_task1.loss_bbox: 0.2901, aux_task2.loss_heatmap: 0.7213, aux_task2.loss_bbox: 0.2826, aux_task3.loss_heatmap: 0.5859, aux_task3.loss_bbox: 0.2915, aux_task4.loss_heatmap: 0.4228, aux_task4.loss_bbox: 0.2306, aux_task5.loss_heatmap: 0.4414, aux_task5.loss_bbox: 0.2874, loss: 11.5052, grad_norm: 79.9031
2025-04-05 20:29:08,521 - mmdet - INFO - Epoch [4][750/61790]	lr: 2.374e-05, eta: 7 days, 0:25:30, time: 0.577, data_time: 0.007, memory: 7525, loss_cls: 0.3585, loss_bbox: 0.6421, d0.loss_cls: 0.4483, d0.loss_bbox: 0.7910, d1.loss_cls: 0.4047, d1.loss_bbox: 0.6884, d2.loss_cls: 0.3807, d2.loss_bbox: 0.6671, d3.loss_cls: 0.3682, d3.loss_bbox: 0.6548, d4.loss_cls: 0.3594, d4.loss_bbox: 0.6473, aux_task0.loss_heatmap: 0.5321, aux_task0.loss_bbox: 0.2058, aux_task1.loss_heatmap: 0.7176, aux_task1.loss_bbox: 0.2804, aux_task2.loss_heatmap: 0.7370, aux_task2.loss_bbox: 0.2870, aux_task3.loss_heatmap: 0.5396, aux_task3.loss_bbox: 0.2781, aux_task4.loss_heatmap: 0.3788, aux_task4.loss_bbox: 0.2272, aux_task5.loss_heatmap: 0.4132, aux_task5.loss_bbox: 0.2755, loss: 11.2830, grad_norm: 86.0926
2025-04-05 20:29:37,164 - mmdet - INFO - Epoch [4][800/61790]	lr: 2.374e-05, eta: 7 days, 0:19:46, time: 0.573, data_time: 0.006, memory: 7525, loss_cls: 0.3463, loss_bbox: 0.6331, d0.loss_cls: 0.4345, d0.loss_bbox: 0.7848, d1.loss_cls: 0.3920, d1.loss_bbox: 0.6814, d2.loss_cls: 0.3659, d2.loss_bbox: 0.6578, d3.loss_cls: 0.3538, d3.loss_bbox: 0.6468, d4.loss_cls: 0.3476, d4.loss_bbox: 0.6381, aux_task0.loss_heatmap: 0.5185, aux_task0.loss_bbox: 0.2052, aux_task1.loss_heatmap: 0.6878, aux_task1.loss_bbox: 0.2722, aux_task2.loss_heatmap: 0.7118, aux_task2.loss_bbox: 0.2822, aux_task3.loss_heatmap: 0.5292, aux_task3.loss_bbox: 0.2627, aux_task4.loss_heatmap: 0.3776, aux_task4.loss_bbox: 0.2221, aux_task5.loss_heatmap: 0.4069, aux_task5.loss_bbox: 0.2739, loss: 11.0323, grad_norm: 87.2771
2025-04-05 20:30:05,710 - mmdet - INFO - Epoch [4][850/61790]	lr: 2.375e-05, eta: 7 days, 0:12:42, time: 0.571, data_time: 0.007, memory: 7525, loss_cls: 0.3587, loss_bbox: 0.6431, d0.loss_cls: 0.4405, d0.loss_bbox: 0.7844, d1.loss_cls: 0.4054, d1.loss_bbox: 0.6866, d2.loss_cls: 0.3797, d2.loss_bbox: 0.6673, d3.loss_cls: 0.3659, d3.loss_bbox: 0.6572, d4.loss_cls: 0.3595, d4.loss_bbox: 0.6496, aux_task0.loss_heatmap: 0.5347, aux_task0.loss_bbox: 0.2110, aux_task1.loss_heatmap: 0.6872, aux_task1.loss_bbox: 0.2785, aux_task2.loss_heatmap: 0.7498, aux_task2.loss_bbox: 0.2952, aux_task3.loss_heatmap: 0.5163, aux_task3.loss_bbox: 0.2540, aux_task4.loss_heatmap: 0.4142, aux_task4.loss_bbox: 0.2292, aux_task5.loss_heatmap: 0.3935, aux_task5.loss_bbox: 0.2791, loss: 11.2404, grad_norm: 94.6236
2025-04-05 20:30:34,394 - mmdet - INFO - Epoch [4][900/61790]	lr: 2.376e-05, eta: 7 days, 0:09:02, time: 0.574, data_time: 0.007, memory: 7525, loss_cls: 0.3516, loss_bbox: 0.6481, d0.loss_cls: 0.4435, d0.loss_bbox: 0.7922, d1.loss_cls: 0.3994, d1.loss_bbox: 0.6884, d2.loss_cls: 0.3691, d2.loss_bbox: 0.6695, d3.loss_cls: 0.3565, d3.loss_bbox: 0.6612, d4.loss_cls: 0.3512, d4.loss_bbox: 0.6540, aux_task0.loss_heatmap: 0.4927, aux_task0.loss_bbox: 0.2016, aux_task1.loss_heatmap: 0.6617, aux_task1.loss_bbox: 0.2697, aux_task2.loss_heatmap: 0.7354, aux_task2.loss_bbox: 0.2866, aux_task3.loss_heatmap: 0.5065, aux_task3.loss_bbox: 0.2600, aux_task4.loss_heatmap: 0.4382, aux_task4.loss_bbox: 0.2316, aux_task5.loss_heatmap: 0.4118, aux_task5.loss_bbox: 0.2780, loss: 11.1583, grad_norm: 95.3974
2025-04-05 20:31:02,862 - mmdet - INFO - Epoch [4][950/61790]	lr: 2.377e-05, eta: 7 days, 0:01:43, time: 0.569, data_time: 0.006, memory: 7525, loss_cls: 0.3665, loss_bbox: 0.6533, d0.loss_cls: 0.4586, d0.loss_bbox: 0.8037, d1.loss_cls: 0.4155, d1.loss_bbox: 0.7008, d2.loss_cls: 0.3872, d2.loss_bbox: 0.6805, d3.loss_cls: 0.3753, d3.loss_bbox: 0.6665, d4.loss_cls: 0.3690, d4.loss_bbox: 0.6583, aux_task0.loss_heatmap: 0.5354, aux_task0.loss_bbox: 0.2078, aux_task1.loss_heatmap: 0.7738, aux_task1.loss_bbox: 0.2796, aux_task2.loss_heatmap: 0.7462, aux_task2.loss_bbox: 0.2926, aux_task3.loss_heatmap: 0.4973, aux_task3.loss_bbox: 0.2542, aux_task4.loss_heatmap: 0.3892, aux_task4.loss_bbox: 0.2244, aux_task5.loss_heatmap: 0.4323, aux_task5.loss_bbox: 0.2807, loss: 11.4488, grad_norm: 85.1296
2025-04-05 20:31:31,533 - mmdet - INFO - Epoch [4][1000/61790]	lr: 2.378e-05, eta: 6 days, 23:58:38, time: 0.573, data_time: 0.007, memory: 7525, loss_cls: 0.3496, loss_bbox: 0.6370, d0.loss_cls: 0.4397, d0.loss_bbox: 0.7858, d1.loss_cls: 0.3974, d1.loss_bbox: 0.6800, d2.loss_cls: 0.3701, d2.loss_bbox: 0.6591, d3.loss_cls: 0.3575, d3.loss_bbox: 0.6478, d4.loss_cls: 0.3514, d4.loss_bbox: 0.6404, aux_task0.loss_heatmap: 0.4879, aux_task0.loss_bbox: 0.1940, aux_task1.loss_heatmap: 0.6877, aux_task1.loss_bbox: 0.2576, aux_task2.loss_heatmap: 0.7378, aux_task2.loss_bbox: 0.2910, aux_task3.loss_heatmap: 0.5198, aux_task3.loss_bbox: 0.2712, aux_task4.loss_heatmap: 0.4010, aux_task4.loss_bbox: 0.2348, aux_task5.loss_heatmap: 0.4080, aux_task5.loss_bbox: 0.2775, loss: 11.0841, grad_norm: 102.3330
2025-04-05 20:31:59,996 - mmdet - INFO - Epoch [4][1050/61790]	lr: 2.379e-05, eta: 6 days, 23:52:21, time: 0.569, data_time: 0.007, memory: 7525, loss_cls: 0.3485, loss_bbox: 0.6285, d0.loss_cls: 0.4344, d0.loss_bbox: 0.7755, d1.loss_cls: 0.3922, d1.loss_bbox: 0.6730, d2.loss_cls: 0.3658, d2.loss_bbox: 0.6537, d3.loss_cls: 0.3548, d3.loss_bbox: 0.6421, d4.loss_cls: 0.3507, d4.loss_bbox: 0.6324, aux_task0.loss_heatmap: 0.4930, aux_task0.loss_bbox: 0.2006, aux_task1.loss_heatmap: 0.6792, aux_task1.loss_bbox: 0.2568, aux_task2.loss_heatmap: 0.7514, aux_task2.loss_bbox: 0.2895, aux_task3.loss_heatmap: 0.5265, aux_task3.loss_bbox: 0.2580, aux_task4.loss_heatmap: 0.4134, aux_task4.loss_bbox: 0.2300, aux_task5.loss_heatmap: 0.3838, aux_task5.loss_bbox: 0.2754, loss: 11.0094, grad_norm: 82.2450
2025-04-05 20:32:28,694 - mmdet - INFO - Epoch [4][1100/61790]	lr: 2.379e-05, eta: 6 days, 23:50:19, time: 0.574, data_time: 0.007, memory: 7525, loss_cls: 0.3503, loss_bbox: 0.6499, d0.loss_cls: 0.4468, d0.loss_bbox: 0.8027, d1.loss_cls: 0.4015, d1.loss_bbox: 0.6986, d2.loss_cls: 0.3716, d2.loss_bbox: 0.6776, d3.loss_cls: 0.3600, d3.loss_bbox: 0.6657, d4.loss_cls: 0.3518, d4.loss_bbox: 0.6566, aux_task0.loss_heatmap: 0.5151, aux_task0.loss_bbox: 0.2037, aux_task1.loss_heatmap: 0.7232, aux_task1.loss_bbox: 0.2701, aux_task2.loss_heatmap: 0.7356, aux_task2.loss_bbox: 0.2792, aux_task3.loss_heatmap: 0.4791, aux_task3.loss_bbox: 0.2855, aux_task4.loss_heatmap: 0.3510, aux_task4.loss_bbox: 0.2221, aux_task5.loss_heatmap: 0.4098, aux_task5.loss_bbox: 0.2810, loss: 11.1886, grad_norm: 84.1500
2025-04-05 20:32:56,927 - mmdet - INFO - Epoch [4][1150/61790]	lr: 2.380e-05, eta: 6 days, 23:41:21, time: 0.565, data_time: 0.007, memory: 7525, loss_cls: 0.3522, loss_bbox: 0.6520, d0.loss_cls: 0.4471, d0.loss_bbox: 0.8147, d1.loss_cls: 0.4058, d1.loss_bbox: 0.6983, d2.loss_cls: 0.3730, d2.loss_bbox: 0.6784, d3.loss_cls: 0.3593, d3.loss_bbox: 0.6654, d4.loss_cls: 0.3551, d4.loss_bbox: 0.6556, aux_task0.loss_heatmap: 0.5298, aux_task0.loss_bbox: 0.2117, aux_task1.loss_heatmap: 0.6988, aux_task1.loss_bbox: 0.2719, aux_task2.loss_heatmap: 0.7725, aux_task2.loss_bbox: 0.2909, aux_task3.loss_heatmap: 0.5829, aux_task3.loss_bbox: 0.2495, aux_task4.loss_heatmap: 0.3729, aux_task4.loss_bbox: 0.2217, aux_task5.loss_heatmap: 0.4151, aux_task5.loss_bbox: 0.2756, loss: 11.3503, grad_norm: 97.2582
2025-04-05 20:33:25,710 - mmdet - INFO - Epoch [4][1200/61790]	lr: 2.381e-05, eta: 6 days, 23:41:06, time: 0.576, data_time: 0.006, memory: 7525, loss_cls: 0.3512, loss_bbox: 0.6497, d0.loss_cls: 0.4388, d0.loss_bbox: 0.8047, d1.loss_cls: 0.3970, d1.loss_bbox: 0.7038, d2.loss_cls: 0.3723, d2.loss_bbox: 0.6778, d3.loss_cls: 0.3606, d3.loss_bbox: 0.6635, d4.loss_cls: 0.3538, d4.loss_bbox: 0.6535, aux_task0.loss_heatmap: 0.5324, aux_task0.loss_bbox: 0.2133, aux_task1.loss_heatmap: 0.7526, aux_task1.loss_bbox: 0.2798, aux_task2.loss_heatmap: 0.6530, aux_task2.loss_bbox: 0.2823, aux_task3.loss_heatmap: 0.5315, aux_task3.loss_bbox: 0.2744, aux_task4.loss_heatmap: 0.3815, aux_task4.loss_bbox: 0.2238, aux_task5.loss_heatmap: 0.4215, aux_task5.loss_bbox: 0.2785, loss: 11.2516, grad_norm: 108.9533
2025-04-05 20:33:54,521 - mmdet - INFO - Epoch [4][1250/61790]	lr: 2.382e-05, eta: 6 days, 23:41:14, time: 0.576, data_time: 0.007, memory: 7525, loss_cls: 0.3508, loss_bbox: 0.6418, d0.loss_cls: 0.4362, d0.loss_bbox: 0.7837, d1.loss_cls: 0.3969, d1.loss_bbox: 0.6826, d2.loss_cls: 0.3718, d2.loss_bbox: 0.6634, d3.loss_cls: 0.3596, d3.loss_bbox: 0.6544, d4.loss_cls: 0.3529, d4.loss_bbox: 0.6469, aux_task0.loss_heatmap: 0.4803, aux_task0.loss_bbox: 0.2035, aux_task1.loss_heatmap: 0.6730, aux_task1.loss_bbox: 0.2683, aux_task2.loss_heatmap: 0.7195, aux_task2.loss_bbox: 0.2788, aux_task3.loss_heatmap: 0.5810, aux_task3.loss_bbox: 0.2714, aux_task4.loss_heatmap: 0.3946, aux_task4.loss_bbox: 0.2293, aux_task5.loss_heatmap: 0.4432, aux_task5.loss_bbox: 0.2815, loss: 11.1653, grad_norm: 81.8498
2025-04-05 20:34:22,991 - mmdet - INFO - Epoch [4][1300/61790]	lr: 2.383e-05, eta: 6 days, 23:36:44, time: 0.569, data_time: 0.008, memory: 7525, loss_cls: 0.3638, loss_bbox: 0.6396, d0.loss_cls: 0.4467, d0.loss_bbox: 0.7871, d1.loss_cls: 0.4076, d1.loss_bbox: 0.6854, d2.loss_cls: 0.3815, d2.loss_bbox: 0.6646, d3.loss_cls: 0.3709, d3.loss_bbox: 0.6546, d4.loss_cls: 0.3631, d4.loss_bbox: 0.6448, aux_task0.loss_heatmap: 0.5268, aux_task0.loss_bbox: 0.2061, aux_task1.loss_heatmap: 0.6870, aux_task1.loss_bbox: 0.2746, aux_task2.loss_heatmap: 0.7247, aux_task2.loss_bbox: 0.2896, aux_task3.loss_heatmap: 0.5739, aux_task3.loss_bbox: 0.2580, aux_task4.loss_heatmap: 0.4179, aux_task4.loss_bbox: 0.2240, aux_task5.loss_heatmap: 0.4039, aux_task5.loss_bbox: 0.2771, loss: 11.2735, grad_norm: 90.3672
2025-04-05 20:34:51,458 - mmdet - INFO - Epoch [4][1350/61790]	lr: 2.384e-05, eta: 6 days, 23:32:30, time: 0.569, data_time: 0.006, memory: 7525, loss_cls: 0.3550, loss_bbox: 0.6404, d0.loss_cls: 0.4374, d0.loss_bbox: 0.7815, d1.loss_cls: 0.4006, d1.loss_bbox: 0.6865, d2.loss_cls: 0.3752, d2.loss_bbox: 0.6627, d3.loss_cls: 0.3626, d3.loss_bbox: 0.6542, d4.loss_cls: 0.3575, d4.loss_bbox: 0.6447, aux_task0.loss_heatmap: 0.4558, aux_task0.loss_bbox: 0.1903, aux_task1.loss_heatmap: 0.7446, aux_task1.loss_bbox: 0.2946, aux_task2.loss_heatmap: 0.7270, aux_task2.loss_bbox: 0.2840, aux_task3.loss_heatmap: 0.5488, aux_task3.loss_bbox: 0.2548, aux_task4.loss_heatmap: 0.3763, aux_task4.loss_bbox: 0.2312, aux_task5.loss_heatmap: 0.4396, aux_task5.loss_bbox: 0.2826, loss: 11.1878, grad_norm: 88.7841
2025-04-05 20:35:19,796 - mmdet - INFO - Epoch [4][1400/61790]	lr: 2.384e-05, eta: 6 days, 23:26:55, time: 0.567, data_time: 0.008, memory: 7525, loss_cls: 0.3497, loss_bbox: 0.6151, d0.loss_cls: 0.4402, d0.loss_bbox: 0.7648, d1.loss_cls: 0.3971, d1.loss_bbox: 0.6582, d2.loss_cls: 0.3701, d2.loss_bbox: 0.6373, d3.loss_cls: 0.3590, d3.loss_bbox: 0.6277, d4.loss_cls: 0.3518, d4.loss_bbox: 0.6211, aux_task0.loss_heatmap: 0.4927, aux_task0.loss_bbox: 0.1905, aux_task1.loss_heatmap: 0.6793, aux_task1.loss_bbox: 0.2704, aux_task2.loss_heatmap: 0.6873, aux_task2.loss_bbox: 0.2847, aux_task3.loss_heatmap: 0.4489, aux_task3.loss_bbox: 0.2611, aux_task4.loss_heatmap: 0.3981, aux_task4.loss_bbox: 0.2233, aux_task5.loss_heatmap: 0.4198, aux_task5.loss_bbox: 0.2785, loss: 10.8265, grad_norm: 81.1530
2025-04-05 20:35:49,851 - mmdet - INFO - Epoch [4][1450/61790]	lr: 2.385e-05, eta: 6 days, 23:42:23, time: 0.601, data_time: 0.007, memory: 7525, loss_cls: 0.3480, loss_bbox: 0.6422, d0.loss_cls: 0.4360, d0.loss_bbox: 0.7836, d1.loss_cls: 0.3943, d1.loss_bbox: 0.6864, d2.loss_cls: 0.3694, d2.loss_bbox: 0.6671, d3.loss_cls: 0.3555, d3.loss_bbox: 0.6555, d4.loss_cls: 0.3490, d4.loss_bbox: 0.6467, aux_task0.loss_heatmap: 0.4883, aux_task0.loss_bbox: 0.2030, aux_task1.loss_heatmap: 0.7148, aux_task1.loss_bbox: 0.2733, aux_task2.loss_heatmap: 0.7282, aux_task2.loss_bbox: 0.2821, aux_task3.loss_heatmap: 0.5625, aux_task3.loss_bbox: 0.2695, aux_task4.loss_heatmap: 0.3828, aux_task4.loss_bbox: 0.2402, aux_task5.loss_heatmap: 0.3944, aux_task5.loss_bbox: 0.2760, loss: 11.1486, grad_norm: 88.9679
2025-04-05 20:36:18,612 - mmdet - INFO - Epoch [4][1500/61790]	lr: 2.386e-05, eta: 6 days, 23:41:42, time: 0.575, data_time: 0.007, memory: 7525, loss_cls: 0.3594, loss_bbox: 0.6430, d0.loss_cls: 0.4453, d0.loss_bbox: 0.7903, d1.loss_cls: 0.4055, d1.loss_bbox: 0.6894, d2.loss_cls: 0.3768, d2.loss_bbox: 0.6678, d3.loss_cls: 0.3675, d3.loss_bbox: 0.6551, d4.loss_cls: 0.3602, d4.loss_bbox: 0.6475, aux_task0.loss_heatmap: 0.5320, aux_task0.loss_bbox: 0.2120, aux_task1.loss_heatmap: 0.7437, aux_task1.loss_bbox: 0.2696, aux_task2.loss_heatmap: 0.6744, aux_task2.loss_bbox: 0.2747, aux_task3.loss_heatmap: 0.5703, aux_task3.loss_bbox: 0.2626, aux_task4.loss_heatmap: 0.4050, aux_task4.loss_bbox: 0.2399, aux_task5.loss_heatmap: 0.4380, aux_task5.loss_bbox: 0.2798, loss: 11.3098, grad_norm: 86.3700
2025-04-05 20:36:47,137 - mmdet - INFO - Epoch [4][1550/61790]	lr: 2.387e-05, eta: 6 days, 23:38:23, time: 0.570, data_time: 0.008, memory: 7525, loss_cls: 0.3410, loss_bbox: 0.6221, d0.loss_cls: 0.4278, d0.loss_bbox: 0.7672, d1.loss_cls: 0.3857, d1.loss_bbox: 0.6692, d2.loss_cls: 0.3616, d2.loss_bbox: 0.6489, d3.loss_cls: 0.3490, d3.loss_bbox: 0.6344, d4.loss_cls: 0.3424, d4.loss_bbox: 0.6273, aux_task0.loss_heatmap: 0.4850, aux_task0.loss_bbox: 0.2026, aux_task1.loss_heatmap: 0.7354, aux_task1.loss_bbox: 0.2694, aux_task2.loss_heatmap: 0.6996, aux_task2.loss_bbox: 0.2815, aux_task3.loss_heatmap: 0.5094, aux_task3.loss_bbox: 0.2481, aux_task4.loss_heatmap: 0.3952, aux_task4.loss_bbox: 0.2267, aux_task5.loss_heatmap: 0.3937, aux_task5.loss_bbox: 0.2739, loss: 10.8971, grad_norm: 82.7506
2025-04-05 20:37:15,767 - mmdet - INFO - Epoch [4][1600/61790]	lr: 2.388e-05, eta: 6 days, 23:36:23, time: 0.573, data_time: 0.007, memory: 7525, loss_cls: 0.3503, loss_bbox: 0.6331, d0.loss_cls: 0.4395, d0.loss_bbox: 0.7809, d1.loss_cls: 0.3957, d1.loss_bbox: 0.6777, d2.loss_cls: 0.3709, d2.loss_bbox: 0.6556, d3.loss_cls: 0.3554, d3.loss_bbox: 0.6464, d4.loss_cls: 0.3509, d4.loss_bbox: 0.6381, aux_task0.loss_heatmap: 0.5603, aux_task0.loss_bbox: 0.2118, aux_task1.loss_heatmap: 0.6374, aux_task1.loss_bbox: 0.2592, aux_task2.loss_heatmap: 0.6623, aux_task2.loss_bbox: 0.2802, aux_task3.loss_heatmap: 0.4954, aux_task3.loss_bbox: 0.2625, aux_task4.loss_heatmap: 0.4089, aux_task4.loss_bbox: 0.2333, aux_task5.loss_heatmap: 0.4255, aux_task5.loss_bbox: 0.2782, loss: 11.0096, grad_norm: 96.2265
2025-04-05 20:37:44,209 - mmdet - INFO - Epoch [4][1650/61790]	lr: 2.388e-05, eta: 6 days, 23:32:29, time: 0.569, data_time: 0.007, memory: 7525, loss_cls: 0.3589, loss_bbox: 0.6408, d0.loss_cls: 0.4449, d0.loss_bbox: 0.7799, d1.loss_cls: 0.4017, d1.loss_bbox: 0.6818, d2.loss_cls: 0.3768, d2.loss_bbox: 0.6621, d3.loss_cls: 0.3657, d3.loss_bbox: 0.6542, d4.loss_cls: 0.3600, d4.loss_bbox: 0.6460, aux_task0.loss_heatmap: 0.5441, aux_task0.loss_bbox: 0.1998, aux_task1.loss_heatmap: 0.7338, aux_task1.loss_bbox: 0.2823, aux_task2.loss_heatmap: 0.6644, aux_task2.loss_bbox: 0.2894, aux_task3.loss_heatmap: 0.5360, aux_task3.loss_bbox: 0.2687, aux_task4.loss_heatmap: 0.4029, aux_task4.loss_bbox: 0.2361, aux_task5.loss_heatmap: 0.3910, aux_task5.loss_bbox: 0.2746, loss: 11.1958, grad_norm: 84.3401
2025-04-05 20:38:12,619 - mmdet - INFO - Epoch [4][1700/61790]	lr: 2.389e-05, eta: 6 days, 23:28:27, time: 0.568, data_time: 0.008, memory: 7525, loss_cls: 0.3466, loss_bbox: 0.6307, d0.loss_cls: 0.4349, d0.loss_bbox: 0.7696, d1.loss_cls: 0.3905, d1.loss_bbox: 0.6728, d2.loss_cls: 0.3668, d2.loss_bbox: 0.6536, d3.loss_cls: 0.3527, d3.loss_bbox: 0.6440, d4.loss_cls: 0.3477, d4.loss_bbox: 0.6369, aux_task0.loss_heatmap: 0.5271, aux_task0.loss_bbox: 0.2025, aux_task1.loss_heatmap: 0.7152, aux_task1.loss_bbox: 0.2738, aux_task2.loss_heatmap: 0.7032, aux_task2.loss_bbox: 0.2881, aux_task3.loss_heatmap: 0.5200, aux_task3.loss_bbox: 0.2607, aux_task4.loss_heatmap: 0.3786, aux_task4.loss_bbox: 0.2202, aux_task5.loss_heatmap: 0.3658, aux_task5.loss_bbox: 0.2761, loss: 10.9780, grad_norm: 99.5721
2025-04-05 20:38:41,398 - mmdet - INFO - Epoch [4][1750/61790]	lr: 2.390e-05, eta: 6 days, 23:28:19, time: 0.576, data_time: 0.007, memory: 7525, loss_cls: 0.3635, loss_bbox: 0.6437, d0.loss_cls: 0.4440, d0.loss_bbox: 0.7906, d1.loss_cls: 0.4059, d1.loss_bbox: 0.6944, d2.loss_cls: 0.3818, d2.loss_bbox: 0.6712, d3.loss_cls: 0.3677, d3.loss_bbox: 0.6591, d4.loss_cls: 0.3632, d4.loss_bbox: 0.6500, aux_task0.loss_heatmap: 0.5214, aux_task0.loss_bbox: 0.2025, aux_task1.loss_heatmap: 0.7015, aux_task1.loss_bbox: 0.2724, aux_task2.loss_heatmap: 0.7118, aux_task2.loss_bbox: 0.2995, aux_task3.loss_heatmap: 0.5451, aux_task3.loss_bbox: 0.2556, aux_task4.loss_heatmap: 0.4194, aux_task4.loss_bbox: 0.2331, aux_task5.loss_heatmap: 0.4056, aux_task5.loss_bbox: 0.2766, loss: 11.2795, grad_norm: 93.2422
2025-04-05 20:39:10,073 - mmdet - INFO - Epoch [4][1800/61790]	lr: 2.391e-05, eta: 6 days, 23:27:09, time: 0.573, data_time: 0.010, memory: 7525, loss_cls: 0.3603, loss_bbox: 0.6265, d0.loss_cls: 0.4436, d0.loss_bbox: 0.7741, d1.loss_cls: 0.4041, d1.loss_bbox: 0.6739, d2.loss_cls: 0.3777, d2.loss_bbox: 0.6560, d3.loss_cls: 0.3667, d3.loss_bbox: 0.6418, d4.loss_cls: 0.3627, d4.loss_bbox: 0.6296, aux_task0.loss_heatmap: 0.5117, aux_task0.loss_bbox: 0.2089, aux_task1.loss_heatmap: 0.7307, aux_task1.loss_bbox: 0.2751, aux_task2.loss_heatmap: 0.6946, aux_task2.loss_bbox: 0.2863, aux_task3.loss_heatmap: 0.5093, aux_task3.loss_bbox: 0.2657, aux_task4.loss_heatmap: 0.4008, aux_task4.loss_bbox: 0.2201, aux_task5.loss_heatmap: 0.3813, aux_task5.loss_bbox: 0.2755, loss: 11.0772, grad_norm: 86.8165
2025-04-05 20:39:38,502 - mmdet - INFO - Epoch [4][1850/61790]	lr: 2.392e-05, eta: 6 days, 23:23:41, time: 0.569, data_time: 0.007, memory: 7532, loss_cls: 0.3663, loss_bbox: 0.6332, d0.loss_cls: 0.4524, d0.loss_bbox: 0.7868, d1.loss_cls: 0.4137, d1.loss_bbox: 0.6847, d2.loss_cls: 0.3863, d2.loss_bbox: 0.6611, d3.loss_cls: 0.3750, d3.loss_bbox: 0.6478, d4.loss_cls: 0.3687, d4.loss_bbox: 0.6403, aux_task0.loss_heatmap: 0.4983, aux_task0.loss_bbox: 0.2070, aux_task1.loss_heatmap: 0.7599, aux_task1.loss_bbox: 0.2776, aux_task2.loss_heatmap: 0.7008, aux_task2.loss_bbox: 0.2736, aux_task3.loss_heatmap: 0.5411, aux_task3.loss_bbox: 0.2614, aux_task4.loss_heatmap: 0.3643, aux_task4.loss_bbox: 0.2224, aux_task5.loss_heatmap: 0.4193, aux_task5.loss_bbox: 0.2782, loss: 11.2204, grad_norm: 91.6954
2025-04-05 20:40:07,062 - mmdet - INFO - Epoch [4][1900/61790]	lr: 2.393e-05, eta: 6 days, 23:21:36, time: 0.571, data_time: 0.007, memory: 7532, loss_cls: 0.3349, loss_bbox: 0.6180, d0.loss_cls: 0.4215, d0.loss_bbox: 0.7598, d1.loss_cls: 0.3758, d1.loss_bbox: 0.6578, d2.loss_cls: 0.3517, d2.loss_bbox: 0.6386, d3.loss_cls: 0.3422, d3.loss_bbox: 0.6276, d4.loss_cls: 0.3371, d4.loss_bbox: 0.6212, aux_task0.loss_heatmap: 0.4756, aux_task0.loss_bbox: 0.1960, aux_task1.loss_heatmap: 0.6532, aux_task1.loss_bbox: 0.2556, aux_task2.loss_heatmap: 0.6690, aux_task2.loss_bbox: 0.2647, aux_task3.loss_heatmap: 0.5638, aux_task3.loss_bbox: 0.2723, aux_task4.loss_heatmap: 0.3812, aux_task4.loss_bbox: 0.2245, aux_task5.loss_heatmap: 0.4106, aux_task5.loss_bbox: 0.2714, loss: 10.7239, grad_norm: 82.8663
2025-04-05 20:40:35,649 - mmdet - INFO - Epoch [4][1950/61790]	lr: 2.393e-05, eta: 6 days, 23:19:50, time: 0.572, data_time: 0.006, memory: 7532, loss_cls: 0.3584, loss_bbox: 0.6342, d0.loss_cls: 0.4377, d0.loss_bbox: 0.7791, d1.loss_cls: 0.3975, d1.loss_bbox: 0.6785, d2.loss_cls: 0.3755, d2.loss_bbox: 0.6594, d3.loss_cls: 0.3652, d3.loss_bbox: 0.6490, d4.loss_cls: 0.3593, d4.loss_bbox: 0.6410, aux_task0.loss_heatmap: 0.4964, aux_task0.loss_bbox: 0.1995, aux_task1.loss_heatmap: 0.7559, aux_task1.loss_bbox: 0.2706, aux_task2.loss_heatmap: 0.7654, aux_task2.loss_bbox: 0.2895, aux_task3.loss_heatmap: 0.5092, aux_task3.loss_bbox: 0.2787, aux_task4.loss_heatmap: 0.3821, aux_task4.loss_bbox: 0.2221, aux_task5.loss_heatmap: 0.4226, aux_task5.loss_bbox: 0.2824, loss: 11.2090, grad_norm: 81.7381
2025-04-05 20:41:04,328 - mmdet - INFO - Epoch [4][2000/61790]	lr: 2.394e-05, eta: 6 days, 23:18:56, time: 0.574, data_time: 0.007, memory: 7532, loss_cls: 0.3427, loss_bbox: 0.6373, d0.loss_cls: 0.4260, d0.loss_bbox: 0.7785, d1.loss_cls: 0.3847, d1.loss_bbox: 0.6822, d2.loss_cls: 0.3599, d2.loss_bbox: 0.6640, d3.loss_cls: 0.3492, d3.loss_bbox: 0.6520, d4.loss_cls: 0.3432, d4.loss_bbox: 0.6429, aux_task0.loss_heatmap: 0.5387, aux_task0.loss_bbox: 0.2089, aux_task1.loss_heatmap: 0.7473, aux_task1.loss_bbox: 0.2766, aux_task2.loss_heatmap: 0.6526, aux_task2.loss_bbox: 0.2800, aux_task3.loss_heatmap: 0.4783, aux_task3.loss_bbox: 0.2618, aux_task4.loss_heatmap: 0.3573, aux_task4.loss_bbox: 0.2291, aux_task5.loss_heatmap: 0.4136, aux_task5.loss_bbox: 0.2775, loss: 10.9843, grad_norm: 81.2968
2025-04-05 20:41:32,850 - mmdet - INFO - Epoch [4][2050/61790]	lr: 2.395e-05, eta: 6 days, 23:16:43, time: 0.570, data_time: 0.007, memory: 7532, loss_cls: 0.3427, loss_bbox: 0.6354, d0.loss_cls: 0.4322, d0.loss_bbox: 0.7885, d1.loss_cls: 0.3892, d1.loss_bbox: 0.6827, d2.loss_cls: 0.3634, d2.loss_bbox: 0.6601, d3.loss_cls: 0.3500, d3.loss_bbox: 0.6492, d4.loss_cls: 0.3456, d4.loss_bbox: 0.6391, aux_task0.loss_heatmap: 0.5090, aux_task0.loss_bbox: 0.2044, aux_task1.loss_heatmap: 0.6970, aux_task1.loss_bbox: 0.2720, aux_task2.loss_heatmap: 0.6911, aux_task2.loss_bbox: 0.2850, aux_task3.loss_heatmap: 0.5338, aux_task3.loss_bbox: 0.2522, aux_task4.loss_heatmap: 0.3906, aux_task4.loss_bbox: 0.2298, aux_task5.loss_heatmap: 0.4002, aux_task5.loss_bbox: 0.2890, loss: 11.0322, grad_norm: 87.8258
2025-04-05 20:42:01,401 - mmdet - INFO - Epoch [4][2100/61790]	lr: 2.396e-05, eta: 6 days, 23:14:50, time: 0.571, data_time: 0.007, memory: 7532, loss_cls: 0.3507, loss_bbox: 0.6459, d0.loss_cls: 0.4407, d0.loss_bbox: 0.7924, d1.loss_cls: 0.3974, d1.loss_bbox: 0.6914, d2.loss_cls: 0.3707, d2.loss_bbox: 0.6717, d3.loss_cls: 0.3576, d3.loss_bbox: 0.6594, d4.loss_cls: 0.3501, d4.loss_bbox: 0.6515, aux_task0.loss_heatmap: 0.5225, aux_task0.loss_bbox: 0.2111, aux_task1.loss_heatmap: 0.7301, aux_task1.loss_bbox: 0.2712, aux_task2.loss_heatmap: 0.6796, aux_task2.loss_bbox: 0.2744, aux_task3.loss_heatmap: 0.5605, aux_task3.loss_bbox: 0.2822, aux_task4.loss_heatmap: 0.3901, aux_task4.loss_bbox: 0.2348, aux_task5.loss_heatmap: 0.3906, aux_task5.loss_bbox: 0.2748, loss: 11.2014, grad_norm: 95.3116
2025-04-05 20:42:29,814 - mmdet - INFO - Epoch [4][2150/61790]	lr: 2.397e-05, eta: 6 days, 23:11:52, time: 0.568, data_time: 0.008, memory: 7532, loss_cls: 0.3386, loss_bbox: 0.6286, d0.loss_cls: 0.4333, d0.loss_bbox: 0.7780, d1.loss_cls: 0.3894, d1.loss_bbox: 0.6726, d2.loss_cls: 0.3628, d2.loss_bbox: 0.6521, d3.loss_cls: 0.3503, d3.loss_bbox: 0.6398, d4.loss_cls: 0.3414, d4.loss_bbox: 0.6329, aux_task0.loss_heatmap: 0.4826, aux_task0.loss_bbox: 0.1958, aux_task1.loss_heatmap: 0.7249, aux_task1.loss_bbox: 0.2818, aux_task2.loss_heatmap: 0.6816, aux_task2.loss_bbox: 0.2710, aux_task3.loss_heatmap: 0.5090, aux_task3.loss_bbox: 0.2499, aux_task4.loss_heatmap: 0.3549, aux_task4.loss_bbox: 0.2222, aux_task5.loss_heatmap: 0.3962, aux_task5.loss_bbox: 0.2692, loss: 10.8590, grad_norm: 104.1206
2025-04-05 20:42:58,132 - mmdet - INFO - Epoch [4][2200/61790]	lr: 2.398e-05, eta: 6 days, 23:08:17, time: 0.566, data_time: 0.008, memory: 7532, loss_cls: 0.3414, loss_bbox: 0.6362, d0.loss_cls: 0.4347, d0.loss_bbox: 0.7834, d1.loss_cls: 0.3894, d1.loss_bbox: 0.6829, d2.loss_cls: 0.3614, d2.loss_bbox: 0.6624, d3.loss_cls: 0.3503, d3.loss_bbox: 0.6505, d4.loss_cls: 0.3440, d4.loss_bbox: 0.6406, aux_task0.loss_heatmap: 0.5143, aux_task0.loss_bbox: 0.2099, aux_task1.loss_heatmap: 0.7116, aux_task1.loss_bbox: 0.2725, aux_task2.loss_heatmap: 0.6682, aux_task2.loss_bbox: 0.2789, aux_task3.loss_heatmap: 0.5296, aux_task3.loss_bbox: 0.2719, aux_task4.loss_heatmap: 0.3906, aux_task4.loss_bbox: 0.2267, aux_task5.loss_heatmap: 0.4205, aux_task5.loss_bbox: 0.2731, loss: 11.0448, grad_norm: 84.8727
2025-04-05 20:43:26,776 - mmdet - INFO - Epoch [4][2250/61790]	lr: 2.398e-05, eta: 6 days, 23:07:22, time: 0.573, data_time: 0.007, memory: 7532, loss_cls: 0.3546, loss_bbox: 0.6422, d0.loss_cls: 0.4487, d0.loss_bbox: 0.7848, d1.loss_cls: 0.4012, d1.loss_bbox: 0.6824, d2.loss_cls: 0.3729, d2.loss_bbox: 0.6649, d3.loss_cls: 0.3604, d3.loss_bbox: 0.6561, d4.loss_cls: 0.3554, d4.loss_bbox: 0.6458, aux_task0.loss_heatmap: 0.5633, aux_task0.loss_bbox: 0.2102, aux_task1.loss_heatmap: 0.6967, aux_task1.loss_bbox: 0.2615, aux_task2.loss_heatmap: 0.7097, aux_task2.loss_bbox: 0.2944, aux_task3.loss_heatmap: 0.5526, aux_task3.loss_bbox: 0.2493, aux_task4.loss_heatmap: 0.4038, aux_task4.loss_bbox: 0.2228, aux_task5.loss_heatmap: 0.3994, aux_task5.loss_bbox: 0.2792, loss: 11.2119, grad_norm: 88.6762
2025-04-05 20:43:55,466 - mmdet - INFO - Epoch [4][2300/61790]	lr: 2.399e-05, eta: 6 days, 23:06:49, time: 0.574, data_time: 0.006, memory: 7532, loss_cls: 0.3436, loss_bbox: 0.6363, d0.loss_cls: 0.4306, d0.loss_bbox: 0.7797, d1.loss_cls: 0.3874, d1.loss_bbox: 0.6815, d2.loss_cls: 0.3604, d2.loss_bbox: 0.6643, d3.loss_cls: 0.3487, d3.loss_bbox: 0.6540, d4.loss_cls: 0.3442, d4.loss_bbox: 0.6424, aux_task0.loss_heatmap: 0.5191, aux_task0.loss_bbox: 0.2034, aux_task1.loss_heatmap: 0.6912, aux_task1.loss_bbox: 0.2727, aux_task2.loss_heatmap: 0.6786, aux_task2.loss_bbox: 0.2830, aux_task3.loss_heatmap: 0.3912, aux_task3.loss_bbox: 0.2605, aux_task4.loss_heatmap: 0.3925, aux_task4.loss_bbox: 0.2272, aux_task5.loss_heatmap: 0.4143, aux_task5.loss_bbox: 0.2744, loss: 10.8813, grad_norm: 86.5758
2025-04-05 20:44:23,720 - mmdet - INFO - Epoch [4][2350/61790]	lr: 2.400e-05, eta: 6 days, 23:03:01, time: 0.565, data_time: 0.007, memory: 7532, loss_cls: 0.3495, loss_bbox: 0.6356, d0.loss_cls: 0.4385, d0.loss_bbox: 0.7828, d1.loss_cls: 0.3960, d1.loss_bbox: 0.6823, d2.loss_cls: 0.3700, d2.loss_bbox: 0.6618, d3.loss_cls: 0.3573, d3.loss_bbox: 0.6501, d4.loss_cls: 0.3524, d4.loss_bbox: 0.6413, aux_task0.loss_heatmap: 0.5071, aux_task0.loss_bbox: 0.2009, aux_task1.loss_heatmap: 0.7410, aux_task1.loss_bbox: 0.2784, aux_task2.loss_heatmap: 0.6809, aux_task2.loss_bbox: 0.2750, aux_task3.loss_heatmap: 0.5114, aux_task3.loss_bbox: 0.2574, aux_task4.loss_heatmap: 0.3883, aux_task4.loss_bbox: 0.2249, aux_task5.loss_heatmap: 0.4262, aux_task5.loss_bbox: 0.2826, loss: 11.0918, grad_norm: 83.5130
2025-04-05 20:44:52,153 - mmdet - INFO - Epoch [4][2400/61790]	lr: 2.401e-05, eta: 6 days, 23:00:41, time: 0.569, data_time: 0.008, memory: 7532, loss_cls: 0.3417, loss_bbox: 0.6222, d0.loss_cls: 0.4334, d0.loss_bbox: 0.7748, d1.loss_cls: 0.3882, d1.loss_bbox: 0.6715, d2.loss_cls: 0.3643, d2.loss_bbox: 0.6464, d3.loss_cls: 0.3500, d3.loss_bbox: 0.6340, d4.loss_cls: 0.3449, d4.loss_bbox: 0.6252, aux_task0.loss_heatmap: 0.4979, aux_task0.loss_bbox: 0.2068, aux_task1.loss_heatmap: 0.6633, aux_task1.loss_bbox: 0.2635, aux_task2.loss_heatmap: 0.6746, aux_task2.loss_bbox: 0.2818, aux_task3.loss_heatmap: 0.5544, aux_task3.loss_bbox: 0.2626, aux_task4.loss_heatmap: 0.3702, aux_task4.loss_bbox: 0.2283, aux_task5.loss_heatmap: 0.4085, aux_task5.loss_bbox: 0.2780, loss: 10.8864, grad_norm: 84.9300
2025-04-05 20:45:20,544 - mmdet - INFO - Epoch [4][2450/61790]	lr: 2.402e-05, eta: 6 days, 22:58:06, time: 0.568, data_time: 0.006, memory: 7532, loss_cls: 0.3543, loss_bbox: 0.6386, d0.loss_cls: 0.4537, d0.loss_bbox: 0.7808, d1.loss_cls: 0.4079, d1.loss_bbox: 0.6805, d2.loss_cls: 0.3768, d2.loss_bbox: 0.6619, d3.loss_cls: 0.3622, d3.loss_bbox: 0.6511, d4.loss_cls: 0.3576, d4.loss_bbox: 0.6415, aux_task0.loss_heatmap: 0.5176, aux_task0.loss_bbox: 0.2079, aux_task1.loss_heatmap: 0.7130, aux_task1.loss_bbox: 0.2721, aux_task2.loss_heatmap: 0.7089, aux_task2.loss_bbox: 0.2851, aux_task3.loss_heatmap: 0.5210, aux_task3.loss_bbox: 0.2560, aux_task4.loss_heatmap: 0.4085, aux_task4.loss_bbox: 0.2241, aux_task5.loss_heatmap: 0.4384, aux_task5.loss_bbox: 0.2771, loss: 11.1968, grad_norm: 84.2492
2025-04-05 20:45:49,187 - mmdet - INFO - Epoch [4][2500/61790]	lr: 2.403e-05, eta: 6 days, 22:57:23, time: 0.573, data_time: 0.007, memory: 7532, loss_cls: 0.3444, loss_bbox: 0.6275, d0.loss_cls: 0.4396, d0.loss_bbox: 0.7761, d1.loss_cls: 0.3949, d1.loss_bbox: 0.6732, d2.loss_cls: 0.3642, d2.loss_bbox: 0.6558, d3.loss_cls: 0.3514, d3.loss_bbox: 0.6414, d4.loss_cls: 0.3477, d4.loss_bbox: 0.6307, aux_task0.loss_heatmap: 0.5101, aux_task0.loss_bbox: 0.2038, aux_task1.loss_heatmap: 0.7233, aux_task1.loss_bbox: 0.2683, aux_task2.loss_heatmap: 0.6725, aux_task2.loss_bbox: 0.2673, aux_task3.loss_heatmap: 0.5092, aux_task3.loss_bbox: 0.2569, aux_task4.loss_heatmap: 0.3588, aux_task4.loss_bbox: 0.2188, aux_task5.loss_heatmap: 0.4727, aux_task5.loss_bbox: 0.2744, loss: 10.9829, grad_norm: 87.8223
2025-04-05 20:46:17,517 - mmdet - INFO - Epoch [4][2550/61790]	lr: 2.403e-05, eta: 6 days, 22:54:31, time: 0.567, data_time: 0.007, memory: 7532, loss_cls: 0.3363, loss_bbox: 0.6149, d0.loss_cls: 0.4270, d0.loss_bbox: 0.7655, d1.loss_cls: 0.3802, d1.loss_bbox: 0.6606, d2.loss_cls: 0.3530, d2.loss_bbox: 0.6411, d3.loss_cls: 0.3443, d3.loss_bbox: 0.6267, d4.loss_cls: 0.3395, d4.loss_bbox: 0.6178, aux_task0.loss_heatmap: 0.5103, aux_task0.loss_bbox: 0.1993, aux_task1.loss_heatmap: 0.6669, aux_task1.loss_bbox: 0.2681, aux_task2.loss_heatmap: 0.6583, aux_task2.loss_bbox: 0.2779, aux_task3.loss_heatmap: 0.5147, aux_task3.loss_bbox: 0.2560, aux_task4.loss_heatmap: 0.3607, aux_task4.loss_bbox: 0.2158, aux_task5.loss_heatmap: 0.4314, aux_task5.loss_bbox: 0.2806, loss: 10.7471, grad_norm: 97.4028
2025-04-05 20:46:46,010 - mmdet - INFO - Epoch [4][2600/61790]	lr: 2.404e-05, eta: 6 days, 22:52:51, time: 0.570, data_time: 0.006, memory: 7532, loss_cls: 0.3513, loss_bbox: 0.6344, d0.loss_cls: 0.4438, d0.loss_bbox: 0.7892, d1.loss_cls: 0.4020, d1.loss_bbox: 0.6815, d2.loss_cls: 0.3718, d2.loss_bbox: 0.6621, d3.loss_cls: 0.3611, d3.loss_bbox: 0.6482, d4.loss_cls: 0.3533, d4.loss_bbox: 0.6402, aux_task0.loss_heatmap: 0.5145, aux_task0.loss_bbox: 0.2100, aux_task1.loss_heatmap: 0.6865, aux_task1.loss_bbox: 0.2608, aux_task2.loss_heatmap: 0.6882, aux_task2.loss_bbox: 0.2797, aux_task3.loss_heatmap: 0.6493, aux_task3.loss_bbox: 0.2954, aux_task4.loss_heatmap: 0.3762, aux_task4.loss_bbox: 0.2292, aux_task5.loss_heatmap: 0.4091, aux_task5.loss_bbox: 0.2755, loss: 11.2132, grad_norm: 94.8758
2025-04-05 20:47:14,123 - mmdet - INFO - Epoch [4][2650/61790]	lr: 2.405e-05, eta: 6 days, 22:48:43, time: 0.562, data_time: 0.007, memory: 7532, loss_cls: 0.3375, loss_bbox: 0.6106, d0.loss_cls: 0.4309, d0.loss_bbox: 0.7619, d1.loss_cls: 0.3879, d1.loss_bbox: 0.6571, d2.loss_cls: 0.3577, d2.loss_bbox: 0.6364, d3.loss_cls: 0.3440, d3.loss_bbox: 0.6233, d4.loss_cls: 0.3375, d4.loss_bbox: 0.6161, aux_task0.loss_heatmap: 0.4641, aux_task0.loss_bbox: 0.1888, aux_task1.loss_heatmap: 0.6426, aux_task1.loss_bbox: 0.2588, aux_task2.loss_heatmap: 0.7202, aux_task2.loss_bbox: 0.2873, aux_task3.loss_heatmap: 0.5550, aux_task3.loss_bbox: 0.2663, aux_task4.loss_heatmap: 0.3502, aux_task4.loss_bbox: 0.2184, aux_task5.loss_heatmap: 0.3949, aux_task5.loss_bbox: 0.2827, loss: 10.7301, grad_norm: 91.8238
2025-04-05 20:47:42,695 - mmdet - INFO - Epoch [4][2700/61790]	lr: 2.406e-05, eta: 6 days, 22:47:41, time: 0.571, data_time: 0.006, memory: 7532, loss_cls: 0.3440, loss_bbox: 0.6475, d0.loss_cls: 0.4270, d0.loss_bbox: 0.7891, d1.loss_cls: 0.3879, d1.loss_bbox: 0.6898, d2.loss_cls: 0.3631, d2.loss_bbox: 0.6720, d3.loss_cls: 0.3502, d3.loss_bbox: 0.6601, d4.loss_cls: 0.3454, d4.loss_bbox: 0.6539, aux_task0.loss_heatmap: 0.5253, aux_task0.loss_bbox: 0.2062, aux_task1.loss_heatmap: 0.6740, aux_task1.loss_bbox: 0.2644, aux_task2.loss_heatmap: 0.6363, aux_task2.loss_bbox: 0.2781, aux_task3.loss_heatmap: 0.4675, aux_task3.loss_bbox: 0.2760, aux_task4.loss_heatmap: 0.3861, aux_task4.loss_bbox: 0.2257, aux_task5.loss_heatmap: 0.4087, aux_task5.loss_bbox: 0.2791, loss: 10.9574, grad_norm: 82.1835
2025-04-05 20:48:11,489 - mmdet - INFO - Epoch [4][2750/61790]	lr: 2.407e-05, eta: 6 days, 22:48:05, time: 0.576, data_time: 0.008, memory: 7532, loss_cls: 0.3432, loss_bbox: 0.6321, d0.loss_cls: 0.4294, d0.loss_bbox: 0.7851, d1.loss_cls: 0.3937, d1.loss_bbox: 0.6788, d2.loss_cls: 0.3623, d2.loss_bbox: 0.6572, d3.loss_cls: 0.3520, d3.loss_bbox: 0.6454, d4.loss_cls: 0.3448, d4.loss_bbox: 0.6369, aux_task0.loss_heatmap: 0.5156, aux_task0.loss_bbox: 0.2013, aux_task1.loss_heatmap: 0.6604, aux_task1.loss_bbox: 0.2674, aux_task2.loss_heatmap: 0.6566, aux_task2.loss_bbox: 0.2739, aux_task3.loss_heatmap: 0.5173, aux_task3.loss_bbox: 0.2477, aux_task4.loss_heatmap: 0.3803, aux_task4.loss_bbox: 0.2178, aux_task5.loss_heatmap: 0.4343, aux_task5.loss_bbox: 0.2815, loss: 10.9150, grad_norm: 90.3574
2025-04-05 20:48:39,788 - mmdet - INFO - Epoch [4][2800/61790]	lr: 2.408e-05, eta: 6 days, 22:45:22, time: 0.566, data_time: 0.006, memory: 7566, loss_cls: 0.3424, loss_bbox: 0.6472, d0.loss_cls: 0.4344, d0.loss_bbox: 0.7933, d1.loss_cls: 0.3922, d1.loss_bbox: 0.6940, d2.loss_cls: 0.3623, d2.loss_bbox: 0.6740, d3.loss_cls: 0.3500, d3.loss_bbox: 0.6618, d4.loss_cls: 0.3437, d4.loss_bbox: 0.6530, aux_task0.loss_heatmap: 0.5054, aux_task0.loss_bbox: 0.2085, aux_task1.loss_heatmap: 0.7325, aux_task1.loss_bbox: 0.2809, aux_task2.loss_heatmap: 0.6291, aux_task2.loss_bbox: 0.2648, aux_task3.loss_heatmap: 0.4835, aux_task3.loss_bbox: 0.2653, aux_task4.loss_heatmap: 0.4002, aux_task4.loss_bbox: 0.2285, aux_task5.loss_heatmap: 0.3910, aux_task5.loss_bbox: 0.2731, loss: 11.0111, grad_norm: 94.4060
2025-04-05 20:49:08,359 - mmdet - INFO - Epoch [4][2850/61790]	lr: 2.408e-05, eta: 6 days, 22:44:24, time: 0.571, data_time: 0.007, memory: 7566, loss_cls: 0.3531, loss_bbox: 0.6339, d0.loss_cls: 0.4436, d0.loss_bbox: 0.7817, d1.loss_cls: 0.4014, d1.loss_bbox: 0.6783, d2.loss_cls: 0.3728, d2.loss_bbox: 0.6610, d3.loss_cls: 0.3597, d3.loss_bbox: 0.6498, d4.loss_cls: 0.3537, d4.loss_bbox: 0.6394, aux_task0.loss_heatmap: 0.5093, aux_task0.loss_bbox: 0.1976, aux_task1.loss_heatmap: 0.6872, aux_task1.loss_bbox: 0.2690, aux_task2.loss_heatmap: 0.6784, aux_task2.loss_bbox: 0.2749, aux_task3.loss_heatmap: 0.5478, aux_task3.loss_bbox: 0.2849, aux_task4.loss_heatmap: 0.4077, aux_task4.loss_bbox: 0.2332, aux_task5.loss_heatmap: 0.4371, aux_task5.loss_bbox: 0.2791, loss: 11.1347, grad_norm: 87.0924
2025-04-05 20:49:36,751 - mmdet - INFO - Epoch [4][2900/61790]	lr: 2.409e-05, eta: 6 days, 22:42:22, time: 0.568, data_time: 0.007, memory: 7566, loss_cls: 0.3472, loss_bbox: 0.6405, d0.loss_cls: 0.4439, d0.loss_bbox: 0.7815, d1.loss_cls: 0.4015, d1.loss_bbox: 0.6814, d2.loss_cls: 0.3721, d2.loss_bbox: 0.6652, d3.loss_cls: 0.3590, d3.loss_bbox: 0.6525, d4.loss_cls: 0.3503, d4.loss_bbox: 0.6449, aux_task0.loss_heatmap: 0.5369, aux_task0.loss_bbox: 0.2100, aux_task1.loss_heatmap: 0.7055, aux_task1.loss_bbox: 0.2769, aux_task2.loss_heatmap: 0.6762, aux_task2.loss_bbox: 0.2814, aux_task3.loss_heatmap: 0.5293, aux_task3.loss_bbox: 0.2634, aux_task4.loss_heatmap: 0.3924, aux_task4.loss_bbox: 0.2260, aux_task5.loss_heatmap: 0.3947, aux_task5.loss_bbox: 0.2839, loss: 11.1166, grad_norm: 83.6002
2025-04-05 20:50:05,218 - mmdet - INFO - Epoch [4][2950/61790]	lr: 2.410e-05, eta: 6 days, 22:40:50, time: 0.569, data_time: 0.009, memory: 7566, loss_cls: 0.3563, loss_bbox: 0.6435, d0.loss_cls: 0.4484, d0.loss_bbox: 0.7831, d1.loss_cls: 0.4073, d1.loss_bbox: 0.6836, d2.loss_cls: 0.3789, d2.loss_bbox: 0.6664, d3.loss_cls: 0.3673, d3.loss_bbox: 0.6550, d4.loss_cls: 0.3595, d4.loss_bbox: 0.6488, aux_task0.loss_heatmap: 0.5152, aux_task0.loss_bbox: 0.2004, aux_task1.loss_heatmap: 0.7613, aux_task1.loss_bbox: 0.2757, aux_task2.loss_heatmap: 0.6514, aux_task2.loss_bbox: 0.2895, aux_task3.loss_heatmap: 0.5233, aux_task3.loss_bbox: 0.2748, aux_task4.loss_heatmap: 0.3751, aux_task4.loss_bbox: 0.2220, aux_task5.loss_heatmap: 0.4335, aux_task5.loss_bbox: 0.2794, loss: 11.1998, grad_norm: 86.8174
2025-04-05 20:50:33,315 - mmdet - INFO - Epoch [4][3000/61790]	lr: 2.411e-05, eta: 6 days, 22:37:11, time: 0.562, data_time: 0.006, memory: 7566, loss_cls: 0.3434, loss_bbox: 0.6218, d0.loss_cls: 0.4343, d0.loss_bbox: 0.7656, d1.loss_cls: 0.3907, d1.loss_bbox: 0.6690, d2.loss_cls: 0.3622, d2.loss_bbox: 0.6491, d3.loss_cls: 0.3510, d3.loss_bbox: 0.6349, d4.loss_cls: 0.3444, d4.loss_bbox: 0.6279, aux_task0.loss_heatmap: 0.5027, aux_task0.loss_bbox: 0.2048, aux_task1.loss_heatmap: 0.6866, aux_task1.loss_bbox: 0.2701, aux_task2.loss_heatmap: 0.7566, aux_task2.loss_bbox: 0.2915, aux_task3.loss_heatmap: 0.6023, aux_task3.loss_bbox: 0.2762, aux_task4.loss_heatmap: 0.3752, aux_task4.loss_bbox: 0.2237, aux_task5.loss_heatmap: 0.3794, aux_task5.loss_bbox: 0.2706, loss: 11.0340, grad_norm: 87.4040
2025-04-05 20:51:03,261 - mmdet - INFO - Epoch [4][3050/61790]	lr: 2.412e-05, eta: 6 days, 22:44:13, time: 0.599, data_time: 0.007, memory: 7566, loss_cls: 0.3540, loss_bbox: 0.6394, d0.loss_cls: 0.4544, d0.loss_bbox: 0.7813, d1.loss_cls: 0.4038, d1.loss_bbox: 0.6779, d2.loss_cls: 0.3765, d2.loss_bbox: 0.6603, d3.loss_cls: 0.3656, d3.loss_bbox: 0.6490, d4.loss_cls: 0.3563, d4.loss_bbox: 0.6440, aux_task0.loss_heatmap: 0.5268, aux_task0.loss_bbox: 0.2052, aux_task1.loss_heatmap: 0.7119, aux_task1.loss_bbox: 0.2759, aux_task2.loss_heatmap: 0.7123, aux_task2.loss_bbox: 0.2791, aux_task3.loss_heatmap: 0.5304, aux_task3.loss_bbox: 0.2826, aux_task4.loss_heatmap: 0.3904, aux_task4.loss_bbox: 0.2237, aux_task5.loss_heatmap: 0.4250, aux_task5.loss_bbox: 0.2785, loss: 11.2046, grad_norm: 87.3240
2025-04-05 20:51:32,162 - mmdet - INFO - Epoch [4][3100/61790]	lr: 2.413e-05, eta: 6 days, 22:45:07, time: 0.578, data_time: 0.006, memory: 7566, loss_cls: 0.3531, loss_bbox: 0.6515, d0.loss_cls: 0.4410, d0.loss_bbox: 0.7988, d1.loss_cls: 0.4018, d1.loss_bbox: 0.6945, d2.loss_cls: 0.3722, d2.loss_bbox: 0.6755, d3.loss_cls: 0.3599, d3.loss_bbox: 0.6644, d4.loss_cls: 0.3545, d4.loss_bbox: 0.6561, aux_task0.loss_heatmap: 0.5331, aux_task0.loss_bbox: 0.2099, aux_task1.loss_heatmap: 0.7238, aux_task1.loss_bbox: 0.2774, aux_task2.loss_heatmap: 0.7008, aux_task2.loss_bbox: 0.2864, aux_task3.loss_heatmap: 0.4910, aux_task3.loss_bbox: 0.2616, aux_task4.loss_heatmap: 0.3937, aux_task4.loss_bbox: 0.2308, aux_task5.loss_heatmap: 0.4373, aux_task5.loss_bbox: 0.2800, loss: 11.2494, grad_norm: 82.5724
2025-04-05 20:52:00,498 - mmdet - INFO - Epoch [4][3150/61790]	lr: 2.413e-05, eta: 6 days, 22:42:51, time: 0.567, data_time: 0.006, memory: 7566, loss_cls: 0.3569, loss_bbox: 0.6152, d0.loss_cls: 0.4415, d0.loss_bbox: 0.7712, d1.loss_cls: 0.3995, d1.loss_bbox: 0.6685, d2.loss_cls: 0.3741, d2.loss_bbox: 0.6434, d3.loss_cls: 0.3649, d3.loss_bbox: 0.6288, d4.loss_cls: 0.3581, d4.loss_bbox: 0.6203, aux_task0.loss_heatmap: 0.4971, aux_task0.loss_bbox: 0.2020, aux_task1.loss_heatmap: 0.6920, aux_task1.loss_bbox: 0.2727, aux_task2.loss_heatmap: 0.7028, aux_task2.loss_bbox: 0.2910, aux_task3.loss_heatmap: 0.4937, aux_task3.loss_bbox: 0.2560, aux_task4.loss_heatmap: 0.4038, aux_task4.loss_bbox: 0.2197, aux_task5.loss_heatmap: 0.4316, aux_task5.loss_bbox: 0.2776, loss: 10.9822, grad_norm: 80.6134
2025-04-05 20:52:29,132 - mmdet - INFO - Epoch [4][3200/61790]	lr: 2.414e-05, eta: 6 days, 22:42:16, time: 0.573, data_time: 0.007, memory: 7566, loss_cls: 0.3452, loss_bbox: 0.6443, d0.loss_cls: 0.4309, d0.loss_bbox: 0.7935, d1.loss_cls: 0.3932, d1.loss_bbox: 0.6957, d2.loss_cls: 0.3639, d2.loss_bbox: 0.6721, d3.loss_cls: 0.3521, d3.loss_bbox: 0.6571, d4.loss_cls: 0.3457, d4.loss_bbox: 0.6478, aux_task0.loss_heatmap: 0.5152, aux_task0.loss_bbox: 0.2058, aux_task1.loss_heatmap: 0.7294, aux_task1.loss_bbox: 0.2770, aux_task2.loss_heatmap: 0.6850, aux_task2.loss_bbox: 0.2816, aux_task3.loss_heatmap: 0.5324, aux_task3.loss_bbox: 0.2616, aux_task4.loss_heatmap: 0.3832, aux_task4.loss_bbox: 0.2220, aux_task5.loss_heatmap: 0.3983, aux_task5.loss_bbox: 0.2822, loss: 11.1152, grad_norm: 79.8519
2025-04-05 20:52:57,818 - mmdet - INFO - Epoch [4][3250/61790]	lr: 2.415e-05, eta: 6 days, 22:41:58, time: 0.574, data_time: 0.007, memory: 7566, loss_cls: 0.3425, loss_bbox: 0.6360, d0.loss_cls: 0.4283, d0.loss_bbox: 0.7804, d1.loss_cls: 0.3911, d1.loss_bbox: 0.6769, d2.loss_cls: 0.3609, d2.loss_bbox: 0.6576, d3.loss_cls: 0.3523, d3.loss_bbox: 0.6455, d4.loss_cls: 0.3459, d4.loss_bbox: 0.6388, aux_task0.loss_heatmap: 0.4738, aux_task0.loss_bbox: 0.1957, aux_task1.loss_heatmap: 0.6705, aux_task1.loss_bbox: 0.2725, aux_task2.loss_heatmap: 0.6852, aux_task2.loss_bbox: 0.2893, aux_task3.loss_heatmap: 0.5127, aux_task3.loss_bbox: 0.2485, aux_task4.loss_heatmap: 0.3839, aux_task4.loss_bbox: 0.2275, aux_task5.loss_heatmap: 0.3983, aux_task5.loss_bbox: 0.2747, loss: 10.8888, grad_norm: 82.5466
2025-04-05 20:53:26,578 - mmdet - INFO - Epoch [4][3300/61790]	lr: 2.416e-05, eta: 6 days, 22:42:02, time: 0.575, data_time: 0.007, memory: 7566, loss_cls: 0.3504, loss_bbox: 0.6424, d0.loss_cls: 0.4383, d0.loss_bbox: 0.7902, d1.loss_cls: 0.3988, d1.loss_bbox: 0.6918, d2.loss_cls: 0.3676, d2.loss_bbox: 0.6757, d3.loss_cls: 0.3580, d3.loss_bbox: 0.6575, d4.loss_cls: 0.3506, d4.loss_bbox: 0.6473, aux_task0.loss_heatmap: 0.5023, aux_task0.loss_bbox: 0.1972, aux_task1.loss_heatmap: 0.6770, aux_task1.loss_bbox: 0.2768, aux_task2.loss_heatmap: 0.7532, aux_task2.loss_bbox: 0.3012, aux_task3.loss_heatmap: 0.4629, aux_task3.loss_bbox: 0.2824, aux_task4.loss_heatmap: 0.4063, aux_task4.loss_bbox: 0.2245, aux_task5.loss_heatmap: 0.4182, aux_task5.loss_bbox: 0.2784, loss: 11.1489, grad_norm: 91.5281
2025-04-05 20:53:55,166 - mmdet - INFO - Epoch [4][3350/61790]	lr: 2.417e-05, eta: 6 days, 22:41:12, time: 0.572, data_time: 0.007, memory: 7566, loss_cls: 0.3667, loss_bbox: 0.6346, d0.loss_cls: 0.4481, d0.loss_bbox: 0.7885, d1.loss_cls: 0.4112, d1.loss_bbox: 0.6831, d2.loss_cls: 0.3861, d2.loss_bbox: 0.6613, d3.loss_cls: 0.3755, d3.loss_bbox: 0.6495, d4.loss_cls: 0.3682, d4.loss_bbox: 0.6405, aux_task0.loss_heatmap: 0.4936, aux_task0.loss_bbox: 0.2017, aux_task1.loss_heatmap: 0.7230, aux_task1.loss_bbox: 0.2652, aux_task2.loss_heatmap: 0.7157, aux_task2.loss_bbox: 0.2948, aux_task3.loss_heatmap: 0.6441, aux_task3.loss_bbox: 0.2683, aux_task4.loss_heatmap: 0.3943, aux_task4.loss_bbox: 0.2284, aux_task5.loss_heatmap: 0.4576, aux_task5.loss_bbox: 0.2851, loss: 11.3850, grad_norm: 83.4471
2025-04-05 20:54:23,842 - mmdet - INFO - Epoch [4][3400/61790]	lr: 2.418e-05, eta: 6 days, 22:40:50, time: 0.574, data_time: 0.008, memory: 7566, loss_cls: 0.3544, loss_bbox: 0.6407, d0.loss_cls: 0.4458, d0.loss_bbox: 0.7911, d1.loss_cls: 0.4055, d1.loss_bbox: 0.6886, d2.loss_cls: 0.3757, d2.loss_bbox: 0.6697, d3.loss_cls: 0.3642, d3.loss_bbox: 0.6543, d4.loss_cls: 0.3565, d4.loss_bbox: 0.6448, aux_task0.loss_heatmap: 0.5366, aux_task0.loss_bbox: 0.2048, aux_task1.loss_heatmap: 0.7165, aux_task1.loss_bbox: 0.2806, aux_task2.loss_heatmap: 0.6591, aux_task2.loss_bbox: 0.2792, aux_task3.loss_heatmap: 0.5139, aux_task3.loss_bbox: 0.2698, aux_task4.loss_heatmap: 0.4005, aux_task4.loss_bbox: 0.2314, aux_task5.loss_heatmap: 0.4572, aux_task5.loss_bbox: 0.2744, loss: 11.2154, grad_norm: 132.5546
2025-04-05 20:54:52,296 - mmdet - INFO - Epoch [4][3450/61790]	lr: 2.418e-05, eta: 6 days, 22:39:21, time: 0.569, data_time: 0.006, memory: 7566, loss_cls: 0.3568, loss_bbox: 0.6392, d0.loss_cls: 0.4449, d0.loss_bbox: 0.7781, d1.loss_cls: 0.4062, d1.loss_bbox: 0.6745, d2.loss_cls: 0.3783, d2.loss_bbox: 0.6601, d3.loss_cls: 0.3660, d3.loss_bbox: 0.6476, d4.loss_cls: 0.3585, d4.loss_bbox: 0.6421, aux_task0.loss_heatmap: 0.5205, aux_task0.loss_bbox: 0.1965, aux_task1.loss_heatmap: 0.7126, aux_task1.loss_bbox: 0.2812, aux_task2.loss_heatmap: 0.6435, aux_task2.loss_bbox: 0.2772, aux_task3.loss_heatmap: 0.5713, aux_task3.loss_bbox: 0.2662, aux_task4.loss_heatmap: 0.3890, aux_task4.loss_bbox: 0.2240, aux_task5.loss_heatmap: 0.4238, aux_task5.loss_bbox: 0.2745, loss: 11.1325, grad_norm: 83.3069
2025-04-05 20:55:20,817 - mmdet - INFO - Epoch [4][3500/61790]	lr: 2.419e-05, eta: 6 days, 22:38:13, time: 0.570, data_time: 0.006, memory: 7566, loss_cls: 0.3456, loss_bbox: 0.6398, d0.loss_cls: 0.4321, d0.loss_bbox: 0.7885, d1.loss_cls: 0.3951, d1.loss_bbox: 0.6823, d2.loss_cls: 0.3681, d2.loss_bbox: 0.6626, d3.loss_cls: 0.3538, d3.loss_bbox: 0.6528, d4.loss_cls: 0.3468, d4.loss_bbox: 0.6466, aux_task0.loss_heatmap: 0.5160, aux_task0.loss_bbox: 0.2012, aux_task1.loss_heatmap: 0.6928, aux_task1.loss_bbox: 0.2754, aux_task2.loss_heatmap: 0.6467, aux_task2.loss_bbox: 0.2738, aux_task3.loss_heatmap: 0.4753, aux_task3.loss_bbox: 0.2755, aux_task4.loss_heatmap: 0.3806, aux_task4.loss_bbox: 0.2270, aux_task5.loss_heatmap: 0.4342, aux_task5.loss_bbox: 0.2767, loss: 10.9893, grad_norm: 83.0340
2025-04-05 20:55:49,205 - mmdet - INFO - Epoch [4][3550/61790]	lr: 2.420e-05, eta: 6 days, 22:36:27, time: 0.568, data_time: 0.007, memory: 7566, loss_cls: 0.3508, loss_bbox: 0.6281, d0.loss_cls: 0.4339, d0.loss_bbox: 0.7759, d1.loss_cls: 0.3957, d1.loss_bbox: 0.6743, d2.loss_cls: 0.3669, d2.loss_bbox: 0.6574, d3.loss_cls: 0.3576, d3.loss_bbox: 0.6438, d4.loss_cls: 0.3516, d4.loss_bbox: 0.6342, aux_task0.loss_heatmap: 0.4783, aux_task0.loss_bbox: 0.1974, aux_task1.loss_heatmap: 0.7111, aux_task1.loss_bbox: 0.2715, aux_task2.loss_heatmap: 0.6650, aux_task2.loss_bbox: 0.2882, aux_task3.loss_heatmap: 0.5346, aux_task3.loss_bbox: 0.2669, aux_task4.loss_heatmap: 0.3836, aux_task4.loss_bbox: 0.2228, aux_task5.loss_heatmap: 0.4278, aux_task5.loss_bbox: 0.2767, loss: 10.9940, grad_norm: 82.1592
2025-04-05 20:56:17,684 - mmdet - INFO - Epoch [4][3600/61790]	lr: 2.421e-05, eta: 6 days, 22:35:09, time: 0.570, data_time: 0.006, memory: 7566, loss_cls: 0.3506, loss_bbox: 0.6455, d0.loss_cls: 0.4466, d0.loss_bbox: 0.7983, d1.loss_cls: 0.4015, d1.loss_bbox: 0.6904, d2.loss_cls: 0.3719, d2.loss_bbox: 0.6723, d3.loss_cls: 0.3605, d3.loss_bbox: 0.6578, d4.loss_cls: 0.3534, d4.loss_bbox: 0.6508, aux_task0.loss_heatmap: 0.4930, aux_task0.loss_bbox: 0.2023, aux_task1.loss_heatmap: 0.7225, aux_task1.loss_bbox: 0.2713, aux_task2.loss_heatmap: 0.6978, aux_task2.loss_bbox: 0.3018, aux_task3.loss_heatmap: 0.5272, aux_task3.loss_bbox: 0.2748, aux_task4.loss_heatmap: 0.3783, aux_task4.loss_bbox: 0.2260, aux_task5.loss_heatmap: 0.4072, aux_task5.loss_bbox: 0.2850, loss: 11.1868, grad_norm: 90.9437
2025-04-05 20:56:46,226 - mmdet - INFO - Epoch [4][3650/61790]	lr: 2.422e-05, eta: 6 days, 22:34:11, time: 0.571, data_time: 0.006, memory: 7566, loss_cls: 0.3426, loss_bbox: 0.6301, d0.loss_cls: 0.4345, d0.loss_bbox: 0.7635, d1.loss_cls: 0.3952, d1.loss_bbox: 0.6669, d2.loss_cls: 0.3638, d2.loss_bbox: 0.6552, d3.loss_cls: 0.3507, d3.loss_bbox: 0.6438, d4.loss_cls: 0.3437, d4.loss_bbox: 0.6362, aux_task0.loss_heatmap: 0.4805, aux_task0.loss_bbox: 0.1938, aux_task1.loss_heatmap: 0.7377, aux_task1.loss_bbox: 0.2759, aux_task2.loss_heatmap: 0.6777, aux_task2.loss_bbox: 0.2833, aux_task3.loss_heatmap: 0.4658, aux_task3.loss_bbox: 0.2511, aux_task4.loss_heatmap: 0.3802, aux_task4.loss_bbox: 0.2181, aux_task5.loss_heatmap: 0.3719, aux_task5.loss_bbox: 0.2745, loss: 10.8367, grad_norm: 85.1222
2025-04-05 20:57:14,575 - mmdet - INFO - Epoch [4][3700/61790]	lr: 2.423e-05, eta: 6 days, 22:32:20, time: 0.567, data_time: 0.007, memory: 7566, loss_cls: 0.3469, loss_bbox: 0.6290, d0.loss_cls: 0.4311, d0.loss_bbox: 0.7866, d1.loss_cls: 0.3928, d1.loss_bbox: 0.6778, d2.loss_cls: 0.3640, d2.loss_bbox: 0.6582, d3.loss_cls: 0.3519, d3.loss_bbox: 0.6440, d4.loss_cls: 0.3490, d4.loss_bbox: 0.6346, aux_task0.loss_heatmap: 0.5006, aux_task0.loss_bbox: 0.2073, aux_task1.loss_heatmap: 0.7189, aux_task1.loss_bbox: 0.2745, aux_task2.loss_heatmap: 0.6709, aux_task2.loss_bbox: 0.2761, aux_task3.loss_heatmap: 0.4631, aux_task3.loss_bbox: 0.2496, aux_task4.loss_heatmap: 0.3732, aux_task4.loss_bbox: 0.2242, aux_task5.loss_heatmap: 0.4316, aux_task5.loss_bbox: 0.2783, loss: 10.9342, grad_norm: 84.3460
2025-04-05 20:57:42,934 - mmdet - INFO - Epoch [4][3750/61790]	lr: 2.423e-05, eta: 6 days, 22:30:33, time: 0.567, data_time: 0.007, memory: 7566, loss_cls: 0.3491, loss_bbox: 0.6386, d0.loss_cls: 0.4369, d0.loss_bbox: 0.7847, d1.loss_cls: 0.3938, d1.loss_bbox: 0.6818, d2.loss_cls: 0.3674, d2.loss_bbox: 0.6609, d3.loss_cls: 0.3553, d3.loss_bbox: 0.6525, d4.loss_cls: 0.3490, d4.loss_bbox: 0.6438, aux_task0.loss_heatmap: 0.4702, aux_task0.loss_bbox: 0.1968, aux_task1.loss_heatmap: 0.7445, aux_task1.loss_bbox: 0.2784, aux_task2.loss_heatmap: 0.6852, aux_task2.loss_bbox: 0.2831, aux_task3.loss_heatmap: 0.4902, aux_task3.loss_bbox: 0.2607, aux_task4.loss_heatmap: 0.3939, aux_task4.loss_bbox: 0.2301, aux_task5.loss_heatmap: 0.3872, aux_task5.loss_bbox: 0.2710, loss: 11.0050, grad_norm: 88.9931
2025-04-05 20:58:11,759 - mmdet - INFO - Epoch [4][3800/61790]	lr: 2.424e-05, eta: 6 days, 22:30:56, time: 0.576, data_time: 0.008, memory: 7566, loss_cls: 0.3557, loss_bbox: 0.6280, d0.loss_cls: 0.4431, d0.loss_bbox: 0.7783, d1.loss_cls: 0.3983, d1.loss_bbox: 0.6782, d2.loss_cls: 0.3741, d2.loss_bbox: 0.6562, d3.loss_cls: 0.3621, d3.loss_bbox: 0.6415, d4.loss_cls: 0.3575, d4.loss_bbox: 0.6341, aux_task0.loss_heatmap: 0.5288, aux_task0.loss_bbox: 0.2030, aux_task1.loss_heatmap: 0.6736, aux_task1.loss_bbox: 0.2692, aux_task2.loss_heatmap: 0.6628, aux_task2.loss_bbox: 0.2834, aux_task3.loss_heatmap: 0.5730, aux_task3.loss_bbox: 0.2481, aux_task4.loss_heatmap: 0.4028, aux_task4.loss_bbox: 0.2280, aux_task5.loss_heatmap: 0.4311, aux_task5.loss_bbox: 0.2768, loss: 11.0878, grad_norm: 82.6482
2025-04-05 20:58:40,252 - mmdet - INFO - Epoch [4][3850/61790]	lr: 2.425e-05, eta: 6 days, 22:29:49, time: 0.570, data_time: 0.008, memory: 7566, loss_cls: 0.3710, loss_bbox: 0.6366, d0.loss_cls: 0.4521, d0.loss_bbox: 0.7804, d1.loss_cls: 0.4159, d1.loss_bbox: 0.6815, d2.loss_cls: 0.3919, d2.loss_bbox: 0.6600, d3.loss_cls: 0.3797, d3.loss_bbox: 0.6521, d4.loss_cls: 0.3723, d4.loss_bbox: 0.6426, aux_task0.loss_heatmap: 0.5332, aux_task0.loss_bbox: 0.2178, aux_task1.loss_heatmap: 0.7251, aux_task1.loss_bbox: 0.2715, aux_task2.loss_heatmap: 0.6617, aux_task2.loss_bbox: 0.2713, aux_task3.loss_heatmap: 0.5854, aux_task3.loss_bbox: 0.2573, aux_task4.loss_heatmap: 0.4012, aux_task4.loss_bbox: 0.2249, aux_task5.loss_heatmap: 0.4301, aux_task5.loss_bbox: 0.2804, loss: 11.2959, grad_norm: 82.1371
2025-04-05 20:59:08,720 - mmdet - INFO - Epoch [4][3900/61790]	lr: 2.426e-05, eta: 6 days, 22:28:35, time: 0.569, data_time: 0.007, memory: 7566, loss_cls: 0.3421, loss_bbox: 0.6195, d0.loss_cls: 0.4315, d0.loss_bbox: 0.7642, d1.loss_cls: 0.3881, d1.loss_bbox: 0.6627, d2.loss_cls: 0.3624, d2.loss_bbox: 0.6464, d3.loss_cls: 0.3489, d3.loss_bbox: 0.6355, d4.loss_cls: 0.3428, d4.loss_bbox: 0.6260, aux_task0.loss_heatmap: 0.4770, aux_task0.loss_bbox: 0.1928, aux_task1.loss_heatmap: 0.7245, aux_task1.loss_bbox: 0.2732, aux_task2.loss_heatmap: 0.6924, aux_task2.loss_bbox: 0.2778, aux_task3.loss_heatmap: 0.5452, aux_task3.loss_bbox: 0.2528, aux_task4.loss_heatmap: 0.3604, aux_task4.loss_bbox: 0.2184, aux_task5.loss_heatmap: 0.4625, aux_task5.loss_bbox: 0.2813, loss: 10.9285, grad_norm: 93.8180
2025-04-05 20:59:37,172 - mmdet - INFO - Epoch [4][3950/61790]	lr: 2.427e-05, eta: 6 days, 22:27:18, time: 0.569, data_time: 0.007, memory: 7566, loss_cls: 0.3478, loss_bbox: 0.6431, d0.loss_cls: 0.4285, d0.loss_bbox: 0.7900, d1.loss_cls: 0.3927, d1.loss_bbox: 0.6852, d2.loss_cls: 0.3673, d2.loss_bbox: 0.6659, d3.loss_cls: 0.3549, d3.loss_bbox: 0.6555, d4.loss_cls: 0.3489, d4.loss_bbox: 0.6477, aux_task0.loss_heatmap: 0.4978, aux_task0.loss_bbox: 0.2053, aux_task1.loss_heatmap: 0.6849, aux_task1.loss_bbox: 0.2645, aux_task2.loss_heatmap: 0.6899, aux_task2.loss_bbox: 0.2832, aux_task3.loss_heatmap: 0.5459, aux_task3.loss_bbox: 0.2577, aux_task4.loss_heatmap: 0.3906, aux_task4.loss_bbox: 0.2326, aux_task5.loss_heatmap: 0.4015, aux_task5.loss_bbox: 0.2779, loss: 11.0594, grad_norm: 78.9304
2025-04-05 21:00:05,693 - mmdet - INFO - Epoch [4][4000/61790]	lr: 2.428e-05, eta: 6 days, 22:26:21, time: 0.570, data_time: 0.007, memory: 7566, loss_cls: 0.3451, loss_bbox: 0.6297, d0.loss_cls: 0.4356, d0.loss_bbox: 0.7741, d1.loss_cls: 0.3950, d1.loss_bbox: 0.6735, d2.loss_cls: 0.3678, d2.loss_bbox: 0.6519, d3.loss_cls: 0.3540, d3.loss_bbox: 0.6418, d4.loss_cls: 0.3473, d4.loss_bbox: 0.6361, aux_task0.loss_heatmap: 0.5060, aux_task0.loss_bbox: 0.1989, aux_task1.loss_heatmap: 0.7300, aux_task1.loss_bbox: 0.2729, aux_task2.loss_heatmap: 0.7068, aux_task2.loss_bbox: 0.2869, aux_task3.loss_heatmap: 0.5825, aux_task3.loss_bbox: 0.2624, aux_task4.loss_heatmap: 0.3719, aux_task4.loss_bbox: 0.2233, aux_task5.loss_heatmap: 0.3923, aux_task5.loss_bbox: 0.2814, loss: 11.0673, grad_norm: 85.8718
2025-04-05 21:00:34,171 - mmdet - INFO - Epoch [4][4050/61790]	lr: 2.428e-05, eta: 6 days, 22:25:13, time: 0.570, data_time: 0.006, memory: 7566, loss_cls: 0.3450, loss_bbox: 0.6288, d0.loss_cls: 0.4374, d0.loss_bbox: 0.7731, d1.loss_cls: 0.3913, d1.loss_bbox: 0.6724, d2.loss_cls: 0.3658, d2.loss_bbox: 0.6503, d3.loss_cls: 0.3541, d3.loss_bbox: 0.6405, d4.loss_cls: 0.3487, d4.loss_bbox: 0.6323, aux_task0.loss_heatmap: 0.4884, aux_task0.loss_bbox: 0.2016, aux_task1.loss_heatmap: 0.7227, aux_task1.loss_bbox: 0.2726, aux_task2.loss_heatmap: 0.6894, aux_task2.loss_bbox: 0.2842, aux_task3.loss_heatmap: 0.5521, aux_task3.loss_bbox: 0.2787, aux_task4.loss_heatmap: 0.3761, aux_task4.loss_bbox: 0.2199, aux_task5.loss_heatmap: 0.4025, aux_task5.loss_bbox: 0.2804, loss: 11.0081, grad_norm: 91.7069
2025-04-05 21:01:03,061 - mmdet - INFO - Epoch [4][4100/61790]	lr: 2.429e-05, eta: 6 days, 22:25:51, time: 0.578, data_time: 0.007, memory: 7566, loss_cls: 0.3661, loss_bbox: 0.6364, d0.loss_cls: 0.4543, d0.loss_bbox: 0.7877, d1.loss_cls: 0.4114, d1.loss_bbox: 0.6850, d2.loss_cls: 0.3823, d2.loss_bbox: 0.6655, d3.loss_cls: 0.3733, d3.loss_bbox: 0.6521, d4.loss_cls: 0.3656, d4.loss_bbox: 0.6417, aux_task0.loss_heatmap: 0.5051, aux_task0.loss_bbox: 0.1913, aux_task1.loss_heatmap: 0.7256, aux_task1.loss_bbox: 0.2768, aux_task2.loss_heatmap: 0.7313, aux_task2.loss_bbox: 0.2888, aux_task3.loss_heatmap: 0.4916, aux_task3.loss_bbox: 0.2785, aux_task4.loss_heatmap: 0.3913, aux_task4.loss_bbox: 0.2228, aux_task5.loss_heatmap: 0.4413, aux_task5.loss_bbox: 0.2794, loss: 11.2452, grad_norm: 85.2845
2025-04-05 21:01:31,849 - mmdet - INFO - Epoch [4][4150/61790]	lr: 2.430e-05, eta: 6 days, 22:26:02, time: 0.576, data_time: 0.007, memory: 7566, loss_cls: 0.3431, loss_bbox: 0.6273, d0.loss_cls: 0.4195, d0.loss_bbox: 0.7692, d1.loss_cls: 0.3825, d1.loss_bbox: 0.6717, d2.loss_cls: 0.3603, d2.loss_bbox: 0.6512, d3.loss_cls: 0.3496, d3.loss_bbox: 0.6404, d4.loss_cls: 0.3452, d4.loss_bbox: 0.6322, aux_task0.loss_heatmap: 0.5094, aux_task0.loss_bbox: 0.1932, aux_task1.loss_heatmap: 0.6758, aux_task1.loss_bbox: 0.2797, aux_task2.loss_heatmap: 0.6867, aux_task2.loss_bbox: 0.2916, aux_task3.loss_heatmap: 0.4884, aux_task3.loss_bbox: 0.2724, aux_task4.loss_heatmap: 0.3768, aux_task4.loss_bbox: 0.2266, aux_task5.loss_heatmap: 0.3882, aux_task5.loss_bbox: 0.2789, loss: 10.8598, grad_norm: 96.2560
2025-04-05 21:02:00,569 - mmdet - INFO - Epoch [4][4200/61790]	lr: 2.431e-05, eta: 6 days, 22:25:55, time: 0.574, data_time: 0.008, memory: 7566, loss_cls: 0.3569, loss_bbox: 0.6419, d0.loss_cls: 0.4465, d0.loss_bbox: 0.7848, d1.loss_cls: 0.4024, d1.loss_bbox: 0.6902, d2.loss_cls: 0.3770, d2.loss_bbox: 0.6718, d3.loss_cls: 0.3658, d3.loss_bbox: 0.6571, d4.loss_cls: 0.3605, d4.loss_bbox: 0.6478, aux_task0.loss_heatmap: 0.5025, aux_task0.loss_bbox: 0.2081, aux_task1.loss_heatmap: 0.7201, aux_task1.loss_bbox: 0.2706, aux_task2.loss_heatmap: 0.6981, aux_task2.loss_bbox: 0.2820, aux_task3.loss_heatmap: 0.4936, aux_task3.loss_bbox: 0.2603, aux_task4.loss_heatmap: 0.4000, aux_task4.loss_bbox: 0.2278, aux_task5.loss_heatmap: 0.4287, aux_task5.loss_bbox: 0.2767, loss: 11.1711, grad_norm: 88.9780
2025-04-05 21:02:29,102 - mmdet - INFO - Epoch [4][4250/61790]	lr: 2.432e-05, eta: 6 days, 22:25:01, time: 0.571, data_time: 0.006, memory: 7566, loss_cls: 0.3706, loss_bbox: 0.6462, d0.loss_cls: 0.4596, d0.loss_bbox: 0.7827, d1.loss_cls: 0.4147, d1.loss_bbox: 0.6871, d2.loss_cls: 0.3879, d2.loss_bbox: 0.6696, d3.loss_cls: 0.3773, d3.loss_bbox: 0.6571, d4.loss_cls: 0.3714, d4.loss_bbox: 0.6501, aux_task0.loss_heatmap: 0.5911, aux_task0.loss_bbox: 0.2053, aux_task1.loss_heatmap: 0.6943, aux_task1.loss_bbox: 0.2679, aux_task2.loss_heatmap: 0.7320, aux_task2.loss_bbox: 0.2839, aux_task3.loss_heatmap: 0.6019, aux_task3.loss_bbox: 0.2772, aux_task4.loss_heatmap: 0.3816, aux_task4.loss_bbox: 0.2210, aux_task5.loss_heatmap: 0.4071, aux_task5.loss_bbox: 0.2760, loss: 11.4136, grad_norm: 85.2136
2025-04-05 21:02:57,599 - mmdet - INFO - Epoch [4][4300/61790]	lr: 2.433e-05, eta: 6 days, 22:24:00, time: 0.570, data_time: 0.006, memory: 7566, loss_cls: 0.3503, loss_bbox: 0.6437, d0.loss_cls: 0.4406, d0.loss_bbox: 0.7901, d1.loss_cls: 0.4005, d1.loss_bbox: 0.6919, d2.loss_cls: 0.3669, d2.loss_bbox: 0.6724, d3.loss_cls: 0.3573, d3.loss_bbox: 0.6573, d4.loss_cls: 0.3509, d4.loss_bbox: 0.6500, aux_task0.loss_heatmap: 0.4933, aux_task0.loss_bbox: 0.2000, aux_task1.loss_heatmap: 0.6876, aux_task1.loss_bbox: 0.2695, aux_task2.loss_heatmap: 0.6979, aux_task2.loss_bbox: 0.2838, aux_task3.loss_heatmap: 0.5512, aux_task3.loss_bbox: 0.2627, aux_task4.loss_heatmap: 0.4293, aux_task4.loss_bbox: 0.2308, aux_task5.loss_heatmap: 0.3953, aux_task5.loss_bbox: 0.2728, loss: 11.1461, grad_norm: 81.5050
2025-04-05 21:03:26,227 - mmdet - INFO - Epoch [4][4350/61790]	lr: 2.433e-05, eta: 6 days, 22:23:30, time: 0.573, data_time: 0.007, memory: 7566, loss_cls: 0.3583, loss_bbox: 0.6439, d0.loss_cls: 0.4479, d0.loss_bbox: 0.7885, d1.loss_cls: 0.4074, d1.loss_bbox: 0.6867, d2.loss_cls: 0.3797, d2.loss_bbox: 0.6673, d3.loss_cls: 0.3683, d3.loss_bbox: 0.6561, d4.loss_cls: 0.3609, d4.loss_bbox: 0.6484, aux_task0.loss_heatmap: 0.5162, aux_task0.loss_bbox: 0.2052, aux_task1.loss_heatmap: 0.6985, aux_task1.loss_bbox: 0.2701, aux_task2.loss_heatmap: 0.7113, aux_task2.loss_bbox: 0.2887, aux_task3.loss_heatmap: 0.5418, aux_task3.loss_bbox: 0.2686, aux_task4.loss_heatmap: 0.3852, aux_task4.loss_bbox: 0.2237, aux_task5.loss_heatmap: 0.4062, aux_task5.loss_bbox: 0.2704, loss: 11.1995, grad_norm: 87.0133
2025-04-05 21:03:54,813 - mmdet - INFO - Epoch [4][4400/61790]	lr: 2.434e-05, eta: 6 days, 22:22:51, time: 0.572, data_time: 0.009, memory: 7566, loss_cls: 0.3508, loss_bbox: 0.6194, d0.loss_cls: 0.4416, d0.loss_bbox: 0.7721, d1.loss_cls: 0.3948, d1.loss_bbox: 0.6681, d2.loss_cls: 0.3697, d2.loss_bbox: 0.6466, d3.loss_cls: 0.3592, d3.loss_bbox: 0.6315, d4.loss_cls: 0.3531, d4.loss_bbox: 0.6234, aux_task0.loss_heatmap: 0.5218, aux_task0.loss_bbox: 0.2015, aux_task1.loss_heatmap: 0.6723, aux_task1.loss_bbox: 0.2651, aux_task2.loss_heatmap: 0.6488, aux_task2.loss_bbox: 0.2848, aux_task3.loss_heatmap: 0.5558, aux_task3.loss_bbox: 0.2657, aux_task4.loss_heatmap: 0.3831, aux_task4.loss_bbox: 0.2288, aux_task5.loss_heatmap: 0.4288, aux_task5.loss_bbox: 0.2763, loss: 10.9632, grad_norm: 87.9245
2025-04-05 21:04:23,590 - mmdet - INFO - Epoch [4][4450/61790]	lr: 2.435e-05, eta: 6 days, 22:22:56, time: 0.576, data_time: 0.008, memory: 7566, loss_cls: 0.3527, loss_bbox: 0.6455, d0.loss_cls: 0.4444, d0.loss_bbox: 0.7806, d1.loss_cls: 0.3972, d1.loss_bbox: 0.6862, d2.loss_cls: 0.3676, d2.loss_bbox: 0.6695, d3.loss_cls: 0.3583, d3.loss_bbox: 0.6597, d4.loss_cls: 0.3530, d4.loss_bbox: 0.6528, aux_task0.loss_heatmap: 0.5261, aux_task0.loss_bbox: 0.2081, aux_task1.loss_heatmap: 0.7298, aux_task1.loss_bbox: 0.2823, aux_task2.loss_heatmap: 0.6084, aux_task2.loss_bbox: 0.2803, aux_task3.loss_heatmap: 0.5537, aux_task3.loss_bbox: 0.2713, aux_task4.loss_heatmap: 0.3872, aux_task4.loss_bbox: 0.2235, aux_task5.loss_heatmap: 0.4217, aux_task5.loss_bbox: 0.2806, loss: 11.1404, grad_norm: 83.2779
2025-04-05 21:04:52,084 - mmdet - INFO - Epoch [4][4500/61790]	lr: 2.436e-05, eta: 6 days, 22:21:56, time: 0.570, data_time: 0.007, memory: 7566, loss_cls: 0.3421, loss_bbox: 0.6305, d0.loss_cls: 0.4254, d0.loss_bbox: 0.7737, d1.loss_cls: 0.3831, d1.loss_bbox: 0.6798, d2.loss_cls: 0.3563, d2.loss_bbox: 0.6587, d3.loss_cls: 0.3483, d3.loss_bbox: 0.6440, d4.loss_cls: 0.3432, d4.loss_bbox: 0.6357, aux_task0.loss_heatmap: 0.4879, aux_task0.loss_bbox: 0.2060, aux_task1.loss_heatmap: 0.7205, aux_task1.loss_bbox: 0.2632, aux_task2.loss_heatmap: 0.7182, aux_task2.loss_bbox: 0.2852, aux_task3.loss_heatmap: 0.5064, aux_task3.loss_bbox: 0.2659, aux_task4.loss_heatmap: 0.3958, aux_task4.loss_bbox: 0.2305, aux_task5.loss_heatmap: 0.3883, aux_task5.loss_bbox: 0.2783, loss: 10.9670, grad_norm: 77.3263
2025-04-05 21:05:20,548 - mmdet - INFO - Epoch [4][4550/61790]	lr: 2.437e-05, eta: 6 days, 22:20:49, time: 0.569, data_time: 0.008, memory: 7566, loss_cls: 0.3518, loss_bbox: 0.6409, d0.loss_cls: 0.4407, d0.loss_bbox: 0.7810, d1.loss_cls: 0.4006, d1.loss_bbox: 0.6768, d2.loss_cls: 0.3749, d2.loss_bbox: 0.6597, d3.loss_cls: 0.3587, d3.loss_bbox: 0.6532, d4.loss_cls: 0.3539, d4.loss_bbox: 0.6442, aux_task0.loss_heatmap: 0.5050, aux_task0.loss_bbox: 0.1987, aux_task1.loss_heatmap: 0.7266, aux_task1.loss_bbox: 0.2644, aux_task2.loss_heatmap: 0.7092, aux_task2.loss_bbox: 0.2846, aux_task3.loss_heatmap: 0.5006, aux_task3.loss_bbox: 0.2655, aux_task4.loss_heatmap: 0.4014, aux_task4.loss_bbox: 0.2297, aux_task5.loss_heatmap: 0.4438, aux_task5.loss_bbox: 0.2728, loss: 11.1386, grad_norm: 97.9196
2025-04-05 21:05:49,247 - mmdet - INFO - Epoch [4][4600/61790]	lr: 2.438e-05, eta: 6 days, 22:20:36, time: 0.574, data_time: 0.008, memory: 7566, loss_cls: 0.3624, loss_bbox: 0.6450, d0.loss_cls: 0.4501, d0.loss_bbox: 0.7905, d1.loss_cls: 0.4057, d1.loss_bbox: 0.6927, d2.loss_cls: 0.3788, d2.loss_bbox: 0.6740, d3.loss_cls: 0.3673, d3.loss_bbox: 0.6616, d4.loss_cls: 0.3624, d4.loss_bbox: 0.6520, aux_task0.loss_heatmap: 0.5012, aux_task0.loss_bbox: 0.1947, aux_task1.loss_heatmap: 0.7037, aux_task1.loss_bbox: 0.2721, aux_task2.loss_heatmap: 0.7141, aux_task2.loss_bbox: 0.3055, aux_task3.loss_heatmap: 0.5565, aux_task3.loss_bbox: 0.2712, aux_task4.loss_heatmap: 0.4106, aux_task4.loss_bbox: 0.2302, aux_task5.loss_heatmap: 0.4127, aux_task5.loss_bbox: 0.2790, loss: 11.2939, grad_norm: 81.2363
2025-04-05 21:06:18,094 - mmdet - INFO - Epoch [4][4650/61790]	lr: 2.438e-05, eta: 6 days, 22:20:56, time: 0.577, data_time: 0.007, memory: 7566, loss_cls: 0.3584, loss_bbox: 0.6587, d0.loss_cls: 0.4472, d0.loss_bbox: 0.8018, d1.loss_cls: 0.4034, d1.loss_bbox: 0.7030, d2.loss_cls: 0.3777, d2.loss_bbox: 0.6828, d3.loss_cls: 0.3651, d3.loss_bbox: 0.6727, d4.loss_cls: 0.3583, d4.loss_bbox: 0.6650, aux_task0.loss_heatmap: 0.5486, aux_task0.loss_bbox: 0.2079, aux_task1.loss_heatmap: 0.6777, aux_task1.loss_bbox: 0.2806, aux_task2.loss_heatmap: 0.7341, aux_task2.loss_bbox: 0.2875, aux_task3.loss_heatmap: 0.5202, aux_task3.loss_bbox: 0.2898, aux_task4.loss_heatmap: 0.3813, aux_task4.loss_bbox: 0.2276, aux_task5.loss_heatmap: 0.4066, aux_task5.loss_bbox: 0.2749, loss: 11.3310, grad_norm: 95.2336
2025-04-05 21:06:46,831 - mmdet - INFO - Epoch [4][4700/61790]	lr: 2.439e-05, eta: 6 days, 22:20:51, time: 0.575, data_time: 0.007, memory: 7566, loss_cls: 0.3471, loss_bbox: 0.6369, d0.loss_cls: 0.4350, d0.loss_bbox: 0.7883, d1.loss_cls: 0.3970, d1.loss_bbox: 0.6877, d2.loss_cls: 0.3711, d2.loss_bbox: 0.6611, d3.loss_cls: 0.3550, d3.loss_bbox: 0.6514, d4.loss_cls: 0.3464, d4.loss_bbox: 0.6426, aux_task0.loss_heatmap: 0.5067, aux_task0.loss_bbox: 0.2094, aux_task1.loss_heatmap: 0.7129, aux_task1.loss_bbox: 0.2723, aux_task2.loss_heatmap: 0.6998, aux_task2.loss_bbox: 0.2835, aux_task3.loss_heatmap: 0.5777, aux_task3.loss_bbox: 0.2737, aux_task4.loss_heatmap: 0.3908, aux_task4.loss_bbox: 0.2255, aux_task5.loss_heatmap: 0.3945, aux_task5.loss_bbox: 0.2783, loss: 11.1446, grad_norm: 84.3334
2025-04-05 21:07:15,354 - mmdet - INFO - Epoch [4][4750/61790]	lr: 2.440e-05, eta: 6 days, 22:19:58, time: 0.570, data_time: 0.007, memory: 7566, loss_cls: 0.3447, loss_bbox: 0.6408, d0.loss_cls: 0.4363, d0.loss_bbox: 0.7832, d1.loss_cls: 0.3914, d1.loss_bbox: 0.6840, d2.loss_cls: 0.3643, d2.loss_bbox: 0.6654, d3.loss_cls: 0.3497, d3.loss_bbox: 0.6545, d4.loss_cls: 0.3454, d4.loss_bbox: 0.6459, aux_task0.loss_heatmap: 0.4913, aux_task0.loss_bbox: 0.2008, aux_task1.loss_heatmap: 0.7191, aux_task1.loss_bbox: 0.2716, aux_task2.loss_heatmap: 0.7239, aux_task2.loss_bbox: 0.2933, aux_task3.loss_heatmap: 0.5203, aux_task3.loss_bbox: 0.2641, aux_task4.loss_heatmap: 0.3949, aux_task4.loss_bbox: 0.2215, aux_task5.loss_heatmap: 0.3947, aux_task5.loss_bbox: 0.2787, loss: 11.0799, grad_norm: 143.2486
2025-04-05 21:07:43,620 - mmdet - INFO - Epoch [4][4800/61790]	lr: 2.441e-05, eta: 6 days, 22:18:09, time: 0.565, data_time: 0.008, memory: 7566, loss_cls: 0.3529, loss_bbox: 0.6362, d0.loss_cls: 0.4362, d0.loss_bbox: 0.7816, d1.loss_cls: 0.3971, d1.loss_bbox: 0.6818, d2.loss_cls: 0.3676, d2.loss_bbox: 0.6652, d3.loss_cls: 0.3578, d3.loss_bbox: 0.6527, d4.loss_cls: 0.3537, d4.loss_bbox: 0.6415, aux_task0.loss_heatmap: 0.5291, aux_task0.loss_bbox: 0.2113, aux_task1.loss_heatmap: 0.7112, aux_task1.loss_bbox: 0.2634, aux_task2.loss_heatmap: 0.7148, aux_task2.loss_bbox: 0.2892, aux_task3.loss_heatmap: 0.5118, aux_task3.loss_bbox: 0.2568, aux_task4.loss_heatmap: 0.3958, aux_task4.loss_bbox: 0.2222, aux_task5.loss_heatmap: 0.4033, aux_task5.loss_bbox: 0.2761, loss: 11.1096, grad_norm: 79.2970
2025-04-05 21:08:12,088 - mmdet - INFO - Epoch [4][4850/61790]	lr: 2.442e-05, eta: 6 days, 22:17:06, time: 0.569, data_time: 0.007, memory: 7566, loss_cls: 0.3519, loss_bbox: 0.6373, d0.loss_cls: 0.4376, d0.loss_bbox: 0.7767, d1.loss_cls: 0.3964, d1.loss_bbox: 0.6776, d2.loss_cls: 0.3711, d2.loss_bbox: 0.6584, d3.loss_cls: 0.3607, d3.loss_bbox: 0.6479, d4.loss_cls: 0.3535, d4.loss_bbox: 0.6414, aux_task0.loss_heatmap: 0.5091, aux_task0.loss_bbox: 0.1955, aux_task1.loss_heatmap: 0.6918, aux_task1.loss_bbox: 0.2699, aux_task2.loss_heatmap: 0.7183, aux_task2.loss_bbox: 0.2915, aux_task3.loss_heatmap: 0.5292, aux_task3.loss_bbox: 0.2699, aux_task4.loss_heatmap: 0.3891, aux_task4.loss_bbox: 0.2190, aux_task5.loss_heatmap: 0.4268, aux_task5.loss_bbox: 0.2725, loss: 11.0930, grad_norm: 97.0641
2025-04-05 21:08:40,519 - mmdet - INFO - Epoch [4][4900/61790]	lr: 2.443e-05, eta: 6 days, 22:15:56, time: 0.569, data_time: 0.008, memory: 7566, loss_cls: 0.3418, loss_bbox: 0.6152, d0.loss_cls: 0.4324, d0.loss_bbox: 0.7715, d1.loss_cls: 0.3899, d1.loss_bbox: 0.6662, d2.loss_cls: 0.3628, d2.loss_bbox: 0.6450, d3.loss_cls: 0.3494, d3.loss_bbox: 0.6323, d4.loss_cls: 0.3447, d4.loss_bbox: 0.6204, aux_task0.loss_heatmap: 0.4913, aux_task0.loss_bbox: 0.1967, aux_task1.loss_heatmap: 0.6586, aux_task1.loss_bbox: 0.2637, aux_task2.loss_heatmap: 0.6750, aux_task2.loss_bbox: 0.2939, aux_task3.loss_heatmap: 0.5188, aux_task3.loss_bbox: 0.2681, aux_task4.loss_heatmap: 0.4071, aux_task4.loss_bbox: 0.2200, aux_task5.loss_heatmap: 0.4108, aux_task5.loss_bbox: 0.2793, loss: 10.8550, grad_norm: 84.1478
2025-04-05 21:09:09,074 - mmdet - INFO - Epoch [4][4950/61790]	lr: 2.443e-05, eta: 6 days, 22:15:12, time: 0.571, data_time: 0.007, memory: 7566, loss_cls: 0.3489, loss_bbox: 0.6162, d0.loss_cls: 0.4359, d0.loss_bbox: 0.7664, d1.loss_cls: 0.3966, d1.loss_bbox: 0.6651, d2.loss_cls: 0.3691, d2.loss_bbox: 0.6434, d3.loss_cls: 0.3575, d3.loss_bbox: 0.6294, d4.loss_cls: 0.3507, d4.loss_bbox: 0.6211, aux_task0.loss_heatmap: 0.4943, aux_task0.loss_bbox: 0.1893, aux_task1.loss_heatmap: 0.6876, aux_task1.loss_bbox: 0.2584, aux_task2.loss_heatmap: 0.7137, aux_task2.loss_bbox: 0.3052, aux_task3.loss_heatmap: 0.4909, aux_task3.loss_bbox: 0.2386, aux_task4.loss_heatmap: 0.4074, aux_task4.loss_bbox: 0.2297, aux_task5.loss_heatmap: 0.4311, aux_task5.loss_bbox: 0.2782, loss: 10.9247, grad_norm: 88.8943
2025-04-05 21:09:37,662 - mmdet - INFO - Epoch [4][5000/61790]	lr: 2.444e-05, eta: 6 days, 22:14:36, time: 0.572, data_time: 0.007, memory: 7566, loss_cls: 0.3587, loss_bbox: 0.6427, d0.loss_cls: 0.4453, d0.loss_bbox: 0.7919, d1.loss_cls: 0.4061, d1.loss_bbox: 0.6889, d2.loss_cls: 0.3780, d2.loss_bbox: 0.6687, d3.loss_cls: 0.3662, d3.loss_bbox: 0.6577, d4.loss_cls: 0.3595, d4.loss_bbox: 0.6484, aux_task0.loss_heatmap: 0.5223, aux_task0.loss_bbox: 0.2075, aux_task1.loss_heatmap: 0.6961, aux_task1.loss_bbox: 0.2737, aux_task2.loss_heatmap: 0.7002, aux_task2.loss_bbox: 0.2866, aux_task3.loss_heatmap: 0.4933, aux_task3.loss_bbox: 0.2591, aux_task4.loss_heatmap: 0.4125, aux_task4.loss_bbox: 0.2309, aux_task5.loss_heatmap: 0.4188, aux_task5.loss_bbox: 0.2721, loss: 11.1852, grad_norm: 80.7807
2025-04-05 21:10:06,484 - mmdet - INFO - Epoch [4][5050/61790]	lr: 2.445e-05, eta: 6 days, 22:14:49, time: 0.576, data_time: 0.006, memory: 7566, loss_cls: 0.3571, loss_bbox: 0.6350, d0.loss_cls: 0.4457, d0.loss_bbox: 0.7826, d1.loss_cls: 0.4021, d1.loss_bbox: 0.6822, d2.loss_cls: 0.3772, d2.loss_bbox: 0.6632, d3.loss_cls: 0.3628, d3.loss_bbox: 0.6512, d4.loss_cls: 0.3572, d4.loss_bbox: 0.6415, aux_task0.loss_heatmap: 0.5048, aux_task0.loss_bbox: 0.2078, aux_task1.loss_heatmap: 0.7129, aux_task1.loss_bbox: 0.2734, aux_task2.loss_heatmap: 0.7243, aux_task2.loss_bbox: 0.2836, aux_task3.loss_heatmap: 0.5402, aux_task3.loss_bbox: 0.2693, aux_task4.loss_heatmap: 0.3988, aux_task4.loss_bbox: 0.2262, aux_task5.loss_heatmap: 0.4374, aux_task5.loss_bbox: 0.2715, loss: 11.2079, grad_norm: 86.7954
2025-04-05 21:10:34,789 - mmdet - INFO - Epoch [4][5100/61790]	lr: 2.446e-05, eta: 6 days, 22:13:14, time: 0.566, data_time: 0.006, memory: 7566, loss_cls: 0.3425, loss_bbox: 0.6400, d0.loss_cls: 0.4323, d0.loss_bbox: 0.7815, d1.loss_cls: 0.3917, d1.loss_bbox: 0.6837, d2.loss_cls: 0.3645, d2.loss_bbox: 0.6666, d3.loss_cls: 0.3535, d3.loss_bbox: 0.6509, d4.loss_cls: 0.3443, d4.loss_bbox: 0.6438, aux_task0.loss_heatmap: 0.5028, aux_task0.loss_bbox: 0.2030, aux_task1.loss_heatmap: 0.7056, aux_task1.loss_bbox: 0.2745, aux_task2.loss_heatmap: 0.6786, aux_task2.loss_bbox: 0.2895, aux_task3.loss_heatmap: 0.5272, aux_task3.loss_bbox: 0.2680, aux_task4.loss_heatmap: 0.3988, aux_task4.loss_bbox: 0.2283, aux_task5.loss_heatmap: 0.4127, aux_task5.loss_bbox: 0.2781, loss: 11.0623, grad_norm: 79.2455
2025-04-05 21:11:03,075 - mmdet - INFO - Epoch [4][5150/61790]	lr: 2.447e-05, eta: 6 days, 22:11:37, time: 0.566, data_time: 0.007, memory: 7566, loss_cls: 0.3434, loss_bbox: 0.6296, d0.loss_cls: 0.4314, d0.loss_bbox: 0.7706, d1.loss_cls: 0.3887, d1.loss_bbox: 0.6687, d2.loss_cls: 0.3625, d2.loss_bbox: 0.6534, d3.loss_cls: 0.3501, d3.loss_bbox: 0.6422, d4.loss_cls: 0.3458, d4.loss_bbox: 0.6352, aux_task0.loss_heatmap: 0.4961, aux_task0.loss_bbox: 0.2054, aux_task1.loss_heatmap: 0.6808, aux_task1.loss_bbox: 0.2664, aux_task2.loss_heatmap: 0.6933, aux_task2.loss_bbox: 0.2832, aux_task3.loss_heatmap: 0.4818, aux_task3.loss_bbox: 0.2699, aux_task4.loss_heatmap: 0.3658, aux_task4.loss_bbox: 0.2253, aux_task5.loss_heatmap: 0.3881, aux_task5.loss_bbox: 0.2794, loss: 10.8571, grad_norm: 82.2824
2025-04-05 21:11:31,682 - mmdet - INFO - Epoch [4][5200/61790]	lr: 2.448e-05, eta: 6 days, 22:11:06, time: 0.572, data_time: 0.007, memory: 7566, loss_cls: 0.3528, loss_bbox: 0.6175, d0.loss_cls: 0.4378, d0.loss_bbox: 0.7585, d1.loss_cls: 0.3942, d1.loss_bbox: 0.6660, d2.loss_cls: 0.3711, d2.loss_bbox: 0.6491, d3.loss_cls: 0.3592, d3.loss_bbox: 0.6340, d4.loss_cls: 0.3541, d4.loss_bbox: 0.6236, aux_task0.loss_heatmap: 0.4666, aux_task0.loss_bbox: 0.1903, aux_task1.loss_heatmap: 0.7163, aux_task1.loss_bbox: 0.2698, aux_task2.loss_heatmap: 0.6662, aux_task2.loss_bbox: 0.2821, aux_task3.loss_heatmap: 0.5433, aux_task3.loss_bbox: 0.2984, aux_task4.loss_heatmap: 0.4075, aux_task4.loss_bbox: 0.2260, aux_task5.loss_heatmap: 0.4105, aux_task5.loss_bbox: 0.2759, loss: 10.9709, grad_norm: 89.1303
2025-04-05 21:11:59,836 - mmdet - INFO - Epoch [4][5250/61790]	lr: 2.448e-05, eta: 6 days, 22:09:05, time: 0.563, data_time: 0.006, memory: 7566, loss_cls: 0.3503, loss_bbox: 0.6296, d0.loss_cls: 0.4371, d0.loss_bbox: 0.7726, d1.loss_cls: 0.3944, d1.loss_bbox: 0.6747, d2.loss_cls: 0.3682, d2.loss_bbox: 0.6550, d3.loss_cls: 0.3577, d3.loss_bbox: 0.6433, d4.loss_cls: 0.3504, d4.loss_bbox: 0.6351, aux_task0.loss_heatmap: 0.4666, aux_task0.loss_bbox: 0.1969, aux_task1.loss_heatmap: 0.6903, aux_task1.loss_bbox: 0.2718, aux_task2.loss_heatmap: 0.6783, aux_task2.loss_bbox: 0.2857, aux_task3.loss_heatmap: 0.5948, aux_task3.loss_bbox: 0.2767, aux_task4.loss_heatmap: 0.3906, aux_task4.loss_bbox: 0.2256, aux_task5.loss_heatmap: 0.3867, aux_task5.loss_bbox: 0.2846, loss: 11.0171, grad_norm: 92.0284
2025-04-05 21:12:28,126 - mmdet - INFO - Epoch [4][5300/61790]	lr: 2.449e-05, eta: 6 days, 22:07:32, time: 0.566, data_time: 0.007, memory: 7566, loss_cls: 0.3437, loss_bbox: 0.6278, d0.loss_cls: 0.4273, d0.loss_bbox: 0.7727, d1.loss_cls: 0.3860, d1.loss_bbox: 0.6760, d2.loss_cls: 0.3585, d2.loss_bbox: 0.6564, d3.loss_cls: 0.3484, d3.loss_bbox: 0.6453, d4.loss_cls: 0.3443, d4.loss_bbox: 0.6335, aux_task0.loss_heatmap: 0.4815, aux_task0.loss_bbox: 0.1915, aux_task1.loss_heatmap: 0.6741, aux_task1.loss_bbox: 0.2638, aux_task2.loss_heatmap: 0.6453, aux_task2.loss_bbox: 0.2761, aux_task3.loss_heatmap: 0.4825, aux_task3.loss_bbox: 0.2729, aux_task4.loss_heatmap: 0.4070, aux_task4.loss_bbox: 0.2228, aux_task5.loss_heatmap: 0.3966, aux_task5.loss_bbox: 0.2748, loss: 10.8089, grad_norm: 93.8350
2025-04-05 21:12:56,952 - mmdet - INFO - Epoch [4][5350/61790]	lr: 2.450e-05, eta: 6 days, 22:07:46, time: 0.577, data_time: 0.008, memory: 7566, loss_cls: 0.3412, loss_bbox: 0.6346, d0.loss_cls: 0.4295, d0.loss_bbox: 0.7835, d1.loss_cls: 0.3899, d1.loss_bbox: 0.6847, d2.loss_cls: 0.3637, d2.loss_bbox: 0.6622, d3.loss_cls: 0.3495, d3.loss_bbox: 0.6495, d4.loss_cls: 0.3425, d4.loss_bbox: 0.6405, aux_task0.loss_heatmap: 0.4950, aux_task0.loss_bbox: 0.1989, aux_task1.loss_heatmap: 0.7167, aux_task1.loss_bbox: 0.2732, aux_task2.loss_heatmap: 0.6810, aux_task2.loss_bbox: 0.2861, aux_task3.loss_heatmap: 0.4146, aux_task3.loss_bbox: 0.2680, aux_task4.loss_heatmap: 0.3820, aux_task4.loss_bbox: 0.2273, aux_task5.loss_heatmap: 0.4238, aux_task5.loss_bbox: 0.2827, loss: 10.9204, grad_norm: 79.1123
2025-04-05 21:13:25,373 - mmdet - INFO - Epoch [4][5400/61790]	lr: 2.451e-05, eta: 6 days, 22:06:40, time: 0.568, data_time: 0.006, memory: 7623, loss_cls: 0.3470, loss_bbox: 0.6263, d0.loss_cls: 0.4342, d0.loss_bbox: 0.7711, d1.loss_cls: 0.3944, d1.loss_bbox: 0.6730, d2.loss_cls: 0.3678, d2.loss_bbox: 0.6530, d3.loss_cls: 0.3569, d3.loss_bbox: 0.6407, d4.loss_cls: 0.3487, d4.loss_bbox: 0.6334, aux_task0.loss_heatmap: 0.4896, aux_task0.loss_bbox: 0.1954, aux_task1.loss_heatmap: 0.6925, aux_task1.loss_bbox: 0.2659, aux_task2.loss_heatmap: 0.7233, aux_task2.loss_bbox: 0.2892, aux_task3.loss_heatmap: 0.4961, aux_task3.loss_bbox: 0.2540, aux_task4.loss_heatmap: 0.3873, aux_task4.loss_bbox: 0.2344, aux_task5.loss_heatmap: 0.4009, aux_task5.loss_bbox: 0.2759, loss: 10.9511, grad_norm: 83.1196
2025-04-05 21:13:54,209 - mmdet - INFO - Epoch [4][5450/61790]	lr: 2.452e-05, eta: 6 days, 22:06:54, time: 0.577, data_time: 0.015, memory: 7623, loss_cls: 0.3544, loss_bbox: 0.6223, d0.loss_cls: 0.4441, d0.loss_bbox: 0.7700, d1.loss_cls: 0.3980, d1.loss_bbox: 0.6759, d2.loss_cls: 0.3731, d2.loss_bbox: 0.6551, d3.loss_cls: 0.3599, d3.loss_bbox: 0.6438, d4.loss_cls: 0.3548, d4.loss_bbox: 0.6304, aux_task0.loss_heatmap: 0.5031, aux_task0.loss_bbox: 0.2027, aux_task1.loss_heatmap: 0.7050, aux_task1.loss_bbox: 0.2595, aux_task2.loss_heatmap: 0.7119, aux_task2.loss_bbox: 0.2915, aux_task3.loss_heatmap: 0.5487, aux_task3.loss_bbox: 0.2626, aux_task4.loss_heatmap: 0.3918, aux_task4.loss_bbox: 0.2238, aux_task5.loss_heatmap: 0.4053, aux_task5.loss_bbox: 0.2774, loss: 11.0651, grad_norm: 81.3351
2025-04-05 21:14:22,379 - mmdet - INFO - Epoch [4][5500/61790]	lr: 2.453e-05, eta: 6 days, 22:05:01, time: 0.563, data_time: 0.007, memory: 7623, loss_cls: 0.3551, loss_bbox: 0.6371, d0.loss_cls: 0.4429, d0.loss_bbox: 0.7741, d1.loss_cls: 0.4014, d1.loss_bbox: 0.6784, d2.loss_cls: 0.3748, d2.loss_bbox: 0.6607, d3.loss_cls: 0.3627, d3.loss_bbox: 0.6521, d4.loss_cls: 0.3572, d4.loss_bbox: 0.6421, aux_task0.loss_heatmap: 0.5282, aux_task0.loss_bbox: 0.2027, aux_task1.loss_heatmap: 0.6855, aux_task1.loss_bbox: 0.2705, aux_task2.loss_heatmap: 0.6929, aux_task2.loss_bbox: 0.2924, aux_task3.loss_heatmap: 0.5665, aux_task3.loss_bbox: 0.2783, aux_task4.loss_heatmap: 0.3782, aux_task4.loss_bbox: 0.2212, aux_task5.loss_heatmap: 0.4528, aux_task5.loss_bbox: 0.2847, loss: 11.1924, grad_norm: 96.5664
2025-04-05 21:14:50,902 - mmdet - INFO - Epoch [4][5550/61790]	lr: 2.453e-05, eta: 6 days, 22:04:16, time: 0.570, data_time: 0.009, memory: 7623, loss_cls: 0.3554, loss_bbox: 0.6241, d0.loss_cls: 0.4500, d0.loss_bbox: 0.7661, d1.loss_cls: 0.4051, d1.loss_bbox: 0.6652, d2.loss_cls: 0.3784, d2.loss_bbox: 0.6518, d3.loss_cls: 0.3598, d3.loss_bbox: 0.6417, d4.loss_cls: 0.3544, d4.loss_bbox: 0.6325, aux_task0.loss_heatmap: 0.4936, aux_task0.loss_bbox: 0.1973, aux_task1.loss_heatmap: 0.7155, aux_task1.loss_bbox: 0.2671, aux_task2.loss_heatmap: 0.6917, aux_task2.loss_bbox: 0.2822, aux_task3.loss_heatmap: 0.5628, aux_task3.loss_bbox: 0.2568, aux_task4.loss_heatmap: 0.4079, aux_task4.loss_bbox: 0.2358, aux_task5.loss_heatmap: 0.3990, aux_task5.loss_bbox: 0.2698, loss: 11.0640, grad_norm: 91.6046
2025-04-05 21:15:19,497 - mmdet - INFO - Epoch [4][5600/61790]	lr: 2.454e-05, eta: 6 days, 22:03:45, time: 0.572, data_time: 0.008, memory: 7623, loss_cls: 0.3503, loss_bbox: 0.6368, d0.loss_cls: 0.4394, d0.loss_bbox: 0.7729, d1.loss_cls: 0.3944, d1.loss_bbox: 0.6784, d2.loss_cls: 0.3679, d2.loss_bbox: 0.6606, d3.loss_cls: 0.3565, d3.loss_bbox: 0.6509, d4.loss_cls: 0.3516, d4.loss_bbox: 0.6426, aux_task0.loss_heatmap: 0.5102, aux_task0.loss_bbox: 0.1975, aux_task1.loss_heatmap: 0.7016, aux_task1.loss_bbox: 0.2797, aux_task2.loss_heatmap: 0.7174, aux_task2.loss_bbox: 0.2816, aux_task3.loss_heatmap: 0.4932, aux_task3.loss_bbox: 0.2575, aux_task4.loss_heatmap: 0.3747, aux_task4.loss_bbox: 0.2185, aux_task5.loss_heatmap: 0.3983, aux_task5.loss_bbox: 0.2763, loss: 11.0088, grad_norm: 79.0752
2025-04-05 21:15:47,977 - mmdet - INFO - Epoch [4][5650/61790]	lr: 2.455e-05, eta: 6 days, 22:02:53, time: 0.570, data_time: 0.012, memory: 7623, loss_cls: 0.3463, loss_bbox: 0.6286, d0.loss_cls: 0.4443, d0.loss_bbox: 0.7759, d1.loss_cls: 0.3979, d1.loss_bbox: 0.6757, d2.loss_cls: 0.3699, d2.loss_bbox: 0.6558, d3.loss_cls: 0.3569, d3.loss_bbox: 0.6409, d4.loss_cls: 0.3482, d4.loss_bbox: 0.6345, aux_task0.loss_heatmap: 0.5181, aux_task0.loss_bbox: 0.2068, aux_task1.loss_heatmap: 0.6863, aux_task1.loss_bbox: 0.2696, aux_task2.loss_heatmap: 0.6768, aux_task2.loss_bbox: 0.2669, aux_task3.loss_heatmap: 0.5055, aux_task3.loss_bbox: 0.2612, aux_task4.loss_heatmap: 0.4047, aux_task4.loss_bbox: 0.2249, aux_task5.loss_heatmap: 0.3936, aux_task5.loss_bbox: 0.2797, loss: 10.9689, grad_norm: 97.9606
2025-04-05 21:16:16,836 - mmdet - INFO - Epoch [4][5700/61790]	lr: 2.456e-05, eta: 6 days, 22:03:10, time: 0.577, data_time: 0.012, memory: 7623, loss_cls: 0.3700, loss_bbox: 0.6674, d0.loss_cls: 0.4622, d0.loss_bbox: 0.8045, d1.loss_cls: 0.4159, d1.loss_bbox: 0.7106, d2.loss_cls: 0.3894, d2.loss_bbox: 0.6944, d3.loss_cls: 0.3766, d3.loss_bbox: 0.6824, d4.loss_cls: 0.3692, d4.loss_bbox: 0.6754, aux_task0.loss_heatmap: 0.5565, aux_task0.loss_bbox: 0.2193, aux_task1.loss_heatmap: 0.7301, aux_task1.loss_bbox: 0.2739, aux_task2.loss_heatmap: 0.7545, aux_task2.loss_bbox: 0.2987, aux_task3.loss_heatmap: 0.5218, aux_task3.loss_bbox: 0.2807, aux_task4.loss_heatmap: 0.3913, aux_task4.loss_bbox: 0.2248, aux_task5.loss_heatmap: 0.4164, aux_task5.loss_bbox: 0.2767, loss: 11.5629, grad_norm: 86.8790
2025-04-05 21:16:45,857 - mmdet - INFO - Epoch [4][5750/61790]	lr: 2.457e-05, eta: 6 days, 22:03:56, time: 0.580, data_time: 0.011, memory: 7623, loss_cls: 0.3521, loss_bbox: 0.6442, d0.loss_cls: 0.4445, d0.loss_bbox: 0.7854, d1.loss_cls: 0.3994, d1.loss_bbox: 0.6873, d2.loss_cls: 0.3727, d2.loss_bbox: 0.6664, d3.loss_cls: 0.3604, d3.loss_bbox: 0.6569, d4.loss_cls: 0.3530, d4.loss_bbox: 0.6489, aux_task0.loss_heatmap: 0.5171, aux_task0.loss_bbox: 0.2042, aux_task1.loss_heatmap: 0.7476, aux_task1.loss_bbox: 0.2666, aux_task2.loss_heatmap: 0.7154, aux_task2.loss_bbox: 0.2800, aux_task3.loss_heatmap: 0.5064, aux_task3.loss_bbox: 0.2592, aux_task4.loss_heatmap: 0.3605, aux_task4.loss_bbox: 0.2210, aux_task5.loss_heatmap: 0.4130, aux_task5.loss_bbox: 0.2771, loss: 11.1394, grad_norm: 88.1482
2025-04-05 21:17:14,541 - mmdet - INFO - Epoch [4][5800/61790]	lr: 2.458e-05, eta: 6 days, 22:03:40, time: 0.574, data_time: 0.011, memory: 7623, loss_cls: 0.3441, loss_bbox: 0.6268, d0.loss_cls: 0.4322, d0.loss_bbox: 0.7673, d1.loss_cls: 0.3888, d1.loss_bbox: 0.6702, d2.loss_cls: 0.3624, d2.loss_bbox: 0.6515, d3.loss_cls: 0.3508, d3.loss_bbox: 0.6416, d4.loss_cls: 0.3457, d4.loss_bbox: 0.6319, aux_task0.loss_heatmap: 0.4821, aux_task0.loss_bbox: 0.1939, aux_task1.loss_heatmap: 0.6856, aux_task1.loss_bbox: 0.2670, aux_task2.loss_heatmap: 0.7084, aux_task2.loss_bbox: 0.2793, aux_task3.loss_heatmap: 0.5182, aux_task3.loss_bbox: 0.2703, aux_task4.loss_heatmap: 0.3761, aux_task4.loss_bbox: 0.2305, aux_task5.loss_heatmap: 0.3829, aux_task5.loss_bbox: 0.2769, loss: 10.8843, grad_norm: 89.7991
2025-04-05 21:17:43,369 - mmdet - INFO - Epoch [4][5850/61790]	lr: 2.459e-05, eta: 6 days, 22:03:50, time: 0.577, data_time: 0.009, memory: 7623, loss_cls: 0.3451, loss_bbox: 0.6278, d0.loss_cls: 0.4298, d0.loss_bbox: 0.7692, d1.loss_cls: 0.3870, d1.loss_bbox: 0.6779, d2.loss_cls: 0.3642, d2.loss_bbox: 0.6561, d3.loss_cls: 0.3504, d3.loss_bbox: 0.6451, d4.loss_cls: 0.3454, d4.loss_bbox: 0.6335, aux_task0.loss_heatmap: 0.5457, aux_task0.loss_bbox: 0.2100, aux_task1.loss_heatmap: 0.7154, aux_task1.loss_bbox: 0.2754, aux_task2.loss_heatmap: 0.7089, aux_task2.loss_bbox: 0.2831, aux_task3.loss_heatmap: 0.5039, aux_task3.loss_bbox: 0.2509, aux_task4.loss_heatmap: 0.3839, aux_task4.loss_bbox: 0.2213, aux_task5.loss_heatmap: 0.3919, aux_task5.loss_bbox: 0.2823, loss: 11.0041, grad_norm: 95.0592
2025-04-05 21:18:12,282 - mmdet - INFO - Epoch [4][5900/61790]	lr: 2.459e-05, eta: 6 days, 22:04:14, time: 0.578, data_time: 0.010, memory: 7623, loss_cls: 0.3573, loss_bbox: 0.6274, d0.loss_cls: 0.4393, d0.loss_bbox: 0.7637, d1.loss_cls: 0.3955, d1.loss_bbox: 0.6689, d2.loss_cls: 0.3734, d2.loss_bbox: 0.6494, d3.loss_cls: 0.3629, d3.loss_bbox: 0.6408, d4.loss_cls: 0.3586, d4.loss_bbox: 0.6315, aux_task0.loss_heatmap: 0.5137, aux_task0.loss_bbox: 0.2067, aux_task1.loss_heatmap: 0.7006, aux_task1.loss_bbox: 0.2680, aux_task2.loss_heatmap: 0.6696, aux_task2.loss_bbox: 0.2775, aux_task3.loss_heatmap: 0.4631, aux_task3.loss_bbox: 0.2519, aux_task4.loss_heatmap: 0.3841, aux_task4.loss_bbox: 0.2221, aux_task5.loss_heatmap: 0.4502, aux_task5.loss_bbox: 0.2788, loss: 10.9549, grad_norm: 86.3834
2025-04-05 21:18:41,729 - mmdet - INFO - Epoch [4][5950/61790]	lr: 2.460e-05, eta: 6 days, 22:06:11, time: 0.589, data_time: 0.024, memory: 7623, loss_cls: 0.3724, loss_bbox: 0.6299, d0.loss_cls: 0.4563, d0.loss_bbox: 0.7771, d1.loss_cls: 0.4157, d1.loss_bbox: 0.6790, d2.loss_cls: 0.3899, d2.loss_bbox: 0.6629, d3.loss_cls: 0.3776, d3.loss_bbox: 0.6486, d4.loss_cls: 0.3748, d4.loss_bbox: 0.6368, aux_task0.loss_heatmap: 0.5287, aux_task0.loss_bbox: 0.1986, aux_task1.loss_heatmap: 0.7456, aux_task1.loss_bbox: 0.2702, aux_task2.loss_heatmap: 0.7403, aux_task2.loss_bbox: 0.2826, aux_task3.loss_heatmap: 0.4822, aux_task3.loss_bbox: 0.2551, aux_task4.loss_heatmap: 0.3920, aux_task4.loss_bbox: 0.2208, aux_task5.loss_heatmap: 0.4688, aux_task5.loss_bbox: 0.2808, loss: 11.2868, grad_norm: 86.1400
2025-04-05 21:19:10,705 - mmdet - INFO - Epoch [4][6000/61790]	lr: 2.461e-05, eta: 6 days, 22:06:43, time: 0.580, data_time: 0.011, memory: 7623, loss_cls: 0.3369, loss_bbox: 0.6159, d0.loss_cls: 0.4256, d0.loss_bbox: 0.7548, d1.loss_cls: 0.3815, d1.loss_bbox: 0.6602, d2.loss_cls: 0.3533, d2.loss_bbox: 0.6409, d3.loss_cls: 0.3422, d3.loss_bbox: 0.6320, d4.loss_cls: 0.3376, d4.loss_bbox: 0.6224, aux_task0.loss_heatmap: 0.5079, aux_task0.loss_bbox: 0.2009, aux_task1.loss_heatmap: 0.6770, aux_task1.loss_bbox: 0.2539, aux_task2.loss_heatmap: 0.7065, aux_task2.loss_bbox: 0.2840, aux_task3.loss_heatmap: 0.4794, aux_task3.loss_bbox: 0.2531, aux_task4.loss_heatmap: 0.3634, aux_task4.loss_bbox: 0.2197, aux_task5.loss_heatmap: 0.4051, aux_task5.loss_bbox: 0.2726, loss: 10.7267, grad_norm: 94.8275
2025-04-05 21:19:39,428 - mmdet - INFO - Epoch [4][6050/61790]	lr: 2.462e-05, eta: 6 days, 22:06:31, time: 0.574, data_time: 0.011, memory: 7623, loss_cls: 0.3444, loss_bbox: 0.6148, d0.loss_cls: 0.4327, d0.loss_bbox: 0.7642, d1.loss_cls: 0.3898, d1.loss_bbox: 0.6646, d2.loss_cls: 0.3604, d2.loss_bbox: 0.6439, d3.loss_cls: 0.3514, d3.loss_bbox: 0.6312, d4.loss_cls: 0.3435, d4.loss_bbox: 0.6228, aux_task0.loss_heatmap: 0.5154, aux_task0.loss_bbox: 0.1973, aux_task1.loss_heatmap: 0.6934, aux_task1.loss_bbox: 0.2760, aux_task2.loss_heatmap: 0.7273, aux_task2.loss_bbox: 0.2842, aux_task3.loss_heatmap: 0.5416, aux_task3.loss_bbox: 0.2648, aux_task4.loss_heatmap: 0.3905, aux_task4.loss_bbox: 0.2250, aux_task5.loss_heatmap: 0.4038, aux_task5.loss_bbox: 0.2692, loss: 10.9522, grad_norm: 81.8835
2025-04-05 21:20:08,369 - mmdet - INFO - Epoch [4][6100/61790]	lr: 2.463e-05, eta: 6 days, 22:06:55, time: 0.579, data_time: 0.010, memory: 7623, loss_cls: 0.3462, loss_bbox: 0.6264, d0.loss_cls: 0.4296, d0.loss_bbox: 0.7756, d1.loss_cls: 0.3923, d1.loss_bbox: 0.6741, d2.loss_cls: 0.3664, d2.loss_bbox: 0.6504, d3.loss_cls: 0.3547, d3.loss_bbox: 0.6379, d4.loss_cls: 0.3486, d4.loss_bbox: 0.6314, aux_task0.loss_heatmap: 0.5141, aux_task0.loss_bbox: 0.2105, aux_task1.loss_heatmap: 0.7028, aux_task1.loss_bbox: 0.2737, aux_task2.loss_heatmap: 0.6878, aux_task2.loss_bbox: 0.2850, aux_task3.loss_heatmap: 0.5677, aux_task3.loss_bbox: 0.2726, aux_task4.loss_heatmap: 0.3888, aux_task4.loss_bbox: 0.2215, aux_task5.loss_heatmap: 0.4365, aux_task5.loss_bbox: 0.2848, loss: 11.0795, grad_norm: 117.9828
2025-04-05 21:20:37,501 - mmdet - INFO - Epoch [4][6150/61790]	lr: 2.464e-05, eta: 6 days, 22:07:52, time: 0.583, data_time: 0.010, memory: 7623, loss_cls: 0.3401, loss_bbox: 0.6296, d0.loss_cls: 0.4358, d0.loss_bbox: 0.7713, d1.loss_cls: 0.3875, d1.loss_bbox: 0.6750, d2.loss_cls: 0.3603, d2.loss_bbox: 0.6550, d3.loss_cls: 0.3466, d3.loss_bbox: 0.6462, d4.loss_cls: 0.3439, d4.loss_bbox: 0.6338, aux_task0.loss_heatmap: 0.5353, aux_task0.loss_bbox: 0.2124, aux_task1.loss_heatmap: 0.7050, aux_task1.loss_bbox: 0.2691, aux_task2.loss_heatmap: 0.6292, aux_task2.loss_bbox: 0.2686, aux_task3.loss_heatmap: 0.5622, aux_task3.loss_bbox: 0.2729, aux_task4.loss_heatmap: 0.3750, aux_task4.loss_bbox: 0.2192, aux_task5.loss_heatmap: 0.4111, aux_task5.loss_bbox: 0.2740, loss: 10.9590, grad_norm: 82.2036
2025-04-05 21:21:06,111 - mmdet - INFO - Epoch [4][6200/61790]	lr: 2.464e-05, eta: 6 days, 22:07:19, time: 0.572, data_time: 0.009, memory: 7623, loss_cls: 0.3445, loss_bbox: 0.6261, d0.loss_cls: 0.4239, d0.loss_bbox: 0.7773, d1.loss_cls: 0.3887, d1.loss_bbox: 0.6789, d2.loss_cls: 0.3613, d2.loss_bbox: 0.6571, d3.loss_cls: 0.3498, d3.loss_bbox: 0.6452, d4.loss_cls: 0.3463, d4.loss_bbox: 0.6321, aux_task0.loss_heatmap: 0.5168, aux_task0.loss_bbox: 0.1959, aux_task1.loss_heatmap: 0.7017, aux_task1.loss_bbox: 0.2661, aux_task2.loss_heatmap: 0.6938, aux_task2.loss_bbox: 0.2911, aux_task3.loss_heatmap: 0.4974, aux_task3.loss_bbox: 0.2650, aux_task4.loss_heatmap: 0.3900, aux_task4.loss_bbox: 0.2224, aux_task5.loss_heatmap: 0.3912, aux_task5.loss_bbox: 0.2757, loss: 10.9385, grad_norm: 85.7842
2025-04-05 21:21:35,056 - mmdet - INFO - Epoch [4][6250/61790]	lr: 2.465e-05, eta: 6 days, 22:07:42, time: 0.579, data_time: 0.012, memory: 7623, loss_cls: 0.3554, loss_bbox: 0.6307, d0.loss_cls: 0.4437, d0.loss_bbox: 0.7832, d1.loss_cls: 0.4019, d1.loss_bbox: 0.6760, d2.loss_cls: 0.3728, d2.loss_bbox: 0.6575, d3.loss_cls: 0.3611, d3.loss_bbox: 0.6471, d4.loss_cls: 0.3554, d4.loss_bbox: 0.6382, aux_task0.loss_heatmap: 0.4872, aux_task0.loss_bbox: 0.1965, aux_task1.loss_heatmap: 0.7072, aux_task1.loss_bbox: 0.2696, aux_task2.loss_heatmap: 0.7331, aux_task2.loss_bbox: 0.2954, aux_task3.loss_heatmap: 0.5344, aux_task3.loss_bbox: 0.2717, aux_task4.loss_heatmap: 0.3986, aux_task4.loss_bbox: 0.2235, aux_task5.loss_heatmap: 0.4153, aux_task5.loss_bbox: 0.2763, loss: 11.1319, grad_norm: 85.8273
2025-04-05 21:22:03,618 - mmdet - INFO - Epoch [4][6300/61790]	lr: 2.466e-05, eta: 6 days, 22:07:00, time: 0.571, data_time: 0.009, memory: 7623, loss_cls: 0.3489, loss_bbox: 0.6277, d0.loss_cls: 0.4308, d0.loss_bbox: 0.7851, d1.loss_cls: 0.3934, d1.loss_bbox: 0.6851, d2.loss_cls: 0.3666, d2.loss_bbox: 0.6619, d3.loss_cls: 0.3523, d3.loss_bbox: 0.6455, d4.loss_cls: 0.3480, d4.loss_bbox: 0.6359, aux_task0.loss_heatmap: 0.5346, aux_task0.loss_bbox: 0.2150, aux_task1.loss_heatmap: 0.6714, aux_task1.loss_bbox: 0.2658, aux_task2.loss_heatmap: 0.7490, aux_task2.loss_bbox: 0.2870, aux_task3.loss_heatmap: 0.5337, aux_task3.loss_bbox: 0.2541, aux_task4.loss_heatmap: 0.3846, aux_task4.loss_bbox: 0.2270, aux_task5.loss_heatmap: 0.4010, aux_task5.loss_bbox: 0.2730, loss: 11.0774, grad_norm: 138.4423
2025-04-05 21:22:32,372 - mmdet - INFO - Epoch [4][6350/61790]	lr: 2.467e-05, eta: 6 days, 22:06:51, time: 0.575, data_time: 0.012, memory: 7623, loss_cls: 0.3564, loss_bbox: 0.6324, d0.loss_cls: 0.4468, d0.loss_bbox: 0.7768, d1.loss_cls: 0.4025, d1.loss_bbox: 0.6770, d2.loss_cls: 0.3749, d2.loss_bbox: 0.6570, d3.loss_cls: 0.3615, d3.loss_bbox: 0.6468, d4.loss_cls: 0.3566, d4.loss_bbox: 0.6362, aux_task0.loss_heatmap: 0.5067, aux_task0.loss_bbox: 0.2016, aux_task1.loss_heatmap: 0.6894, aux_task1.loss_bbox: 0.2685, aux_task2.loss_heatmap: 0.7397, aux_task2.loss_bbox: 0.2832, aux_task3.loss_heatmap: 0.5063, aux_task3.loss_bbox: 0.2648, aux_task4.loss_heatmap: 0.4001, aux_task4.loss_bbox: 0.2280, aux_task5.loss_heatmap: 0.4168, aux_task5.loss_bbox: 0.2731, loss: 11.1030, grad_norm: 82.2651
2025-04-05 21:23:01,122 - mmdet - INFO - Epoch [4][6400/61790]	lr: 2.468e-05, eta: 6 days, 22:06:41, time: 0.575, data_time: 0.010, memory: 7623, loss_cls: 0.3555, loss_bbox: 0.6369, d0.loss_cls: 0.4400, d0.loss_bbox: 0.7714, d1.loss_cls: 0.4010, d1.loss_bbox: 0.6788, d2.loss_cls: 0.3754, d2.loss_bbox: 0.6599, d3.loss_cls: 0.3642, d3.loss_bbox: 0.6475, d4.loss_cls: 0.3561, d4.loss_bbox: 0.6409, aux_task0.loss_heatmap: 0.5542, aux_task0.loss_bbox: 0.2030, aux_task1.loss_heatmap: 0.6813, aux_task1.loss_bbox: 0.2666, aux_task2.loss_heatmap: 0.6914, aux_task2.loss_bbox: 0.2847, aux_task3.loss_heatmap: 0.4878, aux_task3.loss_bbox: 0.2545, aux_task4.loss_heatmap: 0.3743, aux_task4.loss_bbox: 0.2284, aux_task5.loss_heatmap: 0.4254, aux_task5.loss_bbox: 0.2761, loss: 11.0555, grad_norm: 80.2019
2025-04-05 21:23:29,777 - mmdet - INFO - Epoch [4][6450/61790]	lr: 2.469e-05, eta: 6 days, 22:06:14, time: 0.573, data_time: 0.010, memory: 7623, loss_cls: 0.3492, loss_bbox: 0.6266, d0.loss_cls: 0.4336, d0.loss_bbox: 0.7720, d1.loss_cls: 0.3942, d1.loss_bbox: 0.6697, d2.loss_cls: 0.3657, d2.loss_bbox: 0.6501, d3.loss_cls: 0.3547, d3.loss_bbox: 0.6398, d4.loss_cls: 0.3488, d4.loss_bbox: 0.6326, aux_task0.loss_heatmap: 0.5029, aux_task0.loss_bbox: 0.1970, aux_task1.loss_heatmap: 0.6571, aux_task1.loss_bbox: 0.2568, aux_task2.loss_heatmap: 0.7400, aux_task2.loss_bbox: 0.2794, aux_task3.loss_heatmap: 0.5145, aux_task3.loss_bbox: 0.2620, aux_task4.loss_heatmap: 0.3712, aux_task4.loss_bbox: 0.2262, aux_task5.loss_heatmap: 0.4274, aux_task5.loss_bbox: 0.2781, loss: 10.9498, grad_norm: 88.0343
2025-04-05 21:23:58,689 - mmdet - INFO - Epoch [4][6500/61790]	lr: 2.469e-05, eta: 6 days, 22:06:30, time: 0.578, data_time: 0.009, memory: 7623, loss_cls: 0.3560, loss_bbox: 0.6486, d0.loss_cls: 0.4459, d0.loss_bbox: 0.7940, d1.loss_cls: 0.4023, d1.loss_bbox: 0.6942, d2.loss_cls: 0.3777, d2.loss_bbox: 0.6735, d3.loss_cls: 0.3653, d3.loss_bbox: 0.6615, d4.loss_cls: 0.3568, d4.loss_bbox: 0.6535, aux_task0.loss_heatmap: 0.5338, aux_task0.loss_bbox: 0.2091, aux_task1.loss_heatmap: 0.7365, aux_task1.loss_bbox: 0.2756, aux_task2.loss_heatmap: 0.7289, aux_task2.loss_bbox: 0.2899, aux_task3.loss_heatmap: 0.4894, aux_task3.loss_bbox: 0.2539, aux_task4.loss_heatmap: 0.3956, aux_task4.loss_bbox: 0.2239, aux_task5.loss_heatmap: 0.4108, aux_task5.loss_bbox: 0.2695, loss: 11.2459, grad_norm: 80.3557
2025-04-05 21:24:27,140 - mmdet - INFO - Epoch [4][6550/61790]	lr: 2.470e-05, eta: 6 days, 22:05:31, time: 0.569, data_time: 0.011, memory: 7623, loss_cls: 0.3512, loss_bbox: 0.6258, d0.loss_cls: 0.4367, d0.loss_bbox: 0.7746, d1.loss_cls: 0.3965, d1.loss_bbox: 0.6710, d2.loss_cls: 0.3698, d2.loss_bbox: 0.6502, d3.loss_cls: 0.3578, d3.loss_bbox: 0.6392, d4.loss_cls: 0.3533, d4.loss_bbox: 0.6299, aux_task0.loss_heatmap: 0.5092, aux_task0.loss_bbox: 0.2066, aux_task1.loss_heatmap: 0.6948, aux_task1.loss_bbox: 0.2792, aux_task2.loss_heatmap: 0.6795, aux_task2.loss_bbox: 0.2688, aux_task3.loss_heatmap: 0.5932, aux_task3.loss_bbox: 0.2575, aux_task4.loss_heatmap: 0.3916, aux_task4.loss_bbox: 0.2199, aux_task5.loss_heatmap: 0.4177, aux_task5.loss_bbox: 0.2728, loss: 11.0467, grad_norm: 82.9141
2025-04-05 21:24:55,771 - mmdet - INFO - Epoch [4][6600/61790]	lr: 2.471e-05, eta: 6 days, 22:05:01, time: 0.573, data_time: 0.011, memory: 7623, loss_cls: 0.3482, loss_bbox: 0.6193, d0.loss_cls: 0.4355, d0.loss_bbox: 0.7670, d1.loss_cls: 0.3935, d1.loss_bbox: 0.6689, d2.loss_cls: 0.3656, d2.loss_bbox: 0.6472, d3.loss_cls: 0.3552, d3.loss_bbox: 0.6331, d4.loss_cls: 0.3492, d4.loss_bbox: 0.6247, aux_task0.loss_heatmap: 0.4900, aux_task0.loss_bbox: 0.1932, aux_task1.loss_heatmap: 0.7288, aux_task1.loss_bbox: 0.2748, aux_task2.loss_heatmap: 0.7467, aux_task2.loss_bbox: 0.2813, aux_task3.loss_heatmap: 0.4698, aux_task3.loss_bbox: 0.2692, aux_task4.loss_heatmap: 0.3848, aux_task4.loss_bbox: 0.2203, aux_task5.loss_heatmap: 0.4180, aux_task5.loss_bbox: 0.2764, loss: 10.9606, grad_norm: 82.0623
2025-04-05 21:25:24,431 - mmdet - INFO - Epoch [4][6650/61790]	lr: 2.472e-05, eta: 6 days, 22:04:35, time: 0.573, data_time: 0.011, memory: 7623, loss_cls: 0.3585, loss_bbox: 0.6379, d0.loss_cls: 0.4427, d0.loss_bbox: 0.7837, d1.loss_cls: 0.4005, d1.loss_bbox: 0.6841, d2.loss_cls: 0.3728, d2.loss_bbox: 0.6657, d3.loss_cls: 0.3632, d3.loss_bbox: 0.6527, d4.loss_cls: 0.3579, d4.loss_bbox: 0.6449, aux_task0.loss_heatmap: 0.5314, aux_task0.loss_bbox: 0.2122, aux_task1.loss_heatmap: 0.7204, aux_task1.loss_bbox: 0.2770, aux_task2.loss_heatmap: 0.6561, aux_task2.loss_bbox: 0.2780, aux_task3.loss_heatmap: 0.5171, aux_task3.loss_bbox: 0.2670, aux_task4.loss_heatmap: 0.4101, aux_task4.loss_bbox: 0.2312, aux_task5.loss_heatmap: 0.4366, aux_task5.loss_bbox: 0.2823, loss: 11.1842, grad_norm: 80.7426
2025-04-05 21:25:52,880 - mmdet - INFO - Epoch [4][6700/61790]	lr: 2.473e-05, eta: 6 days, 22:03:37, time: 0.569, data_time: 0.009, memory: 7623, loss_cls: 0.3530, loss_bbox: 0.6400, d0.loss_cls: 0.4427, d0.loss_bbox: 0.7800, d1.loss_cls: 0.3995, d1.loss_bbox: 0.6820, d2.loss_cls: 0.3714, d2.loss_bbox: 0.6613, d3.loss_cls: 0.3602, d3.loss_bbox: 0.6510, d4.loss_cls: 0.3533, d4.loss_bbox: 0.6454, aux_task0.loss_heatmap: 0.5109, aux_task0.loss_bbox: 0.2051, aux_task1.loss_heatmap: 0.7137, aux_task1.loss_bbox: 0.2754, aux_task2.loss_heatmap: 0.6455, aux_task2.loss_bbox: 0.2782, aux_task3.loss_heatmap: 0.5741, aux_task3.loss_bbox: 0.2768, aux_task4.loss_heatmap: 0.4029, aux_task4.loss_bbox: 0.2248, aux_task5.loss_heatmap: 0.4512, aux_task5.loss_bbox: 0.2832, loss: 11.1817, grad_norm: 81.8704
2025-04-05 21:26:21,547 - mmdet - INFO - Epoch [4][6750/61790]	lr: 2.474e-05, eta: 6 days, 22:03:12, time: 0.573, data_time: 0.009, memory: 7623, loss_cls: 0.3577, loss_bbox: 0.6360, d0.loss_cls: 0.4528, d0.loss_bbox: 0.7812, d1.loss_cls: 0.4079, d1.loss_bbox: 0.6825, d2.loss_cls: 0.3800, d2.loss_bbox: 0.6617, d3.loss_cls: 0.3674, d3.loss_bbox: 0.6495, d4.loss_cls: 0.3594, d4.loss_bbox: 0.6410, aux_task0.loss_heatmap: 0.5371, aux_task0.loss_bbox: 0.2115, aux_task1.loss_heatmap: 0.7505, aux_task1.loss_bbox: 0.2801, aux_task2.loss_heatmap: 0.6842, aux_task2.loss_bbox: 0.2880, aux_task3.loss_heatmap: 0.6185, aux_task3.loss_bbox: 0.2465, aux_task4.loss_heatmap: 0.4124, aux_task4.loss_bbox: 0.2249, aux_task5.loss_heatmap: 0.3967, aux_task5.loss_bbox: 0.2758, loss: 11.3029, grad_norm: 83.2901
2025-04-05 21:26:50,197 - mmdet - INFO - Epoch [4][6800/61790]	lr: 2.474e-05, eta: 6 days, 22:02:46, time: 0.573, data_time: 0.011, memory: 7623, loss_cls: 0.3463, loss_bbox: 0.6184, d0.loss_cls: 0.4352, d0.loss_bbox: 0.7687, d1.loss_cls: 0.3936, d1.loss_bbox: 0.6644, d2.loss_cls: 0.3644, d2.loss_bbox: 0.6458, d3.loss_cls: 0.3528, d3.loss_bbox: 0.6326, d4.loss_cls: 0.3476, d4.loss_bbox: 0.6223, aux_task0.loss_heatmap: 0.5044, aux_task0.loss_bbox: 0.1987, aux_task1.loss_heatmap: 0.7141, aux_task1.loss_bbox: 0.2635, aux_task2.loss_heatmap: 0.6862, aux_task2.loss_bbox: 0.2799, aux_task3.loss_heatmap: 0.5860, aux_task3.loss_bbox: 0.2543, aux_task4.loss_heatmap: 0.3630, aux_task4.loss_bbox: 0.2177, aux_task5.loss_heatmap: 0.4150, aux_task5.loss_bbox: 0.2754, loss: 10.9502, grad_norm: 79.8567
2025-04-05 21:27:18,947 - mmdet - INFO - Epoch [4][6850/61790]	lr: 2.475e-05, eta: 6 days, 22:02:34, time: 0.575, data_time: 0.014, memory: 7623, loss_cls: 0.3538, loss_bbox: 0.6318, d0.loss_cls: 0.4380, d0.loss_bbox: 0.7717, d1.loss_cls: 0.3980, d1.loss_bbox: 0.6770, d2.loss_cls: 0.3723, d2.loss_bbox: 0.6599, d3.loss_cls: 0.3613, d3.loss_bbox: 0.6471, d4.loss_cls: 0.3557, d4.loss_bbox: 0.6373, aux_task0.loss_heatmap: 0.5093, aux_task0.loss_bbox: 0.2036, aux_task1.loss_heatmap: 0.6832, aux_task1.loss_bbox: 0.2598, aux_task2.loss_heatmap: 0.7401, aux_task2.loss_bbox: 0.2906, aux_task3.loss_heatmap: 0.5408, aux_task3.loss_bbox: 0.2668, aux_task4.loss_heatmap: 0.4010, aux_task4.loss_bbox: 0.2273, aux_task5.loss_heatmap: 0.3768, aux_task5.loss_bbox: 0.2738, loss: 11.0769, grad_norm: 106.8657
2025-04-05 21:27:47,957 - mmdet - INFO - Epoch [4][6900/61790]	lr: 2.476e-05, eta: 6 days, 22:03:01, time: 0.580, data_time: 0.010, memory: 7623, loss_cls: 0.3524, loss_bbox: 0.6267, d0.loss_cls: 0.4389, d0.loss_bbox: 0.7667, d1.loss_cls: 0.3985, d1.loss_bbox: 0.6722, d2.loss_cls: 0.3749, d2.loss_bbox: 0.6504, d3.loss_cls: 0.3607, d3.loss_bbox: 0.6420, d4.loss_cls: 0.3541, d4.loss_bbox: 0.6332, aux_task0.loss_heatmap: 0.5071, aux_task0.loss_bbox: 0.1962, aux_task1.loss_heatmap: 0.6835, aux_task1.loss_bbox: 0.2578, aux_task2.loss_heatmap: 0.7291, aux_task2.loss_bbox: 0.2847, aux_task3.loss_heatmap: 0.4856, aux_task3.loss_bbox: 0.2537, aux_task4.loss_heatmap: 0.4089, aux_task4.loss_bbox: 0.2344, aux_task5.loss_heatmap: 0.4278, aux_task5.loss_bbox: 0.2724, loss: 11.0119, grad_norm: 79.5317
2025-04-05 21:28:18,281 - mmdet - INFO - Epoch [4][6950/61790]	lr: 2.477e-05, eta: 6 days, 22:06:45, time: 0.606, data_time: 0.010, memory: 7623, loss_cls: 0.3505, loss_bbox: 0.6435, d0.loss_cls: 0.4350, d0.loss_bbox: 0.7859, d1.loss_cls: 0.3953, d1.loss_bbox: 0.6879, d2.loss_cls: 0.3674, d2.loss_bbox: 0.6670, d3.loss_cls: 0.3560, d3.loss_bbox: 0.6558, d4.loss_cls: 0.3516, d4.loss_bbox: 0.6492, aux_task0.loss_heatmap: 0.5007, aux_task0.loss_bbox: 0.1993, aux_task1.loss_heatmap: 0.6767, aux_task1.loss_bbox: 0.2633, aux_task2.loss_heatmap: 0.7440, aux_task2.loss_bbox: 0.2873, aux_task3.loss_heatmap: 0.4971, aux_task3.loss_bbox: 0.2820, aux_task4.loss_heatmap: 0.3836, aux_task4.loss_bbox: 0.2217, aux_task5.loss_heatmap: 0.3868, aux_task5.loss_bbox: 0.2782, loss: 11.0659, grad_norm: 127.7680
2025-04-05 21:28:46,989 - mmdet - INFO - Epoch [4][7000/61790]	lr: 2.478e-05, eta: 6 days, 22:06:24, time: 0.574, data_time: 0.009, memory: 7623, loss_cls: 0.3583, loss_bbox: 0.6318, d0.loss_cls: 0.4465, d0.loss_bbox: 0.7815, d1.loss_cls: 0.4027, d1.loss_bbox: 0.6743, d2.loss_cls: 0.3742, d2.loss_bbox: 0.6581, d3.loss_cls: 0.3652, d3.loss_bbox: 0.6448, d4.loss_cls: 0.3601, d4.loss_bbox: 0.6366, aux_task0.loss_heatmap: 0.5110, aux_task0.loss_bbox: 0.2030, aux_task1.loss_heatmap: 0.7063, aux_task1.loss_bbox: 0.2685, aux_task2.loss_heatmap: 0.7076, aux_task2.loss_bbox: 0.2799, aux_task3.loss_heatmap: 0.5447, aux_task3.loss_bbox: 0.2748, aux_task4.loss_heatmap: 0.4107, aux_task4.loss_bbox: 0.2316, aux_task5.loss_heatmap: 0.4018, aux_task5.loss_bbox: 0.2749, loss: 11.1487, grad_norm: 83.5841
2025-04-05 21:29:15,748 - mmdet - INFO - Epoch [4][7050/61790]	lr: 2.479e-05, eta: 6 days, 22:06:11, time: 0.575, data_time: 0.009, memory: 7623, loss_cls: 0.3619, loss_bbox: 0.6467, d0.loss_cls: 0.4576, d0.loss_bbox: 0.7981, d1.loss_cls: 0.4129, d1.loss_bbox: 0.6982, d2.loss_cls: 0.3818, d2.loss_bbox: 0.6810, d3.loss_cls: 0.3684, d3.loss_bbox: 0.6631, d4.loss_cls: 0.3653, d4.loss_bbox: 0.6535, aux_task0.loss_heatmap: 0.5127, aux_task0.loss_bbox: 0.2153, aux_task1.loss_heatmap: 0.7338, aux_task1.loss_bbox: 0.2720, aux_task2.loss_heatmap: 0.7158, aux_task2.loss_bbox: 0.2936, aux_task3.loss_heatmap: 0.5965, aux_task3.loss_bbox: 0.2495, aux_task4.loss_heatmap: 0.3723, aux_task4.loss_bbox: 0.2283, aux_task5.loss_heatmap: 0.4338, aux_task5.loss_bbox: 0.2870, loss: 11.3989, grad_norm: 96.8564
2025-04-05 21:29:44,297 - mmdet - INFO - Epoch [4][7100/61790]	lr: 2.479e-05, eta: 6 days, 22:05:27, time: 0.571, data_time: 0.009, memory: 7623, loss_cls: 0.3536, loss_bbox: 0.6465, d0.loss_cls: 0.4457, d0.loss_bbox: 0.7914, d1.loss_cls: 0.3980, d1.loss_bbox: 0.6920, d2.loss_cls: 0.3718, d2.loss_bbox: 0.6724, d3.loss_cls: 0.3620, d3.loss_bbox: 0.6596, d4.loss_cls: 0.3552, d4.loss_bbox: 0.6530, aux_task0.loss_heatmap: 0.5338, aux_task0.loss_bbox: 0.2054, aux_task1.loss_heatmap: 0.7336, aux_task1.loss_bbox: 0.2710, aux_task2.loss_heatmap: 0.7025, aux_task2.loss_bbox: 0.2918, aux_task3.loss_heatmap: 0.5031, aux_task3.loss_bbox: 0.2689, aux_task4.loss_heatmap: 0.3893, aux_task4.loss_bbox: 0.2250, aux_task5.loss_heatmap: 0.4170, aux_task5.loss_bbox: 0.2743, loss: 11.2171, grad_norm: 79.8837
2025-04-05 21:30:13,018 - mmdet - INFO - Epoch [4][7150/61790]	lr: 2.480e-05, eta: 6 days, 22:05:08, time: 0.574, data_time: 0.011, memory: 7623, loss_cls: 0.3582, loss_bbox: 0.6467, d0.loss_cls: 0.4442, d0.loss_bbox: 0.7853, d1.loss_cls: 0.3991, d1.loss_bbox: 0.6877, d2.loss_cls: 0.3767, d2.loss_bbox: 0.6699, d3.loss_cls: 0.3647, d3.loss_bbox: 0.6600, d4.loss_cls: 0.3608, d4.loss_bbox: 0.6515, aux_task0.loss_heatmap: 0.4950, aux_task0.loss_bbox: 0.1951, aux_task1.loss_heatmap: 0.7578, aux_task1.loss_bbox: 0.2813, aux_task2.loss_heatmap: 0.6966, aux_task2.loss_bbox: 0.2910, aux_task3.loss_heatmap: 0.5196, aux_task3.loss_bbox: 0.2762, aux_task4.loss_heatmap: 0.3982, aux_task4.loss_bbox: 0.2304, aux_task5.loss_heatmap: 0.3966, aux_task5.loss_bbox: 0.2804, loss: 11.2230, grad_norm: 87.7255
2025-04-05 21:30:41,699 - mmdet - INFO - Epoch [4][7200/61790]	lr: 2.481e-05, eta: 6 days, 22:04:43, time: 0.574, data_time: 0.009, memory: 7623, loss_cls: 0.3489, loss_bbox: 0.6185, d0.loss_cls: 0.4322, d0.loss_bbox: 0.7730, d1.loss_cls: 0.3953, d1.loss_bbox: 0.6702, d2.loss_cls: 0.3669, d2.loss_bbox: 0.6458, d3.loss_cls: 0.3582, d3.loss_bbox: 0.6297, d4.loss_cls: 0.3509, d4.loss_bbox: 0.6227, aux_task0.loss_heatmap: 0.4955, aux_task0.loss_bbox: 0.2011, aux_task1.loss_heatmap: 0.7010, aux_task1.loss_bbox: 0.2813, aux_task2.loss_heatmap: 0.7380, aux_task2.loss_bbox: 0.2855, aux_task3.loss_heatmap: 0.4797, aux_task3.loss_bbox: 0.2484, aux_task4.loss_heatmap: 0.4036, aux_task4.loss_bbox: 0.2297, aux_task5.loss_heatmap: 0.3857, aux_task5.loss_bbox: 0.2754, loss: 10.9374, grad_norm: 75.7857
2025-04-05 21:31:10,487 - mmdet - INFO - Epoch [4][7250/61790]	lr: 2.482e-05, eta: 6 days, 22:04:33, time: 0.576, data_time: 0.011, memory: 7623, loss_cls: 0.3560, loss_bbox: 0.6300, d0.loss_cls: 0.4466, d0.loss_bbox: 0.7747, d1.loss_cls: 0.4000, d1.loss_bbox: 0.6740, d2.loss_cls: 0.3751, d2.loss_bbox: 0.6553, d3.loss_cls: 0.3613, d3.loss_bbox: 0.6453, d4.loss_cls: 0.3561, d4.loss_bbox: 0.6368, aux_task0.loss_heatmap: 0.4863, aux_task0.loss_bbox: 0.1961, aux_task1.loss_heatmap: 0.7356, aux_task1.loss_bbox: 0.2716, aux_task2.loss_heatmap: 0.6622, aux_task2.loss_bbox: 0.2792, aux_task3.loss_heatmap: 0.5117, aux_task3.loss_bbox: 0.2724, aux_task4.loss_heatmap: 0.3741, aux_task4.loss_bbox: 0.2237, aux_task5.loss_heatmap: 0.4464, aux_task5.loss_bbox: 0.2815, loss: 11.0522, grad_norm: 88.5744
