2025-05-06 16:52:18,132 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1,2,3: NVIDIA GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.6, V11.6.55
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~18.04) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.4
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+
spconv2.0: True
------------------------------------------------------------

2025-05-06 16:52:20,388 - mmdet - INFO - 分布式训练: True
2025-05-06 16:52:22,539 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=False,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='ObjectSample',
        db_sampler=dict(
            data_root='data/nuscenes/',
            info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
            rate=1.0,
            prepare=dict(
                filter_by_difficulty=[-1],
                filter_by_min_points=dict(
                    car=5,
                    truck=5,
                    bus=5,
                    trailer=5,
                    construction_vehicle=5,
                    traffic_cone=5,
                    barrier=5,
                    motorcycle=5,
                    bicycle=5,
                    pedestrian=5)),
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            sample_groups=dict(
                car=2,
                truck=3,
                construction_vehicle=7,
                bus=4,
                trailer=6,
                barrier=2,
                motorcycle=6,
                bicycle=6,
                pedestrian=2,
                traffic_cone=2),
            points_loader=dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk')))),
    dict(
        type='GlobalRotScaleTrans',
        rot_range=[-0.785, 0.785],
        scale_ratio_range=[0.9, 1.1],
        translation_std=[0.5, 0.5, 0.5]),
    dict(
        type='RandomFlip3D',
        sync_2d=False,
        flip_ratio_bev_horizontal=0.5,
        flip_ratio_bev_vertical=0.5),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(type='Collect3D', keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='CBGSDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=10,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[-0.3925, 0.3925],
                scale_ratio_range=[0.95, 1.05],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D', flip_ratio_bev_horizontal=0.5),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-50, -50, -5, 50, 50, 3]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-50, -50, -5, 50, 50, 3]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                    'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                    'barrier'
                ]),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                    'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                    'barrier'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        dataset=dict(
            type='NuScenesDataset',
            data_root='data/nuscenes/',
            ann_file='data/nuscenes/nuscenes_infos_train.pkl',
            pipeline=[
                dict(
                    type='LoadPointsFromFile',
                    coord_type='LIDAR',
                    load_dim=5,
                    use_dim=5,
                    file_client_args=dict(backend='disk')),
                dict(
                    type='LoadPointsFromMultiSweeps',
                    sweeps_num=9,
                    use_dim=[0, 1, 2, 3, 4],
                    file_client_args=dict(backend='disk'),
                    pad_empty_sweeps=True,
                    remove_close=True),
                dict(
                    type='LoadAnnotations3D',
                    with_bbox_3d=True,
                    with_label_3d=True),
                dict(
                    type='ObjectSample',
                    db_sampler=dict(
                        data_root='data/nuscenes/',
                        info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
                        rate=1.0,
                        prepare=dict(
                            filter_by_difficulty=[-1],
                            filter_by_min_points=dict(
                                car=5,
                                truck=5,
                                bus=5,
                                trailer=5,
                                construction_vehicle=5,
                                traffic_cone=5,
                                barrier=5,
                                motorcycle=5,
                                bicycle=5,
                                pedestrian=5)),
                        classes=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        sample_groups=dict(
                            car=2,
                            truck=3,
                            construction_vehicle=7,
                            bus=4,
                            trailer=6,
                            barrier=2,
                            motorcycle=6,
                            bicycle=6,
                            pedestrian=2,
                            traffic_cone=2),
                        points_loader=dict(
                            type='LoadPointsFromFile',
                            coord_type='LIDAR',
                            load_dim=5,
                            use_dim=[0, 1, 2, 3, 4],
                            file_client_args=dict(backend='disk')))),
                dict(
                    type='GlobalRotScaleTrans',
                    rot_range=[-0.785, 0.785],
                    scale_ratio_range=[0.9, 1.1],
                    translation_std=[0.5, 0.5, 0.5]),
                dict(
                    type='RandomFlip3D',
                    sync_2d=False,
                    flip_ratio_bev_horizontal=0.5,
                    flip_ratio_bev_vertical=0.5),
                dict(
                    type='PointsRangeFilter',
                    point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                dict(
                    type='ObjectRangeFilter',
                    point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                dict(
                    type='ObjectNameFilter',
                    classes=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ]),
                dict(type='PointShuffle'),
                dict(
                    type='DefaultFormatBundle3D',
                    class_names=[
                        'car', 'truck', 'construction_vehicle', 'bus',
                        'trailer', 'barrier', 'motorcycle', 'bicycle',
                        'pedestrian', 'traffic_cone'
                    ]),
                dict(
                    type='Collect3D',
                    keys=['points', 'gt_bboxes_3d', 'gt_labels_3d'])
            ],
            classes=[
                'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                'traffic_cone'
            ],
            test_mode=False,
            use_valid_flag=True,
            box_type_3d='LiDAR')),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=False,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
optimizer = dict(type='AdamW', lr=1.25e-05, weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='cyclic',
    target_ratio=(10, 0.0001),
    cyclic_times=1,
    step_ratio_up=0.4)
momentum_config = dict(
    policy='cyclic',
    target_ratio=(0.8947368421052632, 1),
    cyclic_times=1,
    step_ratio_up=0.4)
runner = dict(type='EpochBasedRunner', max_epochs=20)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/TinyMortonXYDrop01/全量训练'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    aux_weight=0.5,
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=True,
        dss_batch_first=False,
        dss_drop_prob=0.1,
        dss_mamba_prenorm=False,
        dss_mamba_cfg=dict(),
        dss_mamba_version='DSSMamba_Tiny',
        dss_num_layers=2,
        dss_rope=False,
        dss_morton_rearrange=True,
        dss_conv_path=False,
        dss_xy=True,
        anchor_size=3,
        use_aux=True,
        aux_head=dict(
            type='CenterHead',
            in_channels=512,
            tasks=[
                dict(num_class=1, class_names=['car']),
                dict(
                    num_class=2, class_names=['truck',
                                              'construction_vehicle']),
                dict(num_class=2, class_names=['bus', 'trailer']),
                dict(num_class=1, class_names=['barrier']),
                dict(num_class=2, class_names=['motorcycle', 'bicycle']),
                dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
            ],
            common_heads=dict(
                reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
            share_conv_channel=64,
            bbox_coder=dict(
                type='CenterPointBBoxCoder',
                pc_range=[-54, -54],
                post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
                max_num=500,
                score_threshold=0.1,
                out_size_factor=8,
                voxel_size=[0.075, 0.075],
                code_size=9),
            separate_head=dict(
                type='SeparateHead', init_bias=-2.19, final_kernel=3),
            loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
            loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
            norm_bbox=True),
        mix_selection=False,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(type='FUTR3DAttention', embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
custom_hooks = [dict(type='FadeOjectSampleHook', num_last_epochs=5)]
gpu_ids = range(0, 4)

2025-05-06 16:52:22,540 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-05-06 16:52:23,918 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.0.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_f - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_b - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_f - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_b - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_f_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.A_log_b_xy - torch.Size([256, 4]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_f_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.D_b_xy - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.in_proj.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.in_proj_xy.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_f.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_b.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_f_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.x_proj_b_xy.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_f_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b_xy.weight - torch.Size([256, 16]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.dt_proj_b_xy.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.out_proj.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.global_proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.layers.1.mamba.global_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.conv.weight - torch.Size([64, 512, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.shared_conv.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.reg.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.1.weight - torch.Size([1, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.height.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.1.weight - torch.Size([3, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.dim.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.rot.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.vel.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight - torch.Size([64, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight - torch.Size([2, 64, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias - torch.Size([2]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-05-06 16:52:24,195 - mmdet - INFO - Model:
FUTR3D(
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): SyncBatchNorm(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): SyncBatchNorm(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): DSS(
                (layers): ModuleList(
                  (0): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): Identity()
                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  )
                  (1): ModuleDict(
                    (mamba): DSSMamba(
                      (in_proj): Linear(in_features=256, out_features=1024, bias=False)
                      (in_proj_xy): Linear(in_features=256, out_features=1024, bias=False)
                      (act_f): SiLU()
                      (act_b): SiLU()
                      (x_proj_f): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b): Linear(in_features=16, out_features=256, bias=True)
                      (act_f_xy): SiLU()
                      (act_b_xy): SiLU()
                      (x_proj_f_xy): Linear(in_features=256, out_features=24, bias=False)
                      (x_proj_b_xy): Linear(in_features=256, out_features=24, bias=False)
                      (dt_proj_f_xy): Linear(in_features=16, out_features=256, bias=True)
                      (dt_proj_b_xy): Linear(in_features=16, out_features=256, bias=True)
                      (out_proj): Linear(in_features=1024, out_features=256, bias=False)
                      (global_proj): Linear(in_features=1024, out_features=1024, bias=True)
                    )
                    (dropout): DropPath(drop_prob=0.100)
                    (norm): Identity()
                  )
                )
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=10, bias=True)
      (1): Linear(in_features=256, out_features=10, bias=True)
      (2): Linear(in_features=256, out_features=10, bias=True)
      (3): Linear(in_features=256, out_features=10, bias=True)
      (4): Linear(in_features=256, out_features=10, bias=True)
      (5): Linear(in_features=256, out_features=10, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
    (aux_head): CenterHead(
      (loss_cls): GaussianFocalLoss()
      (loss_bbox): L1Loss()
      (shared_conv): ConvModule(
        (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (task_heads): ModuleList(
        (0): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (1): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (2): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (3): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (4): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
        (5): SeparateHead(
          (reg): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (height): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (dim): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (rot): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (vel): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (heatmap): Sequential(
            (0): ConvModule(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (activate): ReLU(inplace=True)
            )
            (1): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
      )
    )
  )
)
2025-05-06 16:53:02,062 - mmdet - INFO - Start running, host: root@mlkd-server2, work_dir: /root/disk/projects/FUTR3D/work_dirs/TinyMortonXYDrop01/全量训练
2025-05-06 16:53:02,063 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(NORMAL      ) FadeOjectSampleHook                
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CyclicLrUpdaterHook                
(HIGH        ) CyclicMomentumUpdaterHook          
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-05-06 16:53:02,063 - mmdet - INFO - workflow: [('train', 1)], max: 20 epochs
2025-05-06 16:53:02,063 - mmdet - INFO - Checkpoints will be saved to /root/disk/projects/FUTR3D/work_dirs/TinyMortonXYDrop01/全量训练 by HardDiskBackend.
2025-05-06 16:54:39,187 - mmdet - INFO - Epoch [1][50/30895]	lr: 1.250e-05, eta: 13 days, 20:59:49, time: 1.940, data_time: 0.272, memory: 8482, loss_cls: 1.5454, loss_bbox: 2.2675, d0.loss_cls: 1.8733, d0.loss_bbox: 4.4387, d1.loss_cls: 1.6397, d1.loss_bbox: 2.3012, d2.loss_cls: 1.6012, d2.loss_bbox: 2.2877, d3.loss_cls: 1.5472, d3.loss_bbox: 2.2805, d4.loss_cls: 1.5264, d4.loss_bbox: 2.2728, aux_task0.loss_heatmap: 425.3757, aux_task0.loss_bbox: 0.8548, aux_task1.loss_heatmap: 789.4768, aux_task1.loss_bbox: 1.0646, aux_task2.loss_heatmap: 1150.6194, aux_task2.loss_bbox: 1.1986, aux_task3.loss_heatmap: 966.1384, aux_task3.loss_bbox: 0.8188, aux_task4.loss_heatmap: 537.3321, aux_task4.loss_bbox: 0.8352, aux_task5.loss_heatmap: 729.3249, aux_task5.loss_bbox: 0.8117, loss: 4629.4324, grad_norm: 88619.0720
2025-05-06 16:55:52,380 - mmdet - INFO - Epoch [1][100/30895]	lr: 1.250e-05, eta: 12 days, 4:06:28, time: 1.464, data_time: 0.005, memory: 8882, loss_cls: 1.1877, loss_bbox: 2.1715, d0.loss_cls: 1.4069, d0.loss_bbox: 4.3565, d1.loss_cls: 1.2157, d1.loss_bbox: 2.2369, d2.loss_cls: 1.1960, d2.loss_bbox: 2.2132, d3.loss_cls: 1.1908, d3.loss_bbox: 2.1968, d4.loss_cls: 1.1883, d4.loss_bbox: 2.1804, aux_task0.loss_heatmap: 215.7712, aux_task0.loss_bbox: 0.7288, aux_task1.loss_heatmap: 526.7815, aux_task1.loss_bbox: 0.8977, aux_task2.loss_heatmap: 709.3178, aux_task2.loss_bbox: 1.1363, aux_task3.loss_heatmap: 416.1797, aux_task3.loss_bbox: 0.6582, aux_task4.loss_heatmap: 315.9546, aux_task4.loss_bbox: 0.5935, aux_task5.loss_heatmap: 392.9418, aux_task5.loss_bbox: 0.6540, loss: 2604.3558, grad_norm: 20542.5635
2025-05-06 16:57:05,967 - mmdet - INFO - Epoch [1][150/30895]	lr: 1.250e-05, eta: 11 days, 14:53:46, time: 1.472, data_time: 0.005, memory: 8882, loss_cls: 1.1590, loss_bbox: 1.9421, d0.loss_cls: 1.1705, d0.loss_bbox: 4.0726, d1.loss_cls: 1.1645, d1.loss_bbox: 2.0920, d2.loss_cls: 1.1669, d2.loss_bbox: 2.0420, d3.loss_cls: 1.1640, d3.loss_bbox: 2.0030, d4.loss_cls: 1.1595, d4.loss_bbox: 1.9676, aux_task0.loss_heatmap: 99.2839, aux_task0.loss_bbox: 0.6060, aux_task1.loss_heatmap: 273.1876, aux_task1.loss_bbox: 0.7573, aux_task2.loss_heatmap: 388.0898, aux_task2.loss_bbox: 0.9589, aux_task3.loss_heatmap: 182.4734, aux_task3.loss_bbox: 0.4965, aux_task4.loss_heatmap: 170.9744, aux_task4.loss_bbox: 0.4999, aux_task5.loss_heatmap: 191.6847, aux_task5.loss_bbox: 0.5334, loss: 1330.6495, grad_norm: 7751.1834
2025-05-06 16:58:18,554 - mmdet - INFO - Epoch [1][200/30895]	lr: 1.250e-05, eta: 11 days, 7:25:48, time: 1.452, data_time: 0.005, memory: 8882, loss_cls: 1.1657, loss_bbox: 1.8499, d0.loss_cls: 1.1722, d0.loss_bbox: 3.1439, d1.loss_cls: 1.1658, d1.loss_bbox: 1.8504, d2.loss_cls: 1.1673, d2.loss_bbox: 1.8001, d3.loss_cls: 1.1651, d3.loss_bbox: 1.7959, d4.loss_cls: 1.1670, d4.loss_bbox: 1.8155, aux_task0.loss_heatmap: 50.3364, aux_task0.loss_bbox: 0.5174, aux_task1.loss_heatmap: 160.4044, aux_task1.loss_bbox: 0.6646, aux_task2.loss_heatmap: 243.4681, aux_task2.loss_bbox: 0.8365, aux_task3.loss_heatmap: 103.7197, aux_task3.loss_bbox: 0.4629, aux_task4.loss_heatmap: 101.0127, aux_task4.loss_bbox: 0.4541, aux_task5.loss_heatmap: 108.2051, aux_task5.loss_bbox: 0.5022, loss: 789.8427, grad_norm: 5114.7421
2025-05-06 16:59:31,548 - mmdet - INFO - Epoch [1][250/30895]	lr: 1.250e-05, eta: 11 days, 3:13:12, time: 1.460, data_time: 0.005, memory: 8882, loss_cls: 1.1552, loss_bbox: 1.9609, d0.loss_cls: 1.1772, d0.loss_bbox: 2.0305, d1.loss_cls: 1.1747, d1.loss_bbox: 1.7732, d2.loss_cls: 1.1735, d2.loss_bbox: 1.8233, d3.loss_cls: 1.1661, d3.loss_bbox: 1.8772, d4.loss_cls: 1.1603, d4.loss_bbox: 1.9274, aux_task0.loss_heatmap: 38.4209, aux_task0.loss_bbox: 0.4959, aux_task1.loss_heatmap: 100.0508, aux_task1.loss_bbox: 0.5978, aux_task2.loss_heatmap: 165.5664, aux_task2.loss_bbox: 0.7288, aux_task3.loss_heatmap: 72.2789, aux_task3.loss_bbox: 0.4593, aux_task4.loss_heatmap: 65.2463, aux_task4.loss_bbox: 0.4479, aux_task5.loss_heatmap: 74.0283, aux_task5.loss_bbox: 0.4738, loss: 537.1949, grad_norm: 3735.1375
2025-05-06 17:00:44,230 - mmdet - INFO - Epoch [1][300/30895]	lr: 1.250e-05, eta: 11 days, 0:11:49, time: 1.453, data_time: 0.005, memory: 8882, loss_cls: 1.1272, loss_bbox: 1.8329, d0.loss_cls: 1.1656, d0.loss_bbox: 1.7647, d1.loss_cls: 1.1632, d1.loss_bbox: 1.7665, d2.loss_cls: 1.1533, d2.loss_bbox: 1.7961, d3.loss_cls: 1.1435, d3.loss_bbox: 1.8246, d4.loss_cls: 1.1276, d4.loss_bbox: 1.8409, aux_task0.loss_heatmap: 27.5971, aux_task0.loss_bbox: 0.4693, aux_task1.loss_heatmap: 67.4906, aux_task1.loss_bbox: 0.5421, aux_task2.loss_heatmap: 110.3462, aux_task2.loss_bbox: 0.6789, aux_task3.loss_heatmap: 53.5250, aux_task3.loss_bbox: 0.4274, aux_task4.loss_heatmap: 43.7852, aux_task4.loss_bbox: 0.4438, aux_task5.loss_heatmap: 44.2984, aux_task5.loss_bbox: 0.4779, loss: 367.7880, grad_norm: 2706.0887
2025-05-06 17:01:57,053 - mmdet - INFO - Epoch [1][350/30895]	lr: 1.250e-05, eta: 10 days, 22:09:15, time: 1.458, data_time: 0.006, memory: 8882, loss_cls: 1.1223, loss_bbox: 1.7702, d0.loss_cls: 1.1524, d0.loss_bbox: 1.7490, d1.loss_cls: 1.1514, d1.loss_bbox: 1.7599, d2.loss_cls: 1.1429, d2.loss_bbox: 1.7692, d3.loss_cls: 1.1345, d3.loss_bbox: 1.7755, d4.loss_cls: 1.1202, d4.loss_bbox: 1.7732, aux_task0.loss_heatmap: 17.0580, aux_task0.loss_bbox: 0.4599, aux_task1.loss_heatmap: 45.6168, aux_task1.loss_bbox: 0.5245, aux_task2.loss_heatmap: 79.4060, aux_task2.loss_bbox: 0.6555, aux_task3.loss_heatmap: 33.8031, aux_task3.loss_bbox: 0.4278, aux_task4.loss_heatmap: 28.9065, aux_task4.loss_bbox: 0.4373, aux_task5.loss_heatmap: 26.1464, aux_task5.loss_bbox: 0.4624, loss: 251.3249, grad_norm: 1923.0906
2025-05-06 17:03:09,988 - mmdet - INFO - Epoch [1][400/30895]	lr: 1.250e-05, eta: 10 days, 20:38:15, time: 1.458, data_time: 0.004, memory: 8882, loss_cls: 1.1341, loss_bbox: 1.7724, d0.loss_cls: 1.1629, d0.loss_bbox: 1.7783, d1.loss_cls: 1.1598, d1.loss_bbox: 1.7764, d2.loss_cls: 1.1493, d2.loss_bbox: 1.7782, d3.loss_cls: 1.1412, d3.loss_bbox: 1.7791, d4.loss_cls: 1.1301, d4.loss_bbox: 1.7734, aux_task0.loss_heatmap: 13.0151, aux_task0.loss_bbox: 0.4755, aux_task1.loss_heatmap: 30.9333, aux_task1.loss_bbox: 0.5219, aux_task2.loss_heatmap: 50.0811, aux_task2.loss_bbox: 0.5893, aux_task3.loss_heatmap: 23.5913, aux_task3.loss_bbox: 0.4491, aux_task4.loss_heatmap: 19.6849, aux_task4.loss_bbox: 0.4450, aux_task5.loss_heatmap: 19.9517, aux_task5.loss_bbox: 0.4673, loss: 177.7407, grad_norm: 1322.5900
2025-05-06 17:04:22,809 - mmdet - INFO - Epoch [1][450/30895]	lr: 1.250e-05, eta: 10 days, 19:24:51, time: 1.456, data_time: 0.004, memory: 8882, loss_cls: 1.1164, loss_bbox: 1.7496, d0.loss_cls: 1.1536, d0.loss_bbox: 1.7690, d1.loss_cls: 1.1505, d1.loss_bbox: 1.7660, d2.loss_cls: 1.1404, d2.loss_bbox: 1.7668, d3.loss_cls: 1.1333, d3.loss_bbox: 1.7654, d4.loss_cls: 1.1166, d4.loss_bbox: 1.7489, aux_task0.loss_heatmap: 10.6627, aux_task0.loss_bbox: 0.4677, aux_task1.loss_heatmap: 19.1125, aux_task1.loss_bbox: 0.4828, aux_task2.loss_heatmap: 35.0667, aux_task2.loss_bbox: 0.5573, aux_task3.loss_heatmap: 15.6264, aux_task3.loss_bbox: 0.4238, aux_task4.loss_heatmap: 13.2717, aux_task4.loss_bbox: 0.4399, aux_task5.loss_heatmap: 12.9384, aux_task5.loss_bbox: 0.4601, loss: 126.8865, grad_norm: 920.1205
2025-05-06 17:05:35,795 - mmdet - INFO - Epoch [1][500/30895]	lr: 1.250e-05, eta: 10 days, 18:29:18, time: 1.460, data_time: 0.005, memory: 8882, loss_cls: 1.0829, loss_bbox: 1.7278, d0.loss_cls: 1.1529, d0.loss_bbox: 1.7606, d1.loss_cls: 1.1485, d1.loss_bbox: 1.7545, d2.loss_cls: 1.1361, d2.loss_bbox: 1.7522, d3.loss_cls: 1.1264, d3.loss_bbox: 1.7520, d4.loss_cls: 1.0877, d4.loss_bbox: 1.7248, aux_task0.loss_heatmap: 6.2807, aux_task0.loss_bbox: 0.4539, aux_task1.loss_heatmap: 14.0005, aux_task1.loss_bbox: 0.4881, aux_task2.loss_heatmap: 24.6993, aux_task2.loss_bbox: 0.5396, aux_task3.loss_heatmap: 12.5026, aux_task3.loss_bbox: 0.4224, aux_task4.loss_heatmap: 9.4185, aux_task4.loss_bbox: 0.4324, aux_task5.loss_heatmap: 8.2400, aux_task5.loss_bbox: 0.4642, loss: 95.1484, grad_norm: 659.3868
2025-05-06 17:06:49,037 - mmdet - INFO - Epoch [1][550/30895]	lr: 1.250e-05, eta: 10 days, 17:48:26, time: 1.465, data_time: 0.004, memory: 8882, loss_cls: 1.0582, loss_bbox: 1.6931, d0.loss_cls: 1.1513, d0.loss_bbox: 1.7315, d1.loss_cls: 1.1429, d1.loss_bbox: 1.7260, d2.loss_cls: 1.1216, d2.loss_bbox: 1.7295, d3.loss_cls: 1.1056, d3.loss_bbox: 1.7246, d4.loss_cls: 1.0589, d4.loss_bbox: 1.7017, aux_task0.loss_heatmap: 4.4758, aux_task0.loss_bbox: 0.4450, aux_task1.loss_heatmap: 9.2311, aux_task1.loss_bbox: 0.4871, aux_task2.loss_heatmap: 16.2833, aux_task2.loss_bbox: 0.5269, aux_task3.loss_heatmap: 8.8123, aux_task3.loss_bbox: 0.4052, aux_task4.loss_heatmap: 6.3808, aux_task4.loss_bbox: 0.4169, aux_task5.loss_heatmap: 6.0047, aux_task5.loss_bbox: 0.4526, loss: 70.8664, grad_norm: 439.3194
2025-05-06 17:08:01,698 - mmdet - INFO - Epoch [1][600/30895]	lr: 1.250e-05, eta: 10 days, 17:03:27, time: 1.452, data_time: 0.004, memory: 8882, loss_cls: 1.0521, loss_bbox: 1.6703, d0.loss_cls: 1.1485, d0.loss_bbox: 1.7261, d1.loss_cls: 1.1401, d1.loss_bbox: 1.7131, d2.loss_cls: 1.1172, d2.loss_bbox: 1.7113, d3.loss_cls: 1.0920, d3.loss_bbox: 1.6940, d4.loss_cls: 1.0497, d4.loss_bbox: 1.6782, aux_task0.loss_heatmap: 3.8392, aux_task0.loss_bbox: 0.4374, aux_task1.loss_heatmap: 6.4621, aux_task1.loss_bbox: 0.4926, aux_task2.loss_heatmap: 12.1610, aux_task2.loss_bbox: 0.5179, aux_task3.loss_heatmap: 6.5488, aux_task3.loss_bbox: 0.4031, aux_task4.loss_heatmap: 4.7199, aux_task4.loss_bbox: 0.4250, aux_task5.loss_heatmap: 4.3283, aux_task5.loss_bbox: 0.4543, loss: 57.5822, grad_norm: 322.8275
2025-05-06 17:09:15,326 - mmdet - INFO - Epoch [1][650/30895]	lr: 1.250e-05, eta: 10 days, 16:41:47, time: 1.473, data_time: 0.006, memory: 8882, loss_cls: 1.0459, loss_bbox: 1.6458, d0.loss_cls: 1.1425, d0.loss_bbox: 1.7126, d1.loss_cls: 1.1337, d1.loss_bbox: 1.6979, d2.loss_cls: 1.0963, d2.loss_bbox: 1.6842, d3.loss_cls: 1.0588, d3.loss_bbox: 1.6635, d4.loss_cls: 1.0414, d4.loss_bbox: 1.6536, aux_task0.loss_heatmap: 3.0081, aux_task0.loss_bbox: 0.4338, aux_task1.loss_heatmap: 4.6147, aux_task1.loss_bbox: 0.4806, aux_task2.loss_heatmap: 7.9304, aux_task2.loss_bbox: 0.4949, aux_task3.loss_heatmap: 4.7326, aux_task3.loss_bbox: 0.4023, aux_task4.loss_heatmap: 3.4724, aux_task4.loss_bbox: 0.4178, aux_task5.loss_heatmap: 3.4858, aux_task5.loss_bbox: 0.4560, loss: 46.5057, grad_norm: 211.8190
2025-05-06 17:10:27,210 - mmdet - INFO - Epoch [1][700/30895]	lr: 1.250e-05, eta: 10 days, 15:56:58, time: 1.438, data_time: 0.005, memory: 8882, loss_cls: 1.0467, loss_bbox: 1.6403, d0.loss_cls: 1.1439, d0.loss_bbox: 1.7264, d1.loss_cls: 1.1317, d1.loss_bbox: 1.7123, d2.loss_cls: 1.0731, d2.loss_bbox: 1.6832, d3.loss_cls: 1.0544, d3.loss_bbox: 1.6633, d4.loss_cls: 1.0397, d4.loss_bbox: 1.6467, aux_task0.loss_heatmap: 2.2261, aux_task0.loss_bbox: 0.4439, aux_task1.loss_heatmap: 3.5272, aux_task1.loss_bbox: 0.4919, aux_task2.loss_heatmap: 5.4432, aux_task2.loss_bbox: 0.4758, aux_task3.loss_heatmap: 3.8285, aux_task3.loss_bbox: 0.3937, aux_task4.loss_heatmap: 2.7175, aux_task4.loss_bbox: 0.4122, aux_task5.loss_heatmap: 2.8277, aux_task5.loss_bbox: 0.4666, loss: 39.8160, grad_norm: 142.3181
2025-05-06 17:11:41,807 - mmdet - INFO - Epoch [1][750/30895]	lr: 1.250e-05, eta: 10 days, 15:54:41, time: 1.491, data_time: 0.005, memory: 8882, loss_cls: 1.0509, loss_bbox: 1.6388, d0.loss_cls: 1.1422, d0.loss_bbox: 1.7404, d1.loss_cls: 1.1264, d1.loss_bbox: 1.7238, d2.loss_cls: 1.0569, d2.loss_bbox: 1.6755, d3.loss_cls: 1.0500, d3.loss_bbox: 1.6668, d4.loss_cls: 1.0486, d4.loss_bbox: 1.6475, aux_task0.loss_heatmap: 2.1914, aux_task0.loss_bbox: 0.4255, aux_task1.loss_heatmap: 2.8042, aux_task1.loss_bbox: 0.4851, aux_task2.loss_heatmap: 3.9165, aux_task2.loss_bbox: 0.4799, aux_task3.loss_heatmap: 2.8259, aux_task3.loss_bbox: 0.4031, aux_task4.loss_heatmap: 2.2428, aux_task4.loss_bbox: 0.4290, aux_task5.loss_heatmap: 2.2828, aux_task5.loss_bbox: 0.4456, loss: 35.4996, grad_norm: 94.4992
2025-05-06 17:17:41,257 - mmdet - INFO - Epoch [1][800/30895]	lr: 1.250e-05, eta: 13 days, 4:55:02, time: 7.189, data_time: 4.229, memory: 8882, loss_cls: 1.0596, loss_bbox: 1.6384, d0.loss_cls: 1.1485, d0.loss_bbox: 1.7498, d1.loss_cls: 1.1319, d1.loss_bbox: 1.7239, d2.loss_cls: 1.0581, d2.loss_bbox: 1.6724, d3.loss_cls: 1.0551, d3.loss_bbox: 1.6548, d4.loss_cls: 1.0599, d4.loss_bbox: 1.6329, aux_task0.loss_heatmap: 1.7971, aux_task0.loss_bbox: 0.4315, aux_task1.loss_heatmap: 2.2971, aux_task1.loss_bbox: 0.4910, aux_task2.loss_heatmap: 2.9889, aux_task2.loss_bbox: 0.4982, aux_task3.loss_heatmap: 2.3522, aux_task3.loss_bbox: 0.3871, aux_task4.loss_heatmap: 1.9545, aux_task4.loss_bbox: 0.4170, aux_task5.loss_heatmap: 2.0518, aux_task5.loss_bbox: 0.4488, loss: 32.7003, grad_norm: 66.0465
2025-05-06 17:29:36,149 - mmdet - INFO - Epoch [1][850/30895]	lr: 1.250e-05, eta: 18 days, 10:24:48, time: 14.298, data_time: 5.830, memory: 8978, loss_cls: 1.0534, loss_bbox: 1.6024, d0.loss_cls: 1.1411, d0.loss_bbox: 1.7351, d1.loss_cls: 1.1169, d1.loss_bbox: 1.7128, d2.loss_cls: 1.0467, d2.loss_bbox: 1.6439, d3.loss_cls: 1.0443, d3.loss_bbox: 1.6193, d4.loss_cls: 1.0481, d4.loss_bbox: 1.6094, aux_task0.loss_heatmap: 1.6268, aux_task0.loss_bbox: 0.4241, aux_task1.loss_heatmap: 1.9704, aux_task1.loss_bbox: 0.4742, aux_task2.loss_heatmap: 2.4228, aux_task2.loss_bbox: 0.4724, aux_task3.loss_heatmap: 2.0173, aux_task3.loss_bbox: 0.3952, aux_task4.loss_heatmap: 1.7700, aux_task4.loss_bbox: 0.4304, aux_task5.loss_heatmap: 1.8534, aux_task5.loss_bbox: 0.4467, loss: 30.6772, grad_norm: 54.4542
2025-05-06 17:45:14,868 - mmdet - INFO - Epoch [1][900/30895]	lr: 1.250e-05, eta: 24 days, 20:33:44, time: 18.774, data_time: 8.001, memory: 8978, loss_cls: 1.0478, loss_bbox: 1.6051, d0.loss_cls: 1.1346, d0.loss_bbox: 1.7314, d1.loss_cls: 1.1055, d1.loss_bbox: 1.7014, d2.loss_cls: 1.0414, d2.loss_bbox: 1.6310, d3.loss_cls: 1.0397, d3.loss_bbox: 1.6095, d4.loss_cls: 1.0457, d4.loss_bbox: 1.6079, aux_task0.loss_heatmap: 1.5777, aux_task0.loss_bbox: 0.4380, aux_task1.loss_heatmap: 1.8071, aux_task1.loss_bbox: 0.4598, aux_task2.loss_heatmap: 2.2379, aux_task2.loss_bbox: 0.4605, aux_task3.loss_heatmap: 1.8077, aux_task3.loss_bbox: 0.3980, aux_task4.loss_heatmap: 1.5762, aux_task4.loss_bbox: 0.4142, aux_task5.loss_heatmap: 1.7004, aux_task5.loss_bbox: 0.4464, loss: 29.6250, grad_norm: 46.0768
2025-05-06 17:56:08,533 - mmdet - INFO - Epoch [1][950/30895]	lr: 1.250e-05, eta: 28 days, 11:01:25, time: 13.072, data_time: 0.540, memory: 8978, loss_cls: 1.0308, loss_bbox: 1.5915, d0.loss_cls: 1.1144, d0.loss_bbox: 1.7339, d1.loss_cls: 1.0742, d1.loss_bbox: 1.6821, d2.loss_cls: 1.0206, d2.loss_bbox: 1.6204, d3.loss_cls: 1.0199, d3.loss_bbox: 1.6021, d4.loss_cls: 1.0259, d4.loss_bbox: 1.5941, aux_task0.loss_heatmap: 1.5140, aux_task0.loss_bbox: 0.4235, aux_task1.loss_heatmap: 1.7268, aux_task1.loss_bbox: 0.4613, aux_task2.loss_heatmap: 2.1267, aux_task2.loss_bbox: 0.4758, aux_task3.loss_heatmap: 1.6590, aux_task3.loss_bbox: 0.3773, aux_task4.loss_heatmap: 1.4337, aux_task4.loss_bbox: 0.4266, aux_task5.loss_heatmap: 1.6180, aux_task5.loss_bbox: 0.4396, loss: 28.7921, grad_norm: 44.8191
2025-05-06 18:04:07,033 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-06 18:04:07,033 - mmdet - INFO - Epoch [1][1000/30895]	lr: 1.250e-05, eta: 30 days, 10:49:39, time: 9.571, data_time: 2.862, memory: 8978, loss_cls: 0.9910, loss_bbox: 1.5796, d0.loss_cls: 1.0851, d0.loss_bbox: 1.7266, d1.loss_cls: 1.0268, d1.loss_bbox: 1.6622, d2.loss_cls: 0.9929, d2.loss_bbox: 1.6077, d3.loss_cls: 0.9855, d3.loss_bbox: 1.5892, d4.loss_cls: 0.9912, d4.loss_bbox: 1.5829, aux_task0.loss_heatmap: 1.4550, aux_task0.loss_bbox: 0.4326, aux_task1.loss_heatmap: 1.6739, aux_task1.loss_bbox: 0.4605, aux_task2.loss_heatmap: 2.0493, aux_task2.loss_bbox: 0.4677, aux_task3.loss_heatmap: 1.6257, aux_task3.loss_bbox: 0.3912, aux_task4.loss_heatmap: 1.3518, aux_task4.loss_bbox: 0.4191, aux_task5.loss_heatmap: 1.5508, aux_task5.loss_bbox: 0.4433, loss: 28.1415, grad_norm: 45.3274
2025-05-06 18:15:05,374 - mmdet - INFO - Epoch [1][1050/30895]	lr: 1.251e-05, eta: 33 days, 11:24:16, time: 13.167, data_time: 9.938, memory: 8978, loss_cls: 0.9721, loss_bbox: 1.5925, d0.loss_cls: 1.0609, d0.loss_bbox: 1.7500, d1.loss_cls: 0.9990, d1.loss_bbox: 1.6730, d2.loss_cls: 0.9760, d2.loss_bbox: 1.6209, d3.loss_cls: 0.9672, d3.loss_bbox: 1.6069, d4.loss_cls: 0.9696, d4.loss_bbox: 1.6008, aux_task0.loss_heatmap: 1.4115, aux_task0.loss_bbox: 0.4220, aux_task1.loss_heatmap: 1.6580, aux_task1.loss_bbox: 0.4600, aux_task2.loss_heatmap: 1.9177, aux_task2.loss_bbox: 0.4817, aux_task3.loss_heatmap: 1.5229, aux_task3.loss_bbox: 0.3776, aux_task4.loss_heatmap: 1.2606, aux_task4.loss_bbox: 0.4042, aux_task5.loss_heatmap: 1.4567, aux_task5.loss_bbox: 0.4385, loss: 27.6002, grad_norm: 42.3581
2025-05-06 18:27:31,452 - mmdet - INFO - Epoch [1][1100/30895]	lr: 1.251e-05, eta: 36 days, 19:01:47, time: 14.921, data_time: 5.156, memory: 8990, loss_cls: 0.9563, loss_bbox: 1.5652, d0.loss_cls: 1.0397, d0.loss_bbox: 1.7358, d1.loss_cls: 0.9729, d1.loss_bbox: 1.6425, d2.loss_cls: 0.9576, d2.loss_bbox: 1.5931, d3.loss_cls: 0.9514, d3.loss_bbox: 1.5769, d4.loss_cls: 0.9507, d4.loss_bbox: 1.5706, aux_task0.loss_heatmap: 1.4292, aux_task0.loss_bbox: 0.4253, aux_task1.loss_heatmap: 1.5774, aux_task1.loss_bbox: 0.4463, aux_task2.loss_heatmap: 1.8329, aux_task2.loss_bbox: 0.4747, aux_task3.loss_heatmap: 1.4670, aux_task3.loss_bbox: 0.3835, aux_task4.loss_heatmap: 1.1955, aux_task4.loss_bbox: 0.3931, aux_task5.loss_heatmap: 1.4270, aux_task5.loss_bbox: 0.4388, loss: 27.0035, grad_norm: 44.0526
2025-05-06 18:39:38,238 - mmdet - INFO - Epoch [1][1150/30895]	lr: 1.251e-05, eta: 39 days, 16:50:30, time: 14.536, data_time: 4.868, memory: 8990, loss_cls: 0.9412, loss_bbox: 1.5623, d0.loss_cls: 1.0263, d0.loss_bbox: 1.7283, d1.loss_cls: 0.9539, d1.loss_bbox: 1.6326, d2.loss_cls: 0.9441, d2.loss_bbox: 1.5899, d3.loss_cls: 0.9407, d3.loss_bbox: 1.5740, d4.loss_cls: 0.9394, d4.loss_bbox: 1.5684, aux_task0.loss_heatmap: 1.3340, aux_task0.loss_bbox: 0.4076, aux_task1.loss_heatmap: 1.5932, aux_task1.loss_bbox: 0.4422, aux_task2.loss_heatmap: 1.7951, aux_task2.loss_bbox: 0.4747, aux_task3.loss_heatmap: 1.4813, aux_task3.loss_bbox: 0.3690, aux_task4.loss_heatmap: 1.1324, aux_task4.loss_bbox: 0.3976, aux_task5.loss_heatmap: 1.4045, aux_task5.loss_bbox: 0.4283, loss: 26.6607, grad_norm: 40.0969
2025-05-06 18:54:43,402 - mmdet - INFO - Epoch [1][1200/30895]	lr: 1.251e-05, eta: 43 days, 10:16:45, time: 18.103, data_time: 0.610, memory: 8990, loss_cls: 0.9417, loss_bbox: 1.5605, d0.loss_cls: 1.0254, d0.loss_bbox: 1.7371, d1.loss_cls: 0.9561, d1.loss_bbox: 1.6274, d2.loss_cls: 0.9471, d2.loss_bbox: 1.5858, d3.loss_cls: 0.9406, d3.loss_bbox: 1.5680, d4.loss_cls: 0.9387, d4.loss_bbox: 1.5630, aux_task0.loss_heatmap: 1.3631, aux_task0.loss_bbox: 0.4145, aux_task1.loss_heatmap: 1.5652, aux_task1.loss_bbox: 0.4446, aux_task2.loss_heatmap: 1.8007, aux_task2.loss_bbox: 0.4634, aux_task3.loss_heatmap: 1.4532, aux_task3.loss_bbox: 0.3805, aux_task4.loss_heatmap: 1.0987, aux_task4.loss_bbox: 0.3962, aux_task5.loss_heatmap: 1.3522, aux_task5.loss_bbox: 0.4198, loss: 26.5436, grad_norm: 45.4868
2025-05-06 19:10:10,886 - mmdet - INFO - Epoch [1][1250/30895]	lr: 1.251e-05, eta: 46 days, 23:36:24, time: 18.550, data_time: 4.863, memory: 8990, loss_cls: 0.9242, loss_bbox: 1.5379, d0.loss_cls: 1.0065, d0.loss_bbox: 1.7187, d1.loss_cls: 0.9392, d1.loss_bbox: 1.5987, d2.loss_cls: 0.9294, d2.loss_bbox: 1.5605, d3.loss_cls: 0.9278, d3.loss_bbox: 1.5436, d4.loss_cls: 0.9246, d4.loss_bbox: 1.5375, aux_task0.loss_heatmap: 1.3304, aux_task0.loss_bbox: 0.3989, aux_task1.loss_heatmap: 1.5603, aux_task1.loss_bbox: 0.4410, aux_task2.loss_heatmap: 1.7808, aux_task2.loss_bbox: 0.4765, aux_task3.loss_heatmap: 1.4071, aux_task3.loss_bbox: 0.3585, aux_task4.loss_heatmap: 1.0435, aux_task4.loss_bbox: 0.3856, aux_task5.loss_heatmap: 1.3266, aux_task5.loss_bbox: 0.4083, loss: 26.0660, grad_norm: 42.8701
2025-05-06 19:24:30,944 - mmdet - INFO - Epoch [1][1300/30895]	lr: 1.251e-05, eta: 49 days, 21:27:49, time: 17.201, data_time: 3.755, memory: 8990, loss_cls: 0.9215, loss_bbox: 1.5521, d0.loss_cls: 0.9997, d0.loss_bbox: 1.7383, d1.loss_cls: 0.9291, d1.loss_bbox: 1.6152, d2.loss_cls: 0.9218, d2.loss_bbox: 1.5782, d3.loss_cls: 0.9185, d3.loss_bbox: 1.5607, d4.loss_cls: 0.9153, d4.loss_bbox: 1.5564, aux_task0.loss_heatmap: 1.3336, aux_task0.loss_bbox: 0.4046, aux_task1.loss_heatmap: 1.5224, aux_task1.loss_bbox: 0.4434, aux_task2.loss_heatmap: 1.7382, aux_task2.loss_bbox: 0.4674, aux_task3.loss_heatmap: 1.4050, aux_task3.loss_bbox: 0.3500, aux_task4.loss_heatmap: 1.0393, aux_task4.loss_bbox: 0.3886, aux_task5.loss_heatmap: 1.3056, aux_task5.loss_bbox: 0.4078, loss: 26.0128, grad_norm: 40.6783
2025-05-06 19:38:58,547 - mmdet - INFO - Epoch [1][1350/30895]	lr: 1.251e-05, eta: 52 days, 15:05:01, time: 17.352, data_time: 2.707, memory: 8990, loss_cls: 0.9214, loss_bbox: 1.5105, d0.loss_cls: 0.9880, d0.loss_bbox: 1.7194, d1.loss_cls: 0.9239, d1.loss_bbox: 1.5808, d2.loss_cls: 0.9207, d2.loss_bbox: 1.5421, d3.loss_cls: 0.9175, d3.loss_bbox: 1.5214, d4.loss_cls: 0.9185, d4.loss_bbox: 1.5142, aux_task0.loss_heatmap: 1.3044, aux_task0.loss_bbox: 0.3917, aux_task1.loss_heatmap: 1.5121, aux_task1.loss_bbox: 0.4289, aux_task2.loss_heatmap: 1.7650, aux_task2.loss_bbox: 0.4555, aux_task3.loss_heatmap: 1.3983, aux_task3.loss_bbox: 0.3662, aux_task4.loss_heatmap: 0.9951, aux_task4.loss_bbox: 0.3758, aux_task5.loss_heatmap: 1.2843, aux_task5.loss_bbox: 0.4051, loss: 25.6609, grad_norm: 43.5829
2025-05-06 19:55:51,329 - mmdet - INFO - Epoch [1][1400/30895]	lr: 1.251e-05, eta: 55 days, 21:45:33, time: 20.256, data_time: 4.699, memory: 8990, loss_cls: 0.9044, loss_bbox: 1.5140, d0.loss_cls: 0.9717, d0.loss_bbox: 1.7223, d1.loss_cls: 0.9122, d1.loss_bbox: 1.5802, d2.loss_cls: 0.9091, d2.loss_bbox: 1.5447, d3.loss_cls: 0.9043, d3.loss_bbox: 1.5243, d4.loss_cls: 0.9020, d4.loss_bbox: 1.5184, aux_task0.loss_heatmap: 1.2838, aux_task0.loss_bbox: 0.3950, aux_task1.loss_heatmap: 1.4669, aux_task1.loss_bbox: 0.4156, aux_task2.loss_heatmap: 1.7670, aux_task2.loss_bbox: 0.4512, aux_task3.loss_heatmap: 1.3538, aux_task3.loss_bbox: 0.3573, aux_task4.loss_heatmap: 1.0240, aux_task4.loss_bbox: 0.3797, aux_task5.loss_heatmap: 1.2657, aux_task5.loss_bbox: 0.3925, loss: 25.4602, grad_norm: 44.9805
2025-05-06 20:12:20,142 - mmdet - INFO - Epoch [1][1450/30895]	lr: 1.251e-05, eta: 58 days, 20:09:40, time: 19.777, data_time: 15.315, memory: 8990, loss_cls: 0.8955, loss_bbox: 1.5055, d0.loss_cls: 0.9640, d0.loss_bbox: 1.7151, d1.loss_cls: 0.9043, d1.loss_bbox: 1.5669, d2.loss_cls: 0.8984, d2.loss_bbox: 1.5340, d3.loss_cls: 0.8931, d3.loss_bbox: 1.5147, d4.loss_cls: 0.8923, d4.loss_bbox: 1.5094, aux_task0.loss_heatmap: 1.3058, aux_task0.loss_bbox: 0.3887, aux_task1.loss_heatmap: 1.4845, aux_task1.loss_bbox: 0.4243, aux_task2.loss_heatmap: 1.7634, aux_task2.loss_bbox: 0.4398, aux_task3.loss_heatmap: 1.3319, aux_task3.loss_bbox: 0.3493, aux_task4.loss_heatmap: 0.9681, aux_task4.loss_bbox: 0.3779, aux_task5.loss_heatmap: 1.2500, aux_task5.loss_bbox: 0.3884, loss: 25.2653, grad_norm: 47.4507
2025-05-06 20:25:34,228 - mmdet - INFO - Epoch [1][1500/30895]	lr: 1.251e-05, eta: 60 days, 15:37:13, time: 15.881, data_time: 9.455, memory: 8990, loss_cls: 0.8930, loss_bbox: 1.4676, d0.loss_cls: 0.9586, d0.loss_bbox: 1.6764, d1.loss_cls: 0.9048, d1.loss_bbox: 1.5324, d2.loss_cls: 0.9009, d2.loss_bbox: 1.4990, d3.loss_cls: 0.8940, d3.loss_bbox: 1.4774, d4.loss_cls: 0.8928, d4.loss_bbox: 1.4709, aux_task0.loss_heatmap: 1.2531, aux_task0.loss_bbox: 0.3859, aux_task1.loss_heatmap: 1.4397, aux_task1.loss_bbox: 0.4145, aux_task2.loss_heatmap: 1.7371, aux_task2.loss_bbox: 0.4334, aux_task3.loss_heatmap: 1.3267, aux_task3.loss_bbox: 0.3423, aux_task4.loss_heatmap: 0.9456, aux_task4.loss_bbox: 0.3729, aux_task5.loss_heatmap: 1.2452, aux_task5.loss_bbox: 0.3916, loss: 24.8559, grad_norm: 47.9995
2025-05-06 20:40:47,976 - mmdet - INFO - Epoch [1][1550/30895]	lr: 1.251e-05, eta: 62 days, 21:28:41, time: 18.275, data_time: 13.623, memory: 8990, loss_cls: 0.8921, loss_bbox: 1.4695, d0.loss_cls: 0.9557, d0.loss_bbox: 1.6921, d1.loss_cls: 0.9015, d1.loss_bbox: 1.5361, d2.loss_cls: 0.8990, d2.loss_bbox: 1.5042, d3.loss_cls: 0.8925, d3.loss_bbox: 1.4822, d4.loss_cls: 0.8897, d4.loss_bbox: 1.4754, aux_task0.loss_heatmap: 1.2484, aux_task0.loss_bbox: 0.3800, aux_task1.loss_heatmap: 1.4283, aux_task1.loss_bbox: 0.4084, aux_task2.loss_heatmap: 1.7346, aux_task2.loss_bbox: 0.4541, aux_task3.loss_heatmap: 1.3318, aux_task3.loss_bbox: 0.3517, aux_task4.loss_heatmap: 0.9315, aux_task4.loss_bbox: 0.3640, aux_task5.loss_heatmap: 1.2304, aux_task5.loss_bbox: 0.3953, loss: 24.8485, grad_norm: 46.9273
2025-05-06 20:55:29,227 - mmdet - INFO - Epoch [1][1600/30895]	lr: 1.251e-05, eta: 64 days, 20:28:57, time: 17.626, data_time: 12.977, memory: 8990, loss_cls: 0.8836, loss_bbox: 1.4450, d0.loss_cls: 0.9423, d0.loss_bbox: 1.6775, d1.loss_cls: 0.8915, d1.loss_bbox: 1.5148, d2.loss_cls: 0.8848, d2.loss_bbox: 1.4833, d3.loss_cls: 0.8818, d3.loss_bbox: 1.4570, d4.loss_cls: 0.8805, d4.loss_bbox: 1.4502, aux_task0.loss_heatmap: 1.2289, aux_task0.loss_bbox: 0.3832, aux_task1.loss_heatmap: 1.4015, aux_task1.loss_bbox: 0.4078, aux_task2.loss_heatmap: 1.7023, aux_task2.loss_bbox: 0.4365, aux_task3.loss_heatmap: 1.2816, aux_task3.loss_bbox: 0.3421, aux_task4.loss_heatmap: 0.9283, aux_task4.loss_bbox: 0.3738, aux_task5.loss_heatmap: 1.2162, aux_task5.loss_bbox: 0.3875, loss: 24.4821, grad_norm: 49.0196
2025-05-06 21:08:42,457 - mmdet - INFO - Epoch [1][1650/30895]	lr: 1.251e-05, eta: 66 days, 7:29:18, time: 15.865, data_time: 7.207, memory: 8990, loss_cls: 0.8694, loss_bbox: 1.4315, d0.loss_cls: 0.9297, d0.loss_bbox: 1.6655, d1.loss_cls: 0.8752, d1.loss_bbox: 1.5040, d2.loss_cls: 0.8723, d2.loss_bbox: 1.4712, d3.loss_cls: 0.8684, d3.loss_bbox: 1.4458, d4.loss_cls: 0.8684, d4.loss_bbox: 1.4345, aux_task0.loss_heatmap: 1.2465, aux_task0.loss_bbox: 0.3915, aux_task1.loss_heatmap: 1.4243, aux_task1.loss_bbox: 0.4156, aux_task2.loss_heatmap: 1.7154, aux_task2.loss_bbox: 0.4459, aux_task3.loss_heatmap: 1.2511, aux_task3.loss_bbox: 0.3427, aux_task4.loss_heatmap: 0.9266, aux_task4.loss_bbox: 0.3711, aux_task5.loss_heatmap: 1.1955, aux_task5.loss_bbox: 0.3830, loss: 24.3452, grad_norm: 49.0800
2025-05-06 21:20:36,102 - mmdet - INFO - Epoch [1][1700/30895]	lr: 1.251e-05, eta: 67 days, 8:24:25, time: 14.273, data_time: 7.997, memory: 8990, loss_cls: 0.8780, loss_bbox: 1.4303, d0.loss_cls: 0.9318, d0.loss_bbox: 1.6724, d1.loss_cls: 0.8825, d1.loss_bbox: 1.5020, d2.loss_cls: 0.8801, d2.loss_bbox: 1.4720, d3.loss_cls: 0.8750, d3.loss_bbox: 1.4470, d4.loss_cls: 0.8758, d4.loss_bbox: 1.4376, aux_task0.loss_heatmap: 1.2440, aux_task0.loss_bbox: 0.3924, aux_task1.loss_heatmap: 1.4135, aux_task1.loss_bbox: 0.4075, aux_task2.loss_heatmap: 1.7121, aux_task2.loss_bbox: 0.4392, aux_task3.loss_heatmap: 1.2594, aux_task3.loss_bbox: 0.3393, aux_task4.loss_heatmap: 0.9024, aux_task4.loss_bbox: 0.3633, aux_task5.loss_heatmap: 1.1863, aux_task5.loss_bbox: 0.3900, loss: 24.3339, grad_norm: 49.8163
2025-05-06 21:33:14,523 - mmdet - INFO - Epoch [1][1750/30895]	lr: 1.251e-05, eta: 68 days, 12:16:23, time: 15.169, data_time: 7.241, memory: 8990, loss_cls: 0.8777, loss_bbox: 1.4145, d0.loss_cls: 0.9308, d0.loss_bbox: 1.6736, d1.loss_cls: 0.8826, d1.loss_bbox: 1.4983, d2.loss_cls: 0.8795, d2.loss_bbox: 1.4658, d3.loss_cls: 0.8771, d3.loss_bbox: 1.4329, d4.loss_cls: 0.8760, d4.loss_bbox: 1.4213, aux_task0.loss_heatmap: 1.1751, aux_task0.loss_bbox: 0.3847, aux_task1.loss_heatmap: 1.3826, aux_task1.loss_bbox: 0.4070, aux_task2.loss_heatmap: 1.6821, aux_task2.loss_bbox: 0.4391, aux_task3.loss_heatmap: 1.2445, aux_task3.loss_bbox: 0.3474, aux_task4.loss_heatmap: 0.9333, aux_task4.loss_bbox: 0.3612, aux_task5.loss_heatmap: 1.1579, aux_task5.loss_bbox: 0.3823, loss: 24.1273, grad_norm: 49.5564
2025-05-06 21:48:10,461 - mmdet - INFO - Epoch [1][1800/30895]	lr: 1.251e-05, eta: 70 days, 3:38:55, time: 17.918, data_time: 4.898, memory: 8990, loss_cls: 0.8695, loss_bbox: 1.3851, d0.loss_cls: 0.9262, d0.loss_bbox: 1.6408, d1.loss_cls: 0.8755, d1.loss_bbox: 1.4694, d2.loss_cls: 0.8703, d2.loss_bbox: 1.4390, d3.loss_cls: 0.8682, d3.loss_bbox: 1.4067, d4.loss_cls: 0.8692, d4.loss_bbox: 1.3923, aux_task0.loss_heatmap: 1.2431, aux_task0.loss_bbox: 0.3957, aux_task1.loss_heatmap: 1.3715, aux_task1.loss_bbox: 0.4100, aux_task2.loss_heatmap: 1.6809, aux_task2.loss_bbox: 0.4487, aux_task3.loss_heatmap: 1.3009, aux_task3.loss_bbox: 0.3433, aux_task4.loss_heatmap: 0.9152, aux_task4.loss_bbox: 0.3775, aux_task5.loss_heatmap: 1.1146, aux_task5.loss_bbox: 0.3789, loss: 23.9925, grad_norm: 52.3597
2025-05-06 22:03:50,798 - mmdet - INFO - Epoch [1][1850/30895]	lr: 1.252e-05, eta: 71 days, 20:59:42, time: 18.807, data_time: 12.455, memory: 8990, loss_cls: 0.8697, loss_bbox: 1.3593, d0.loss_cls: 0.9217, d0.loss_bbox: 1.6359, d1.loss_cls: 0.8729, d1.loss_bbox: 1.4562, d2.loss_cls: 0.8675, d2.loss_bbox: 1.4227, d3.loss_cls: 0.8682, d3.loss_bbox: 1.3850, d4.loss_cls: 0.8664, d4.loss_bbox: 1.3699, aux_task0.loss_heatmap: 1.1967, aux_task0.loss_bbox: 0.3806, aux_task1.loss_heatmap: 1.3884, aux_task1.loss_bbox: 0.4111, aux_task2.loss_heatmap: 1.6140, aux_task2.loss_bbox: 0.4255, aux_task3.loss_heatmap: 1.2457, aux_task3.loss_bbox: 0.3384, aux_task4.loss_heatmap: 0.8658, aux_task4.loss_bbox: 0.3564, aux_task5.loss_heatmap: 1.1575, aux_task5.loss_bbox: 0.3792, loss: 23.6545, grad_norm: 53.1191
2025-05-06 22:20:45,356 - mmdet - INFO - Epoch [1][1900/30895]	lr: 1.252e-05, eta: 73 days, 18:49:59, time: 20.291, data_time: 4.943, memory: 8990, loss_cls: 0.8708, loss_bbox: 1.3588, d0.loss_cls: 0.9204, d0.loss_bbox: 1.6429, d1.loss_cls: 0.8755, d1.loss_bbox: 1.4580, d2.loss_cls: 0.8700, d2.loss_bbox: 1.4237, d3.loss_cls: 0.8656, d3.loss_bbox: 1.3859, d4.loss_cls: 0.8661, d4.loss_bbox: 1.3699, aux_task0.loss_heatmap: 1.1788, aux_task0.loss_bbox: 0.3858, aux_task1.loss_heatmap: 1.4034, aux_task1.loss_bbox: 0.4063, aux_task2.loss_heatmap: 1.6101, aux_task2.loss_bbox: 0.4370, aux_task3.loss_heatmap: 1.2537, aux_task3.loss_bbox: 0.3357, aux_task4.loss_heatmap: 0.8691, aux_task4.loss_bbox: 0.3536, aux_task5.loss_heatmap: 1.1003, aux_task5.loss_bbox: 0.3774, loss: 23.6187, grad_norm: 51.4333
2025-05-06 22:33:31,086 - mmdet - INFO - Epoch [1][1950/30895]	lr: 1.252e-05, eta: 74 days, 16:28:26, time: 15.315, data_time: 4.469, memory: 8990, loss_cls: 0.8615, loss_bbox: 1.3389, d0.loss_cls: 0.9068, d0.loss_bbox: 1.6245, d1.loss_cls: 0.8605, d1.loss_bbox: 1.4418, d2.loss_cls: 0.8580, d2.loss_bbox: 1.4083, d3.loss_cls: 0.8577, d3.loss_bbox: 1.3690, d4.loss_cls: 0.8600, d4.loss_bbox: 1.3508, aux_task0.loss_heatmap: 1.1547, aux_task0.loss_bbox: 0.3714, aux_task1.loss_heatmap: 1.3614, aux_task1.loss_bbox: 0.3961, aux_task2.loss_heatmap: 1.6590, aux_task2.loss_bbox: 0.4382, aux_task3.loss_heatmap: 1.2506, aux_task3.loss_bbox: 0.3360, aux_task4.loss_heatmap: 0.8819, aux_task4.loss_bbox: 0.3637, aux_task5.loss_heatmap: 1.1063, aux_task5.loss_bbox: 0.3784, loss: 23.4356, grad_norm: 53.1407
2025-05-06 22:55:45,993 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-06 22:55:45,993 - mmdet - INFO - Epoch [1][2000/30895]	lr: 1.252e-05, eta: 77 days, 13:42:39, time: 26.698, data_time: 10.773, memory: 8990, loss_cls: 0.8558, loss_bbox: 1.3153, d0.loss_cls: 0.9013, d0.loss_bbox: 1.6016, d1.loss_cls: 0.8567, d1.loss_bbox: 1.4188, d2.loss_cls: 0.8531, d2.loss_bbox: 1.3846, d3.loss_cls: 0.8520, d3.loss_bbox: 1.3451, d4.loss_cls: 0.8544, d4.loss_bbox: 1.3269, aux_task0.loss_heatmap: 1.1805, aux_task0.loss_bbox: 0.3881, aux_task1.loss_heatmap: 1.3442, aux_task1.loss_bbox: 0.3994, aux_task2.loss_heatmap: 1.6599, aux_task2.loss_bbox: 0.4387, aux_task3.loss_heatmap: 1.2334, aux_task3.loss_bbox: 0.3329, aux_task4.loss_heatmap: 0.8643, aux_task4.loss_bbox: 0.3616, aux_task5.loss_heatmap: 1.0919, aux_task5.loss_bbox: 0.3754, loss: 23.2359, grad_norm: 53.6350
2025-05-06 23:11:45,867 - mmdet - INFO - Epoch [1][2050/30895]	lr: 1.252e-05, eta: 79 days, 0:15:24, time: 19.198, data_time: 0.897, memory: 8990, loss_cls: 0.8653, loss_bbox: 1.3154, d0.loss_cls: 0.9030, d0.loss_bbox: 1.6080, d1.loss_cls: 0.8617, d1.loss_bbox: 1.4207, d2.loss_cls: 0.8603, d2.loss_bbox: 1.3869, d3.loss_cls: 0.8590, d3.loss_bbox: 1.3459, d4.loss_cls: 0.8627, d4.loss_bbox: 1.3264, aux_task0.loss_heatmap: 1.1746, aux_task0.loss_bbox: 0.3820, aux_task1.loss_heatmap: 1.3057, aux_task1.loss_bbox: 0.3996, aux_task2.loss_heatmap: 1.6578, aux_task2.loss_bbox: 0.4257, aux_task3.loss_heatmap: 1.2155, aux_task3.loss_bbox: 0.3368, aux_task4.loss_heatmap: 0.8758, aux_task4.loss_bbox: 0.3616, aux_task5.loss_heatmap: 1.0804, aux_task5.loss_bbox: 0.3692, loss: 23.1997, grad_norm: 55.1751
2025-05-06 23:23:29,948 - mmdet - INFO - Epoch [1][2100/30895]	lr: 1.252e-05, eta: 79 days, 12:18:29, time: 14.082, data_time: 7.935, memory: 8990, loss_cls: 0.8575, loss_bbox: 1.2915, d0.loss_cls: 0.8958, d0.loss_bbox: 1.5939, d1.loss_cls: 0.8557, d1.loss_bbox: 1.3962, d2.loss_cls: 0.8516, d2.loss_bbox: 1.3619, d3.loss_cls: 0.8508, d3.loss_bbox: 1.3229, d4.loss_cls: 0.8550, d4.loss_bbox: 1.3028, aux_task0.loss_heatmap: 1.1782, aux_task0.loss_bbox: 0.3910, aux_task1.loss_heatmap: 1.2853, aux_task1.loss_bbox: 0.3986, aux_task2.loss_heatmap: 1.6205, aux_task2.loss_bbox: 0.4456, aux_task3.loss_heatmap: 1.2165, aux_task3.loss_bbox: 0.3399, aux_task4.loss_heatmap: 0.8783, aux_task4.loss_bbox: 0.3648, aux_task5.loss_heatmap: 1.0597, aux_task5.loss_bbox: 0.3690, loss: 22.9826, grad_norm: 55.0905
2025-05-06 23:40:22,334 - mmdet - INFO - Epoch [1][2150/30895]	lr: 1.252e-05, eta: 81 days, 0:18:59, time: 20.248, data_time: 5.348, memory: 8990, loss_cls: 0.8418, loss_bbox: 1.2887, d0.loss_cls: 0.8837, d0.loss_bbox: 1.5848, d1.loss_cls: 0.8432, d1.loss_bbox: 1.3942, d2.loss_cls: 0.8403, d2.loss_bbox: 1.3573, d3.loss_cls: 0.8362, d3.loss_bbox: 1.3167, d4.loss_cls: 0.8412, d4.loss_bbox: 1.2998, aux_task0.loss_heatmap: 1.1394, aux_task0.loss_bbox: 0.3810, aux_task1.loss_heatmap: 1.3361, aux_task1.loss_bbox: 0.4085, aux_task2.loss_heatmap: 1.6358, aux_task2.loss_bbox: 0.4539, aux_task3.loss_heatmap: 1.2346, aux_task3.loss_bbox: 0.3376, aux_task4.loss_heatmap: 0.8264, aux_task4.loss_bbox: 0.3510, aux_task5.loss_heatmap: 1.0885, aux_task5.loss_bbox: 0.3697, loss: 22.8905, grad_norm: 58.6725
2025-05-06 23:56:16,597 - mmdet - INFO - Epoch [1][2200/30895]	lr: 1.252e-05, eta: 82 days, 6:09:27, time: 19.085, data_time: 0.005, memory: 8990, loss_cls: 0.8483, loss_bbox: 1.2800, d0.loss_cls: 0.8865, d0.loss_bbox: 1.5694, d1.loss_cls: 0.8474, d1.loss_bbox: 1.3738, d2.loss_cls: 0.8421, d2.loss_bbox: 1.3412, d3.loss_cls: 0.8420, d3.loss_bbox: 1.3042, d4.loss_cls: 0.8473, d4.loss_bbox: 1.2866, aux_task0.loss_heatmap: 1.1278, aux_task0.loss_bbox: 0.3813, aux_task1.loss_heatmap: 1.3113, aux_task1.loss_bbox: 0.3967, aux_task2.loss_heatmap: 1.6351, aux_task2.loss_bbox: 0.4447, aux_task3.loss_heatmap: 1.2304, aux_task3.loss_bbox: 0.3429, aux_task4.loss_heatmap: 0.8272, aux_task4.loss_bbox: 0.3597, aux_task5.loss_heatmap: 1.0642, aux_task5.loss_bbox: 0.3687, loss: 22.7586, grad_norm: 61.6253
2025-05-07 00:12:47,608 - mmdet - INFO - Epoch [1][2250/30895]	lr: 1.252e-05, eta: 83 days, 13:27:10, time: 19.820, data_time: 2.325, memory: 8990, loss_cls: 0.8454, loss_bbox: 1.2705, d0.loss_cls: 0.8831, d0.loss_bbox: 1.5656, d1.loss_cls: 0.8438, d1.loss_bbox: 1.3711, d2.loss_cls: 0.8400, d2.loss_bbox: 1.3347, d3.loss_cls: 0.8407, d3.loss_bbox: 1.2972, d4.loss_cls: 0.8422, d4.loss_bbox: 1.2791, aux_task0.loss_heatmap: 1.1104, aux_task0.loss_bbox: 0.3758, aux_task1.loss_heatmap: 1.3308, aux_task1.loss_bbox: 0.4024, aux_task2.loss_heatmap: 1.6286, aux_task2.loss_bbox: 0.4400, aux_task3.loss_heatmap: 1.2339, aux_task3.loss_bbox: 0.3376, aux_task4.loss_heatmap: 0.8332, aux_task4.loss_bbox: 0.3513, aux_task5.loss_heatmap: 1.0668, aux_task5.loss_bbox: 0.3761, loss: 22.7001, grad_norm: 56.5114
2025-05-07 00:29:29,016 - mmdet - INFO - Epoch [1][2300/30895]	lr: 1.252e-05, eta: 84 days, 20:08:58, time: 20.028, data_time: 0.005, memory: 8990, loss_cls: 0.8502, loss_bbox: 1.2813, d0.loss_cls: 0.8869, d0.loss_bbox: 1.5696, d1.loss_cls: 0.8476, d1.loss_bbox: 1.3725, d2.loss_cls: 0.8420, d2.loss_bbox: 1.3378, d3.loss_cls: 0.8421, d3.loss_bbox: 1.3012, d4.loss_cls: 0.8479, d4.loss_bbox: 1.2880, aux_task0.loss_heatmap: 1.1479, aux_task0.loss_bbox: 0.3800, aux_task1.loss_heatmap: 1.3027, aux_task1.loss_bbox: 0.4028, aux_task2.loss_heatmap: 1.6205, aux_task2.loss_bbox: 0.4409, aux_task3.loss_heatmap: 1.2248, aux_task3.loss_bbox: 0.3360, aux_task4.loss_heatmap: 0.8101, aux_task4.loss_bbox: 0.3491, aux_task5.loss_heatmap: 1.0714, aux_task5.loss_bbox: 0.3747, loss: 22.7278, grad_norm: 57.8809
2025-05-07 00:45:59,014 - mmdet - INFO - Epoch [1][2350/30895]	lr: 1.253e-05, eta: 86 days, 0:41:49, time: 19.800, data_time: 0.004, memory: 8990, loss_cls: 0.8321, loss_bbox: 1.2365, d0.loss_cls: 0.8633, d0.loss_bbox: 1.5347, d1.loss_cls: 0.8271, d1.loss_bbox: 1.3367, d2.loss_cls: 0.8241, d2.loss_bbox: 1.2981, d3.loss_cls: 0.8241, d3.loss_bbox: 1.2614, d4.loss_cls: 0.8298, d4.loss_bbox: 1.2454, aux_task0.loss_heatmap: 1.1128, aux_task0.loss_bbox: 0.3733, aux_task1.loss_heatmap: 1.3505, aux_task1.loss_bbox: 0.3926, aux_task2.loss_heatmap: 1.6001, aux_task2.loss_bbox: 0.4364, aux_task3.loss_heatmap: 1.2145, aux_task3.loss_bbox: 0.3396, aux_task4.loss_heatmap: 0.7903, aux_task4.loss_bbox: 0.3568, aux_task5.loss_heatmap: 1.0302, aux_task5.loss_bbox: 0.3662, loss: 22.2764, grad_norm: 55.6330
2025-05-07 01:05:37,530 - mmdet - INFO - Epoch [1][2400/30895]	lr: 1.253e-05, eta: 87 days, 17:28:31, time: 23.571, data_time: 0.005, memory: 8990, loss_cls: 0.8407, loss_bbox: 1.2341, d0.loss_cls: 0.8773, d0.loss_bbox: 1.5212, d1.loss_cls: 0.8361, d1.loss_bbox: 1.3294, d2.loss_cls: 0.8339, d2.loss_bbox: 1.2942, d3.loss_cls: 0.8359, d3.loss_bbox: 1.2565, d4.loss_cls: 0.8383, d4.loss_bbox: 1.2414, aux_task0.loss_heatmap: 1.1047, aux_task0.loss_bbox: 0.3666, aux_task1.loss_heatmap: 1.2939, aux_task1.loss_bbox: 0.3961, aux_task2.loss_heatmap: 1.5948, aux_task2.loss_bbox: 0.4333, aux_task3.loss_heatmap: 1.2833, aux_task3.loss_bbox: 0.3415, aux_task4.loss_heatmap: 0.8210, aux_task4.loss_bbox: 0.3509, aux_task5.loss_heatmap: 1.0464, aux_task5.loss_bbox: 0.3678, loss: 22.3392, grad_norm: 57.2086
2025-05-07 01:18:34,871 - mmdet - INFO - Epoch [1][2450/30895]	lr: 1.253e-05, eta: 88 days, 4:34:48, time: 15.547, data_time: 1.564, memory: 8990, loss_cls: 0.8329, loss_bbox: 1.2354, d0.loss_cls: 0.8705, d0.loss_bbox: 1.5183, d1.loss_cls: 0.8322, d1.loss_bbox: 1.3254, d2.loss_cls: 0.8318, d2.loss_bbox: 1.2881, d3.loss_cls: 0.8286, d3.loss_bbox: 1.2554, d4.loss_cls: 0.8326, d4.loss_bbox: 1.2412, aux_task0.loss_heatmap: 1.0772, aux_task0.loss_bbox: 0.3666, aux_task1.loss_heatmap: 1.2513, aux_task1.loss_bbox: 0.3888, aux_task2.loss_heatmap: 1.5891, aux_task2.loss_bbox: 0.4415, aux_task3.loss_heatmap: 1.2062, aux_task3.loss_bbox: 0.3351, aux_task4.loss_heatmap: 0.8379, aux_task4.loss_bbox: 0.3509, aux_task5.loss_heatmap: 0.9853, aux_task5.loss_bbox: 0.3663, loss: 22.0887, grad_norm: 59.4425
2025-05-07 01:35:41,936 - mmdet - INFO - Epoch [1][2500/30895]	lr: 1.253e-05, eta: 89 days, 8:18:28, time: 20.541, data_time: 0.005, memory: 8990, loss_cls: 0.8332, loss_bbox: 1.2326, d0.loss_cls: 0.8683, d0.loss_bbox: 1.5126, d1.loss_cls: 0.8318, d1.loss_bbox: 1.3168, d2.loss_cls: 0.8287, d2.loss_bbox: 1.2802, d3.loss_cls: 0.8288, d3.loss_bbox: 1.2494, d4.loss_cls: 0.8302, d4.loss_bbox: 1.2377, aux_task0.loss_heatmap: 1.1128, aux_task0.loss_bbox: 0.3797, aux_task1.loss_heatmap: 1.2308, aux_task1.loss_bbox: 0.3969, aux_task2.loss_heatmap: 1.6136, aux_task2.loss_bbox: 0.4384, aux_task3.loss_heatmap: 1.2260, aux_task3.loss_bbox: 0.3364, aux_task4.loss_heatmap: 0.8516, aux_task4.loss_bbox: 0.3672, aux_task5.loss_heatmap: 1.0224, aux_task5.loss_bbox: 0.3673, loss: 22.1935, grad_norm: 59.2377
2025-05-07 01:53:04,976 - mmdet - INFO - Epoch [1][2550/30895]	lr: 1.253e-05, eta: 90 days, 12:00:21, time: 20.860, data_time: 0.005, memory: 8990, loss_cls: 0.8160, loss_bbox: 1.2371, d0.loss_cls: 0.8543, d0.loss_bbox: 1.5211, d1.loss_cls: 0.8168, d1.loss_bbox: 1.3281, d2.loss_cls: 0.8153, d2.loss_bbox: 1.2884, d3.loss_cls: 0.8128, d3.loss_bbox: 1.2575, d4.loss_cls: 0.8140, d4.loss_bbox: 1.2451, aux_task0.loss_heatmap: 1.1067, aux_task0.loss_bbox: 0.3767, aux_task1.loss_heatmap: 1.2411, aux_task1.loss_bbox: 0.3995, aux_task2.loss_heatmap: 1.6074, aux_task2.loss_bbox: 0.4409, aux_task3.loss_heatmap: 1.1933, aux_task3.loss_bbox: 0.3359, aux_task4.loss_heatmap: 0.8479, aux_task4.loss_bbox: 0.3515, aux_task5.loss_heatmap: 0.9888, aux_task5.loss_bbox: 0.3594, loss: 22.0556, grad_norm: 55.4580
2025-05-07 02:06:32,523 - mmdet - INFO - Epoch [1][2600/30895]	lr: 1.253e-05, eta: 90 days, 23:09:06, time: 16.152, data_time: 1.157, memory: 8990, loss_cls: 0.8188, loss_bbox: 1.2190, d0.loss_cls: 0.8515, d0.loss_bbox: 1.5075, d1.loss_cls: 0.8165, d1.loss_bbox: 1.3054, d2.loss_cls: 0.8127, d2.loss_bbox: 1.2703, d3.loss_cls: 0.8142, d3.loss_bbox: 1.2397, d4.loss_cls: 0.8170, d4.loss_bbox: 1.2276, aux_task0.loss_heatmap: 1.0765, aux_task0.loss_bbox: 0.3678, aux_task1.loss_heatmap: 1.2486, aux_task1.loss_bbox: 0.4056, aux_task2.loss_heatmap: 1.5736, aux_task2.loss_bbox: 0.4365, aux_task3.loss_heatmap: 1.2092, aux_task3.loss_bbox: 0.3336, aux_task4.loss_heatmap: 0.8446, aux_task4.loss_bbox: 0.3572, aux_task5.loss_heatmap: 1.0096, aux_task5.loss_bbox: 0.3652, loss: 21.9281, grad_norm: 57.9029
2025-05-07 02:26:03,843 - mmdet - INFO - Epoch [1][2650/30895]	lr: 1.253e-05, eta: 92 days, 9:19:35, time: 23.426, data_time: 0.005, memory: 8990, loss_cls: 0.8205, loss_bbox: 1.2147, d0.loss_cls: 0.8533, d0.loss_bbox: 1.4954, d1.loss_cls: 0.8181, d1.loss_bbox: 1.2983, d2.loss_cls: 0.8127, d2.loss_bbox: 1.2636, d3.loss_cls: 0.8143, d3.loss_bbox: 1.2337, d4.loss_cls: 0.8166, d4.loss_bbox: 1.2209, aux_task0.loss_heatmap: 1.0716, aux_task0.loss_bbox: 0.3772, aux_task1.loss_heatmap: 1.2156, aux_task1.loss_bbox: 0.3947, aux_task2.loss_heatmap: 1.5452, aux_task2.loss_bbox: 0.4355, aux_task3.loss_heatmap: 1.1683, aux_task3.loss_bbox: 0.3287, aux_task4.loss_heatmap: 0.8082, aux_task4.loss_bbox: 0.3462, aux_task5.loss_heatmap: 1.0182, aux_task5.loss_bbox: 0.3652, loss: 21.7369, grad_norm: 62.1806
2025-05-07 02:42:28,154 - mmdet - INFO - Epoch [1][2700/30895]	lr: 1.253e-05, eta: 93 days, 6:23:14, time: 19.686, data_time: 0.619, memory: 8990, loss_cls: 0.8240, loss_bbox: 1.2136, d0.loss_cls: 0.8564, d0.loss_bbox: 1.5082, d1.loss_cls: 0.8205, d1.loss_bbox: 1.3029, d2.loss_cls: 0.8198, d2.loss_bbox: 1.2654, d3.loss_cls: 0.8174, d3.loss_bbox: 1.2348, d4.loss_cls: 0.8210, d4.loss_bbox: 1.2213, aux_task0.loss_heatmap: 1.1223, aux_task0.loss_bbox: 0.3734, aux_task1.loss_heatmap: 1.3013, aux_task1.loss_bbox: 0.3944, aux_task2.loss_heatmap: 1.5690, aux_task2.loss_bbox: 0.4253, aux_task3.loss_heatmap: 1.1470, aux_task3.loss_bbox: 0.3254, aux_task4.loss_heatmap: 0.7995, aux_task4.loss_bbox: 0.3606, aux_task5.loss_heatmap: 0.9855, aux_task5.loss_bbox: 0.3598, loss: 21.8687, grad_norm: 56.9571
2025-05-07 03:15:00,936 - mmdet - INFO - Epoch [1][2750/30895]	lr: 1.253e-05, eta: 96 days, 14:50:54, time: 39.055, data_time: 0.005, memory: 8990, loss_cls: 0.8087, loss_bbox: 1.2013, d0.loss_cls: 0.8474, d0.loss_bbox: 1.4752, d1.loss_cls: 0.8109, d1.loss_bbox: 1.2843, d2.loss_cls: 0.8080, d2.loss_bbox: 1.2507, d3.loss_cls: 0.8077, d3.loss_bbox: 1.2209, d4.loss_cls: 0.8072, d4.loss_bbox: 1.2086, aux_task0.loss_heatmap: 1.0748, aux_task0.loss_bbox: 0.3702, aux_task1.loss_heatmap: 1.2464, aux_task1.loss_bbox: 0.3869, aux_task2.loss_heatmap: 1.5987, aux_task2.loss_bbox: 0.4313, aux_task3.loss_heatmap: 1.1335, aux_task3.loss_bbox: 0.3355, aux_task4.loss_heatmap: 0.7869, aux_task4.loss_bbox: 0.3520, aux_task5.loss_heatmap: 0.9679, aux_task5.loss_bbox: 0.3584, loss: 21.5736, grad_norm: 59.0894
2025-05-07 03:40:35,845 - mmdet - INFO - Epoch [1][2800/30895]	lr: 1.254e-05, eta: 98 days, 18:55:09, time: 30.699, data_time: 0.625, memory: 8990, loss_cls: 0.8166, loss_bbox: 1.2146, d0.loss_cls: 0.8462, d0.loss_bbox: 1.4824, d1.loss_cls: 0.8138, d1.loss_bbox: 1.2932, d2.loss_cls: 0.8135, d2.loss_bbox: 1.2583, d3.loss_cls: 0.8113, d3.loss_bbox: 1.2336, d4.loss_cls: 0.8146, d4.loss_bbox: 1.2210, aux_task0.loss_heatmap: 1.0959, aux_task0.loss_bbox: 0.3673, aux_task1.loss_heatmap: 1.2197, aux_task1.loss_bbox: 0.3901, aux_task2.loss_heatmap: 1.5425, aux_task2.loss_bbox: 0.4237, aux_task3.loss_heatmap: 1.1882, aux_task3.loss_bbox: 0.3377, aux_task4.loss_heatmap: 0.8172, aux_task4.loss_bbox: 0.3591, aux_task5.loss_heatmap: 0.9561, aux_task5.loss_bbox: 0.3623, loss: 21.6791, grad_norm: 64.6812
2025-05-07 04:11:22,838 - mmdet - INFO - Epoch [1][2850/30895]	lr: 1.254e-05, eta: 101 days, 15:51:19, time: 36.940, data_time: 0.979, memory: 8990, loss_cls: 0.8165, loss_bbox: 1.2034, d0.loss_cls: 0.8475, d0.loss_bbox: 1.4706, d1.loss_cls: 0.8140, d1.loss_bbox: 1.2837, d2.loss_cls: 0.8126, d2.loss_bbox: 1.2465, d3.loss_cls: 0.8133, d3.loss_bbox: 1.2206, d4.loss_cls: 0.8147, d4.loss_bbox: 1.2091, aux_task0.loss_heatmap: 1.0718, aux_task0.loss_bbox: 0.3745, aux_task1.loss_heatmap: 1.2716, aux_task1.loss_bbox: 0.3873, aux_task2.loss_heatmap: 1.5780, aux_task2.loss_bbox: 0.4427, aux_task3.loss_heatmap: 1.1515, aux_task3.loss_bbox: 0.3331, aux_task4.loss_heatmap: 0.7957, aux_task4.loss_bbox: 0.3513, aux_task5.loss_heatmap: 0.9586, aux_task5.loss_bbox: 0.3571, loss: 21.6258, grad_norm: 64.4703
2025-05-07 04:38:39,190 - mmdet - INFO - Epoch [1][2900/30895]	lr: 1.254e-05, eta: 103 days, 21:59:16, time: 32.727, data_time: 0.005, memory: 8990, loss_cls: 0.8107, loss_bbox: 1.1763, d0.loss_cls: 0.8387, d0.loss_bbox: 1.4372, d1.loss_cls: 0.8045, d1.loss_bbox: 1.2562, d2.loss_cls: 0.8058, d2.loss_bbox: 1.2198, d3.loss_cls: 0.8051, d3.loss_bbox: 1.1958, d4.loss_cls: 0.8078, d4.loss_bbox: 1.1821, aux_task0.loss_heatmap: 1.0574, aux_task0.loss_bbox: 0.3667, aux_task1.loss_heatmap: 1.2107, aux_task1.loss_bbox: 0.3807, aux_task2.loss_heatmap: 1.5226, aux_task2.loss_bbox: 0.4286, aux_task3.loss_heatmap: 1.1285, aux_task3.loss_bbox: 0.3321, aux_task4.loss_heatmap: 0.7887, aux_task4.loss_bbox: 0.3527, aux_task5.loss_heatmap: 0.9486, aux_task5.loss_bbox: 0.3594, loss: 21.2166, grad_norm: 67.7118
2025-05-07 05:00:44,655 - mmdet - INFO - Epoch [1][2950/30895]	lr: 1.254e-05, eta: 105 days, 8:16:06, time: 26.509, data_time: 0.005, memory: 8990, loss_cls: 0.8017, loss_bbox: 1.1767, d0.loss_cls: 0.8366, d0.loss_bbox: 1.4411, d1.loss_cls: 0.8006, d1.loss_bbox: 1.2558, d2.loss_cls: 0.7989, d2.loss_bbox: 1.2218, d3.loss_cls: 0.7963, d3.loss_bbox: 1.1957, d4.loss_cls: 0.7984, d4.loss_bbox: 1.1842, aux_task0.loss_heatmap: 1.0700, aux_task0.loss_bbox: 0.3586, aux_task1.loss_heatmap: 1.1972, aux_task1.loss_bbox: 0.3891, aux_task2.loss_heatmap: 1.5433, aux_task2.loss_bbox: 0.4131, aux_task3.loss_heatmap: 1.1433, aux_task3.loss_bbox: 0.3346, aux_task4.loss_heatmap: 0.8185, aux_task4.loss_bbox: 0.3593, aux_task5.loss_heatmap: 0.9426, aux_task5.loss_bbox: 0.3591, loss: 21.2364, grad_norm: 62.2508
2025-05-07 05:30:16,361 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-07 05:30:16,361 - mmdet - INFO - Epoch [1][3000/30895]	lr: 1.254e-05, eta: 107 days, 18:48:02, time: 35.434, data_time: 10.683, memory: 8990, loss_cls: 0.7939, loss_bbox: 1.1778, d0.loss_cls: 0.8288, d0.loss_bbox: 1.4350, d1.loss_cls: 0.7930, d1.loss_bbox: 1.2540, d2.loss_cls: 0.7906, d2.loss_bbox: 1.2191, d3.loss_cls: 0.7899, d3.loss_bbox: 1.1953, d4.loss_cls: 0.7940, d4.loss_bbox: 1.1836, aux_task0.loss_heatmap: 1.0698, aux_task0.loss_bbox: 0.3647, aux_task1.loss_heatmap: 1.3121, aux_task1.loss_bbox: 0.3966, aux_task2.loss_heatmap: 1.5584, aux_task2.loss_bbox: 0.4204, aux_task3.loss_heatmap: 1.1565, aux_task3.loss_bbox: 0.3324, aux_task4.loss_heatmap: 0.8048, aux_task4.loss_bbox: 0.3462, aux_task5.loss_heatmap: 0.9329, aux_task5.loss_bbox: 0.3564, loss: 21.3062, grad_norm: 60.9039
2025-05-07 05:47:58,020 - mmdet - INFO - Epoch [1][3050/30895]	lr: 1.254e-05, eta: 108 days, 11:38:11, time: 21.233, data_time: 0.607, memory: 8990, loss_cls: 0.8023, loss_bbox: 1.2048, d0.loss_cls: 0.8347, d0.loss_bbox: 1.4534, d1.loss_cls: 0.8029, d1.loss_bbox: 1.2829, d2.loss_cls: 0.8006, d2.loss_bbox: 1.2484, d3.loss_cls: 0.7974, d3.loss_bbox: 1.2234, d4.loss_cls: 0.8003, d4.loss_bbox: 1.2108, aux_task0.loss_heatmap: 1.0621, aux_task0.loss_bbox: 0.3705, aux_task1.loss_heatmap: 1.2701, aux_task1.loss_bbox: 0.3949, aux_task2.loss_heatmap: 1.5598, aux_task2.loss_bbox: 0.4358, aux_task3.loss_heatmap: 1.0693, aux_task3.loss_bbox: 0.3282, aux_task4.loss_heatmap: 0.7785, aux_task4.loss_bbox: 0.3511, aux_task5.loss_heatmap: 0.9443, aux_task5.loss_bbox: 0.3588, loss: 21.3852, grad_norm: 63.0362
2025-05-07 06:10:50,596 - mmdet - INFO - Epoch [1][3100/30895]	lr: 1.254e-05, eta: 109 days, 21:02:58, time: 27.452, data_time: 6.088, memory: 8990, loss_cls: 0.7887, loss_bbox: 1.1634, d0.loss_cls: 0.8241, d0.loss_bbox: 1.4205, d1.loss_cls: 0.7894, d1.loss_bbox: 1.2447, d2.loss_cls: 0.7844, d2.loss_bbox: 1.2112, d3.loss_cls: 0.7838, d3.loss_bbox: 1.1848, d4.loss_cls: 0.7858, d4.loss_bbox: 1.1722, aux_task0.loss_heatmap: 1.0626, aux_task0.loss_bbox: 0.3657, aux_task1.loss_heatmap: 1.2500, aux_task1.loss_bbox: 0.3965, aux_task2.loss_heatmap: 1.5829, aux_task2.loss_bbox: 0.4254, aux_task3.loss_heatmap: 1.0645, aux_task3.loss_bbox: 0.3242, aux_task4.loss_heatmap: 0.7825, aux_task4.loss_bbox: 0.3333, aux_task5.loss_heatmap: 0.9015, aux_task5.loss_bbox: 0.3466, loss: 20.9887, grad_norm: 66.6196
2025-05-07 06:38:16,718 - mmdet - INFO - Epoch [1][3150/30895]	lr: 1.255e-05, eta: 111 days, 20:13:07, time: 32.923, data_time: 25.928, memory: 8990, loss_cls: 0.7937, loss_bbox: 1.1636, d0.loss_cls: 0.8232, d0.loss_bbox: 1.4067, d1.loss_cls: 0.7942, d1.loss_bbox: 1.2397, d2.loss_cls: 0.7919, d2.loss_bbox: 1.2049, d3.loss_cls: 0.7897, d3.loss_bbox: 1.1804, d4.loss_cls: 0.7906, d4.loss_bbox: 1.1692, aux_task0.loss_heatmap: 1.0551, aux_task0.loss_bbox: 0.3648, aux_task1.loss_heatmap: 1.1966, aux_task1.loss_bbox: 0.3858, aux_task2.loss_heatmap: 1.5506, aux_task2.loss_bbox: 0.4307, aux_task3.loss_heatmap: 1.0281, aux_task3.loss_bbox: 0.3212, aux_task4.loss_heatmap: 0.8155, aux_task4.loss_bbox: 0.3493, aux_task5.loss_heatmap: 0.8942, aux_task5.loss_bbox: 0.3530, loss: 20.8927, grad_norm: 65.5073
2025-05-07 07:15:21,151 - mmdet - INFO - Epoch [1][3200/30895]	lr: 1.255e-05, eta: 115 days, 0:45:21, time: 44.488, data_time: 24.486, memory: 8990, loss_cls: 0.7802, loss_bbox: 1.1583, d0.loss_cls: 0.8103, d0.loss_bbox: 1.4049, d1.loss_cls: 0.7803, d1.loss_bbox: 1.2374, d2.loss_cls: 0.7807, d2.loss_bbox: 1.2006, d3.loss_cls: 0.7773, d3.loss_bbox: 1.1771, d4.loss_cls: 0.7794, d4.loss_bbox: 1.1651, aux_task0.loss_heatmap: 1.0115, aux_task0.loss_bbox: 0.3569, aux_task1.loss_heatmap: 1.2150, aux_task1.loss_bbox: 0.3823, aux_task2.loss_heatmap: 1.5218, aux_task2.loss_bbox: 0.4215, aux_task3.loss_heatmap: 1.1504, aux_task3.loss_bbox: 0.3344, aux_task4.loss_heatmap: 0.7946, aux_task4.loss_bbox: 0.3460, aux_task5.loss_heatmap: 0.8751, aux_task5.loss_bbox: 0.3561, loss: 20.8172, grad_norm: 64.0684
2025-05-07 07:43:29,745 - mmdet - INFO - Epoch [1][3250/30895]	lr: 1.255e-05, eta: 116 days, 22:46:17, time: 33.772, data_time: 20.356, memory: 8990, loss_cls: 0.7797, loss_bbox: 1.1565, d0.loss_cls: 0.8109, d0.loss_bbox: 1.4018, d1.loss_cls: 0.7813, d1.loss_bbox: 1.2358, d2.loss_cls: 0.7791, d2.loss_bbox: 1.1996, d3.loss_cls: 0.7767, d3.loss_bbox: 1.1775, d4.loss_cls: 0.7772, d4.loss_bbox: 1.1648, aux_task0.loss_heatmap: 1.0172, aux_task0.loss_bbox: 0.3679, aux_task1.loss_heatmap: 1.2080, aux_task1.loss_bbox: 0.3861, aux_task2.loss_heatmap: 1.5008, aux_task2.loss_bbox: 0.4250, aux_task3.loss_heatmap: 1.1000, aux_task3.loss_bbox: 0.3339, aux_task4.loss_heatmap: 0.8025, aux_task4.loss_bbox: 0.3449, aux_task5.loss_heatmap: 0.8916, aux_task5.loss_bbox: 0.3554, loss: 20.7742, grad_norm: 63.5715
2025-05-07 08:01:46,262 - mmdet - INFO - Epoch [1][3300/30895]	lr: 1.255e-05, eta: 117 days, 12:44:44, time: 21.930, data_time: 0.284, memory: 8990, loss_cls: 0.7714, loss_bbox: 1.1835, d0.loss_cls: 0.8050, d0.loss_bbox: 1.4172, d1.loss_cls: 0.7742, d1.loss_bbox: 1.2566, d2.loss_cls: 0.7721, d2.loss_bbox: 1.2226, d3.loss_cls: 0.7663, d3.loss_bbox: 1.2010, d4.loss_cls: 0.7674, d4.loss_bbox: 1.1893, aux_task0.loss_heatmap: 1.0034, aux_task0.loss_bbox: 0.3576, aux_task1.loss_heatmap: 1.2492, aux_task1.loss_bbox: 0.4016, aux_task2.loss_heatmap: 1.4858, aux_task2.loss_bbox: 0.4367, aux_task3.loss_heatmap: 1.1302, aux_task3.loss_bbox: 0.3310, aux_task4.loss_heatmap: 0.7740, aux_task4.loss_bbox: 0.3561, aux_task5.loss_heatmap: 0.8858, aux_task5.loss_bbox: 0.3566, loss: 20.8943, grad_norm: 68.1530
2025-05-07 08:23:20,336 - mmdet - INFO - Epoch [1][3350/30895]	lr: 1.255e-05, eta: 118 days, 12:21:48, time: 25.882, data_time: 1.011, memory: 8990, loss_cls: 0.7651, loss_bbox: 1.1532, d0.loss_cls: 0.8030, d0.loss_bbox: 1.3831, d1.loss_cls: 0.7723, d1.loss_bbox: 1.2249, d2.loss_cls: 0.7663, d2.loss_bbox: 1.1932, d3.loss_cls: 0.7646, d3.loss_bbox: 1.1694, d4.loss_cls: 0.7642, d4.loss_bbox: 1.1599, aux_task0.loss_heatmap: 1.0567, aux_task0.loss_bbox: 0.3619, aux_task1.loss_heatmap: 1.2260, aux_task1.loss_bbox: 0.3984, aux_task2.loss_heatmap: 1.5092, aux_task2.loss_bbox: 0.4270, aux_task3.loss_heatmap: 1.0848, aux_task3.loss_bbox: 0.3292, aux_task4.loss_heatmap: 0.7516, aux_task4.loss_bbox: 0.3348, aux_task5.loss_heatmap: 0.8732, aux_task5.loss_bbox: 0.3473, loss: 20.6196, grad_norm: 66.2818
2025-05-07 08:40:41,821 - mmdet - INFO - Epoch [1][3400/30895]	lr: 1.255e-05, eta: 118 days, 22:35:36, time: 20.830, data_time: 0.005, memory: 8990, loss_cls: 0.7713, loss_bbox: 1.1595, d0.loss_cls: 0.8085, d0.loss_bbox: 1.3934, d1.loss_cls: 0.7768, d1.loss_bbox: 1.2367, d2.loss_cls: 0.7712, d2.loss_bbox: 1.2032, d3.loss_cls: 0.7693, d3.loss_bbox: 1.1777, d4.loss_cls: 0.7688, d4.loss_bbox: 1.1658, aux_task0.loss_heatmap: 0.9847, aux_task0.loss_bbox: 0.3479, aux_task1.loss_heatmap: 1.2148, aux_task1.loss_bbox: 0.3847, aux_task2.loss_heatmap: 1.5189, aux_task2.loss_bbox: 0.4325, aux_task3.loss_heatmap: 1.0596, aux_task3.loss_bbox: 0.3243, aux_task4.loss_heatmap: 0.7326, aux_task4.loss_bbox: 0.3449, aux_task5.loss_heatmap: 0.8778, aux_task5.loss_bbox: 0.3535, loss: 20.5785, grad_norm: 69.0309
2025-05-07 08:56:56,786 - mmdet - INFO - Epoch [1][3450/30895]	lr: 1.255e-05, eta: 119 days, 5:13:39, time: 19.499, data_time: 0.005, memory: 8990, loss_cls: 0.7752, loss_bbox: 1.1570, d0.loss_cls: 0.8048, d0.loss_bbox: 1.3738, d1.loss_cls: 0.7785, d1.loss_bbox: 1.2311, d2.loss_cls: 0.7737, d2.loss_bbox: 1.1978, d3.loss_cls: 0.7733, d3.loss_bbox: 1.1713, d4.loss_cls: 0.7722, d4.loss_bbox: 1.1629, aux_task0.loss_heatmap: 1.0011, aux_task0.loss_bbox: 0.3651, aux_task1.loss_heatmap: 1.1797, aux_task1.loss_bbox: 0.3831, aux_task2.loss_heatmap: 1.5125, aux_task2.loss_bbox: 0.4247, aux_task3.loss_heatmap: 1.0820, aux_task3.loss_bbox: 0.3280, aux_task4.loss_heatmap: 0.7844, aux_task4.loss_bbox: 0.3487, aux_task5.loss_heatmap: 0.8715, aux_task5.loss_bbox: 0.3580, loss: 20.6102, grad_norm: 67.3947
2025-05-07 09:14:44,077 - mmdet - INFO - Epoch [1][3500/30895]	lr: 1.256e-05, eta: 119 days, 16:10:00, time: 21.346, data_time: 1.389, memory: 8990, loss_cls: 0.7742, loss_bbox: 1.1538, d0.loss_cls: 0.8031, d0.loss_bbox: 1.3710, d1.loss_cls: 0.7706, d1.loss_bbox: 1.2269, d2.loss_cls: 0.7711, d2.loss_bbox: 1.1935, d3.loss_cls: 0.7693, d3.loss_bbox: 1.1695, d4.loss_cls: 0.7727, d4.loss_bbox: 1.1582, aux_task0.loss_heatmap: 1.0002, aux_task0.loss_bbox: 0.3638, aux_task1.loss_heatmap: 1.2505, aux_task1.loss_bbox: 0.3861, aux_task2.loss_heatmap: 1.4986, aux_task2.loss_bbox: 0.4253, aux_task3.loss_heatmap: 1.0784, aux_task3.loss_bbox: 0.3204, aux_task4.loss_heatmap: 0.7802, aux_task4.loss_bbox: 0.3340, aux_task5.loss_heatmap: 0.8513, aux_task5.loss_bbox: 0.3518, loss: 20.5746, grad_norm: 67.4536
2025-05-07 09:37:10,687 - mmdet - INFO - Epoch [1][3550/30895]	lr: 1.256e-05, eta: 120 days, 16:12:59, time: 26.932, data_time: 5.993, memory: 8990, loss_cls: 0.7782, loss_bbox: 1.1418, d0.loss_cls: 0.8043, d0.loss_bbox: 1.3517, d1.loss_cls: 0.7766, d1.loss_bbox: 1.2132, d2.loss_cls: 0.7732, d2.loss_bbox: 1.1791, d3.loss_cls: 0.7721, d3.loss_bbox: 1.1576, d4.loss_cls: 0.7754, d4.loss_bbox: 1.1475, aux_task0.loss_heatmap: 1.0137, aux_task0.loss_bbox: 0.3600, aux_task1.loss_heatmap: 1.2109, aux_task1.loss_bbox: 0.3853, aux_task2.loss_heatmap: 1.4991, aux_task2.loss_bbox: 0.4284, aux_task3.loss_heatmap: 1.0698, aux_task3.loss_bbox: 0.3305, aux_task4.loss_heatmap: 0.7800, aux_task4.loss_bbox: 0.3400, aux_task5.loss_heatmap: 0.8653, aux_task5.loss_bbox: 0.3532, loss: 20.5070, grad_norm: 64.9640
2025-05-07 09:57:47,502 - mmdet - INFO - Epoch [1][3600/30895]	lr: 1.256e-05, eta: 121 days, 10:22:58, time: 24.736, data_time: 9.153, memory: 8990, loss_cls: 0.7675, loss_bbox: 1.1420, d0.loss_cls: 0.8036, d0.loss_bbox: 1.3591, d1.loss_cls: 0.7715, d1.loss_bbox: 1.2178, d2.loss_cls: 0.7647, d2.loss_bbox: 1.1822, d3.loss_cls: 0.7630, d3.loss_bbox: 1.1597, d4.loss_cls: 0.7639, d4.loss_bbox: 1.1498, aux_task0.loss_heatmap: 1.0117, aux_task0.loss_bbox: 0.3600, aux_task1.loss_heatmap: 1.1726, aux_task1.loss_bbox: 0.3844, aux_task2.loss_heatmap: 1.4827, aux_task2.loss_bbox: 0.4262, aux_task3.loss_heatmap: 1.0184, aux_task3.loss_bbox: 0.3261, aux_task4.loss_heatmap: 0.7847, aux_task4.loss_bbox: 0.3496, aux_task5.loss_heatmap: 0.8632, aux_task5.loss_bbox: 0.3505, loss: 20.3750, grad_norm: 64.4432
2025-05-07 10:18:50,843 - mmdet - INFO - Epoch [1][3650/30895]	lr: 1.256e-05, eta: 122 days, 5:16:59, time: 25.267, data_time: 22.812, memory: 8990, loss_cls: 0.7459, loss_bbox: 1.1337, d0.loss_cls: 0.7854, d0.loss_bbox: 1.3536, d1.loss_cls: 0.7546, d1.loss_bbox: 1.2091, d2.loss_cls: 0.7493, d2.loss_bbox: 1.1739, d3.loss_cls: 0.7467, d3.loss_bbox: 1.1507, d4.loss_cls: 0.7461, d4.loss_bbox: 1.1393, aux_task0.loss_heatmap: 0.9611, aux_task0.loss_bbox: 0.3617, aux_task1.loss_heatmap: 1.2379, aux_task1.loss_bbox: 0.3914, aux_task2.loss_heatmap: 1.4619, aux_task2.loss_bbox: 0.4154, aux_task3.loss_heatmap: 1.0859, aux_task3.loss_bbox: 0.3374, aux_task4.loss_heatmap: 0.7545, aux_task4.loss_bbox: 0.3400, aux_task5.loss_heatmap: 0.8468, aux_task5.loss_bbox: 0.3475, loss: 20.2299, grad_norm: 67.1668
2025-05-07 10:45:29,181 - mmdet - INFO - Epoch [1][3700/30895]	lr: 1.256e-05, eta: 123 days, 15:06:34, time: 31.967, data_time: 13.746, memory: 8990, loss_cls: 0.7566, loss_bbox: 1.1331, d0.loss_cls: 0.7842, d0.loss_bbox: 1.3503, d1.loss_cls: 0.7569, d1.loss_bbox: 1.2094, d2.loss_cls: 0.7542, d2.loss_bbox: 1.1755, d3.loss_cls: 0.7528, d3.loss_bbox: 1.1514, d4.loss_cls: 0.7538, d4.loss_bbox: 1.1399, aux_task0.loss_heatmap: 0.9907, aux_task0.loss_bbox: 0.3662, aux_task1.loss_heatmap: 1.2224, aux_task1.loss_bbox: 0.3894, aux_task2.loss_heatmap: 1.4594, aux_task2.loss_bbox: 0.4194, aux_task3.loss_heatmap: 1.0733, aux_task3.loss_bbox: 0.3202, aux_task4.loss_heatmap: 0.7209, aux_task4.loss_bbox: 0.3524, aux_task5.loss_heatmap: 0.8479, aux_task5.loss_bbox: 0.3479, loss: 20.2281, grad_norm: 67.8252
2025-05-07 11:00:40,183 - mmdet - INFO - Epoch [1][3750/30895]	lr: 1.256e-05, eta: 123 days, 16:45:13, time: 18.220, data_time: 9.980, memory: 8990, loss_cls: 0.7385, loss_bbox: 1.1312, d0.loss_cls: 0.7752, d0.loss_bbox: 1.3376, d1.loss_cls: 0.7434, d1.loss_bbox: 1.2058, d2.loss_cls: 0.7397, d2.loss_bbox: 1.1730, d3.loss_cls: 0.7393, d3.loss_bbox: 1.1494, d4.loss_cls: 0.7365, d4.loss_bbox: 1.1378, aux_task0.loss_heatmap: 0.9785, aux_task0.loss_bbox: 0.3602, aux_task1.loss_heatmap: 1.2010, aux_task1.loss_bbox: 0.3892, aux_task2.loss_heatmap: 1.4558, aux_task2.loss_bbox: 0.4213, aux_task3.loss_heatmap: 1.0272, aux_task3.loss_bbox: 0.3256, aux_task4.loss_heatmap: 0.7378, aux_task4.loss_bbox: 0.3385, aux_task5.loss_heatmap: 0.8207, aux_task5.loss_bbox: 0.3509, loss: 20.0140, grad_norm: 63.6308
2025-05-07 11:24:56,461 - mmdet - INFO - Epoch [1][3800/30895]	lr: 1.257e-05, eta: 124 days, 18:49:32, time: 29.126, data_time: 24.147, memory: 8990, loss_cls: 0.7428, loss_bbox: 1.1202, d0.loss_cls: 0.7766, d0.loss_bbox: 1.3295, d1.loss_cls: 0.7480, d1.loss_bbox: 1.1949, d2.loss_cls: 0.7445, d2.loss_bbox: 1.1609, d3.loss_cls: 0.7408, d3.loss_bbox: 1.1363, d4.loss_cls: 0.7413, d4.loss_bbox: 1.1255, aux_task0.loss_heatmap: 0.9593, aux_task0.loss_bbox: 0.3475, aux_task1.loss_heatmap: 1.1624, aux_task1.loss_bbox: 0.3781, aux_task2.loss_heatmap: 1.5308, aux_task2.loss_bbox: 0.4111, aux_task3.loss_heatmap: 1.0361, aux_task3.loss_bbox: 0.3327, aux_task4.loss_heatmap: 0.7462, aux_task4.loss_bbox: 0.3403, aux_task5.loss_heatmap: 0.8258, aux_task5.loss_bbox: 0.3428, loss: 19.9747, grad_norm: 70.5347
2025-05-07 11:47:27,093 - mmdet - INFO - Epoch [1][3850/30895]	lr: 1.257e-05, eta: 125 days, 15:31:43, time: 27.012, data_time: 13.092, memory: 8990, loss_cls: 0.7469, loss_bbox: 1.1250, d0.loss_cls: 0.7720, d0.loss_bbox: 1.3320, d1.loss_cls: 0.7463, d1.loss_bbox: 1.2011, d2.loss_cls: 0.7467, d2.loss_bbox: 1.1659, d3.loss_cls: 0.7434, d3.loss_bbox: 1.1421, d4.loss_cls: 0.7448, d4.loss_bbox: 1.1316, aux_task0.loss_heatmap: 1.0095, aux_task0.loss_bbox: 0.3559, aux_task1.loss_heatmap: 1.1795, aux_task1.loss_bbox: 0.3840, aux_task2.loss_heatmap: 1.5064, aux_task2.loss_bbox: 0.4143, aux_task3.loss_heatmap: 1.0013, aux_task3.loss_bbox: 0.3229, aux_task4.loss_heatmap: 0.7521, aux_task4.loss_bbox: 0.3448, aux_task5.loss_heatmap: 0.8241, aux_task5.loss_bbox: 0.3483, loss: 20.0410, grad_norm: 65.8010
2025-05-07 12:07:45,831 - mmdet - INFO - Epoch [1][3900/30895]	lr: 1.257e-05, eta: 126 days, 5:55:25, time: 24.375, data_time: 0.876, memory: 8990, loss_cls: 0.7413, loss_bbox: 1.1178, d0.loss_cls: 0.7734, d0.loss_bbox: 1.3298, d1.loss_cls: 0.7420, d1.loss_bbox: 1.2002, d2.loss_cls: 0.7397, d2.loss_bbox: 1.1650, d3.loss_cls: 0.7369, d3.loss_bbox: 1.1400, d4.loss_cls: 0.7385, d4.loss_bbox: 1.1264, aux_task0.loss_heatmap: 0.9792, aux_task0.loss_bbox: 0.3525, aux_task1.loss_heatmap: 1.2078, aux_task1.loss_bbox: 0.3860, aux_task2.loss_heatmap: 1.4766, aux_task2.loss_bbox: 0.4265, aux_task3.loss_heatmap: 1.0116, aux_task3.loss_bbox: 0.3216, aux_task4.loss_heatmap: 0.7033, aux_task4.loss_bbox: 0.3356, aux_task5.loss_heatmap: 0.8489, aux_task5.loss_bbox: 0.3521, loss: 19.9528, grad_norm: 79.0728
2025-05-07 12:30:02,800 - mmdet - INFO - Epoch [1][3950/30895]	lr: 1.257e-05, eta: 127 days, 1:03:00, time: 26.739, data_time: 7.794, memory: 8990, loss_cls: 0.7431, loss_bbox: 1.1140, d0.loss_cls: 0.7743, d0.loss_bbox: 1.3162, d1.loss_cls: 0.7476, d1.loss_bbox: 1.1873, d2.loss_cls: 0.7440, d2.loss_bbox: 1.1532, d3.loss_cls: 0.7390, d3.loss_bbox: 1.1317, d4.loss_cls: 0.7399, d4.loss_bbox: 1.1226, aux_task0.loss_heatmap: 0.9815, aux_task0.loss_bbox: 0.3494, aux_task1.loss_heatmap: 1.2148, aux_task1.loss_bbox: 0.3887, aux_task2.loss_heatmap: 1.4504, aux_task2.loss_bbox: 0.4166, aux_task3.loss_heatmap: 0.9955, aux_task3.loss_bbox: 0.3219, aux_task4.loss_heatmap: 0.7051, aux_task4.loss_bbox: 0.3454, aux_task5.loss_heatmap: 0.8335, aux_task5.loss_bbox: 0.3472, loss: 19.8627, grad_norm: 65.5236
2025-05-07 12:52:11,535 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-07 12:52:11,536 - mmdet - INFO - Epoch [1][4000/30895]	lr: 1.257e-05, eta: 127 days, 19:20:17, time: 26.575, data_time: 6.207, memory: 8990, loss_cls: 0.7417, loss_bbox: 1.1194, d0.loss_cls: 0.7706, d0.loss_bbox: 1.3202, d1.loss_cls: 0.7448, d1.loss_bbox: 1.1957, d2.loss_cls: 0.7411, d2.loss_bbox: 1.1629, d3.loss_cls: 0.7373, d3.loss_bbox: 1.1396, d4.loss_cls: 0.7401, d4.loss_bbox: 1.1267, aux_task0.loss_heatmap: 0.9801, aux_task0.loss_bbox: 0.3645, aux_task1.loss_heatmap: 1.1650, aux_task1.loss_bbox: 0.3916, aux_task2.loss_heatmap: 1.4323, aux_task2.loss_bbox: 0.4210, aux_task3.loss_heatmap: 1.0315, aux_task3.loss_bbox: 0.3238, aux_task4.loss_heatmap: 0.7413, aux_task4.loss_bbox: 0.3427, aux_task5.loss_heatmap: 0.8004, aux_task5.loss_bbox: 0.3474, loss: 19.8817, grad_norm: 66.4084
2025-05-07 13:15:42,747 - mmdet - INFO - Epoch [1][4050/30895]	lr: 1.257e-05, eta: 128 days, 16:38:20, time: 28.225, data_time: 8.829, memory: 8990, loss_cls: 0.7329, loss_bbox: 1.1090, d0.loss_cls: 0.7669, d0.loss_bbox: 1.3208, d1.loss_cls: 0.7373, d1.loss_bbox: 1.1880, d2.loss_cls: 0.7318, d2.loss_bbox: 1.1525, d3.loss_cls: 0.7289, d3.loss_bbox: 1.1272, d4.loss_cls: 0.7302, d4.loss_bbox: 1.1154, aux_task0.loss_heatmap: 0.9851, aux_task0.loss_bbox: 0.3590, aux_task1.loss_heatmap: 1.1397, aux_task1.loss_bbox: 0.3800, aux_task2.loss_heatmap: 1.4577, aux_task2.loss_bbox: 0.4153, aux_task3.loss_heatmap: 1.0032, aux_task3.loss_bbox: 0.3138, aux_task4.loss_heatmap: 0.7363, aux_task4.loss_bbox: 0.3421, aux_task5.loss_heatmap: 0.8339, aux_task5.loss_bbox: 0.3482, loss: 19.7552, grad_norm: 69.1704
2025-05-07 13:36:38,845 - mmdet - INFO - Epoch [1][4100/30895]	lr: 1.258e-05, eta: 129 days, 6:57:35, time: 25.122, data_time: 17.459, memory: 8990, loss_cls: 0.7323, loss_bbox: 1.1131, d0.loss_cls: 0.7630, d0.loss_bbox: 1.3147, d1.loss_cls: 0.7368, d1.loss_bbox: 1.1951, d2.loss_cls: 0.7322, d2.loss_bbox: 1.1598, d3.loss_cls: 0.7301, d3.loss_bbox: 1.1329, d4.loss_cls: 0.7305, d4.loss_bbox: 1.1187, aux_task0.loss_heatmap: 0.9613, aux_task0.loss_bbox: 0.3560, aux_task1.loss_heatmap: 1.1838, aux_task1.loss_bbox: 0.3750, aux_task2.loss_heatmap: 1.4604, aux_task2.loss_bbox: 0.4125, aux_task3.loss_heatmap: 1.0255, aux_task3.loss_bbox: 0.3259, aux_task4.loss_heatmap: 0.7460, aux_task4.loss_bbox: 0.3354, aux_task5.loss_heatmap: 0.7914, aux_task5.loss_bbox: 0.3367, loss: 19.7691, grad_norm: 72.3053
2025-05-07 14:00:00,836 - mmdet - INFO - Epoch [1][4150/30895]	lr: 1.258e-05, eta: 130 days, 2:55:12, time: 28.040, data_time: 2.368, memory: 8990, loss_cls: 0.7182, loss_bbox: 1.1118, d0.loss_cls: 0.7539, d0.loss_bbox: 1.3119, d1.loss_cls: 0.7234, d1.loss_bbox: 1.1878, d2.loss_cls: 0.7173, d2.loss_bbox: 1.1561, d3.loss_cls: 0.7161, d3.loss_bbox: 1.1302, d4.loss_cls: 0.7155, d4.loss_bbox: 1.1188, aux_task0.loss_heatmap: 0.9211, aux_task0.loss_bbox: 0.3550, aux_task1.loss_heatmap: 1.2082, aux_task1.loss_bbox: 0.3914, aux_task2.loss_heatmap: 1.4157, aux_task2.loss_bbox: 0.4086, aux_task3.loss_heatmap: 1.0176, aux_task3.loss_bbox: 0.3163, aux_task4.loss_heatmap: 0.7125, aux_task4.loss_bbox: 0.3428, aux_task5.loss_heatmap: 0.7721, aux_task5.loss_bbox: 0.3393, loss: 19.5614, grad_norm: 65.7240
2025-05-07 14:17:50,308 - mmdet - INFO - Epoch [1][4200/30895]	lr: 1.258e-05, eta: 130 days, 8:53:57, time: 21.389, data_time: 4.092, memory: 8990, loss_cls: 0.7295, loss_bbox: 1.1297, d0.loss_cls: 0.7613, d0.loss_bbox: 1.3222, d1.loss_cls: 0.7313, d1.loss_bbox: 1.2024, d2.loss_cls: 0.7280, d2.loss_bbox: 1.1692, d3.loss_cls: 0.7257, d3.loss_bbox: 1.1470, d4.loss_cls: 0.7274, d4.loss_bbox: 1.1361, aux_task0.loss_heatmap: 0.9765, aux_task0.loss_bbox: 0.3643, aux_task1.loss_heatmap: 1.1782, aux_task1.loss_bbox: 0.3995, aux_task2.loss_heatmap: 1.4203, aux_task2.loss_bbox: 0.4138, aux_task3.loss_heatmap: 1.0033, aux_task3.loss_bbox: 0.3262, aux_task4.loss_heatmap: 0.7550, aux_task4.loss_bbox: 0.3476, aux_task5.loss_heatmap: 0.7980, aux_task5.loss_bbox: 0.3402, loss: 19.8327, grad_norm: 68.8786
2025-05-07 14:40:13,794 - mmdet - INFO - Epoch [1][4250/30895]	lr: 1.258e-05, eta: 131 days, 1:43:16, time: 26.870, data_time: 11.533, memory: 8990, loss_cls: 0.7177, loss_bbox: 1.1142, d0.loss_cls: 0.7494, d0.loss_bbox: 1.3236, d1.loss_cls: 0.7218, d1.loss_bbox: 1.1916, d2.loss_cls: 0.7169, d2.loss_bbox: 1.1588, d3.loss_cls: 0.7162, d3.loss_bbox: 1.1330, d4.loss_cls: 0.7158, d4.loss_bbox: 1.1211, aux_task0.loss_heatmap: 0.9719, aux_task0.loss_bbox: 0.3477, aux_task1.loss_heatmap: 1.1697, aux_task1.loss_bbox: 0.3842, aux_task2.loss_heatmap: 1.4348, aux_task2.loss_bbox: 0.4160, aux_task3.loss_heatmap: 0.9880, aux_task3.loss_bbox: 0.3209, aux_task4.loss_heatmap: 0.7182, aux_task4.loss_bbox: 0.3499, aux_task5.loss_heatmap: 0.7697, aux_task5.loss_bbox: 0.3385, loss: 19.5898, grad_norm: 66.3752
2025-05-07 15:00:21,082 - mmdet - INFO - Epoch [1][4300/30895]	lr: 1.258e-05, eta: 131 days, 12:44:39, time: 24.146, data_time: 15.648, memory: 8990, loss_cls: 0.7090, loss_bbox: 1.0988, d0.loss_cls: 0.7372, d0.loss_bbox: 1.2969, d1.loss_cls: 0.7127, d1.loss_bbox: 1.1721, d2.loss_cls: 0.7064, d2.loss_bbox: 1.1396, d3.loss_cls: 0.7039, d3.loss_bbox: 1.1170, d4.loss_cls: 0.7067, d4.loss_bbox: 1.1054, aux_task0.loss_heatmap: 0.9366, aux_task0.loss_bbox: 0.3509, aux_task1.loss_heatmap: 1.1393, aux_task1.loss_bbox: 0.3754, aux_task2.loss_heatmap: 1.3942, aux_task2.loss_bbox: 0.4239, aux_task3.loss_heatmap: 0.9773, aux_task3.loss_bbox: 0.3269, aux_task4.loss_heatmap: 0.6926, aux_task4.loss_bbox: 0.3429, aux_task5.loss_heatmap: 0.7543, aux_task5.loss_bbox: 0.3384, loss: 19.2583, grad_norm: 67.7889
2025-05-07 15:20:35,038 - mmdet - INFO - Epoch [1][4350/30895]	lr: 1.259e-05, eta: 131 days, 23:46:02, time: 24.279, data_time: 19.957, memory: 8990, loss_cls: 0.7102, loss_bbox: 1.1104, d0.loss_cls: 0.7406, d0.loss_bbox: 1.3070, d1.loss_cls: 0.7144, d1.loss_bbox: 1.1860, d2.loss_cls: 0.7098, d2.loss_bbox: 1.1545, d3.loss_cls: 0.7083, d3.loss_bbox: 1.1268, d4.loss_cls: 0.7068, d4.loss_bbox: 1.1172, aux_task0.loss_heatmap: 0.9159, aux_task0.loss_bbox: 0.3421, aux_task1.loss_heatmap: 1.1693, aux_task1.loss_bbox: 0.3883, aux_task2.loss_heatmap: 1.4092, aux_task2.loss_bbox: 0.4207, aux_task3.loss_heatmap: 0.9481, aux_task3.loss_bbox: 0.3209, aux_task4.loss_heatmap: 0.7403, aux_task4.loss_bbox: 0.3520, aux_task5.loss_heatmap: 0.7331, aux_task5.loss_bbox: 0.3366, loss: 19.3684, grad_norm: 69.1024
2025-05-07 15:38:44,021 - mmdet - INFO - Epoch [1][4400/30895]	lr: 1.259e-05, eta: 132 days, 5:41:32, time: 21.780, data_time: 9.765, memory: 8990, loss_cls: 0.7110, loss_bbox: 1.1053, d0.loss_cls: 0.7465, d0.loss_bbox: 1.2984, d1.loss_cls: 0.7210, d1.loss_bbox: 1.1765, d2.loss_cls: 0.7127, d2.loss_bbox: 1.1441, d3.loss_cls: 0.7101, d3.loss_bbox: 1.1221, d4.loss_cls: 0.7089, d4.loss_bbox: 1.1113, aux_task0.loss_heatmap: 0.9389, aux_task0.loss_bbox: 0.3429, aux_task1.loss_heatmap: 1.1389, aux_task1.loss_bbox: 0.3821, aux_task2.loss_heatmap: 1.3932, aux_task2.loss_bbox: 0.4200, aux_task3.loss_heatmap: 0.9182, aux_task3.loss_bbox: 0.3241, aux_task4.loss_heatmap: 0.7535, aux_task4.loss_bbox: 0.3464, aux_task5.loss_heatmap: 0.7706, aux_task5.loss_bbox: 0.3386, loss: 19.3354, grad_norm: 67.8740
2025-05-07 16:05:41,605 - mmdet - INFO - Epoch [1][4450/30895]	lr: 1.259e-05, eta: 133 days, 7:43:04, time: 32.351, data_time: 29.565, memory: 8990, loss_cls: 0.7081, loss_bbox: 1.0990, d0.loss_cls: 0.7425, d0.loss_bbox: 1.2922, d1.loss_cls: 0.7183, d1.loss_bbox: 1.1733, d2.loss_cls: 0.7108, d2.loss_bbox: 1.1420, d3.loss_cls: 0.7065, d3.loss_bbox: 1.1161, d4.loss_cls: 0.7069, d4.loss_bbox: 1.1048, aux_task0.loss_heatmap: 0.9435, aux_task0.loss_bbox: 0.3537, aux_task1.loss_heatmap: 1.1405, aux_task1.loss_bbox: 0.3784, aux_task2.loss_heatmap: 1.3890, aux_task2.loss_bbox: 0.4299, aux_task3.loss_heatmap: 0.9092, aux_task3.loss_bbox: 0.3193, aux_task4.loss_heatmap: 0.7252, aux_task4.loss_bbox: 0.3434, aux_task5.loss_heatmap: 0.7784, aux_task5.loss_bbox: 0.3362, loss: 19.2671, grad_norm: 67.7025
2025-05-07 16:21:39,865 - mmdet - INFO - Epoch [1][4500/30895]	lr: 1.259e-05, eta: 133 days, 8:11:32, time: 19.166, data_time: 14.725, memory: 8990, loss_cls: 0.7087, loss_bbox: 1.0908, d0.loss_cls: 0.7442, d0.loss_bbox: 1.2851, d1.loss_cls: 0.7161, d1.loss_bbox: 1.1651, d2.loss_cls: 0.7093, d2.loss_bbox: 1.1312, d3.loss_cls: 0.7058, d3.loss_bbox: 1.1081, d4.loss_cls: 0.7062, d4.loss_bbox: 1.0964, aux_task0.loss_heatmap: 0.9480, aux_task0.loss_bbox: 0.3477, aux_task1.loss_heatmap: 1.1139, aux_task1.loss_bbox: 0.3694, aux_task2.loss_heatmap: 1.3994, aux_task2.loss_bbox: 0.4113, aux_task3.loss_heatmap: 0.8943, aux_task3.loss_bbox: 0.3167, aux_task4.loss_heatmap: 0.7164, aux_task4.loss_bbox: 0.3388, aux_task5.loss_heatmap: 0.7781, aux_task5.loss_bbox: 0.3435, loss: 19.1445, grad_norm: 73.0309
2025-05-07 16:45:17,207 - mmdet - INFO - Epoch [1][4550/30895]	lr: 1.259e-05, eta: 134 days, 1:50:21, time: 28.347, data_time: 18.972, memory: 8990, loss_cls: 0.7157, loss_bbox: 1.0809, d0.loss_cls: 0.7469, d0.loss_bbox: 1.2695, d1.loss_cls: 0.7179, d1.loss_bbox: 1.1527, d2.loss_cls: 0.7161, d2.loss_bbox: 1.1222, d3.loss_cls: 0.7121, d3.loss_bbox: 1.0999, d4.loss_cls: 0.7127, d4.loss_bbox: 1.0887, aux_task0.loss_heatmap: 0.9165, aux_task0.loss_bbox: 0.3521, aux_task1.loss_heatmap: 1.1341, aux_task1.loss_bbox: 0.3857, aux_task2.loss_heatmap: 1.3937, aux_task2.loss_bbox: 0.4209, aux_task3.loss_heatmap: 0.9258, aux_task3.loss_bbox: 0.3120, aux_task4.loss_heatmap: 0.7231, aux_task4.loss_bbox: 0.3374, aux_task5.loss_heatmap: 0.7647, aux_task5.loss_bbox: 0.3387, loss: 19.1399, grad_norm: 71.6142
2025-05-07 17:08:11,077 - mmdet - INFO - Epoch [1][4600/30895]	lr: 1.260e-05, eta: 134 days, 17:29:06, time: 27.478, data_time: 13.312, memory: 8990, loss_cls: 0.7018, loss_bbox: 1.0876, d0.loss_cls: 0.7333, d0.loss_bbox: 1.2763, d1.loss_cls: 0.7032, d1.loss_bbox: 1.1575, d2.loss_cls: 0.6993, d2.loss_bbox: 1.1281, d3.loss_cls: 0.6967, d3.loss_bbox: 1.1049, d4.loss_cls: 0.6972, d4.loss_bbox: 1.0955, aux_task0.loss_heatmap: 0.9553, aux_task0.loss_bbox: 0.3589, aux_task1.loss_heatmap: 1.1573, aux_task1.loss_bbox: 0.3909, aux_task2.loss_heatmap: 1.4056, aux_task2.loss_bbox: 0.4206, aux_task3.loss_heatmap: 0.9457, aux_task3.loss_bbox: 0.3169, aux_task4.loss_heatmap: 0.7010, aux_task4.loss_bbox: 0.3367, aux_task5.loss_heatmap: 0.7468, aux_task5.loss_bbox: 0.3381, loss: 19.1553, grad_norm: 70.7010
2025-05-07 17:29:29,241 - mmdet - INFO - Epoch [1][4650/30895]	lr: 1.260e-05, eta: 135 days, 5:16:47, time: 25.563, data_time: 21.600, memory: 8990, loss_cls: 0.7036, loss_bbox: 1.0866, d0.loss_cls: 0.7405, d0.loss_bbox: 1.2811, d1.loss_cls: 0.7094, d1.loss_bbox: 1.1605, d2.loss_cls: 0.7056, d2.loss_bbox: 1.1265, d3.loss_cls: 0.7043, d3.loss_bbox: 1.1029, d4.loss_cls: 0.7035, d4.loss_bbox: 1.0926, aux_task0.loss_heatmap: 0.9308, aux_task0.loss_bbox: 0.3582, aux_task1.loss_heatmap: 1.1465, aux_task1.loss_bbox: 0.3743, aux_task2.loss_heatmap: 1.4210, aux_task2.loss_bbox: 0.4176, aux_task3.loss_heatmap: 0.9941, aux_task3.loss_bbox: 0.3205, aux_task4.loss_heatmap: 0.7087, aux_task4.loss_bbox: 0.3411, aux_task5.loss_heatmap: 0.7546, aux_task5.loss_bbox: 0.3403, loss: 19.2246, grad_norm: 66.0150
2025-05-07 17:49:44,499 - mmdet - INFO - Epoch [1][4700/30895]	lr: 1.260e-05, eta: 135 days, 14:32:09, time: 24.305, data_time: 21.467, memory: 8990, loss_cls: 0.7061, loss_bbox: 1.0862, d0.loss_cls: 0.7361, d0.loss_bbox: 1.2682, d1.loss_cls: 0.7105, d1.loss_bbox: 1.1513, d2.loss_cls: 0.7057, d2.loss_bbox: 1.1221, d3.loss_cls: 0.7018, d3.loss_bbox: 1.1001, d4.loss_cls: 0.7044, d4.loss_bbox: 1.0903, aux_task0.loss_heatmap: 0.9397, aux_task0.loss_bbox: 0.3606, aux_task1.loss_heatmap: 1.1403, aux_task1.loss_bbox: 0.3807, aux_task2.loss_heatmap: 1.3689, aux_task2.loss_bbox: 0.4074, aux_task3.loss_heatmap: 0.9510, aux_task3.loss_bbox: 0.3089, aux_task4.loss_heatmap: 0.7136, aux_task4.loss_bbox: 0.3457, aux_task5.loss_heatmap: 0.7588, aux_task5.loss_bbox: 0.3419, loss: 19.1001, grad_norm: 70.1835
2025-05-07 18:04:18,213 - mmdet - INFO - Epoch [1][4750/30895]	lr: 1.260e-05, eta: 135 days, 11:20:36, time: 17.474, data_time: 16.033, memory: 8990, loss_cls: 0.7003, loss_bbox: 1.0625, d0.loss_cls: 0.7276, d0.loss_bbox: 1.2515, d1.loss_cls: 0.7030, d1.loss_bbox: 1.1364, d2.loss_cls: 0.6991, d2.loss_bbox: 1.1022, d3.loss_cls: 0.6972, d3.loss_bbox: 1.0784, d4.loss_cls: 0.6986, d4.loss_bbox: 1.0680, aux_task0.loss_heatmap: 0.9185, aux_task0.loss_bbox: 0.3529, aux_task1.loss_heatmap: 1.1145, aux_task1.loss_bbox: 0.3728, aux_task2.loss_heatmap: 1.3624, aux_task2.loss_bbox: 0.4011, aux_task3.loss_heatmap: 0.9411, aux_task3.loss_bbox: 0.3119, aux_task4.loss_heatmap: 0.7090, aux_task4.loss_bbox: 0.3371, aux_task5.loss_heatmap: 0.7601, aux_task5.loss_bbox: 0.3337, loss: 18.8397, grad_norm: 75.6295
2025-05-07 18:18:26,200 - mmdet - INFO - Epoch [1][4800/30895]	lr: 1.260e-05, eta: 135 days, 7:18:00, time: 16.960, data_time: 15.496, memory: 8990, loss_cls: 0.6961, loss_bbox: 1.0715, d0.loss_cls: 0.7307, d0.loss_bbox: 1.2586, d1.loss_cls: 0.7004, d1.loss_bbox: 1.1416, d2.loss_cls: 0.6947, d2.loss_bbox: 1.1118, d3.loss_cls: 0.6955, d3.loss_bbox: 1.0896, d4.loss_cls: 0.6950, d4.loss_bbox: 1.0775, aux_task0.loss_heatmap: 0.9457, aux_task0.loss_bbox: 0.3509, aux_task1.loss_heatmap: 1.1398, aux_task1.loss_bbox: 0.3821, aux_task2.loss_heatmap: 1.3849, aux_task2.loss_bbox: 0.4083, aux_task3.loss_heatmap: 0.9485, aux_task3.loss_bbox: 0.3107, aux_task4.loss_heatmap: 0.7006, aux_task4.loss_bbox: 0.3452, aux_task5.loss_heatmap: 0.7527, aux_task5.loss_bbox: 0.3358, loss: 18.9683, grad_norm: 66.9521
2025-05-07 18:29:39,042 - mmdet - INFO - Epoch [1][4850/30895]	lr: 1.261e-05, eta: 134 days, 21:11:05, time: 13.457, data_time: 11.997, memory: 8990, loss_cls: 0.6912, loss_bbox: 1.0800, d0.loss_cls: 0.7233, d0.loss_bbox: 1.2674, d1.loss_cls: 0.6969, d1.loss_bbox: 1.1494, d2.loss_cls: 0.6925, d2.loss_bbox: 1.1172, d3.loss_cls: 0.6901, d3.loss_bbox: 1.0944, d4.loss_cls: 0.6914, d4.loss_bbox: 1.0848, aux_task0.loss_heatmap: 0.8733, aux_task0.loss_bbox: 0.3446, aux_task1.loss_heatmap: 1.1701, aux_task1.loss_bbox: 0.3814, aux_task2.loss_heatmap: 1.3684, aux_task2.loss_bbox: 0.3996, aux_task3.loss_heatmap: 0.8733, aux_task3.loss_bbox: 0.3058, aux_task4.loss_heatmap: 0.7279, aux_task4.loss_bbox: 0.3335, aux_task5.loss_heatmap: 0.7130, aux_task5.loss_bbox: 0.3349, loss: 18.8045, grad_norm: 66.6926
2025-05-07 18:43:32,353 - mmdet - INFO - Epoch [1][4900/30895]	lr: 1.261e-05, eta: 134 days, 16:50:56, time: 16.666, data_time: 15.206, memory: 8990, loss_cls: 0.6866, loss_bbox: 1.0650, d0.loss_cls: 0.7225, d0.loss_bbox: 1.2427, d1.loss_cls: 0.6947, d1.loss_bbox: 1.1309, d2.loss_cls: 0.6884, d2.loss_bbox: 1.0991, d3.loss_cls: 0.6839, d3.loss_bbox: 1.0789, d4.loss_cls: 0.6837, d4.loss_bbox: 1.0692, aux_task0.loss_heatmap: 0.9238, aux_task0.loss_bbox: 0.3534, aux_task1.loss_heatmap: 1.1304, aux_task1.loss_bbox: 0.3783, aux_task2.loss_heatmap: 1.3405, aux_task2.loss_bbox: 0.4031, aux_task3.loss_heatmap: 0.9446, aux_task3.loss_bbox: 0.3143, aux_task4.loss_heatmap: 0.6987, aux_task4.loss_bbox: 0.3325, aux_task5.loss_heatmap: 0.7456, aux_task5.loss_bbox: 0.3353, loss: 18.7462, grad_norm: 68.8384
2025-05-07 18:57:21,001 - mmdet - INFO - Epoch [1][4950/30895]	lr: 1.261e-05, eta: 134 days, 12:26:10, time: 16.573, data_time: 15.120, memory: 8990, loss_cls: 0.6893, loss_bbox: 1.0580, d0.loss_cls: 0.7173, d0.loss_bbox: 1.2496, d1.loss_cls: 0.6935, d1.loss_bbox: 1.1300, d2.loss_cls: 0.6900, d2.loss_bbox: 1.0965, d3.loss_cls: 0.6879, d3.loss_bbox: 1.0732, d4.loss_cls: 0.6877, d4.loss_bbox: 1.0633, aux_task0.loss_heatmap: 0.8938, aux_task0.loss_bbox: 0.3427, aux_task1.loss_heatmap: 1.0901, aux_task1.loss_bbox: 0.3785, aux_task2.loss_heatmap: 1.3184, aux_task2.loss_bbox: 0.4065, aux_task3.loss_heatmap: 0.9252, aux_task3.loss_bbox: 0.3188, aux_task4.loss_heatmap: 0.6746, aux_task4.loss_bbox: 0.3423, aux_task5.loss_heatmap: 0.7102, aux_task5.loss_bbox: 0.3304, loss: 18.5678, grad_norm: 73.6821
2025-05-07 19:16:54,451 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-07 19:16:54,451 - mmdet - INFO - Epoch [1][5000/30895]	lr: 1.261e-05, eta: 134 days, 19:50:48, time: 23.469, data_time: 22.002, memory: 8990, loss_cls: 0.6870, loss_bbox: 1.0685, d0.loss_cls: 0.7220, d0.loss_bbox: 1.2509, d1.loss_cls: 0.6929, d1.loss_bbox: 1.1372, d2.loss_cls: 0.6876, d2.loss_bbox: 1.1065, d3.loss_cls: 0.6877, d3.loss_bbox: 1.0840, d4.loss_cls: 0.6853, d4.loss_bbox: 1.0756, aux_task0.loss_heatmap: 0.9398, aux_task0.loss_bbox: 0.3643, aux_task1.loss_heatmap: 1.1388, aux_task1.loss_bbox: 0.3868, aux_task2.loss_heatmap: 1.3708, aux_task2.loss_bbox: 0.4116, aux_task3.loss_heatmap: 0.9388, aux_task3.loss_bbox: 0.3133, aux_task4.loss_heatmap: 0.6902, aux_task4.loss_bbox: 0.3321, aux_task5.loss_heatmap: 0.7346, aux_task5.loss_bbox: 0.3324, loss: 18.8385, grad_norm: 70.1475
2025-05-07 19:31:11,162 - mmdet - INFO - Epoch [1][5050/30895]	lr: 1.262e-05, eta: 134 days, 16:25:38, time: 17.134, data_time: 15.665, memory: 8990, loss_cls: 0.6861, loss_bbox: 1.0504, d0.loss_cls: 0.7121, d0.loss_bbox: 1.2316, d1.loss_cls: 0.6901, d1.loss_bbox: 1.1182, d2.loss_cls: 0.6863, d2.loss_bbox: 1.0861, d3.loss_cls: 0.6850, d3.loss_bbox: 1.0649, d4.loss_cls: 0.6829, d4.loss_bbox: 1.0570, aux_task0.loss_heatmap: 0.9544, aux_task0.loss_bbox: 0.3609, aux_task1.loss_heatmap: 1.0911, aux_task1.loss_bbox: 0.3702, aux_task2.loss_heatmap: 1.3602, aux_task2.loss_bbox: 0.3985, aux_task3.loss_heatmap: 0.9577, aux_task3.loss_bbox: 0.3188, aux_task4.loss_heatmap: 0.6834, aux_task4.loss_bbox: 0.3415, aux_task5.loss_heatmap: 0.7331, aux_task5.loss_bbox: 0.3337, loss: 18.6542, grad_norm: 71.5733
2025-05-07 19:49:42,787 - mmdet - INFO - Epoch [1][5100/30895]	lr: 1.262e-05, eta: 134 days, 21:34:42, time: 22.233, data_time: 20.762, memory: 8990, loss_cls: 0.6874, loss_bbox: 1.0662, d0.loss_cls: 0.7225, d0.loss_bbox: 1.2455, d1.loss_cls: 0.6952, d1.loss_bbox: 1.1284, d2.loss_cls: 0.6893, d2.loss_bbox: 1.0994, d3.loss_cls: 0.6861, d3.loss_bbox: 1.0807, d4.loss_cls: 0.6859, d4.loss_bbox: 1.0716, aux_task0.loss_heatmap: 0.9344, aux_task0.loss_bbox: 0.3524, aux_task1.loss_heatmap: 1.1041, aux_task1.loss_bbox: 0.3762, aux_task2.loss_heatmap: 1.3507, aux_task2.loss_bbox: 0.4065, aux_task3.loss_heatmap: 0.9182, aux_task3.loss_bbox: 0.3189, aux_task4.loss_heatmap: 0.6858, aux_task4.loss_bbox: 0.3317, aux_task5.loss_heatmap: 0.7498, aux_task5.loss_bbox: 0.3364, loss: 18.7234, grad_norm: 71.0588
2025-05-07 20:04:40,086 - mmdet - INFO - Epoch [1][5150/30895]	lr: 1.262e-05, eta: 134 days, 19:32:24, time: 17.946, data_time: 16.482, memory: 8990, loss_cls: 0.6796, loss_bbox: 1.0656, d0.loss_cls: 0.7196, d0.loss_bbox: 1.2473, d1.loss_cls: 0.6874, d1.loss_bbox: 1.1306, d2.loss_cls: 0.6816, d2.loss_bbox: 1.0997, d3.loss_cls: 0.6790, d3.loss_bbox: 1.0772, d4.loss_cls: 0.6770, d4.loss_bbox: 1.0690, aux_task0.loss_heatmap: 0.8875, aux_task0.loss_bbox: 0.3493, aux_task1.loss_heatmap: 1.0760, aux_task1.loss_bbox: 0.3734, aux_task2.loss_heatmap: 1.3228, aux_task2.loss_bbox: 0.4038, aux_task3.loss_heatmap: 0.9779, aux_task3.loss_bbox: 0.3167, aux_task4.loss_heatmap: 0.6921, aux_task4.loss_bbox: 0.3366, aux_task5.loss_heatmap: 0.7218, aux_task5.loss_bbox: 0.3338, loss: 18.6052, grad_norm: 74.4241
2025-05-07 20:18:57,372 - mmdet - INFO - Epoch [1][5200/30895]	lr: 1.262e-05, eta: 134 days, 16:13:33, time: 17.145, data_time: 15.213, memory: 8990, loss_cls: 0.6830, loss_bbox: 1.0638, d0.loss_cls: 0.7206, d0.loss_bbox: 1.2542, d1.loss_cls: 0.6932, d1.loss_bbox: 1.1344, d2.loss_cls: 0.6872, d2.loss_bbox: 1.1041, d3.loss_cls: 0.6828, d3.loss_bbox: 1.0819, d4.loss_cls: 0.6818, d4.loss_bbox: 1.0715, aux_task0.loss_heatmap: 0.8987, aux_task0.loss_bbox: 0.3538, aux_task1.loss_heatmap: 1.0852, aux_task1.loss_bbox: 0.3686, aux_task2.loss_heatmap: 1.3416, aux_task2.loss_bbox: 0.4084, aux_task3.loss_heatmap: 0.9777, aux_task3.loss_bbox: 0.3254, aux_task4.loss_heatmap: 0.6709, aux_task4.loss_bbox: 0.3470, aux_task5.loss_heatmap: 0.7344, aux_task5.loss_bbox: 0.3356, loss: 18.7057, grad_norm: 71.8068
2025-05-07 20:37:01,251 - mmdet - INFO - Epoch [1][5250/30895]	lr: 1.263e-05, eta: 134 days, 20:18:58, time: 21.678, data_time: 20.213, memory: 8990, loss_cls: 0.6697, loss_bbox: 1.0517, d0.loss_cls: 0.6984, d0.loss_bbox: 1.2327, d1.loss_cls: 0.6753, d1.loss_bbox: 1.1162, d2.loss_cls: 0.6718, d2.loss_bbox: 1.0860, d3.loss_cls: 0.6674, d3.loss_bbox: 1.0660, d4.loss_cls: 0.6680, d4.loss_bbox: 1.0571, aux_task0.loss_heatmap: 0.8738, aux_task0.loss_bbox: 0.3423, aux_task1.loss_heatmap: 1.0866, aux_task1.loss_bbox: 0.3738, aux_task2.loss_heatmap: 1.3615, aux_task2.loss_bbox: 0.4059, aux_task3.loss_heatmap: 0.9143, aux_task3.loss_bbox: 0.3320, aux_task4.loss_heatmap: 0.6568, aux_task4.loss_bbox: 0.3390, aux_task5.loss_heatmap: 0.7350, aux_task5.loss_bbox: 0.3354, loss: 18.4166, grad_norm: 71.7864
2025-05-07 20:58:40,569 - mmdet - INFO - Epoch [1][5300/30895]	lr: 1.263e-05, eta: 135 days, 7:14:25, time: 25.986, data_time: 24.532, memory: 8990, loss_cls: 0.6831, loss_bbox: 1.0451, d0.loss_cls: 0.7107, d0.loss_bbox: 1.2342, d1.loss_cls: 0.6867, d1.loss_bbox: 1.1165, d2.loss_cls: 0.6837, d2.loss_bbox: 1.0847, d3.loss_cls: 0.6823, d3.loss_bbox: 1.0615, d4.loss_cls: 0.6822, d4.loss_bbox: 1.0507, aux_task0.loss_heatmap: 0.8882, aux_task0.loss_bbox: 0.3492, aux_task1.loss_heatmap: 1.0625, aux_task1.loss_bbox: 0.3734, aux_task2.loss_heatmap: 1.3399, aux_task2.loss_bbox: 0.4030, aux_task3.loss_heatmap: 0.9129, aux_task3.loss_bbox: 0.3232, aux_task4.loss_heatmap: 0.6473, aux_task4.loss_bbox: 0.3328, aux_task5.loss_heatmap: 0.7583, aux_task5.loss_bbox: 0.3373, loss: 18.4495, grad_norm: 72.1994
2025-05-07 21:14:58,550 - mmdet - INFO - Epoch [1][5350/30895]	lr: 1.263e-05, eta: 135 days, 7:44:01, time: 19.560, data_time: 18.105, memory: 8990, loss_cls: 0.6725, loss_bbox: 1.0503, d0.loss_cls: 0.7083, d0.loss_bbox: 1.2354, d1.loss_cls: 0.6799, d1.loss_bbox: 1.1190, d2.loss_cls: 0.6754, d2.loss_bbox: 1.0863, d3.loss_cls: 0.6717, d3.loss_bbox: 1.0647, d4.loss_cls: 0.6712, d4.loss_bbox: 1.0545, aux_task0.loss_heatmap: 0.8517, aux_task0.loss_bbox: 0.3455, aux_task1.loss_heatmap: 1.0802, aux_task1.loss_bbox: 0.3750, aux_task2.loss_heatmap: 1.2684, aux_task2.loss_bbox: 0.4008, aux_task3.loss_heatmap: 0.9570, aux_task3.loss_bbox: 0.3269, aux_task4.loss_heatmap: 0.6526, aux_task4.loss_bbox: 0.3345, aux_task5.loss_heatmap: 0.7041, aux_task5.loss_bbox: 0.3384, loss: 18.3245, grad_norm: 71.5185
2025-05-07 21:28:04,868 - mmdet - INFO - Epoch [1][5400/30895]	lr: 1.263e-05, eta: 135 days, 2:10:26, time: 15.726, data_time: 14.272, memory: 8990, loss_cls: 0.6703, loss_bbox: 1.0507, d0.loss_cls: 0.7064, d0.loss_bbox: 1.2238, d1.loss_cls: 0.6785, d1.loss_bbox: 1.1187, d2.loss_cls: 0.6740, d2.loss_bbox: 1.0880, d3.loss_cls: 0.6696, d3.loss_bbox: 1.0651, d4.loss_cls: 0.6695, d4.loss_bbox: 1.0567, aux_task0.loss_heatmap: 0.8891, aux_task0.loss_bbox: 0.3411, aux_task1.loss_heatmap: 1.1186, aux_task1.loss_bbox: 0.3719, aux_task2.loss_heatmap: 1.2728, aux_task2.loss_bbox: 0.4103, aux_task3.loss_heatmap: 0.9226, aux_task3.loss_bbox: 0.3199, aux_task4.loss_heatmap: 0.6982, aux_task4.loss_bbox: 0.3342, aux_task5.loss_heatmap: 0.7032, aux_task5.loss_bbox: 0.3278, loss: 18.3812, grad_norm: 73.3758
2025-05-07 22:02:24,134 - mmdet - INFO - Epoch [1][5450/30895]	lr: 1.263e-05, eta: 136 days, 12:26:52, time: 41.185, data_time: 39.747, memory: 8990, loss_cls: 0.6728, loss_bbox: 1.0324, d0.loss_cls: 0.7094, d0.loss_bbox: 1.2158, d1.loss_cls: 0.6801, d1.loss_bbox: 1.0992, d2.loss_cls: 0.6757, d2.loss_bbox: 1.0674, d3.loss_cls: 0.6723, d3.loss_bbox: 1.0462, d4.loss_cls: 0.6712, d4.loss_bbox: 1.0394, aux_task0.loss_heatmap: 0.8999, aux_task0.loss_bbox: 0.3512, aux_task1.loss_heatmap: 1.0713, aux_task1.loss_bbox: 0.3743, aux_task2.loss_heatmap: 1.3083, aux_task2.loss_bbox: 0.4112, aux_task3.loss_heatmap: 0.8843, aux_task3.loss_bbox: 0.3190, aux_task4.loss_heatmap: 0.6703, aux_task4.loss_bbox: 0.3275, aux_task5.loss_heatmap: 0.7189, aux_task5.loss_bbox: 0.3349, loss: 18.2531, grad_norm: 72.5427
2025-05-07 22:12:27,495 - mmdet - INFO - Epoch [1][5500/30895]	lr: 1.264e-05, eta: 136 days, 1:03:32, time: 12.068, data_time: 10.605, memory: 8990, loss_cls: 0.6709, loss_bbox: 1.0445, d0.loss_cls: 0.7046, d0.loss_bbox: 1.2276, d1.loss_cls: 0.6780, d1.loss_bbox: 1.1117, d2.loss_cls: 0.6731, d2.loss_bbox: 1.0807, d3.loss_cls: 0.6693, d3.loss_bbox: 1.0607, d4.loss_cls: 0.6707, d4.loss_bbox: 1.0512, aux_task0.loss_heatmap: 0.8687, aux_task0.loss_bbox: 0.3367, aux_task1.loss_heatmap: 1.0824, aux_task1.loss_bbox: 0.3760, aux_task2.loss_heatmap: 1.2790, aux_task2.loss_bbox: 0.4047, aux_task3.loss_heatmap: 0.8822, aux_task3.loss_bbox: 0.3109, aux_task4.loss_heatmap: 0.6798, aux_task4.loss_bbox: 0.3404, aux_task5.loss_heatmap: 0.7434, aux_task5.loss_bbox: 0.3336, loss: 18.2807, grad_norm: 66.0745
2025-05-07 22:22:11,159 - mmdet - INFO - Epoch [1][5550/30895]	lr: 1.264e-05, eta: 135 days, 13:16:03, time: 11.673, data_time: 10.211, memory: 8990, loss_cls: 0.6678, loss_bbox: 1.0508, d0.loss_cls: 0.7023, d0.loss_bbox: 1.2309, d1.loss_cls: 0.6758, d1.loss_bbox: 1.1155, d2.loss_cls: 0.6711, d2.loss_bbox: 1.0875, d3.loss_cls: 0.6681, d3.loss_bbox: 1.0676, d4.loss_cls: 0.6664, d4.loss_bbox: 1.0581, aux_task0.loss_heatmap: 0.9016, aux_task0.loss_bbox: 0.3490, aux_task1.loss_heatmap: 1.0765, aux_task1.loss_bbox: 0.3742, aux_task2.loss_heatmap: 1.3204, aux_task2.loss_bbox: 0.4025, aux_task3.loss_heatmap: 0.8788, aux_task3.loss_bbox: 0.3239, aux_task4.loss_heatmap: 0.6998, aux_task4.loss_bbox: 0.3414, aux_task5.loss_heatmap: 0.6939, aux_task5.loss_bbox: 0.3288, loss: 18.3526, grad_norm: 66.6127
2025-05-07 22:32:11,463 - mmdet - INFO - Epoch [1][5600/30895]	lr: 1.264e-05, eta: 135 days, 2:11:23, time: 12.006, data_time: 9.164, memory: 8990, loss_cls: 0.6649, loss_bbox: 1.0505, d0.loss_cls: 0.6985, d0.loss_bbox: 1.2350, d1.loss_cls: 0.6731, d1.loss_bbox: 1.1192, d2.loss_cls: 0.6664, d2.loss_bbox: 1.0867, d3.loss_cls: 0.6636, d3.loss_bbox: 1.0648, d4.loss_cls: 0.6629, d4.loss_bbox: 1.0552, aux_task0.loss_heatmap: 0.8900, aux_task0.loss_bbox: 0.3418, aux_task1.loss_heatmap: 1.1036, aux_task1.loss_bbox: 0.3797, aux_task2.loss_heatmap: 1.2941, aux_task2.loss_bbox: 0.3943, aux_task3.loss_heatmap: 0.9230, aux_task3.loss_bbox: 0.3157, aux_task4.loss_heatmap: 0.6826, aux_task4.loss_bbox: 0.3348, aux_task5.loss_heatmap: 0.7192, aux_task5.loss_bbox: 0.3382, loss: 18.3576, grad_norm: 73.9921
2025-05-07 22:41:30,909 - mmdet - INFO - Epoch [1][5650/30895]	lr: 1.264e-05, eta: 134 days, 14:04:30, time: 11.189, data_time: 5.318, memory: 8990, loss_cls: 0.6632, loss_bbox: 1.0508, d0.loss_cls: 0.6993, d0.loss_bbox: 1.2239, d1.loss_cls: 0.6723, d1.loss_bbox: 1.1180, d2.loss_cls: 0.6663, d2.loss_bbox: 1.0883, d3.loss_cls: 0.6637, d3.loss_bbox: 1.0677, d4.loss_cls: 0.6635, d4.loss_bbox: 1.0572, aux_task0.loss_heatmap: 0.8674, aux_task0.loss_bbox: 0.3495, aux_task1.loss_heatmap: 1.0935, aux_task1.loss_bbox: 0.3804, aux_task2.loss_heatmap: 1.2310, aux_task2.loss_bbox: 0.3971, aux_task3.loss_heatmap: 0.8629, aux_task3.loss_bbox: 0.3225, aux_task4.loss_heatmap: 0.6545, aux_task4.loss_bbox: 0.3385, aux_task5.loss_heatmap: 0.7274, aux_task5.loss_bbox: 0.3367, loss: 18.1959, grad_norm: 70.4499
2025-05-07 22:53:48,277 - mmdet - INFO - Epoch [1][5700/30895]	lr: 1.265e-05, eta: 134 days, 7:28:44, time: 14.748, data_time: 13.206, memory: 8990, loss_cls: 0.6668, loss_bbox: 1.0430, d0.loss_cls: 0.6958, d0.loss_bbox: 1.2328, d1.loss_cls: 0.6755, d1.loss_bbox: 1.1151, d2.loss_cls: 0.6704, d2.loss_bbox: 1.0842, d3.loss_cls: 0.6641, d3.loss_bbox: 1.0624, d4.loss_cls: 0.6672, d4.loss_bbox: 1.0516, aux_task0.loss_heatmap: 0.8868, aux_task0.loss_bbox: 0.3476, aux_task1.loss_heatmap: 1.1043, aux_task1.loss_bbox: 0.3736, aux_task2.loss_heatmap: 1.2764, aux_task2.loss_bbox: 0.4012, aux_task3.loss_heatmap: 0.8514, aux_task3.loss_bbox: 0.3153, aux_task4.loss_heatmap: 0.6865, aux_task4.loss_bbox: 0.3318, aux_task5.loss_heatmap: 0.7340, aux_task5.loss_bbox: 0.3326, loss: 18.2704, grad_norm: 70.2045
2025-05-07 23:02:41,232 - mmdet - INFO - Epoch [1][5750/30895]	lr: 1.265e-05, eta: 133 days, 18:56:53, time: 10.659, data_time: 7.823, memory: 8990, loss_cls: 0.6653, loss_bbox: 1.0418, d0.loss_cls: 0.6977, d0.loss_bbox: 1.2199, d1.loss_cls: 0.6737, d1.loss_bbox: 1.1055, d2.loss_cls: 0.6684, d2.loss_bbox: 1.0766, d3.loss_cls: 0.6636, d3.loss_bbox: 1.0572, d4.loss_cls: 0.6646, d4.loss_bbox: 1.0479, aux_task0.loss_heatmap: 0.8535, aux_task0.loss_bbox: 0.3499, aux_task1.loss_heatmap: 1.0723, aux_task1.loss_bbox: 0.3706, aux_task2.loss_heatmap: 1.3197, aux_task2.loss_bbox: 0.4115, aux_task3.loss_heatmap: 0.8766, aux_task3.loss_bbox: 0.3114, aux_task4.loss_heatmap: 0.6880, aux_task4.loss_bbox: 0.3350, aux_task5.loss_heatmap: 0.7280, aux_task5.loss_bbox: 0.3361, loss: 18.2346, grad_norm: 72.7992
2025-05-07 23:16:46,767 - mmdet - INFO - Epoch [1][5800/30895]	lr: 1.265e-05, eta: 133 days, 15:47:40, time: 16.911, data_time: 15.191, memory: 8990, loss_cls: 0.6602, loss_bbox: 1.0241, d0.loss_cls: 0.6919, d0.loss_bbox: 1.2105, d1.loss_cls: 0.6696, d1.loss_bbox: 1.0956, d2.loss_cls: 0.6638, d2.loss_bbox: 1.0640, d3.loss_cls: 0.6601, d3.loss_bbox: 1.0420, d4.loss_cls: 0.6588, d4.loss_bbox: 1.0315, aux_task0.loss_heatmap: 0.8446, aux_task0.loss_bbox: 0.3315, aux_task1.loss_heatmap: 1.1024, aux_task1.loss_bbox: 0.3759, aux_task2.loss_heatmap: 1.3052, aux_task2.loss_bbox: 0.4018, aux_task3.loss_heatmap: 0.7882, aux_task3.loss_bbox: 0.3049, aux_task4.loss_heatmap: 0.6801, aux_task4.loss_bbox: 0.3319, aux_task5.loss_heatmap: 0.7163, aux_task5.loss_bbox: 0.3316, loss: 17.9865, grad_norm: 71.2378
2025-05-07 23:31:20,698 - mmdet - INFO - Epoch [1][5850/30895]	lr: 1.266e-05, eta: 133 days, 13:30:58, time: 17.479, data_time: 15.948, memory: 8990, loss_cls: 0.6714, loss_bbox: 1.0361, d0.loss_cls: 0.7072, d0.loss_bbox: 1.2207, d1.loss_cls: 0.6773, d1.loss_bbox: 1.1063, d2.loss_cls: 0.6697, d2.loss_bbox: 1.0780, d3.loss_cls: 0.6719, d3.loss_bbox: 1.0526, d4.loss_cls: 0.6711, d4.loss_bbox: 1.0426, aux_task0.loss_heatmap: 0.8543, aux_task0.loss_bbox: 0.3494, aux_task1.loss_heatmap: 1.1337, aux_task1.loss_bbox: 0.3818, aux_task2.loss_heatmap: 1.3207, aux_task2.loss_bbox: 0.4066, aux_task3.loss_heatmap: 0.8248, aux_task3.loss_bbox: 0.3054, aux_task4.loss_heatmap: 0.6855, aux_task4.loss_bbox: 0.3366, aux_task5.loss_heatmap: 0.7106, aux_task5.loss_bbox: 0.3258, loss: 18.2401, grad_norm: 72.3854
2025-05-07 23:41:29,207 - mmdet - INFO - Epoch [1][5900/30895]	lr: 1.266e-05, eta: 133 days, 3:37:25, time: 12.170, data_time: 10.406, memory: 8990, loss_cls: 0.6622, loss_bbox: 1.0499, d0.loss_cls: 0.6926, d0.loss_bbox: 1.2284, d1.loss_cls: 0.6718, d1.loss_bbox: 1.1151, d2.loss_cls: 0.6659, d2.loss_bbox: 1.0850, d3.loss_cls: 0.6647, d3.loss_bbox: 1.0635, d4.loss_cls: 0.6625, d4.loss_bbox: 1.0557, aux_task0.loss_heatmap: 0.8546, aux_task0.loss_bbox: 0.3473, aux_task1.loss_heatmap: 1.0888, aux_task1.loss_bbox: 0.3864, aux_task2.loss_heatmap: 1.2946, aux_task2.loss_bbox: 0.4141, aux_task3.loss_heatmap: 0.8681, aux_task3.loss_bbox: 0.3168, aux_task4.loss_heatmap: 0.6648, aux_task4.loss_bbox: 0.3333, aux_task5.loss_heatmap: 0.6874, aux_task5.loss_bbox: 0.3353, loss: 18.2089, grad_norm: 75.1332
2025-05-08 00:04:28,683 - mmdet - INFO - Epoch [1][5950/30895]	lr: 1.266e-05, eta: 133 days, 15:55:17, time: 27.590, data_time: 26.153, memory: 8990, loss_cls: 0.6592, loss_bbox: 1.0258, d0.loss_cls: 0.6886, d0.loss_bbox: 1.2117, d1.loss_cls: 0.6652, d1.loss_bbox: 1.0967, d2.loss_cls: 0.6606, d2.loss_bbox: 1.0668, d3.loss_cls: 0.6574, d3.loss_bbox: 1.0449, d4.loss_cls: 0.6592, d4.loss_bbox: 1.0336, aux_task0.loss_heatmap: 0.8768, aux_task0.loss_bbox: 0.3457, aux_task1.loss_heatmap: 1.0597, aux_task1.loss_bbox: 0.3743, aux_task2.loss_heatmap: 1.2650, aux_task2.loss_bbox: 0.4096, aux_task3.loss_heatmap: 0.8716, aux_task3.loss_bbox: 0.3223, aux_task4.loss_heatmap: 0.6582, aux_task4.loss_bbox: 0.3365, aux_task5.loss_heatmap: 0.6940, aux_task5.loss_bbox: 0.3247, loss: 18.0082, grad_norm: 70.3737
2025-05-08 00:13:05,915 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-08 00:13:05,916 - mmdet - INFO - Epoch [1][6000/30895]	lr: 1.266e-05, eta: 133 days, 3:34:54, time: 10.345, data_time: 7.812, memory: 8990, loss_cls: 0.6470, loss_bbox: 1.0140, d0.loss_cls: 0.6775, d0.loss_bbox: 1.1982, d1.loss_cls: 0.6519, d1.loss_bbox: 1.0835, d2.loss_cls: 0.6457, d2.loss_bbox: 1.0550, d3.loss_cls: 0.6442, d3.loss_bbox: 1.0318, d4.loss_cls: 0.6436, d4.loss_bbox: 1.0222, aux_task0.loss_heatmap: 0.8493, aux_task0.loss_bbox: 0.3331, aux_task1.loss_heatmap: 1.0746, aux_task1.loss_bbox: 0.3749, aux_task2.loss_heatmap: 1.3191, aux_task2.loss_bbox: 0.4102, aux_task3.loss_heatmap: 0.8473, aux_task3.loss_bbox: 0.3106, aux_task4.loss_heatmap: 0.6740, aux_task4.loss_bbox: 0.3319, aux_task5.loss_heatmap: 0.6838, aux_task5.loss_bbox: 0.3279, loss: 17.8510, grad_norm: 72.1116
2025-05-08 00:25:49,149 - mmdet - INFO - Epoch [1][6050/30895]	lr: 1.267e-05, eta: 132 days, 22:21:14, time: 15.265, data_time: 13.837, memory: 8990, loss_cls: 0.6498, loss_bbox: 1.0220, d0.loss_cls: 0.6858, d0.loss_bbox: 1.2094, d1.loss_cls: 0.6598, d1.loss_bbox: 1.0970, d2.loss_cls: 0.6558, d2.loss_bbox: 1.0650, d3.loss_cls: 0.6502, d3.loss_bbox: 1.0400, d4.loss_cls: 0.6502, d4.loss_bbox: 1.0299, aux_task0.loss_heatmap: 0.8737, aux_task0.loss_bbox: 0.3457, aux_task1.loss_heatmap: 1.1131, aux_task1.loss_bbox: 0.3728, aux_task2.loss_heatmap: 1.3348, aux_task2.loss_bbox: 0.3998, aux_task3.loss_heatmap: 0.8461, aux_task3.loss_bbox: 0.3181, aux_task4.loss_heatmap: 0.6654, aux_task4.loss_bbox: 0.3302, aux_task5.loss_heatmap: 0.7011, aux_task5.loss_bbox: 0.3334, loss: 18.0491, grad_norm: 72.2277
2025-05-08 00:39:34,746 - mmdet - INFO - Epoch [1][6100/30895]	lr: 1.267e-05, eta: 132 days, 18:56:45, time: 16.512, data_time: 15.059, memory: 8990, loss_cls: 0.6611, loss_bbox: 1.0312, d0.loss_cls: 0.6925, d0.loss_bbox: 1.2255, d1.loss_cls: 0.6685, d1.loss_bbox: 1.1060, d2.loss_cls: 0.6615, d2.loss_bbox: 1.0752, d3.loss_cls: 0.6598, d3.loss_bbox: 1.0503, d4.loss_cls: 0.6598, d4.loss_bbox: 1.0408, aux_task0.loss_heatmap: 0.8730, aux_task0.loss_bbox: 0.3413, aux_task1.loss_heatmap: 1.0871, aux_task1.loss_bbox: 0.3719, aux_task2.loss_heatmap: 1.2947, aux_task2.loss_bbox: 0.4074, aux_task3.loss_heatmap: 0.8612, aux_task3.loss_bbox: 0.3181, aux_task4.loss_heatmap: 0.6684, aux_task4.loss_bbox: 0.3348, aux_task5.loss_heatmap: 0.6810, aux_task5.loss_bbox: 0.3315, loss: 18.1025, grad_norm: 71.9738
2025-05-08 00:50:15,103 - mmdet - INFO - Epoch [1][6150/30895]	lr: 1.267e-05, eta: 132 days, 10:28:15, time: 12.807, data_time: 9.232, memory: 8990, loss_cls: 0.6523, loss_bbox: 1.0063, d0.loss_cls: 0.6864, d0.loss_bbox: 1.1840, d1.loss_cls: 0.6615, d1.loss_bbox: 1.0721, d2.loss_cls: 0.6550, d2.loss_bbox: 1.0428, d3.loss_cls: 0.6522, d3.loss_bbox: 1.0219, d4.loss_cls: 0.6528, d4.loss_bbox: 1.0130, aux_task0.loss_heatmap: 0.8704, aux_task0.loss_bbox: 0.3475, aux_task1.loss_heatmap: 1.0634, aux_task1.loss_bbox: 0.3722, aux_task2.loss_heatmap: 1.3008, aux_task2.loss_bbox: 0.3971, aux_task3.loss_heatmap: 0.8515, aux_task3.loss_bbox: 0.3117, aux_task4.loss_heatmap: 0.6421, aux_task4.loss_bbox: 0.3337, aux_task5.loss_heatmap: 0.6904, aux_task5.loss_bbox: 0.3352, loss: 17.8163, grad_norm: 75.2333
2025-05-08 01:02:52,268 - mmdet - INFO - Epoch [1][6200/30895]	lr: 1.267e-05, eta: 132 days, 5:19:52, time: 15.143, data_time: 13.354, memory: 8990, loss_cls: 0.6646, loss_bbox: 1.0238, d0.loss_cls: 0.6953, d0.loss_bbox: 1.2052, d1.loss_cls: 0.6700, d1.loss_bbox: 1.0941, d2.loss_cls: 0.6664, d2.loss_bbox: 1.0645, d3.loss_cls: 0.6658, d3.loss_bbox: 1.0389, d4.loss_cls: 0.6638, d4.loss_bbox: 1.0299, aux_task0.loss_heatmap: 0.8686, aux_task0.loss_bbox: 0.3415, aux_task1.loss_heatmap: 1.0721, aux_task1.loss_bbox: 0.3740, aux_task2.loss_heatmap: 1.2522, aux_task2.loss_bbox: 0.4081, aux_task3.loss_heatmap: 0.8627, aux_task3.loss_bbox: 0.3165, aux_task4.loss_heatmap: 0.6740, aux_task4.loss_bbox: 0.3332, aux_task5.loss_heatmap: 0.7070, aux_task5.loss_bbox: 0.3285, loss: 18.0207, grad_norm: 70.5420
2025-05-08 01:13:41,505 - mmdet - INFO - Epoch [1][6250/30895]	lr: 1.268e-05, eta: 131 days, 21:20:11, time: 12.985, data_time: 11.522, memory: 8990, loss_cls: 0.6454, loss_bbox: 1.0135, d0.loss_cls: 0.6778, d0.loss_bbox: 1.2041, d1.loss_cls: 0.6527, d1.loss_bbox: 1.0849, d2.loss_cls: 0.6468, d2.loss_bbox: 1.0561, d3.loss_cls: 0.6440, d3.loss_bbox: 1.0316, d4.loss_cls: 0.6445, d4.loss_bbox: 1.0227, aux_task0.loss_heatmap: 0.8386, aux_task0.loss_bbox: 0.3452, aux_task1.loss_heatmap: 1.0653, aux_task1.loss_bbox: 0.3735, aux_task2.loss_heatmap: 1.2454, aux_task2.loss_bbox: 0.4036, aux_task3.loss_heatmap: 0.8721, aux_task3.loss_bbox: 0.3156, aux_task4.loss_heatmap: 0.6767, aux_task4.loss_bbox: 0.3335, aux_task5.loss_heatmap: 0.6834, aux_task5.loss_bbox: 0.3279, loss: 17.8048, grad_norm: 75.9286
2025-05-08 01:22:17,646 - mmdet - INFO - Epoch [1][6300/30895]	lr: 1.268e-05, eta: 131 days, 9:52:35, time: 10.323, data_time: 8.874, memory: 8990, loss_cls: 0.6417, loss_bbox: 1.0119, d0.loss_cls: 0.6736, d0.loss_bbox: 1.1996, d1.loss_cls: 0.6503, d1.loss_bbox: 1.0846, d2.loss_cls: 0.6439, d2.loss_bbox: 1.0538, d3.loss_cls: 0.6405, d3.loss_bbox: 1.0301, d4.loss_cls: 0.6391, d4.loss_bbox: 1.0207, aux_task0.loss_heatmap: 0.8491, aux_task0.loss_bbox: 0.3437, aux_task1.loss_heatmap: 1.0513, aux_task1.loss_bbox: 0.3767, aux_task2.loss_heatmap: 1.2379, aux_task2.loss_bbox: 0.3976, aux_task3.loss_heatmap: 0.8773, aux_task3.loss_bbox: 0.3069, aux_task4.loss_heatmap: 0.6694, aux_task4.loss_bbox: 0.3341, aux_task5.loss_heatmap: 0.6974, aux_task5.loss_bbox: 0.3221, loss: 17.7535, grad_norm: 81.7552
2025-05-08 01:31:19,086 - mmdet - INFO - Epoch [1][6350/30895]	lr: 1.268e-05, eta: 130 days, 23:16:17, time: 10.829, data_time: 8.499, memory: 8990, loss_cls: 0.6423, loss_bbox: 1.0232, d0.loss_cls: 0.6786, d0.loss_bbox: 1.2117, d1.loss_cls: 0.6522, d1.loss_bbox: 1.0996, d2.loss_cls: 0.6460, d2.loss_bbox: 1.0702, d3.loss_cls: 0.6436, d3.loss_bbox: 1.0415, d4.loss_cls: 0.6431, d4.loss_bbox: 1.0324, aux_task0.loss_heatmap: 0.8269, aux_task0.loss_bbox: 0.3423, aux_task1.loss_heatmap: 1.0670, aux_task1.loss_bbox: 0.3708, aux_task2.loss_heatmap: 1.2543, aux_task2.loss_bbox: 0.3902, aux_task3.loss_heatmap: 0.8541, aux_task3.loss_bbox: 0.3071, aux_task4.loss_heatmap: 0.6746, aux_task4.loss_bbox: 0.3336, aux_task5.loss_heatmap: 0.6839, aux_task5.loss_bbox: 0.3249, loss: 17.8143, grad_norm: 70.9104
2025-05-08 01:38:25,964 - mmdet - INFO - Epoch [1][6400/30895]	lr: 1.269e-05, eta: 130 days, 9:47:22, time: 8.538, data_time: 4.306, memory: 8990, loss_cls: 0.6360, loss_bbox: 1.0074, d0.loss_cls: 0.6709, d0.loss_bbox: 1.1911, d1.loss_cls: 0.6483, d1.loss_bbox: 1.0793, d2.loss_cls: 0.6400, d2.loss_bbox: 1.0515, d3.loss_cls: 0.6362, d3.loss_bbox: 1.0267, d4.loss_cls: 0.6342, d4.loss_bbox: 1.0174, aux_task0.loss_heatmap: 0.8379, aux_task0.loss_bbox: 0.3447, aux_task1.loss_heatmap: 1.0650, aux_task1.loss_bbox: 0.3777, aux_task2.loss_heatmap: 1.2648, aux_task2.loss_bbox: 0.3916, aux_task3.loss_heatmap: 0.8101, aux_task3.loss_bbox: 0.3038, aux_task4.loss_heatmap: 0.6576, aux_task4.loss_bbox: 0.3346, aux_task5.loss_heatmap: 0.6469, aux_task5.loss_bbox: 0.3274, loss: 17.6010, grad_norm: 69.5495
2025-05-08 01:49:08,081 - mmdet - INFO - Epoch [1][6450/30895]	lr: 1.269e-05, eta: 130 days, 2:10:56, time: 12.842, data_time: 11.111, memory: 8990, loss_cls: 0.6488, loss_bbox: 1.0019, d0.loss_cls: 0.6744, d0.loss_bbox: 1.1916, d1.loss_cls: 0.6525, d1.loss_bbox: 1.0781, d2.loss_cls: 0.6466, d2.loss_bbox: 1.0502, d3.loss_cls: 0.6464, d3.loss_bbox: 1.0239, d4.loss_cls: 0.6472, d4.loss_bbox: 1.0105, aux_task0.loss_heatmap: 0.8588, aux_task0.loss_bbox: 0.3394, aux_task1.loss_heatmap: 1.0732, aux_task1.loss_bbox: 0.3839, aux_task2.loss_heatmap: 1.2487, aux_task2.loss_bbox: 0.3988, aux_task3.loss_heatmap: 0.8054, aux_task3.loss_bbox: 0.3103, aux_task4.loss_heatmap: 0.6512, aux_task4.loss_bbox: 0.3348, aux_task5.loss_heatmap: 0.6967, aux_task5.loss_bbox: 0.3266, loss: 17.7001, grad_norm: 70.3431
2025-05-08 02:00:53,088 - mmdet - INFO - Epoch [1][6500/30895]	lr: 1.269e-05, eta: 129 days, 20:19:59, time: 14.100, data_time: 12.673, memory: 8990, loss_cls: 0.6401, loss_bbox: 1.0098, d0.loss_cls: 0.6722, d0.loss_bbox: 1.1896, d1.loss_cls: 0.6480, d1.loss_bbox: 1.0797, d2.loss_cls: 0.6409, d2.loss_bbox: 1.0519, d3.loss_cls: 0.6380, d3.loss_bbox: 1.0269, d4.loss_cls: 0.6378, d4.loss_bbox: 1.0158, aux_task0.loss_heatmap: 0.8041, aux_task0.loss_bbox: 0.3411, aux_task1.loss_heatmap: 1.0714, aux_task1.loss_bbox: 0.3824, aux_task2.loss_heatmap: 1.2400, aux_task2.loss_bbox: 0.4202, aux_task3.loss_heatmap: 0.8303, aux_task3.loss_bbox: 0.3100, aux_task4.loss_heatmap: 0.6534, aux_task4.loss_bbox: 0.3337, aux_task5.loss_heatmap: 0.6628, aux_task5.loss_bbox: 0.3315, loss: 17.6316, grad_norm: 73.8477
2025-05-08 02:09:33,971 - mmdet - INFO - Epoch [1][6550/30895]	lr: 1.269e-05, eta: 129 days, 9:47:44, time: 10.417, data_time: 8.267, memory: 8990, loss_cls: 0.6341, loss_bbox: 0.9945, d0.loss_cls: 0.6715, d0.loss_bbox: 1.1787, d1.loss_cls: 0.6479, d1.loss_bbox: 1.0634, d2.loss_cls: 0.6386, d2.loss_bbox: 1.0383, d3.loss_cls: 0.6351, d3.loss_bbox: 1.0105, d4.loss_cls: 0.6336, d4.loss_bbox: 1.0003, aux_task0.loss_heatmap: 0.8399, aux_task0.loss_bbox: 0.3377, aux_task1.loss_heatmap: 1.0435, aux_task1.loss_bbox: 0.3659, aux_task2.loss_heatmap: 1.2600, aux_task2.loss_bbox: 0.4145, aux_task3.loss_heatmap: 0.7924, aux_task3.loss_bbox: 0.3045, aux_task4.loss_heatmap: 0.6513, aux_task4.loss_bbox: 0.3360, aux_task5.loss_heatmap: 0.6835, aux_task5.loss_bbox: 0.3316, loss: 17.5075, grad_norm: 73.6346
2025-05-08 02:17:38,239 - mmdet - INFO - Epoch [1][6600/30895]	lr: 1.270e-05, eta: 128 days, 22:28:28, time: 9.686, data_time: 7.533, memory: 8990, loss_cls: 0.6381, loss_bbox: 0.9909, d0.loss_cls: 0.6720, d0.loss_bbox: 1.1742, d1.loss_cls: 0.6495, d1.loss_bbox: 1.0570, d2.loss_cls: 0.6416, d2.loss_bbox: 1.0310, d3.loss_cls: 0.6397, d3.loss_bbox: 1.0045, d4.loss_cls: 0.6388, d4.loss_bbox: 0.9953, aux_task0.loss_heatmap: 0.8292, aux_task0.loss_bbox: 0.3315, aux_task1.loss_heatmap: 1.0277, aux_task1.loss_bbox: 0.3736, aux_task2.loss_heatmap: 1.2569, aux_task2.loss_bbox: 0.4043, aux_task3.loss_heatmap: 0.7969, aux_task3.loss_bbox: 0.3026, aux_task4.loss_heatmap: 0.6383, aux_task4.loss_bbox: 0.3261, aux_task5.loss_heatmap: 0.6850, aux_task5.loss_bbox: 0.3307, loss: 17.4353, grad_norm: 75.7030
2025-05-08 02:24:52,534 - mmdet - INFO - Epoch [1][6650/30895]	lr: 1.270e-05, eta: 128 days, 10:02:40, time: 8.685, data_time: 2.532, memory: 8990, loss_cls: 0.6373, loss_bbox: 1.0099, d0.loss_cls: 0.6663, d0.loss_bbox: 1.2038, d1.loss_cls: 0.6447, d1.loss_bbox: 1.0853, d2.loss_cls: 0.6388, d2.loss_bbox: 1.0551, d3.loss_cls: 0.6367, d3.loss_bbox: 1.0276, d4.loss_cls: 0.6357, d4.loss_bbox: 1.0163, aux_task0.loss_heatmap: 0.8201, aux_task0.loss_bbox: 0.3359, aux_task1.loss_heatmap: 1.0402, aux_task1.loss_bbox: 0.3691, aux_task2.loss_heatmap: 1.2416, aux_task2.loss_bbox: 0.4024, aux_task3.loss_heatmap: 0.8290, aux_task3.loss_bbox: 0.3065, aux_task4.loss_heatmap: 0.6592, aux_task4.loss_bbox: 0.3318, aux_task5.loss_heatmap: 0.6609, aux_task5.loss_bbox: 0.3226, loss: 17.5769, grad_norm: 69.9280
2025-05-08 02:36:09,784 - mmdet - INFO - Epoch [1][6700/30895]	lr: 1.270e-05, eta: 128 days, 3:57:19, time: 13.545, data_time: 0.477, memory: 8990, loss_cls: 0.6377, loss_bbox: 1.0015, d0.loss_cls: 0.6728, d0.loss_bbox: 1.1893, d1.loss_cls: 0.6479, d1.loss_bbox: 1.0791, d2.loss_cls: 0.6411, d2.loss_bbox: 1.0483, d3.loss_cls: 0.6375, d3.loss_bbox: 1.0204, d4.loss_cls: 0.6370, d4.loss_bbox: 1.0088, aux_task0.loss_heatmap: 0.8582, aux_task0.loss_bbox: 0.3436, aux_task1.loss_heatmap: 1.0812, aux_task1.loss_bbox: 0.3724, aux_task2.loss_heatmap: 1.2410, aux_task2.loss_bbox: 0.4044, aux_task3.loss_heatmap: 0.8619, aux_task3.loss_bbox: 0.3158, aux_task4.loss_heatmap: 0.6310, aux_task4.loss_bbox: 0.3294, aux_task5.loss_heatmap: 0.6475, aux_task5.loss_bbox: 0.3234, loss: 17.6313, grad_norm: 83.9420
2025-05-08 02:44:18,673 - mmdet - INFO - Epoch [1][6750/30895]	lr: 1.271e-05, eta: 127 days, 17:13:01, time: 9.778, data_time: 0.099, memory: 8990, loss_cls: 0.6346, loss_bbox: 1.0014, d0.loss_cls: 0.6683, d0.loss_bbox: 1.1849, d1.loss_cls: 0.6444, d1.loss_bbox: 1.0720, d2.loss_cls: 0.6365, d2.loss_bbox: 1.0438, d3.loss_cls: 0.6337, d3.loss_bbox: 1.0184, d4.loss_cls: 0.6330, d4.loss_bbox: 1.0078, aux_task0.loss_heatmap: 0.8445, aux_task0.loss_bbox: 0.3440, aux_task1.loss_heatmap: 1.0607, aux_task1.loss_bbox: 0.3761, aux_task2.loss_heatmap: 1.1793, aux_task2.loss_bbox: 0.4012, aux_task3.loss_heatmap: 0.8413, aux_task3.loss_bbox: 0.3096, aux_task4.loss_heatmap: 0.6523, aux_task4.loss_bbox: 0.3374, aux_task5.loss_heatmap: 0.6843, aux_task5.loss_bbox: 0.3236, loss: 17.5333, grad_norm: 71.7819
2025-05-08 02:55:02,693 - mmdet - INFO - Epoch [1][6800/30895]	lr: 1.271e-05, eta: 127 days, 10:30:22, time: 12.880, data_time: 8.260, memory: 8990, loss_cls: 0.6361, loss_bbox: 1.0129, d0.loss_cls: 0.6722, d0.loss_bbox: 1.1976, d1.loss_cls: 0.6454, d1.loss_bbox: 1.0858, d2.loss_cls: 0.6411, d2.loss_bbox: 1.0563, d3.loss_cls: 0.6350, d3.loss_bbox: 1.0331, d4.loss_cls: 0.6344, d4.loss_bbox: 1.0210, aux_task0.loss_heatmap: 0.8428, aux_task0.loss_bbox: 0.3352, aux_task1.loss_heatmap: 1.0907, aux_task1.loss_bbox: 0.3736, aux_task2.loss_heatmap: 1.1903, aux_task2.loss_bbox: 0.3940, aux_task3.loss_heatmap: 0.8080, aux_task3.loss_bbox: 0.3144, aux_task4.loss_heatmap: 0.6501, aux_task4.loss_bbox: 0.3334, aux_task5.loss_heatmap: 0.6640, aux_task5.loss_bbox: 0.3281, loss: 17.5952, grad_norm: 72.7880
2025-05-08 03:03:20,165 - mmdet - INFO - Epoch [1][6850/30895]	lr: 1.271e-05, eta: 127 days, 0:15:36, time: 9.950, data_time: 5.400, memory: 8990, loss_cls: 0.6391, loss_bbox: 0.9890, d0.loss_cls: 0.6762, d0.loss_bbox: 1.1697, d1.loss_cls: 0.6500, d1.loss_bbox: 1.0561, d2.loss_cls: 0.6421, d2.loss_bbox: 1.0287, d3.loss_cls: 0.6395, d3.loss_bbox: 1.0029, d4.loss_cls: 0.6379, d4.loss_bbox: 0.9930, aux_task0.loss_heatmap: 0.8353, aux_task0.loss_bbox: 0.3474, aux_task1.loss_heatmap: 1.0804, aux_task1.loss_bbox: 0.3698, aux_task2.loss_heatmap: 1.1488, aux_task2.loss_bbox: 0.3900, aux_task3.loss_heatmap: 0.8648, aux_task3.loss_bbox: 0.3128, aux_task4.loss_heatmap: 0.6457, aux_task4.loss_bbox: 0.3351, aux_task5.loss_heatmap: 0.6833, aux_task5.loss_bbox: 0.3256, loss: 17.4631, grad_norm: 72.6496
2025-05-08 03:14:56,525 - mmdet - INFO - Epoch [1][6900/30895]	lr: 1.272e-05, eta: 126 days, 19:03:08, time: 13.927, data_time: 12.313, memory: 8990, loss_cls: 0.6310, loss_bbox: 0.9832, d0.loss_cls: 0.6680, d0.loss_bbox: 1.1693, d1.loss_cls: 0.6405, d1.loss_bbox: 1.0553, d2.loss_cls: 0.6357, d2.loss_bbox: 1.0258, d3.loss_cls: 0.6327, d3.loss_bbox: 1.0000, d4.loss_cls: 0.6301, d4.loss_bbox: 0.9897, aux_task0.loss_heatmap: 0.8044, aux_task0.loss_bbox: 0.3430, aux_task1.loss_heatmap: 1.0680, aux_task1.loss_bbox: 0.3714, aux_task2.loss_heatmap: 1.1631, aux_task2.loss_bbox: 0.3985, aux_task3.loss_heatmap: 0.8205, aux_task3.loss_bbox: 0.3111, aux_task4.loss_heatmap: 0.6313, aux_task4.loss_bbox: 0.3286, aux_task5.loss_heatmap: 0.6932, aux_task5.loss_bbox: 0.3260, loss: 17.3206, grad_norm: 71.1255
2025-05-08 03:26:12,310 - mmdet - INFO - Epoch [1][6950/30895]	lr: 1.272e-05, eta: 126 days, 13:24:50, time: 13.516, data_time: 11.820, memory: 8990, loss_cls: 0.6415, loss_bbox: 0.9930, d0.loss_cls: 0.6737, d0.loss_bbox: 1.1737, d1.loss_cls: 0.6505, d1.loss_bbox: 1.0604, d2.loss_cls: 0.6432, d2.loss_bbox: 1.0320, d3.loss_cls: 0.6410, d3.loss_bbox: 1.0091, d4.loss_cls: 0.6407, d4.loss_bbox: 0.9993, aux_task0.loss_heatmap: 0.8595, aux_task0.loss_bbox: 0.3372, aux_task1.loss_heatmap: 1.0621, aux_task1.loss_bbox: 0.3688, aux_task2.loss_heatmap: 1.1854, aux_task2.loss_bbox: 0.3926, aux_task3.loss_heatmap: 0.8372, aux_task3.loss_bbox: 0.3081, aux_task4.loss_heatmap: 0.6300, aux_task4.loss_bbox: 0.3297, aux_task5.loss_heatmap: 0.6751, aux_task5.loss_bbox: 0.3204, loss: 17.4645, grad_norm: 71.5801
2025-05-08 03:34:26,094 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-08 03:34:26,095 - mmdet - INFO - Epoch [1][7000/30895]	lr: 1.272e-05, eta: 126 days, 3:26:30, time: 9.876, data_time: 5.299, memory: 8990, loss_cls: 0.6327, loss_bbox: 1.0126, d0.loss_cls: 0.6670, d0.loss_bbox: 1.1998, d1.loss_cls: 0.6410, d1.loss_bbox: 1.0847, d2.loss_cls: 0.6359, d2.loss_bbox: 1.0551, d3.loss_cls: 0.6346, d3.loss_bbox: 1.0306, d4.loss_cls: 0.6312, d4.loss_bbox: 1.0203, aux_task0.loss_heatmap: 0.8851, aux_task0.loss_bbox: 0.3448, aux_task1.loss_heatmap: 1.0241, aux_task1.loss_bbox: 0.3680, aux_task2.loss_heatmap: 1.2434, aux_task2.loss_bbox: 0.4044, aux_task3.loss_heatmap: 0.8413, aux_task3.loss_bbox: 0.3136, aux_task4.loss_heatmap: 0.6410, aux_task4.loss_bbox: 0.3328, aux_task5.loss_heatmap: 0.6925, aux_task5.loss_bbox: 0.3273, loss: 17.6640, grad_norm: 68.2972
2025-05-08 03:40:32,838 - mmdet - INFO - Epoch [1][7050/30895]	lr: 1.273e-05, eta: 125 days, 14:33:04, time: 7.335, data_time: 4.736, memory: 8990, loss_cls: 0.6141, loss_bbox: 0.9904, d0.loss_cls: 0.6513, d0.loss_bbox: 1.1753, d1.loss_cls: 0.6263, d1.loss_bbox: 1.0585, d2.loss_cls: 0.6214, d2.loss_bbox: 1.0292, d3.loss_cls: 0.6178, d3.loss_bbox: 1.0061, d4.loss_cls: 0.6155, d4.loss_bbox: 0.9966, aux_task0.loss_heatmap: 0.8407, aux_task0.loss_bbox: 0.3370, aux_task1.loss_heatmap: 1.0208, aux_task1.loss_bbox: 0.3697, aux_task2.loss_heatmap: 1.2069, aux_task2.loss_bbox: 0.3960, aux_task3.loss_heatmap: 0.8687, aux_task3.loss_bbox: 0.3158, aux_task4.loss_heatmap: 0.6381, aux_task4.loss_bbox: 0.3265, aux_task5.loss_heatmap: 0.6474, aux_task5.loss_bbox: 0.3208, loss: 17.2909, grad_norm: 69.9863
2025-05-08 03:50:43,072 - mmdet - INFO - Epoch [1][7100/30895]	lr: 1.273e-05, eta: 125 days, 7:39:35, time: 12.205, data_time: 7.733, memory: 8990, loss_cls: 0.6305, loss_bbox: 1.0089, d0.loss_cls: 0.6670, d0.loss_bbox: 1.1945, d1.loss_cls: 0.6431, d1.loss_bbox: 1.0805, d2.loss_cls: 0.6352, d2.loss_bbox: 1.0481, d3.loss_cls: 0.6307, d3.loss_bbox: 1.0246, d4.loss_cls: 0.6293, d4.loss_bbox: 1.0151, aux_task0.loss_heatmap: 0.8405, aux_task0.loss_bbox: 0.3416, aux_task1.loss_heatmap: 1.0486, aux_task1.loss_bbox: 0.3730, aux_task2.loss_heatmap: 1.2307, aux_task2.loss_bbox: 0.4082, aux_task3.loss_heatmap: 0.7817, aux_task3.loss_bbox: 0.3125, aux_task4.loss_heatmap: 0.6406, aux_task4.loss_bbox: 0.3297, aux_task5.loss_heatmap: 0.6658, aux_task5.loss_bbox: 0.3246, loss: 17.5052, grad_norm: 70.6266
2025-05-08 03:57:00,394 - mmdet - INFO - Epoch [1][7150/30895]	lr: 1.273e-05, eta: 124 days, 19:20:08, time: 7.546, data_time: 1.385, memory: 8990, loss_cls: 0.6195, loss_bbox: 0.9843, d0.loss_cls: 0.6575, d0.loss_bbox: 1.1787, d1.loss_cls: 0.6330, d1.loss_bbox: 1.0577, d2.loss_cls: 0.6232, d2.loss_bbox: 1.0276, d3.loss_cls: 0.6218, d3.loss_bbox: 1.0004, d4.loss_cls: 0.6199, d4.loss_bbox: 0.9901, aux_task0.loss_heatmap: 0.8347, aux_task0.loss_bbox: 0.3445, aux_task1.loss_heatmap: 1.0287, aux_task1.loss_bbox: 0.3714, aux_task2.loss_heatmap: 1.2230, aux_task2.loss_bbox: 0.4073, aux_task3.loss_heatmap: 0.7512, aux_task3.loss_bbox: 0.2984, aux_task4.loss_heatmap: 0.6302, aux_task4.loss_bbox: 0.3328, aux_task5.loss_heatmap: 0.6599, aux_task5.loss_bbox: 0.3250, loss: 17.2206, grad_norm: 75.8958
2025-05-08 04:05:40,351 - mmdet - INFO - Epoch [1][7200/30895]	lr: 1.274e-05, eta: 124 days, 10:32:29, time: 10.399, data_time: 0.005, memory: 8990, loss_cls: 0.6262, loss_bbox: 1.0071, d0.loss_cls: 0.6669, d0.loss_bbox: 1.1845, d1.loss_cls: 0.6403, d1.loss_bbox: 1.0720, d2.loss_cls: 0.6322, d2.loss_bbox: 1.0428, d3.loss_cls: 0.6285, d3.loss_bbox: 1.0211, d4.loss_cls: 0.6272, d4.loss_bbox: 1.0114, aux_task0.loss_heatmap: 0.8479, aux_task0.loss_bbox: 0.3430, aux_task1.loss_heatmap: 1.0707, aux_task1.loss_bbox: 0.3747, aux_task2.loss_heatmap: 1.2348, aux_task2.loss_bbox: 0.4018, aux_task3.loss_heatmap: 0.8402, aux_task3.loss_bbox: 0.3045, aux_task4.loss_heatmap: 0.6448, aux_task4.loss_bbox: 0.3330, aux_task5.loss_heatmap: 0.6547, aux_task5.loss_bbox: 0.3236, loss: 17.5338, grad_norm: 70.8703
2025-05-08 04:10:43,888 - mmdet - INFO - Epoch [1][7250/30895]	lr: 1.274e-05, eta: 123 days, 20:48:14, time: 6.071, data_time: 2.517, memory: 8990, loss_cls: 0.6212, loss_bbox: 0.9844, d0.loss_cls: 0.6579, d0.loss_bbox: 1.1596, d1.loss_cls: 0.6334, d1.loss_bbox: 1.0484, d2.loss_cls: 0.6260, d2.loss_bbox: 1.0200, d3.loss_cls: 0.6209, d3.loss_bbox: 0.9988, d4.loss_cls: 0.6207, d4.loss_bbox: 0.9892, aux_task0.loss_heatmap: 0.8415, aux_task0.loss_bbox: 0.3321, aux_task1.loss_heatmap: 1.0468, aux_task1.loss_bbox: 0.3727, aux_task2.loss_heatmap: 1.2069, aux_task2.loss_bbox: 0.3982, aux_task3.loss_heatmap: 0.8089, aux_task3.loss_bbox: 0.3040, aux_task4.loss_heatmap: 0.6292, aux_task4.loss_bbox: 0.3314, aux_task5.loss_heatmap: 0.6320, aux_task5.loss_bbox: 0.3258, loss: 17.2104, grad_norm: 72.4776
2025-05-08 04:18:15,125 - mmdet - INFO - Epoch [1][7300/30895]	lr: 1.274e-05, eta: 123 days, 10:41:05, time: 9.025, data_time: 3.367, memory: 8990, loss_cls: 0.6245, loss_bbox: 0.9811, d0.loss_cls: 0.6627, d0.loss_bbox: 1.1634, d1.loss_cls: 0.6401, d1.loss_bbox: 1.0443, d2.loss_cls: 0.6310, d2.loss_bbox: 1.0150, d3.loss_cls: 0.6254, d3.loss_bbox: 0.9960, d4.loss_cls: 0.6251, d4.loss_bbox: 0.9858, aux_task0.loss_heatmap: 0.8416, aux_task0.loss_bbox: 0.3263, aux_task1.loss_heatmap: 1.0367, aux_task1.loss_bbox: 0.3683, aux_task2.loss_heatmap: 1.2074, aux_task2.loss_bbox: 0.3953, aux_task3.loss_heatmap: 0.7863, aux_task3.loss_bbox: 0.3094, aux_task4.loss_heatmap: 0.6403, aux_task4.loss_bbox: 0.3279, aux_task5.loss_heatmap: 0.6737, aux_task5.loss_bbox: 0.3262, loss: 17.2337, grad_norm: 74.8522
2025-05-08 04:22:56,534 - mmdet - INFO - Epoch [1][7350/30895]	lr: 1.275e-05, eta: 122 days, 20:46:58, time: 5.628, data_time: 0.880, memory: 8990, loss_cls: 0.6205, loss_bbox: 0.9800, d0.loss_cls: 0.6583, d0.loss_bbox: 1.1575, d1.loss_cls: 0.6362, d1.loss_bbox: 1.0400, d2.loss_cls: 0.6283, d2.loss_bbox: 1.0098, d3.loss_cls: 0.6229, d3.loss_bbox: 0.9926, d4.loss_cls: 0.6213, d4.loss_bbox: 0.9838, aux_task0.loss_heatmap: 0.7935, aux_task0.loss_bbox: 0.3323, aux_task1.loss_heatmap: 1.0232, aux_task1.loss_bbox: 0.3701, aux_task2.loss_heatmap: 1.1711, aux_task2.loss_bbox: 0.3992, aux_task3.loss_heatmap: 0.8532, aux_task3.loss_bbox: 0.3132, aux_task4.loss_heatmap: 0.6315, aux_task4.loss_bbox: 0.3268, aux_task5.loss_heatmap: 0.6744, aux_task5.loss_bbox: 0.3245, loss: 17.1644, grad_norm: 72.7557
2025-05-08 04:30:03,700 - mmdet - INFO - Epoch [1][7400/30895]	lr: 1.275e-05, eta: 122 days, 10:24:28, time: 8.543, data_time: 3.631, memory: 8990, loss_cls: 0.6208, loss_bbox: 0.9721, d0.loss_cls: 0.6594, d0.loss_bbox: 1.1596, d1.loss_cls: 0.6338, d1.loss_bbox: 1.0403, d2.loss_cls: 0.6283, d2.loss_bbox: 1.0099, d3.loss_cls: 0.6237, d3.loss_bbox: 0.9897, d4.loss_cls: 0.6223, d4.loss_bbox: 0.9776, aux_task0.loss_heatmap: 0.8104, aux_task0.loss_bbox: 0.3341, aux_task1.loss_heatmap: 1.0179, aux_task1.loss_bbox: 0.3645, aux_task2.loss_heatmap: 1.1796, aux_task2.loss_bbox: 0.3906, aux_task3.loss_heatmap: 0.7919, aux_task3.loss_bbox: 0.3061, aux_task4.loss_heatmap: 0.6396, aux_task4.loss_bbox: 0.3312, aux_task5.loss_heatmap: 0.6368, aux_task5.loss_bbox: 0.3227, loss: 17.0627, grad_norm: 69.1923
2025-05-08 04:38:54,424 - mmdet - INFO - Epoch [1][7450/30895]	lr: 1.275e-05, eta: 122 days, 2:31:39, time: 10.614, data_time: 8.501, memory: 8990, loss_cls: 0.6149, loss_bbox: 0.9676, d0.loss_cls: 0.6509, d0.loss_bbox: 1.1577, d1.loss_cls: 0.6296, d1.loss_bbox: 1.0343, d2.loss_cls: 0.6212, d2.loss_bbox: 1.0054, d3.loss_cls: 0.6196, d3.loss_bbox: 0.9827, d4.loss_cls: 0.6158, d4.loss_bbox: 0.9722, aux_task0.loss_heatmap: 0.8222, aux_task0.loss_bbox: 0.3321, aux_task1.loss_heatmap: 1.0141, aux_task1.loss_bbox: 0.3659, aux_task2.loss_heatmap: 1.1649, aux_task2.loss_bbox: 0.3961, aux_task3.loss_heatmap: 0.7971, aux_task3.loss_bbox: 0.3100, aux_task4.loss_heatmap: 0.6283, aux_task4.loss_bbox: 0.3266, aux_task5.loss_heatmap: 0.6634, aux_task5.loss_bbox: 0.3247, loss: 17.0172, grad_norm: 73.2522
2025-05-08 04:44:19,807 - mmdet - INFO - Epoch [1][7500/30895]	lr: 1.276e-05, eta: 121 days, 14:06:31, time: 6.508, data_time: 2.895, memory: 8990, loss_cls: 0.6157, loss_bbox: 0.9697, d0.loss_cls: 0.6499, d0.loss_bbox: 1.1542, d1.loss_cls: 0.6280, d1.loss_bbox: 1.0358, d2.loss_cls: 0.6203, d2.loss_bbox: 1.0071, d3.loss_cls: 0.6175, d3.loss_bbox: 0.9838, d4.loss_cls: 0.6175, d4.loss_bbox: 0.9736, aux_task0.loss_heatmap: 0.7798, aux_task0.loss_bbox: 0.3355, aux_task1.loss_heatmap: 1.0404, aux_task1.loss_bbox: 0.3654, aux_task2.loss_heatmap: 1.1693, aux_task2.loss_bbox: 0.3947, aux_task3.loss_heatmap: 0.7631, aux_task3.loss_bbox: 0.2961, aux_task4.loss_heatmap: 0.6442, aux_task4.loss_bbox: 0.3278, aux_task5.loss_heatmap: 0.6287, aux_task5.loss_bbox: 0.3198, loss: 16.9379, grad_norm: 72.6718
2025-05-08 04:49:17,247 - mmdet - INFO - Epoch [1][7550/30895]	lr: 1.276e-05, eta: 121 days, 1:13:30, time: 5.949, data_time: 0.471, memory: 8990, loss_cls: 0.6030, loss_bbox: 0.9749, d0.loss_cls: 0.6426, d0.loss_bbox: 1.1604, d1.loss_cls: 0.6179, d1.loss_bbox: 1.0436, d2.loss_cls: 0.6106, d2.loss_bbox: 1.0129, d3.loss_cls: 0.6086, d3.loss_bbox: 0.9893, d4.loss_cls: 0.6050, d4.loss_bbox: 0.9796, aux_task0.loss_heatmap: 0.8230, aux_task0.loss_bbox: 0.3401, aux_task1.loss_heatmap: 1.0177, aux_task1.loss_bbox: 0.3709, aux_task2.loss_heatmap: 1.2099, aux_task2.loss_bbox: 0.4004, aux_task3.loss_heatmap: 0.7504, aux_task3.loss_bbox: 0.3032, aux_task4.loss_heatmap: 0.6277, aux_task4.loss_bbox: 0.3293, aux_task5.loss_heatmap: 0.6347, aux_task5.loss_bbox: 0.3191, loss: 16.9748, grad_norm: 75.0105
2025-05-08 04:53:49,781 - mmdet - INFO - Epoch [1][7600/30895]	lr: 1.276e-05, eta: 120 days, 11:57:15, time: 5.451, data_time: 0.853, memory: 8990, loss_cls: 0.5942, loss_bbox: 0.9691, d0.loss_cls: 0.6355, d0.loss_bbox: 1.1490, d1.loss_cls: 0.6115, d1.loss_bbox: 1.0289, d2.loss_cls: 0.6038, d2.loss_bbox: 0.9983, d3.loss_cls: 0.5968, d3.loss_bbox: 0.9811, d4.loss_cls: 0.5963, d4.loss_bbox: 0.9720, aux_task0.loss_heatmap: 0.7814, aux_task0.loss_bbox: 0.3281, aux_task1.loss_heatmap: 1.0017, aux_task1.loss_bbox: 0.3663, aux_task2.loss_heatmap: 1.1685, aux_task2.loss_bbox: 0.3922, aux_task3.loss_heatmap: 0.7218, aux_task3.loss_bbox: 0.2994, aux_task4.loss_heatmap: 0.6192, aux_task4.loss_bbox: 0.3270, aux_task5.loss_heatmap: 0.6338, aux_task5.loss_bbox: 0.3223, loss: 16.6983, grad_norm: 75.6605
2025-05-08 04:58:46,018 - mmdet - INFO - Epoch [1][7650/30895]	lr: 1.277e-05, eta: 119 days, 23:22:53, time: 5.925, data_time: 0.531, memory: 8990, loss_cls: 0.5982, loss_bbox: 0.9800, d0.loss_cls: 0.6397, d0.loss_bbox: 1.1636, d1.loss_cls: 0.6152, d1.loss_bbox: 1.0458, d2.loss_cls: 0.6058, d2.loss_bbox: 1.0153, d3.loss_cls: 0.6030, d3.loss_bbox: 0.9940, d4.loss_cls: 0.5999, d4.loss_bbox: 0.9858, aux_task0.loss_heatmap: 0.8217, aux_task0.loss_bbox: 0.3324, aux_task1.loss_heatmap: 1.0149, aux_task1.loss_bbox: 0.3664, aux_task2.loss_heatmap: 1.1349, aux_task2.loss_bbox: 0.3923, aux_task3.loss_heatmap: 0.7820, aux_task3.loss_bbox: 0.3126, aux_task4.loss_heatmap: 0.6413, aux_task4.loss_bbox: 0.3270, aux_task5.loss_heatmap: 0.6321, aux_task5.loss_bbox: 0.3175, loss: 16.9214, grad_norm: 72.2810
2025-05-08 05:03:58,364 - mmdet - INFO - Epoch [1][7700/30895]	lr: 1.277e-05, eta: 119 days, 11:19:33, time: 6.247, data_time: 3.478, memory: 8990, loss_cls: 0.6052, loss_bbox: 0.9762, d0.loss_cls: 0.6379, d0.loss_bbox: 1.1620, d1.loss_cls: 0.6157, d1.loss_bbox: 1.0431, d2.loss_cls: 0.6087, d2.loss_bbox: 1.0134, d3.loss_cls: 0.6043, d3.loss_bbox: 0.9937, d4.loss_cls: 0.6041, d4.loss_bbox: 0.9825, aux_task0.loss_heatmap: 0.7971, aux_task0.loss_bbox: 0.3296, aux_task1.loss_heatmap: 1.0167, aux_task1.loss_bbox: 0.3670, aux_task2.loss_heatmap: 1.1451, aux_task2.loss_bbox: 0.4035, aux_task3.loss_heatmap: 0.7706, aux_task3.loss_bbox: 0.3019, aux_task4.loss_heatmap: 0.6147, aux_task4.loss_bbox: 0.3269, aux_task5.loss_heatmap: 0.6553, aux_task5.loss_bbox: 0.3214, loss: 16.8966, grad_norm: 91.6553
2025-05-08 05:09:31,811 - mmdet - INFO - Epoch [1][7750/30895]	lr: 1.277e-05, eta: 118 days, 23:53:06, time: 6.668, data_time: 0.005, memory: 9064, loss_cls: 0.6066, loss_bbox: 0.9556, d0.loss_cls: 0.6437, d0.loss_bbox: 1.1421, d1.loss_cls: 0.6178, d1.loss_bbox: 1.0238, d2.loss_cls: 0.6130, d2.loss_bbox: 0.9918, d3.loss_cls: 0.6082, d3.loss_bbox: 0.9715, d4.loss_cls: 0.6069, d4.loss_bbox: 0.9618, aux_task0.loss_heatmap: 0.7516, aux_task0.loss_bbox: 0.3218, aux_task1.loss_heatmap: 1.0605, aux_task1.loss_bbox: 0.3643, aux_task2.loss_heatmap: 1.1419, aux_task2.loss_bbox: 0.3961, aux_task3.loss_heatmap: 0.7573, aux_task3.loss_bbox: 0.2999, aux_task4.loss_heatmap: 0.6363, aux_task4.loss_bbox: 0.3280, aux_task5.loss_heatmap: 0.6464, aux_task5.loss_bbox: 0.3155, loss: 16.7624, grad_norm: 70.1541
2025-05-08 05:13:33,690 - mmdet - INFO - Epoch [1][7800/30895]	lr: 1.278e-05, eta: 118 days, 10:36:05, time: 4.838, data_time: 0.006, memory: 9064, loss_cls: 0.5947, loss_bbox: 0.9780, d0.loss_cls: 0.6292, d0.loss_bbox: 1.1558, d1.loss_cls: 0.6069, d1.loss_bbox: 1.0373, d2.loss_cls: 0.5976, d2.loss_bbox: 1.0107, d3.loss_cls: 0.5944, d3.loss_bbox: 0.9929, d4.loss_cls: 0.5944, d4.loss_bbox: 0.9830, aux_task0.loss_heatmap: 0.7865, aux_task0.loss_bbox: 0.3291, aux_task1.loss_heatmap: 1.0094, aux_task1.loss_bbox: 0.3658, aux_task2.loss_heatmap: 1.1572, aux_task2.loss_bbox: 0.3925, aux_task3.loss_heatmap: 0.7197, aux_task3.loss_bbox: 0.3020, aux_task4.loss_heatmap: 0.6217, aux_task4.loss_bbox: 0.3282, aux_task5.loss_heatmap: 0.6199, aux_task5.loss_bbox: 0.3174, loss: 16.7244, grad_norm: 68.4683
2025-05-08 05:16:50,847 - mmdet - INFO - Epoch [1][7850/30895]	lr: 1.278e-05, eta: 117 days, 20:31:13, time: 3.943, data_time: 0.005, memory: 9064, loss_cls: 0.6111, loss_bbox: 0.9555, d0.loss_cls: 0.6480, d0.loss_bbox: 1.1410, d1.loss_cls: 0.6234, d1.loss_bbox: 1.0224, d2.loss_cls: 0.6157, d2.loss_bbox: 0.9927, d3.loss_cls: 0.6130, d3.loss_bbox: 0.9716, d4.loss_cls: 0.6121, d4.loss_bbox: 0.9616, aux_task0.loss_heatmap: 0.7972, aux_task0.loss_bbox: 0.3263, aux_task1.loss_heatmap: 1.0347, aux_task1.loss_bbox: 0.3605, aux_task2.loss_heatmap: 1.1665, aux_task2.loss_bbox: 0.3971, aux_task3.loss_heatmap: 0.7785, aux_task3.loss_bbox: 0.2931, aux_task4.loss_heatmap: 0.6289, aux_task4.loss_bbox: 0.3269, aux_task5.loss_heatmap: 0.6539, aux_task5.loss_bbox: 0.3231, loss: 16.8548, grad_norm: 74.2364
2025-05-08 05:19:59,551 - mmdet - INFO - Epoch [1][7900/30895]	lr: 1.278e-05, eta: 117 days, 6:26:08, time: 3.774, data_time: 0.005, memory: 9064, loss_cls: 0.6024, loss_bbox: 0.9747, d0.loss_cls: 0.6404, d0.loss_bbox: 1.1479, d1.loss_cls: 0.6168, d1.loss_bbox: 1.0347, d2.loss_cls: 0.6092, d2.loss_bbox: 1.0073, d3.loss_cls: 0.6055, d3.loss_bbox: 0.9899, d4.loss_cls: 0.6026, d4.loss_bbox: 0.9807, aux_task0.loss_heatmap: 0.8370, aux_task0.loss_bbox: 0.3351, aux_task1.loss_heatmap: 1.0146, aux_task1.loss_bbox: 0.3656, aux_task2.loss_heatmap: 1.1252, aux_task2.loss_bbox: 0.3914, aux_task3.loss_heatmap: 0.7256, aux_task3.loss_bbox: 0.3094, aux_task4.loss_heatmap: 0.6094, aux_task4.loss_bbox: 0.3282, aux_task5.loss_heatmap: 0.6626, aux_task5.loss_bbox: 0.3248, loss: 16.8410, grad_norm: 70.2235
2025-05-08 05:23:56,223 - mmdet - INFO - Epoch [1][7950/30895]	lr: 1.279e-05, eta: 116 days, 17:32:57, time: 4.733, data_time: 0.919, memory: 9064, loss_cls: 0.5991, loss_bbox: 0.9606, d0.loss_cls: 0.6381, d0.loss_bbox: 1.1411, d1.loss_cls: 0.6139, d1.loss_bbox: 1.0227, d2.loss_cls: 0.6077, d2.loss_bbox: 0.9914, d3.loss_cls: 0.6038, d3.loss_bbox: 0.9730, d4.loss_cls: 0.6006, d4.loss_bbox: 0.9656, aux_task0.loss_heatmap: 0.8137, aux_task0.loss_bbox: 0.3296, aux_task1.loss_heatmap: 1.0054, aux_task1.loss_bbox: 0.3671, aux_task2.loss_heatmap: 1.1537, aux_task2.loss_bbox: 0.3949, aux_task3.loss_heatmap: 0.8143, aux_task3.loss_bbox: 0.3134, aux_task4.loss_heatmap: 0.6334, aux_task4.loss_bbox: 0.3301, aux_task5.loss_heatmap: 0.6481, aux_task5.loss_bbox: 0.3241, loss: 16.8453, grad_norm: 76.6723
2025-05-08 05:28:56,225 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-08 05:28:56,226 - mmdet - INFO - Epoch [1][8000/30895]	lr: 1.279e-05, eta: 116 days, 6:09:52, time: 6.000, data_time: 0.549, memory: 9064, loss_cls: 0.6169, loss_bbox: 0.9802, d0.loss_cls: 0.6555, d0.loss_bbox: 1.1613, d1.loss_cls: 0.6306, d1.loss_bbox: 1.0483, d2.loss_cls: 0.6249, d2.loss_bbox: 1.0166, d3.loss_cls: 0.6189, d3.loss_bbox: 0.9984, d4.loss_cls: 0.6179, d4.loss_bbox: 0.9874, aux_task0.loss_heatmap: 0.8379, aux_task0.loss_bbox: 0.3356, aux_task1.loss_heatmap: 1.0073, aux_task1.loss_bbox: 0.3670, aux_task2.loss_heatmap: 1.1556, aux_task2.loss_bbox: 0.3899, aux_task3.loss_heatmap: 0.8418, aux_task3.loss_bbox: 0.3126, aux_task4.loss_heatmap: 0.6268, aux_task4.loss_bbox: 0.3305, aux_task5.loss_heatmap: 0.6368, aux_task5.loss_bbox: 0.3197, loss: 17.1185, grad_norm: 72.3325
2025-05-08 05:32:44,910 - mmdet - INFO - Epoch [1][8050/30895]	lr: 1.279e-05, eta: 115 days, 17:25:09, time: 4.574, data_time: 1.351, memory: 9064, loss_cls: 0.6050, loss_bbox: 0.9656, d0.loss_cls: 0.6411, d0.loss_bbox: 1.1385, d1.loss_cls: 0.6149, d1.loss_bbox: 1.0258, d2.loss_cls: 0.6053, d2.loss_bbox: 0.9998, d3.loss_cls: 0.6055, d3.loss_bbox: 0.9784, d4.loss_cls: 0.6046, d4.loss_bbox: 0.9702, aux_task0.loss_heatmap: 0.7985, aux_task0.loss_bbox: 0.3273, aux_task1.loss_heatmap: 1.0261, aux_task1.loss_bbox: 0.3645, aux_task2.loss_heatmap: 1.1794, aux_task2.loss_bbox: 0.3992, aux_task3.loss_heatmap: 0.8046, aux_task3.loss_bbox: 0.3037, aux_task4.loss_heatmap: 0.6286, aux_task4.loss_bbox: 0.3272, aux_task5.loss_heatmap: 0.6342, aux_task5.loss_bbox: 0.3216, loss: 16.8697, grad_norm: 77.1842
2025-05-08 05:36:44,812 - mmdet - INFO - Epoch [1][8100/30895]	lr: 1.280e-05, eta: 115 days, 5:03:53, time: 4.798, data_time: 1.357, memory: 9064, loss_cls: 0.5986, loss_bbox: 0.9634, d0.loss_cls: 0.6370, d0.loss_bbox: 1.1420, d1.loss_cls: 0.6108, d1.loss_bbox: 1.0267, d2.loss_cls: 0.6056, d2.loss_bbox: 0.9943, d3.loss_cls: 0.6036, d3.loss_bbox: 0.9746, d4.loss_cls: 0.6005, d4.loss_bbox: 0.9665, aux_task0.loss_heatmap: 0.7749, aux_task0.loss_bbox: 0.3289, aux_task1.loss_heatmap: 1.0112, aux_task1.loss_bbox: 0.3635, aux_task2.loss_heatmap: 1.1526, aux_task2.loss_bbox: 0.4009, aux_task3.loss_heatmap: 0.7624, aux_task3.loss_bbox: 0.3116, aux_task4.loss_heatmap: 0.6242, aux_task4.loss_bbox: 0.3264, aux_task5.loss_heatmap: 0.6858, aux_task5.loss_bbox: 0.3255, loss: 16.7915, grad_norm: 71.6863
2025-05-08 05:43:03,790 - mmdet - INFO - Epoch [1][8150/30895]	lr: 1.280e-05, eta: 114 days, 19:45:08, time: 7.580, data_time: 2.704, memory: 9064, loss_cls: 0.6132, loss_bbox: 0.9762, d0.loss_cls: 0.6461, d0.loss_bbox: 1.1483, d1.loss_cls: 0.6263, d1.loss_bbox: 1.0336, d2.loss_cls: 0.6203, d2.loss_bbox: 1.0051, d3.loss_cls: 0.6175, d3.loss_bbox: 0.9878, d4.loss_cls: 0.6153, d4.loss_bbox: 0.9810, aux_task0.loss_heatmap: 0.7932, aux_task0.loss_bbox: 0.3354, aux_task1.loss_heatmap: 1.0191, aux_task1.loss_bbox: 0.3678, aux_task2.loss_heatmap: 1.1630, aux_task2.loss_bbox: 0.3982, aux_task3.loss_heatmap: 0.8238, aux_task3.loss_bbox: 0.3147, aux_task4.loss_heatmap: 0.6459, aux_task4.loss_bbox: 0.3258, aux_task5.loss_heatmap: 0.6675, aux_task5.loss_bbox: 0.3200, loss: 17.0451, grad_norm: 74.3000
2025-05-08 05:49:12,935 - mmdet - INFO - Epoch [1][8200/30895]	lr: 1.281e-05, eta: 114 days, 10:20:55, time: 7.383, data_time: 2.292, memory: 9064, loss_cls: 0.6031, loss_bbox: 0.9578, d0.loss_cls: 0.6393, d0.loss_bbox: 1.1290, d1.loss_cls: 0.6154, d1.loss_bbox: 1.0174, d2.loss_cls: 0.6071, d2.loss_bbox: 0.9895, d3.loss_cls: 0.6050, d3.loss_bbox: 0.9716, d4.loss_cls: 0.6040, d4.loss_bbox: 0.9617, aux_task0.loss_heatmap: 0.7884, aux_task0.loss_bbox: 0.3255, aux_task1.loss_heatmap: 1.0235, aux_task1.loss_bbox: 0.3706, aux_task2.loss_heatmap: 1.1531, aux_task2.loss_bbox: 0.3884, aux_task3.loss_heatmap: 0.7678, aux_task3.loss_bbox: 0.3004, aux_task4.loss_heatmap: 0.6246, aux_task4.loss_bbox: 0.3223, aux_task5.loss_heatmap: 0.6471, aux_task5.loss_bbox: 0.3203, loss: 16.7328, grad_norm: 70.5836
2025-05-08 05:55:46,125 - mmdet - INFO - Epoch [1][8250/30895]	lr: 1.281e-05, eta: 114 days, 1:33:04, time: 7.864, data_time: 0.005, memory: 9064, loss_cls: 0.5951, loss_bbox: 0.9622, d0.loss_cls: 0.6305, d0.loss_bbox: 1.1451, d1.loss_cls: 0.6049, d1.loss_bbox: 1.0222, d2.loss_cls: 0.5993, d2.loss_bbox: 0.9920, d3.loss_cls: 0.5955, d3.loss_bbox: 0.9755, d4.loss_cls: 0.5950, d4.loss_bbox: 0.9680, aux_task0.loss_heatmap: 0.7764, aux_task0.loss_bbox: 0.3311, aux_task1.loss_heatmap: 1.0006, aux_task1.loss_bbox: 0.3617, aux_task2.loss_heatmap: 1.1561, aux_task2.loss_bbox: 0.3895, aux_task3.loss_heatmap: 0.7502, aux_task3.loss_bbox: 0.3080, aux_task4.loss_heatmap: 0.6171, aux_task4.loss_bbox: 0.3323, aux_task5.loss_heatmap: 0.6319, aux_task5.loss_bbox: 0.3178, loss: 16.6581, grad_norm: 69.6295
2025-05-08 06:05:22,048 - mmdet - INFO - Epoch [1][8300/30895]	lr: 1.281e-05, eta: 113 days, 20:35:10, time: 11.518, data_time: 0.005, memory: 9064, loss_cls: 0.5834, loss_bbox: 0.9587, d0.loss_cls: 0.6190, d0.loss_bbox: 1.1404, d1.loss_cls: 0.5948, d1.loss_bbox: 1.0217, d2.loss_cls: 0.5885, d2.loss_bbox: 0.9888, d3.loss_cls: 0.5849, d3.loss_bbox: 0.9708, d4.loss_cls: 0.5849, d4.loss_bbox: 0.9631, aux_task0.loss_heatmap: 0.8080, aux_task0.loss_bbox: 0.3311, aux_task1.loss_heatmap: 1.0203, aux_task1.loss_bbox: 0.3674, aux_task2.loss_heatmap: 1.1593, aux_task2.loss_bbox: 0.3973, aux_task3.loss_heatmap: 0.7572, aux_task3.loss_bbox: 0.3122, aux_task4.loss_heatmap: 0.5980, aux_task4.loss_bbox: 0.3301, aux_task5.loss_heatmap: 0.6102, aux_task5.loss_bbox: 0.3171, loss: 16.6071, grad_norm: 71.8262
2025-05-08 06:11:35,831 - mmdet - INFO - Epoch [1][8350/30895]	lr: 1.282e-05, eta: 113 days, 11:34:49, time: 7.476, data_time: 0.006, memory: 9064, loss_cls: 0.5997, loss_bbox: 0.9576, d0.loss_cls: 0.6345, d0.loss_bbox: 1.1377, d1.loss_cls: 0.6131, d1.loss_bbox: 1.0180, d2.loss_cls: 0.6070, d2.loss_bbox: 0.9877, d3.loss_cls: 0.6016, d3.loss_bbox: 0.9724, d4.loss_cls: 0.5996, d4.loss_bbox: 0.9629, aux_task0.loss_heatmap: 0.8030, aux_task0.loss_bbox: 0.3386, aux_task1.loss_heatmap: 0.9945, aux_task1.loss_bbox: 0.3666, aux_task2.loss_heatmap: 1.1451, aux_task2.loss_bbox: 0.3971, aux_task3.loss_heatmap: 0.7793, aux_task3.loss_bbox: 0.3045, aux_task4.loss_heatmap: 0.6225, aux_task4.loss_bbox: 0.3277, aux_task5.loss_heatmap: 0.6316, aux_task5.loss_bbox: 0.3161, loss: 16.7185, grad_norm: 73.9575
2025-05-08 06:17:56,714 - mmdet - INFO - Epoch [1][8400/30895]	lr: 1.282e-05, eta: 113 days, 2:49:25, time: 7.618, data_time: 0.006, memory: 9064, loss_cls: 0.6025, loss_bbox: 0.9649, d0.loss_cls: 0.6348, d0.loss_bbox: 1.1368, d1.loss_cls: 0.6147, d1.loss_bbox: 1.0231, d2.loss_cls: 0.6076, d2.loss_bbox: 0.9942, d3.loss_cls: 0.6035, d3.loss_bbox: 0.9785, d4.loss_cls: 0.6027, d4.loss_bbox: 0.9697, aux_task0.loss_heatmap: 0.7710, aux_task0.loss_bbox: 0.3319, aux_task1.loss_heatmap: 1.0069, aux_task1.loss_bbox: 0.3655, aux_task2.loss_heatmap: 1.1087, aux_task2.loss_bbox: 0.3976, aux_task3.loss_heatmap: 0.8433, aux_task3.loss_bbox: 0.3102, aux_task4.loss_heatmap: 0.6271, aux_task4.loss_bbox: 0.3296, aux_task5.loss_heatmap: 0.6144, aux_task5.loss_bbox: 0.3133, loss: 16.7527, grad_norm: 71.3088
2025-05-08 06:28:13,291 - mmdet - INFO - Epoch [1][8450/30895]	lr: 1.282e-05, eta: 112 days, 22:53:26, time: 12.331, data_time: 0.005, memory: 9064, loss_cls: 0.5953, loss_bbox: 0.9759, d0.loss_cls: 0.6311, d0.loss_bbox: 1.1530, d1.loss_cls: 0.6100, d1.loss_bbox: 1.0350, d2.loss_cls: 0.6040, d2.loss_bbox: 1.0044, d3.loss_cls: 0.6005, d3.loss_bbox: 0.9875, d4.loss_cls: 0.5959, d4.loss_bbox: 0.9803, aux_task0.loss_heatmap: 0.7788, aux_task0.loss_bbox: 0.3372, aux_task1.loss_heatmap: 0.9875, aux_task1.loss_bbox: 0.3672, aux_task2.loss_heatmap: 1.1142, aux_task2.loss_bbox: 0.3948, aux_task3.loss_heatmap: 0.8036, aux_task3.loss_bbox: 0.3034, aux_task4.loss_heatmap: 0.6266, aux_task4.loss_bbox: 0.3245, aux_task5.loss_heatmap: 0.6128, aux_task5.loss_bbox: 0.3139, loss: 16.7375, grad_norm: 71.9491
2025-05-08 06:39:36,484 - mmdet - INFO - Epoch [1][8500/30895]	lr: 1.283e-05, eta: 112 days, 20:19:44, time: 13.664, data_time: 0.573, memory: 9064, loss_cls: 0.5923, loss_bbox: 0.9592, d0.loss_cls: 0.6292, d0.loss_bbox: 1.1312, d1.loss_cls: 0.6075, d1.loss_bbox: 1.0143, d2.loss_cls: 0.6002, d2.loss_bbox: 0.9871, d3.loss_cls: 0.5980, d3.loss_bbox: 0.9704, d4.loss_cls: 0.5939, d4.loss_bbox: 0.9637, aux_task0.loss_heatmap: 0.7935, aux_task0.loss_bbox: 0.3386, aux_task1.loss_heatmap: 1.0114, aux_task1.loss_bbox: 0.3717, aux_task2.loss_heatmap: 1.1486, aux_task2.loss_bbox: 0.3916, aux_task3.loss_heatmap: 0.7943, aux_task3.loss_bbox: 0.3068, aux_task4.loss_heatmap: 0.6261, aux_task4.loss_bbox: 0.3268, aux_task5.loss_heatmap: 0.6155, aux_task5.loss_bbox: 0.3136, loss: 16.6855, grad_norm: 71.4741
2025-05-08 06:45:17,753 - mmdet - INFO - Epoch [1][8550/30895]	lr: 1.283e-05, eta: 112 days, 11:01:33, time: 6.825, data_time: 1.517, memory: 9064, loss_cls: 0.5945, loss_bbox: 0.9669, d0.loss_cls: 0.6315, d0.loss_bbox: 1.1401, d1.loss_cls: 0.6057, d1.loss_bbox: 1.0271, d2.loss_cls: 0.6013, d2.loss_bbox: 0.9950, d3.loss_cls: 0.5970, d3.loss_bbox: 0.9795, d4.loss_cls: 0.5971, d4.loss_bbox: 0.9705, aux_task0.loss_heatmap: 0.7929, aux_task0.loss_bbox: 0.3417, aux_task1.loss_heatmap: 1.0047, aux_task1.loss_bbox: 0.3652, aux_task2.loss_heatmap: 1.1423, aux_task2.loss_bbox: 0.3891, aux_task3.loss_heatmap: 0.7651, aux_task3.loss_bbox: 0.3054, aux_task4.loss_heatmap: 0.6094, aux_task4.loss_bbox: 0.3276, aux_task5.loss_heatmap: 0.6400, aux_task5.loss_bbox: 0.3225, loss: 16.7120, grad_norm: 74.7363
2025-05-08 06:52:12,703 - mmdet - INFO - Epoch [1][8600/30895]	lr: 1.284e-05, eta: 112 days, 3:16:48, time: 8.299, data_time: 3.330, memory: 9064, loss_cls: 0.5979, loss_bbox: 0.9411, d0.loss_cls: 0.6335, d0.loss_bbox: 1.1127, d1.loss_cls: 0.6124, d1.loss_bbox: 0.9979, d2.loss_cls: 0.6036, d2.loss_bbox: 0.9698, d3.loss_cls: 0.6007, d3.loss_bbox: 0.9548, d4.loss_cls: 0.5978, d4.loss_bbox: 0.9478, aux_task0.loss_heatmap: 0.7770, aux_task0.loss_bbox: 0.3264, aux_task1.loss_heatmap: 1.0337, aux_task1.loss_bbox: 0.3653, aux_task2.loss_heatmap: 1.1520, aux_task2.loss_bbox: 0.3813, aux_task3.loss_heatmap: 0.7266, aux_task3.loss_bbox: 0.2953, aux_task4.loss_heatmap: 0.6325, aux_task4.loss_bbox: 0.3242, aux_task5.loss_heatmap: 0.6215, aux_task5.loss_bbox: 0.3222, loss: 16.5280, grad_norm: 70.4570
2025-05-08 06:59:03,642 - mmdet - INFO - Epoch [1][8650/30895]	lr: 1.284e-05, eta: 111 days, 19:32:38, time: 8.219, data_time: 4.188, memory: 9064, loss_cls: 0.5866, loss_bbox: 0.9363, d0.loss_cls: 0.6212, d0.loss_bbox: 1.1124, d1.loss_cls: 0.6014, d1.loss_bbox: 0.9916, d2.loss_cls: 0.5943, d2.loss_bbox: 0.9634, d3.loss_cls: 0.5915, d3.loss_bbox: 0.9471, d4.loss_cls: 0.5878, d4.loss_bbox: 0.9406, aux_task0.loss_heatmap: 0.7885, aux_task0.loss_bbox: 0.3306, aux_task1.loss_heatmap: 1.0038, aux_task1.loss_bbox: 0.3642, aux_task2.loss_heatmap: 1.1038, aux_task2.loss_bbox: 0.3745, aux_task3.loss_heatmap: 0.7959, aux_task3.loss_bbox: 0.3022, aux_task4.loss_heatmap: 0.5959, aux_task4.loss_bbox: 0.3199, aux_task5.loss_heatmap: 0.5865, aux_task5.loss_bbox: 0.3121, loss: 16.3521, grad_norm: 73.1149
2025-05-08 07:05:09,024 - mmdet - INFO - Epoch [1][8700/30895]	lr: 1.284e-05, eta: 111 days, 11:00:34, time: 7.308, data_time: 4.091, memory: 9064, loss_cls: 0.5812, loss_bbox: 0.9503, d0.loss_cls: 0.6171, d0.loss_bbox: 1.1256, d1.loss_cls: 0.5952, d1.loss_bbox: 1.0118, d2.loss_cls: 0.5893, d2.loss_bbox: 0.9813, d3.loss_cls: 0.5855, d3.loss_bbox: 0.9648, d4.loss_cls: 0.5825, d4.loss_bbox: 0.9560, aux_task0.loss_heatmap: 0.8087, aux_task0.loss_bbox: 0.3307, aux_task1.loss_heatmap: 0.9865, aux_task1.loss_bbox: 0.3652, aux_task2.loss_heatmap: 1.0879, aux_task2.loss_bbox: 0.3849, aux_task3.loss_heatmap: 0.7082, aux_task3.loss_bbox: 0.2999, aux_task4.loss_heatmap: 0.6040, aux_task4.loss_bbox: 0.3209, aux_task5.loss_heatmap: 0.6006, aux_task5.loss_bbox: 0.3227, loss: 16.3608, grad_norm: 72.2481
2025-05-08 07:10:30,762 - mmdet - INFO - Epoch [1][8750/30895]	lr: 1.285e-05, eta: 111 days, 1:43:38, time: 6.435, data_time: 1.368, memory: 9064, loss_cls: 0.5897, loss_bbox: 0.9343, d0.loss_cls: 0.6194, d0.loss_bbox: 1.1042, d1.loss_cls: 0.5986, d1.loss_bbox: 0.9915, d2.loss_cls: 0.5920, d2.loss_bbox: 0.9632, d3.loss_cls: 0.5909, d3.loss_bbox: 0.9473, d4.loss_cls: 0.5908, d4.loss_bbox: 0.9388, aux_task0.loss_heatmap: 0.7538, aux_task0.loss_bbox: 0.3181, aux_task1.loss_heatmap: 1.0025, aux_task1.loss_bbox: 0.3643, aux_task2.loss_heatmap: 1.0764, aux_task2.loss_bbox: 0.3805, aux_task3.loss_heatmap: 0.7433, aux_task3.loss_bbox: 0.3010, aux_task4.loss_heatmap: 0.6112, aux_task4.loss_bbox: 0.3287, aux_task5.loss_heatmap: 0.6227, aux_task5.loss_bbox: 0.3150, loss: 16.2781, grad_norm: 69.1558
2025-05-08 07:15:34,406 - mmdet - INFO - Epoch [1][8800/30895]	lr: 1.285e-05, eta: 110 days, 16:12:05, time: 6.073, data_time: 3.817, memory: 9064, loss_cls: 0.5770, loss_bbox: 0.9501, d0.loss_cls: 0.6127, d0.loss_bbox: 1.1278, d1.loss_cls: 0.5948, d1.loss_bbox: 1.0076, d2.loss_cls: 0.5856, d2.loss_bbox: 0.9819, d3.loss_cls: 0.5817, d3.loss_bbox: 0.9650, d4.loss_cls: 0.5788, d4.loss_bbox: 0.9564, aux_task0.loss_heatmap: 0.7653, aux_task0.loss_bbox: 0.3258, aux_task1.loss_heatmap: 0.9756, aux_task1.loss_bbox: 0.3649, aux_task2.loss_heatmap: 1.0907, aux_task2.loss_bbox: 0.3838, aux_task3.loss_heatmap: 0.7941, aux_task3.loss_bbox: 0.3006, aux_task4.loss_heatmap: 0.6172, aux_task4.loss_bbox: 0.3272, aux_task5.loss_heatmap: 0.6304, aux_task5.loss_bbox: 0.3179, loss: 16.4127, grad_norm: 72.2803
2025-05-08 07:25:25,125 - mmdet - INFO - Epoch [1][8850/30895]	lr: 1.286e-05, eta: 110 days, 12:16:12, time: 11.814, data_time: 10.371, memory: 9064, loss_cls: 0.5915, loss_bbox: 0.9464, d0.loss_cls: 0.6247, d0.loss_bbox: 1.1138, d1.loss_cls: 0.6049, d1.loss_bbox: 0.9996, d2.loss_cls: 0.5979, d2.loss_bbox: 0.9720, d3.loss_cls: 0.5936, d3.loss_bbox: 0.9571, d4.loss_cls: 0.5945, d4.loss_bbox: 0.9494, aux_task0.loss_heatmap: 0.7792, aux_task0.loss_bbox: 0.3284, aux_task1.loss_heatmap: 1.0140, aux_task1.loss_bbox: 0.3658, aux_task2.loss_heatmap: 1.0768, aux_task2.loss_bbox: 0.3814, aux_task3.loss_heatmap: 0.7710, aux_task3.loss_bbox: 0.3093, aux_task4.loss_heatmap: 0.5989, aux_task4.loss_bbox: 0.3244, aux_task5.loss_heatmap: 0.6086, aux_task5.loss_bbox: 0.3201, loss: 16.4232, grad_norm: 71.6439
2025-05-08 07:33:33,057 - mmdet - INFO - Epoch [1][8900/30895]	lr: 1.286e-05, eta: 110 days, 6:25:38, time: 9.758, data_time: 8.325, memory: 9064, loss_cls: 0.5859, loss_bbox: 0.9529, d0.loss_cls: 0.6207, d0.loss_bbox: 1.1280, d1.loss_cls: 0.6006, d1.loss_bbox: 1.0101, d2.loss_cls: 0.5928, d2.loss_bbox: 0.9819, d3.loss_cls: 0.5913, d3.loss_bbox: 0.9669, d4.loss_cls: 0.5889, d4.loss_bbox: 0.9572, aux_task0.loss_heatmap: 0.7661, aux_task0.loss_bbox: 0.3346, aux_task1.loss_heatmap: 0.9950, aux_task1.loss_bbox: 0.3704, aux_task2.loss_heatmap: 1.0717, aux_task2.loss_bbox: 0.3904, aux_task3.loss_heatmap: 0.7608, aux_task3.loss_bbox: 0.3031, aux_task4.loss_heatmap: 0.6126, aux_task4.loss_bbox: 0.3221, aux_task5.loss_heatmap: 0.6598, aux_task5.loss_bbox: 0.3217, loss: 16.4858, grad_norm: 71.2145
2025-05-08 07:39:07,086 - mmdet - INFO - Epoch [1][8950/30895]	lr: 1.286e-05, eta: 109 days, 21:44:23, time: 6.681, data_time: 3.806, memory: 9064, loss_cls: 0.5778, loss_bbox: 0.9526, d0.loss_cls: 0.6154, d0.loss_bbox: 1.1330, d1.loss_cls: 0.5934, d1.loss_bbox: 1.0151, d2.loss_cls: 0.5835, d2.loss_bbox: 0.9862, d3.loss_cls: 0.5823, d3.loss_bbox: 0.9680, d4.loss_cls: 0.5786, d4.loss_bbox: 0.9589, aux_task0.loss_heatmap: 0.8057, aux_task0.loss_bbox: 0.3373, aux_task1.loss_heatmap: 1.0343, aux_task1.loss_bbox: 0.3689, aux_task2.loss_heatmap: 1.0587, aux_task2.loss_bbox: 0.3784, aux_task3.loss_heatmap: 0.7293, aux_task3.loss_bbox: 0.3063, aux_task4.loss_heatmap: 0.6037, aux_task4.loss_bbox: 0.3232, aux_task5.loss_heatmap: 0.6252, aux_task5.loss_bbox: 0.3230, loss: 16.4388, grad_norm: 70.5038
2025-05-08 07:44:02,570 - mmdet - INFO - Exp name: TinyMortonXYDrop01.py
2025-05-08 07:44:02,570 - mmdet - INFO - Epoch [1][9000/30895]	lr: 1.287e-05, eta: 109 days, 12:25:25, time: 5.910, data_time: 4.402, memory: 9064, loss_cls: 0.5839, loss_bbox: 0.9409, d0.loss_cls: 0.6176, d0.loss_bbox: 1.1126, d1.loss_cls: 0.6000, d1.loss_bbox: 0.9977, d2.loss_cls: 0.5906, d2.loss_bbox: 0.9680, d3.loss_cls: 0.5885, d3.loss_bbox: 0.9521, d4.loss_cls: 0.5863, d4.loss_bbox: 0.9461, aux_task0.loss_heatmap: 0.7944, aux_task0.loss_bbox: 0.3281, aux_task1.loss_heatmap: 0.9738, aux_task1.loss_bbox: 0.3601, aux_task2.loss_heatmap: 1.0677, aux_task2.loss_bbox: 0.3804, aux_task3.loss_heatmap: 0.6836, aux_task3.loss_bbox: 0.2980, aux_task4.loss_heatmap: 0.6163, aux_task4.loss_bbox: 0.3258, aux_task5.loss_heatmap: 0.6234, aux_task5.loss_bbox: 0.3181, loss: 16.2538, grad_norm: 71.9453
2025-05-08 07:52:06,224 - mmdet - INFO - Epoch [1][9050/30895]	lr: 1.287e-05, eta: 109 days, 6:43:33, time: 9.673, data_time: 6.846, memory: 9064, loss_cls: 0.5722, loss_bbox: 0.9350, d0.loss_cls: 0.6067, d0.loss_bbox: 1.1076, d1.loss_cls: 0.5867, d1.loss_bbox: 0.9948, d2.loss_cls: 0.5791, d2.loss_bbox: 0.9663, d3.loss_cls: 0.5746, d3.loss_bbox: 0.9504, d4.loss_cls: 0.5744, d4.loss_bbox: 0.9401, aux_task0.loss_heatmap: 0.7832, aux_task0.loss_bbox: 0.3266, aux_task1.loss_heatmap: 0.9654, aux_task1.loss_bbox: 0.3581, aux_task2.loss_heatmap: 1.0769, aux_task2.loss_bbox: 0.3837, aux_task3.loss_heatmap: 0.7071, aux_task3.loss_bbox: 0.3003, aux_task4.loss_heatmap: 0.6164, aux_task4.loss_bbox: 0.3245, aux_task5.loss_heatmap: 0.6089, aux_task5.loss_bbox: 0.3211, loss: 16.1601, grad_norm: 73.8686
2025-05-08 07:57:03,567 - mmdet - INFO - Epoch [1][9100/30895]	lr: 1.288e-05, eta: 108 days, 21:37:35, time: 5.946, data_time: 4.039, memory: 9064, loss_cls: 0.5733, loss_bbox: 0.9466, d0.loss_cls: 0.6097, d0.loss_bbox: 1.1153, d1.loss_cls: 0.5852, d1.loss_bbox: 1.0082, d2.loss_cls: 0.5807, d2.loss_bbox: 0.9771, d3.loss_cls: 0.5786, d3.loss_bbox: 0.9601, d4.loss_cls: 0.5767, d4.loss_bbox: 0.9505, aux_task0.loss_heatmap: 0.7536, aux_task0.loss_bbox: 0.3369, aux_task1.loss_heatmap: 0.9899, aux_task1.loss_bbox: 0.3651, aux_task2.loss_heatmap: 1.0917, aux_task2.loss_bbox: 0.3991, aux_task3.loss_heatmap: 0.7634, aux_task3.loss_bbox: 0.3019, aux_task4.loss_heatmap: 0.6251, aux_task4.loss_bbox: 0.3279, aux_task5.loss_heatmap: 0.6446, aux_task5.loss_bbox: 0.3208, loss: 16.3817, grad_norm: 70.2519
2025-05-08 08:03:47,354 - mmdet - INFO - Epoch [1][9150/30895]	lr: 1.288e-05, eta: 108 days, 14:35:37, time: 8.076, data_time: 6.609, memory: 9064, loss_cls: 0.5820, loss_bbox: 0.9406, d0.loss_cls: 0.6232, d0.loss_bbox: 1.1168, d1.loss_cls: 0.6018, d1.loss_bbox: 1.0002, d2.loss_cls: 0.5935, d2.loss_bbox: 0.9728, d3.loss_cls: 0.5894, d3.loss_bbox: 0.9548, d4.loss_cls: 0.5849, d4.loss_bbox: 0.9442, aux_task0.loss_heatmap: 0.8213, aux_task0.loss_bbox: 0.3366, aux_task1.loss_heatmap: 1.0032, aux_task1.loss_bbox: 0.3664, aux_task2.loss_heatmap: 1.0822, aux_task2.loss_bbox: 0.4045, aux_task3.loss_heatmap: 0.7638, aux_task3.loss_bbox: 0.3042, aux_task4.loss_heatmap: 0.5989, aux_task4.loss_bbox: 0.3230, aux_task5.loss_heatmap: 0.6001, aux_task5.loss_bbox: 0.3173, loss: 16.4255, grad_norm: 98.2800
2025-05-08 08:11:18,830 - mmdet - INFO - Epoch [1][9200/30895]	lr: 1.288e-05, eta: 108 days, 8:30:42, time: 9.029, data_time: 7.598, memory: 9064, loss_cls: 0.5824, loss_bbox: 0.9473, d0.loss_cls: 0.6161, d0.loss_bbox: 1.1121, d1.loss_cls: 0.5976, d1.loss_bbox: 1.0013, d2.loss_cls: 0.5903, d2.loss_bbox: 0.9727, d3.loss_cls: 0.5872, d3.loss_bbox: 0.9569, d4.loss_cls: 0.5849, d4.loss_bbox: 0.9509, aux_task0.loss_heatmap: 0.7344, aux_task0.loss_bbox: 0.3240, aux_task1.loss_heatmap: 1.0205, aux_task1.loss_bbox: 0.3622, aux_task2.loss_heatmap: 1.0910, aux_task2.loss_bbox: 0.3960, aux_task3.loss_heatmap: 0.7707, aux_task3.loss_bbox: 0.3047, aux_task4.loss_heatmap: 0.5921, aux_task4.loss_bbox: 0.3226, aux_task5.loss_heatmap: 0.6238, aux_task5.loss_bbox: 0.3182, loss: 16.3597, grad_norm: 70.4216
2025-05-08 08:18:32,246 - mmdet - INFO - Epoch [1][9250/30895]	lr: 1.289e-05, eta: 108 days, 2:09:51, time: 8.668, data_time: 6.618, memory: 9064, loss_cls: 0.5843, loss_bbox: 0.9450, d0.loss_cls: 0.6244, d0.loss_bbox: 1.1139, d1.loss_cls: 0.6010, d1.loss_bbox: 1.0020, d2.loss_cls: 0.5906, d2.loss_bbox: 0.9735, d3.loss_cls: 0.5872, d3.loss_bbox: 0.9578, d4.loss_cls: 0.5843, d4.loss_bbox: 0.9506, aux_task0.loss_heatmap: 0.7756, aux_task0.loss_bbox: 0.3350, aux_task1.loss_heatmap: 0.9789, aux_task1.loss_bbox: 0.3592, aux_task2.loss_heatmap: 1.1201, aux_task2.loss_bbox: 0.3973, aux_task3.loss_heatmap: 0.7385, aux_task3.loss_bbox: 0.3027, aux_task4.loss_heatmap: 0.5880, aux_task4.loss_bbox: 0.3209, aux_task5.loss_heatmap: 0.6599, aux_task5.loss_bbox: 0.3185, loss: 16.4095, grad_norm: 84.5537
2025-05-08 08:24:18,381 - mmdet - INFO - Epoch [1][9300/30895]	lr: 1.289e-05, eta: 107 days, 18:17:49, time: 6.923, data_time: 2.351, memory: 9064, loss_cls: 0.5823, loss_bbox: 0.9442, d0.loss_cls: 0.6182, d0.loss_bbox: 1.1165, d1.loss_cls: 0.5997, d1.loss_bbox: 1.0018, d2.loss_cls: 0.5906, d2.loss_bbox: 0.9743, d3.loss_cls: 0.5879, d3.loss_bbox: 0.9590, d4.loss_cls: 0.5858, d4.loss_bbox: 0.9490, aux_task0.loss_heatmap: 0.8241, aux_task0.loss_bbox: 0.3444, aux_task1.loss_heatmap: 1.0064, aux_task1.loss_bbox: 0.3662, aux_task2.loss_heatmap: 1.1081, aux_task2.loss_bbox: 0.3862, aux_task3.loss_heatmap: 0.8085, aux_task3.loss_bbox: 0.3093, aux_task4.loss_heatmap: 0.5894, aux_task4.loss_bbox: 0.3213, aux_task5.loss_heatmap: 0.6119, aux_task5.loss_bbox: 0.3142, loss: 16.4993, grad_norm: 72.6860
2025-05-08 08:30:45,989 - mmdet - INFO - Epoch [1][9350/30895]	lr: 1.290e-05, eta: 107 days, 11:15:47, time: 7.752, data_time: 6.287, memory: 9064, loss_cls: 0.5697, loss_bbox: 0.9410, d0.loss_cls: 0.6057, d0.loss_bbox: 1.1094, d1.loss_cls: 0.5834, d1.loss_bbox: 0.9942, d2.loss_cls: 0.5774, d2.loss_bbox: 0.9660, d3.loss_cls: 0.5746, d3.loss_bbox: 0.9525, d4.loss_cls: 0.5728, d4.loss_bbox: 0.9445, aux_task0.loss_heatmap: 0.7577, aux_task0.loss_bbox: 0.3262, aux_task1.loss_heatmap: 0.9692, aux_task1.loss_bbox: 0.3690, aux_task2.loss_heatmap: 1.1152, aux_task2.loss_bbox: 0.3849, aux_task3.loss_heatmap: 0.7662, aux_task3.loss_bbox: 0.2974, aux_task4.loss_heatmap: 0.6068, aux_task4.loss_bbox: 0.3230, aux_task5.loss_heatmap: 0.6094, aux_task5.loss_bbox: 0.3172, loss: 16.2334, grad_norm: 72.7838
2025-05-08 08:38:11,170 - mmdet - INFO - Epoch [1][9400/30895]	lr: 1.290e-05, eta: 107 days, 5:20:17, time: 8.904, data_time: 7.055, memory: 9064, loss_cls: 0.5650, loss_bbox: 0.9467, d0.loss_cls: 0.6057, d0.loss_bbox: 1.1151, d1.loss_cls: 0.5827, d1.loss_bbox: 1.0007, d2.loss_cls: 0.5768, d2.loss_bbox: 0.9710, d3.loss_cls: 0.5706, d3.loss_bbox: 0.9593, d4.loss_cls: 0.5680, d4.loss_bbox: 0.9515, aux_task0.loss_heatmap: 0.7970, aux_task0.loss_bbox: 0.3364, aux_task1.loss_heatmap: 0.9735, aux_task1.loss_bbox: 0.3659, aux_task2.loss_heatmap: 1.1146, aux_task2.loss_bbox: 0.3812, aux_task3.loss_heatmap: 0.8004, aux_task3.loss_bbox: 0.3082, aux_task4.loss_heatmap: 0.6018, aux_task4.loss_bbox: 0.3243, aux_task5.loss_heatmap: 0.6063, aux_task5.loss_bbox: 0.3180, loss: 16.3405, grad_norm: 71.2072
2025-05-08 08:46:33,894 - mmdet - INFO - Epoch [1][9450/30895]	lr: 1.291e-05, eta: 107 days, 0:30:12, time: 10.055, data_time: 2.709, memory: 9064, loss_cls: 0.5607, loss_bbox: 0.9366, d0.loss_cls: 0.5993, d0.loss_bbox: 1.1039, d1.loss_cls: 0.5773, d1.loss_bbox: 0.9880, d2.loss_cls: 0.5701, d2.loss_bbox: 0.9610, d3.loss_cls: 0.5659, d3.loss_bbox: 0.9480, d4.loss_cls: 0.5635, d4.loss_bbox: 0.9407, aux_task0.loss_heatmap: 0.7724, aux_task0.loss_bbox: 0.3321, aux_task1.loss_heatmap: 0.9687, aux_task1.loss_bbox: 0.3637, aux_task2.loss_heatmap: 1.0911, aux_task2.loss_bbox: 0.3920, aux_task3.loss_heatmap: 0.7427, aux_task3.loss_bbox: 0.3043, aux_task4.loss_heatmap: 0.5896, aux_task4.loss_bbox: 0.3229, aux_task5.loss_heatmap: 0.6105, aux_task5.loss_bbox: 0.3147, loss: 16.1198, grad_norm: 73.2980
2025-05-08 08:53:22,876 - mmdet - INFO - Epoch [1][9500/30895]	lr: 1.291e-05, eta: 106 days, 18:03:00, time: 8.179, data_time: 2.514, memory: 9064, loss_cls: 0.5682, loss_bbox: 0.9323, d0.loss_cls: 0.6064, d0.loss_bbox: 1.0954, d1.loss_cls: 0.5866, d1.loss_bbox: 0.9844, d2.loss_cls: 0.5784, d2.loss_bbox: 0.9546, d3.loss_cls: 0.5732, d3.loss_bbox: 0.9410, d4.loss_cls: 0.5728, d4.loss_bbox: 0.9335, aux_task0.loss_heatmap: 0.7655, aux_task0.loss_bbox: 0.3232, aux_task1.loss_heatmap: 0.9653, aux_task1.loss_bbox: 0.3585, aux_task2.loss_heatmap: 1.1290, aux_task2.loss_bbox: 0.3883, aux_task3.loss_heatmap: 0.8218, aux_task3.loss_bbox: 0.3076, aux_task4.loss_heatmap: 0.6027, aux_task4.loss_bbox: 0.3205, aux_task5.loss_heatmap: 0.6040, aux_task5.loss_bbox: 0.3153, loss: 16.2285, grad_norm: 71.9447
2025-05-08 09:02:46,899 - mmdet - INFO - Epoch [1][9550/30895]	lr: 1.291e-05, eta: 106 days, 14:24:27, time: 11.281, data_time: 7.437, memory: 9064, loss_cls: 0.5683, loss_bbox: 0.9340, d0.loss_cls: 0.6106, d0.loss_bbox: 1.1030, d1.loss_cls: 0.5875, d1.loss_bbox: 0.9919, d2.loss_cls: 0.5799, d2.loss_bbox: 0.9615, d3.loss_cls: 0.5746, d3.loss_bbox: 0.9475, d4.loss_cls: 0.5735, d4.loss_bbox: 0.9384, aux_task0.loss_heatmap: 0.7614, aux_task0.loss_bbox: 0.3222, aux_task1.loss_heatmap: 0.9816, aux_task1.loss_bbox: 0.3646, aux_task2.loss_heatmap: 1.0834, aux_task2.loss_bbox: 0.3884, aux_task3.loss_heatmap: 0.7590, aux_task3.loss_bbox: 0.2938, aux_task4.loss_heatmap: 0.5943, aux_task4.loss_bbox: 0.3207, aux_task5.loss_heatmap: 0.6011, aux_task5.loss_bbox: 0.3153, loss: 16.1564, grad_norm: 74.4634
2025-05-08 09:06:30,415 - mmdet - INFO - Epoch [1][9600/30895]	lr: 1.292e-05, eta: 106 days, 4:48:26, time: 4.470, data_time: 0.879, memory: 9064, loss_cls: 0.5765, loss_bbox: 0.9342, d0.loss_cls: 0.6154, d0.loss_bbox: 1.1034, d1.loss_cls: 0.5923, d1.loss_bbox: 0.9915, d2.loss_cls: 0.5866, d2.loss_bbox: 0.9617, d3.loss_cls: 0.5832, d3.loss_bbox: 0.9469, d4.loss_cls: 0.5820, d4.loss_bbox: 0.9386, aux_task0.loss_heatmap: 0.7734, aux_task0.loss_bbox: 0.3326, aux_task1.loss_heatmap: 0.9526, aux_task1.loss_bbox: 0.3560, aux_task2.loss_heatmap: 1.0761, aux_task2.loss_bbox: 0.3944, aux_task3.loss_heatmap: 0.7298, aux_task3.loss_bbox: 0.2977, aux_task4.loss_heatmap: 0.5806, aux_task4.loss_bbox: 0.3212, aux_task5.loss_heatmap: 0.6187, aux_task5.loss_bbox: 0.3164, loss: 16.1620, grad_norm: 74.2895
2025-05-08 09:14:35,608 - mmdet - INFO - Epoch [1][9650/30895]	lr: 1.292e-05, eta: 105 days, 23:53:16, time: 9.704, data_time: 0.771, memory: 9064, loss_cls: 0.5687, loss_bbox: 0.9371, d0.loss_cls: 0.6078, d0.loss_bbox: 1.1055, d1.loss_cls: 0.5863, d1.loss_bbox: 0.9908, d2.loss_cls: 0.5767, d2.loss_bbox: 0.9631, d3.loss_cls: 0.5742, d3.loss_bbox: 0.9486, d4.loss_cls: 0.5721, d4.loss_bbox: 0.9398, aux_task0.loss_heatmap: 0.7702, aux_task0.loss_bbox: 0.3200, aux_task1.loss_heatmap: 0.9858, aux_task1.loss_bbox: 0.3631, aux_task2.loss_heatmap: 1.1196, aux_task2.loss_bbox: 0.3877, aux_task3.loss_heatmap: 0.7282, aux_task3.loss_bbox: 0.3083, aux_task4.loss_heatmap: 0.6122, aux_task4.loss_bbox: 0.3192, aux_task5.loss_heatmap: 0.6080, aux_task5.loss_bbox: 0.3183, loss: 16.2116, grad_norm: 71.6092
2025-05-08 09:22:21,922 - mmdet - INFO - Epoch [1][9700/30895]	lr: 1.293e-05, eta: 105 days, 18:41:19, time: 9.326, data_time: 6.541, memory: 9064, loss_cls: 0.5590, loss_bbox: 0.9393, d0.loss_cls: 0.6021, d0.loss_bbox: 1.1057, d1.loss_cls: 0.5779, d1.loss_bbox: 0.9935, d2.loss_cls: 0.5708, d2.loss_bbox: 0.9648, d3.loss_cls: 0.5638, d3.loss_bbox: 0.9516, d4.loss_cls: 0.5610, d4.loss_bbox: 0.9440, aux_task0.loss_heatmap: 0.7284, aux_task0.loss_bbox: 0.3305, aux_task1.loss_heatmap: 0.9708, aux_task1.loss_bbox: 0.3623, aux_task2.loss_heatmap: 1.0739, aux_task2.loss_bbox: 0.3895, aux_task3.loss_heatmap: 0.7435, aux_task3.loss_bbox: 0.3032, aux_task4.loss_heatmap: 0.6128, aux_task4.loss_bbox: 0.3257, aux_task5.loss_heatmap: 0.5948, aux_task5.loss_bbox: 0.3161, loss: 16.0852, grad_norm: 70.6020
