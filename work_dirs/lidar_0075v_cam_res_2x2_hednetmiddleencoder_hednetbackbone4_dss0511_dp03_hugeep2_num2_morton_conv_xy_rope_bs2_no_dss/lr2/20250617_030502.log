2025-06-17 03:05:02,425 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 (default, Oct  3 2024, 15:24:27) [GCC 11.2.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+HEAD
spconv2.0: True
------------------------------------------------------------

2025-06-17 03:05:02,972 - mmdet - INFO - 分布式训练: True
2025-06-17 03:05:03,502 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss/lr2'
load_from = 'pretrained/hedres_forced.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=-1,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(-1, -1),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(
        type='DynamicVFE',
        in_channels=5,
        feat_channels=[64, 128],
        with_distance=False,
        with_cluster_center=True,
        with_voxel_center=True,
        voxel_size=[0.075, 0.075, 0.2],
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_middle_encoder=dict(
        type='HEDNet',
        in_channels=128,
        sparse_shape=[41, 1440, 1440],
        model_cfg=dict(
            FEATURE_DIM=128,
            NUM_LAYERS=2,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDE=[1, 2, 2],
            DOWN_KERNEL_SIZE=[3, 3, 3])),
    pts_backbone=dict(
        type='CascadeDEDBackbone',
        in_channels=256,
        model_cfg=dict(
            USE_SECONDMAMBA=False,
            FEATURE_DIM=256,
            NUM_LAYERS=4,
            NUM_SBB=[2, 1, 1],
            DOWN_STRIDES=[1, 2, 2])),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        use_dss=False,
        use_hybrid=False,
        dss_date_version='0511',
        dss_drop_prob=0.3,
        dss_mamba_version='DSSMamba_Huge_EP2',
        dss_num_layers=2,
        dss_use_morton=True,
        dss_use_conv=True,
        dss_use_xy=True,
        dss_use_rope=True,
        dss_stack=True,
        dss_strong_cls=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0002,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
gpu_ids = range(0, 2)

2025-06-17 03:05:03,503 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-06-17 03:05:04,263 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-06-17 03:05:04,292 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-06-17 03:05:04,362 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,362 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,363 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,363 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,363 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,364 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,364 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,364 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,366 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,368 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,370 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,372 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,374 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,376 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,377 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,379 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,381 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,383 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,385 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,387 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,388 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,390 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,392 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,394 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,396 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,398 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,399 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,401 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,403 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,405 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,413 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,420 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,427 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-06-17 03:05:04,446 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_voxel_encoder.vfe_layers.0.0.weight - torch.Size([64, 11]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.0.weight - torch.Size([128, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_voxel_encoder.vfe_layers.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.0.weight - torch.Size([16, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.0.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.0.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.1.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.2.conv2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.2.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.0.weight - torch.Size([32, 3, 3, 3, 16]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv1.3.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv1.3.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.0.blocks.2.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.1.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.conv2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.encoder.2.blocks.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.0.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder.1.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.0.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.0.decoder_norm.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.0.weight - torch.Size([64, 3, 3, 3, 32]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv2.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv2.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.0.blocks.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.1.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.conv2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.encoder.2.blocks.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.0.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.0.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder.1.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.0.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.0.decoder_norm.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.0.weight - torch.Size([128, 3, 3, 3, 64]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv3.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv3.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.0.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.0.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.0.blocks.2.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.0.blocks.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.1.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.1.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.layers.1.encoder.2.blocks.1.conv2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.encoder.2.blocks.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.0.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder.1.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.layers.1.decoder_norm.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.3.weight - torch.Size([128, 3, 1, 1, 128]): 
Initialized by user-defined `init_weights` in HEDNet  

pts_middle_encoder.conv_out.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.0.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.1.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.2.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.0.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.1.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.0.weight - torch.Size([256, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.0.downsample_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv1.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.encoder.2.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.0.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.0.weight - torch.Size([256, 256, 2, 2]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.layers.3.decoder_norm.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.gate_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.up_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.weight - torch.Size([10, 1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.down_proj.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-06-17 03:05:04,461 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=-1, max_voxels=(-1, -1), deterministic=True)
  (pts_voxel_encoder): DynamicVFE(
    (scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
    (vfe_layers): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=11, out_features=64, bias=False)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=False)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (vfe_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=False)
    (cluster_scatter): DynamicScatter(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], average_points=True)
  )
  (pts_middle_encoder): HEDNet(
    (conv1): SparseSequential(
      (0): SparseSequential(
        (0): SubMConv3d(128, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
      (1): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (2): SparseBasicBlock(
        (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (relu): ReLU()
        (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
        (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (3): SparseSequential(
        (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv2): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (conv3): SparseSequential(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SparseSequential(
        (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[1, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
        (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (layers): ModuleList(
      (0): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): SEDLayer(
        (encoder): ModuleList(
          (0): SEDBlock(
            (blocks): SparseSequential(
              (0): Identity()
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
              (2): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (1): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
          (2): SEDBlock(
            (blocks): SparseSequential(
              (0): SparseSequential(
                (0): SparseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
                (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (2): ReLU()
              )
              (1): SparseBasicBlock(
                (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
                (relu): ReLU()
                (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)
                (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
          )
        )
        (decoder): ModuleList(
          (0): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): SparseSequential(
            (0): SparseInverseConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
            (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (4): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (5): ReLU()
    )
  )
  (pts_backbone): CascadeDEDBackbone(
    (layers): ModuleList(
      (0): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (1): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (2): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
      (3): DEDBackbone(
        (encoder): ModuleList(
          (0): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (1): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
          (2): Sequential(
            (0): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
              (downsample_layer): Sequential(
                (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              )
            )
            (1): BasicBlock(
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu1): ReLU()
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (bn2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
              (relu2): ReLU()
            )
          )
        )
        (decoder): ModuleList(
          (0): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
          (1): Sequential(
            (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
            (2): ReLU()
          )
        )
        (decoder_norm): ModuleList(
          (0): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (1): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (2): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (3): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (4): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
      (5): MLP(
        (gate_proj): Linear(in_features=256, out_features=1024, bias=False)
        (up_proj): Linear(in_features=256, out_features=1024, bias=False)
        (act_fn): SiLU()
        (down_proj): Linear(in_features=1024, out_features=10, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-06-17 03:05:29,921 - mmdet - INFO - load checkpoint from local path: pretrained/hedres_forced.pth
2025-06-17 03:05:47,742 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.0.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.1.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.2.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.3.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.4.attentions.0.2.rope.sin_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.in_proj_bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.0.attn.out_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm_before.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.norm.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.0.mlp_norm.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm_before.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_f_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.A_log_b_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_f_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.D_b_xy, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.in_proj_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.x_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_f_xy.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.dt_proj_b_xy.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_f.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_x_xy_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.conv1d_z_xy_b.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.out_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mamba.global_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.norm.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.gate_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.up_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.layers.1.mlp.down_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.inv_freq, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.cos_cached, pts_bbox_head.transformer.decoder.layers.5.attentions.0.2.rope.sin_cached, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias

2025-06-17 03:05:47,757 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss/lr2
2025-06-17 03:05:47,758 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-06-17 03:05:47,758 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-06-17 03:05:47,758 - mmdet - INFO - Checkpoints will be saved to /mnt/sdc/FUTR3D/work_dirs/lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss/lr2 by HardDiskBackend.
2025-06-17 03:07:03,204 - mmdet - INFO - Epoch [1][50/7033]	lr: 7.973e-05, eta: 17:31:49, time: 1.497, data_time: 0.288, memory: 13038, loss_cls: 1.0672, loss_bbox: 1.5704, d0.loss_cls: 1.1510, d0.loss_bbox: 1.7526, d1.loss_cls: 1.1003, d1.loss_bbox: 1.6414, d2.loss_cls: 1.0434, d2.loss_bbox: 1.5961, d3.loss_cls: 1.0561, d3.loss_bbox: 1.6093, d4.loss_cls: 1.0251, d4.loss_bbox: 1.5710, loss: 16.1840, grad_norm: 21.9472
2025-06-17 03:07:49,276 - mmdet - INFO - Epoch [1][100/7033]	lr: 9.307e-05, eta: 14:08:32, time: 0.921, data_time: 0.027, memory: 13192, loss_cls: 0.8428, loss_bbox: 1.2425, d0.loss_cls: 0.9707, d0.loss_bbox: 1.4843, d1.loss_cls: 0.8839, d1.loss_bbox: 1.3321, d2.loss_cls: 0.8268, d2.loss_bbox: 1.2760, d3.loss_cls: 0.8292, d3.loss_bbox: 1.2760, d4.loss_cls: 0.8299, d4.loss_bbox: 1.2519, loss: 13.0460, grad_norm: 17.1250
2025-06-17 03:08:35,315 - mmdet - INFO - Epoch [1][150/7033]	lr: 1.064e-04, eta: 13:00:06, time: 0.921, data_time: 0.027, memory: 13192, loss_cls: 0.7726, loss_bbox: 1.1504, d0.loss_cls: 0.8990, d0.loss_bbox: 1.4016, d1.loss_cls: 0.7923, d1.loss_bbox: 1.2292, d2.loss_cls: 0.7422, d2.loss_bbox: 1.1806, d3.loss_cls: 0.7480, d3.loss_bbox: 1.1786, d4.loss_cls: 0.7652, d4.loss_bbox: 1.1624, loss: 12.0222, grad_norm: 16.4177
2025-06-17 03:09:21,519 - mmdet - INFO - Epoch [1][200/7033]	lr: 1.197e-04, eta: 12:26:05, time: 0.924, data_time: 0.026, memory: 13192, loss_cls: 0.7222, loss_bbox: 1.1270, d0.loss_cls: 0.8334, d0.loss_bbox: 1.3601, d1.loss_cls: 0.7260, d1.loss_bbox: 1.1975, d2.loss_cls: 0.6833, d2.loss_bbox: 1.1550, d3.loss_cls: 0.6977, d3.loss_bbox: 1.1524, d4.loss_cls: 0.7094, d4.loss_bbox: 1.1349, loss: 11.4990, grad_norm: 16.1116
2025-06-17 03:10:07,490 - mmdet - INFO - Epoch [1][250/7033]	lr: 1.331e-04, eta: 12:04:43, time: 0.919, data_time: 0.026, memory: 13192, loss_cls: 0.6980, loss_bbox: 1.0412, d0.loss_cls: 0.8163, d0.loss_bbox: 1.2605, d1.loss_cls: 0.6982, d1.loss_bbox: 1.1070, d2.loss_cls: 0.6562, d2.loss_bbox: 1.0596, d3.loss_cls: 0.6675, d3.loss_bbox: 1.0607, d4.loss_cls: 0.6804, d4.loss_bbox: 1.0498, loss: 10.7954, grad_norm: 14.9718
2025-06-17 03:10:53,382 - mmdet - INFO - Epoch [1][300/7033]	lr: 1.464e-04, eta: 11:50:00, time: 0.918, data_time: 0.027, memory: 13192, loss_cls: 0.6598, loss_bbox: 1.0342, d0.loss_cls: 0.7638, d0.loss_bbox: 1.2735, d1.loss_cls: 0.6669, d1.loss_bbox: 1.1103, d2.loss_cls: 0.6127, d2.loss_bbox: 1.0645, d3.loss_cls: 0.6284, d3.loss_bbox: 1.0665, d4.loss_cls: 0.6433, d4.loss_bbox: 1.0464, loss: 10.5704, grad_norm: 12.3587
2025-06-17 03:11:39,494 - mmdet - INFO - Epoch [1][350/7033]	lr: 1.597e-04, eta: 11:39:45, time: 0.922, data_time: 0.027, memory: 13192, loss_cls: 0.6002, loss_bbox: 1.0144, d0.loss_cls: 0.7366, d0.loss_bbox: 1.2571, d1.loss_cls: 0.6130, d1.loss_bbox: 1.0909, d2.loss_cls: 0.5464, d2.loss_bbox: 1.0420, d3.loss_cls: 0.5647, d3.loss_bbox: 1.0469, d4.loss_cls: 0.5794, d4.loss_bbox: 1.0220, loss: 10.1135, grad_norm: 10.8857
2025-06-17 03:12:25,579 - mmdet - INFO - Epoch [1][400/7033]	lr: 1.731e-04, eta: 11:31:49, time: 0.922, data_time: 0.029, memory: 13211, loss_cls: 0.5355, loss_bbox: 0.9648, d0.loss_cls: 0.6959, d0.loss_bbox: 1.2302, d1.loss_cls: 0.5550, d1.loss_bbox: 1.0529, d2.loss_cls: 0.4830, d2.loss_bbox: 1.0038, d3.loss_cls: 0.4913, d3.loss_bbox: 1.0112, d4.loss_cls: 0.5073, d4.loss_bbox: 0.9758, loss: 9.5068, grad_norm: 11.2774
2025-06-17 03:13:11,518 - mmdet - INFO - Epoch [1][450/7033]	lr: 1.864e-04, eta: 11:25:15, time: 0.919, data_time: 0.028, memory: 13211, loss_cls: 0.4984, loss_bbox: 0.9262, d0.loss_cls: 0.6664, d0.loss_bbox: 1.1943, d1.loss_cls: 0.5275, d1.loss_bbox: 1.0239, d2.loss_cls: 0.4460, d2.loss_bbox: 0.9725, d3.loss_cls: 0.4557, d3.loss_bbox: 0.9742, d4.loss_cls: 0.4787, d4.loss_bbox: 0.9356, loss: 9.0993, grad_norm: 11.4266
2025-06-17 03:13:57,477 - mmdet - INFO - Epoch [1][500/7033]	lr: 1.997e-04, eta: 11:19:51, time: 0.919, data_time: 0.029, memory: 13211, loss_cls: 0.4163, loss_bbox: 0.8569, d0.loss_cls: 0.6683, d0.loss_bbox: 1.1820, d1.loss_cls: 0.5031, d1.loss_bbox: 0.9844, d2.loss_cls: 0.3744, d2.loss_bbox: 0.9102, d3.loss_cls: 0.3810, d3.loss_bbox: 0.9057, d4.loss_cls: 0.3977, d4.loss_bbox: 0.8692, loss: 8.4492, grad_norm: 13.0851
2025-06-17 03:14:43,493 - mmdet - INFO - Epoch [1][550/7033]	lr: 2.000e-04, eta: 11:15:23, time: 0.920, data_time: 0.026, memory: 13262, loss_cls: 0.3970, loss_bbox: 0.8269, d0.loss_cls: 0.6419, d0.loss_bbox: 1.1528, d1.loss_cls: 0.4455, d1.loss_bbox: 0.9450, d2.loss_cls: 0.3610, d2.loss_bbox: 0.8532, d3.loss_cls: 0.3712, d3.loss_bbox: 0.8524, d4.loss_cls: 0.3828, d4.loss_bbox: 0.8240, loss: 8.0537, grad_norm: 15.0218
2025-06-17 03:15:29,794 - mmdet - INFO - Epoch [1][600/7033]	lr: 2.000e-04, eta: 11:11:52, time: 0.926, data_time: 0.027, memory: 13462, loss_cls: 0.3821, loss_bbox: 0.7630, d0.loss_cls: 0.6098, d0.loss_bbox: 1.1541, d1.loss_cls: 0.4095, d1.loss_bbox: 0.8842, d2.loss_cls: 0.3580, d2.loss_bbox: 0.7791, d3.loss_cls: 0.3628, d3.loss_bbox: 0.7800, d4.loss_cls: 0.3724, d4.loss_bbox: 0.7574, loss: 7.6126, grad_norm: 13.6834
2025-06-17 03:16:15,926 - mmdet - INFO - Epoch [1][650/7033]	lr: 2.000e-04, eta: 11:08:35, time: 0.923, data_time: 0.028, memory: 13462, loss_cls: 0.3845, loss_bbox: 0.7037, d0.loss_cls: 0.5980, d0.loss_bbox: 1.1442, d1.loss_cls: 0.3799, d1.loss_bbox: 0.7558, d2.loss_cls: 0.3425, d2.loss_bbox: 0.6808, d3.loss_cls: 0.3529, d3.loss_bbox: 0.6947, d4.loss_cls: 0.3670, d4.loss_bbox: 0.6884, loss: 7.0923, grad_norm: 16.6329
2025-06-17 03:17:02,000 - mmdet - INFO - Epoch [1][700/7033]	lr: 2.000e-04, eta: 11:05:36, time: 0.921, data_time: 0.029, memory: 13462, loss_cls: 0.3615, loss_bbox: 0.5951, d0.loss_cls: 0.6008, d0.loss_bbox: 1.1191, d1.loss_cls: 0.3575, d1.loss_bbox: 0.6247, d2.loss_cls: 0.3357, d2.loss_bbox: 0.5527, d3.loss_cls: 0.3431, d3.loss_bbox: 0.5714, d4.loss_cls: 0.3529, d4.loss_bbox: 0.5759, loss: 6.3905, grad_norm: 16.8050
2025-06-17 03:17:48,050 - mmdet - INFO - Epoch [1][750/7033]	lr: 2.000e-04, eta: 11:02:53, time: 0.921, data_time: 0.026, memory: 13462, loss_cls: 0.3542, loss_bbox: 0.5575, d0.loss_cls: 0.5989, d0.loss_bbox: 1.1217, d1.loss_cls: 0.3398, d1.loss_bbox: 0.5848, d2.loss_cls: 0.3317, d2.loss_bbox: 0.5099, d3.loss_cls: 0.3336, d3.loss_bbox: 0.5236, d4.loss_cls: 0.3408, d4.loss_bbox: 0.5341, loss: 6.1305, grad_norm: 17.7196
2025-06-17 03:18:34,252 - mmdet - INFO - Epoch [1][800/7033]	lr: 2.000e-04, eta: 11:00:33, time: 0.924, data_time: 0.028, memory: 13462, loss_cls: 0.3399, loss_bbox: 0.5143, d0.loss_cls: 0.5829, d0.loss_bbox: 1.1045, d1.loss_cls: 0.3286, d1.loss_bbox: 0.5348, d2.loss_cls: 0.3168, d2.loss_bbox: 0.4651, d3.loss_cls: 0.3152, d3.loss_bbox: 0.4808, d4.loss_cls: 0.3287, d4.loss_bbox: 0.4915, loss: 5.8031, grad_norm: 23.1862
2025-06-17 03:19:20,160 - mmdet - INFO - Epoch [1][850/7033]	lr: 2.000e-04, eta: 10:58:10, time: 0.918, data_time: 0.028, memory: 13462, loss_cls: 0.3491, loss_bbox: 0.4827, d0.loss_cls: 0.5728, d0.loss_bbox: 1.1055, d1.loss_cls: 0.3240, d1.loss_bbox: 0.5167, d2.loss_cls: 0.3220, d2.loss_bbox: 0.4383, d3.loss_cls: 0.3281, d3.loss_bbox: 0.4443, d4.loss_cls: 0.3351, d4.loss_bbox: 0.4581, loss: 5.6766, grad_norm: 24.0988
2025-06-17 03:20:06,279 - mmdet - INFO - Epoch [1][900/7033]	lr: 2.000e-04, eta: 10:56:07, time: 0.922, data_time: 0.028, memory: 13462, loss_cls: 0.3312, loss_bbox: 0.4545, d0.loss_cls: 0.5617, d0.loss_bbox: 1.0914, d1.loss_cls: 0.3088, d1.loss_bbox: 0.5087, d2.loss_cls: 0.3058, d2.loss_bbox: 0.4202, d3.loss_cls: 0.3131, d3.loss_bbox: 0.4239, d4.loss_cls: 0.3185, d4.loss_bbox: 0.4324, loss: 5.4701, grad_norm: 20.3645
2025-06-17 03:20:52,204 - mmdet - INFO - Epoch [1][950/7033]	lr: 2.000e-04, eta: 10:54:04, time: 0.918, data_time: 0.026, memory: 13462, loss_cls: 0.2963, loss_bbox: 0.4148, d0.loss_cls: 0.5402, d0.loss_bbox: 1.0525, d1.loss_cls: 0.2884, d1.loss_bbox: 0.4694, d2.loss_cls: 0.2832, d2.loss_bbox: 0.3842, d3.loss_cls: 0.2816, d3.loss_bbox: 0.3877, d4.loss_cls: 0.2864, d4.loss_bbox: 0.3950, loss: 5.0796, grad_norm: 17.0547
2025-06-17 03:21:38,249 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 03:21:38,249 - mmdet - INFO - Epoch [1][1000/7033]	lr: 2.000e-04, eta: 10:52:14, time: 0.921, data_time: 0.026, memory: 13462, loss_cls: 0.2961, loss_bbox: 0.3960, d0.loss_cls: 0.5243, d0.loss_bbox: 1.0511, d1.loss_cls: 0.2909, d1.loss_bbox: 0.4506, d2.loss_cls: 0.2836, d2.loss_bbox: 0.3690, d3.loss_cls: 0.2860, d3.loss_bbox: 0.3730, d4.loss_cls: 0.2902, d4.loss_bbox: 0.3775, loss: 4.9884, grad_norm: 18.5648
2025-06-17 03:22:24,395 - mmdet - INFO - Epoch [1][1050/7033]	lr: 2.000e-04, eta: 10:50:33, time: 0.923, data_time: 0.029, memory: 13462, loss_cls: 0.2968, loss_bbox: 0.4003, d0.loss_cls: 0.5309, d0.loss_bbox: 1.0525, d1.loss_cls: 0.2967, d1.loss_bbox: 0.4552, d2.loss_cls: 0.2842, d2.loss_bbox: 0.3731, d3.loss_cls: 0.2856, d3.loss_bbox: 0.3802, d4.loss_cls: 0.2888, d4.loss_bbox: 0.3882, loss: 5.0325, grad_norm: 30.8529
2025-06-17 03:23:10,520 - mmdet - INFO - Epoch [1][1100/7033]	lr: 2.000e-04, eta: 10:48:57, time: 0.922, data_time: 0.028, memory: 13462, loss_cls: 0.2933, loss_bbox: 0.3632, d0.loss_cls: 0.5063, d0.loss_bbox: 1.0076, d1.loss_cls: 0.2932, d1.loss_bbox: 0.4165, d2.loss_cls: 0.2805, d2.loss_bbox: 0.3436, d3.loss_cls: 0.2832, d3.loss_bbox: 0.3449, d4.loss_cls: 0.2895, d4.loss_bbox: 0.3480, loss: 4.7698, grad_norm: 20.1758
2025-06-17 03:23:56,764 - mmdet - INFO - Epoch [1][1150/7033]	lr: 2.000e-04, eta: 10:47:29, time: 0.925, data_time: 0.026, memory: 13462, loss_cls: 0.2883, loss_bbox: 0.3702, d0.loss_cls: 0.4714, d0.loss_bbox: 1.0161, d1.loss_cls: 0.2870, d1.loss_bbox: 0.4280, d2.loss_cls: 0.2789, d2.loss_bbox: 0.3449, d3.loss_cls: 0.2785, d3.loss_bbox: 0.3479, d4.loss_cls: 0.2813, d4.loss_bbox: 0.3545, loss: 4.7470, grad_norm: 19.0122
2025-06-17 03:24:42,777 - mmdet - INFO - Epoch [1][1200/7033]	lr: 2.000e-04, eta: 10:45:57, time: 0.920, data_time: 0.026, memory: 13462, loss_cls: 0.2844, loss_bbox: 0.3778, d0.loss_cls: 0.4458, d0.loss_bbox: 1.0174, d1.loss_cls: 0.2842, d1.loss_bbox: 0.4442, d2.loss_cls: 0.2757, d2.loss_bbox: 0.3574, d3.loss_cls: 0.2790, d3.loss_bbox: 0.3590, d4.loss_cls: 0.2848, d4.loss_bbox: 0.3607, loss: 4.7705, grad_norm: 19.4868
2025-06-17 03:25:28,867 - mmdet - INFO - Epoch [1][1250/7033]	lr: 2.000e-04, eta: 10:44:31, time: 0.922, data_time: 0.026, memory: 13462, loss_cls: 0.2805, loss_bbox: 0.3581, d0.loss_cls: 0.4179, d0.loss_bbox: 0.9623, d1.loss_cls: 0.2833, d1.loss_bbox: 0.4086, d2.loss_cls: 0.2732, d2.loss_bbox: 0.3400, d3.loss_cls: 0.2696, d3.loss_bbox: 0.3442, d4.loss_cls: 0.2735, d4.loss_bbox: 0.3463, loss: 4.5574, grad_norm: 21.3924
2025-06-17 03:26:14,986 - mmdet - INFO - Epoch [1][1300/7033]	lr: 2.000e-04, eta: 10:43:10, time: 0.922, data_time: 0.029, memory: 13462, loss_cls: 0.2701, loss_bbox: 0.3474, d0.loss_cls: 0.3818, d0.loss_bbox: 0.8741, d1.loss_cls: 0.2673, d1.loss_bbox: 0.3872, d2.loss_cls: 0.2620, d2.loss_bbox: 0.3182, d3.loss_cls: 0.2606, d3.loss_bbox: 0.3234, d4.loss_cls: 0.2655, d4.loss_bbox: 0.3307, loss: 4.2883, grad_norm: 30.6272
2025-06-17 03:27:00,829 - mmdet - INFO - Epoch [1][1350/7033]	lr: 2.000e-04, eta: 10:41:42, time: 0.917, data_time: 0.027, memory: 13462, loss_cls: 0.2514, loss_bbox: 0.3472, d0.loss_cls: 0.3236, d0.loss_bbox: 0.7952, d1.loss_cls: 0.2526, d1.loss_bbox: 0.3800, d2.loss_cls: 0.2435, d2.loss_bbox: 0.3222, d3.loss_cls: 0.2423, d3.loss_bbox: 0.3265, d4.loss_cls: 0.2450, d4.loss_bbox: 0.3331, loss: 4.0627, grad_norm: 24.8397
2025-06-17 03:27:46,953 - mmdet - INFO - Epoch [1][1400/7033]	lr: 2.000e-04, eta: 10:40:26, time: 0.922, data_time: 0.027, memory: 13462, loss_cls: 0.2584, loss_bbox: 0.3621, d0.loss_cls: 0.3040, d0.loss_bbox: 0.6868, d1.loss_cls: 0.2584, d1.loss_bbox: 0.3882, d2.loss_cls: 0.2499, d2.loss_bbox: 0.3364, d3.loss_cls: 0.2490, d3.loss_bbox: 0.3413, d4.loss_cls: 0.2542, d4.loss_bbox: 0.3485, loss: 4.0371, grad_norm: 21.3559
2025-06-17 03:28:33,273 - mmdet - INFO - Epoch [1][1450/7033]	lr: 2.000e-04, eta: 10:39:17, time: 0.926, data_time: 0.029, memory: 13462, loss_cls: 0.2434, loss_bbox: 0.3511, d0.loss_cls: 0.2664, d0.loss_bbox: 0.6168, d1.loss_cls: 0.2433, d1.loss_bbox: 0.3687, d2.loss_cls: 0.2342, d2.loss_bbox: 0.3238, d3.loss_cls: 0.2360, d3.loss_bbox: 0.3273, d4.loss_cls: 0.2375, d4.loss_bbox: 0.3366, loss: 3.7851, grad_norm: 22.1768
2025-06-17 03:29:19,303 - mmdet - INFO - Epoch [1][1500/7033]	lr: 2.000e-04, eta: 10:38:02, time: 0.921, data_time: 0.028, memory: 13462, loss_cls: 0.2333, loss_bbox: 0.3291, d0.loss_cls: 0.2803, d0.loss_bbox: 0.6000, d1.loss_cls: 0.2363, d1.loss_bbox: 0.3562, d2.loss_cls: 0.2265, d2.loss_bbox: 0.3116, d3.loss_cls: 0.2272, d3.loss_bbox: 0.3138, d4.loss_cls: 0.2294, d4.loss_bbox: 0.3164, loss: 3.6599, grad_norm: 28.6272
2025-06-17 03:30:05,335 - mmdet - INFO - Epoch [1][1550/7033]	lr: 2.000e-04, eta: 10:36:48, time: 0.921, data_time: 0.027, memory: 13462, loss_cls: 0.2243, loss_bbox: 0.3260, d0.loss_cls: 0.2539, d0.loss_bbox: 0.5661, d1.loss_cls: 0.2227, d1.loss_bbox: 0.3503, d2.loss_cls: 0.2178, d2.loss_bbox: 0.3101, d3.loss_cls: 0.2178, d3.loss_bbox: 0.3137, d4.loss_cls: 0.2201, d4.loss_bbox: 0.3158, loss: 3.5386, grad_norm: 21.9387
2025-06-17 03:30:51,501 - mmdet - INFO - Epoch [1][1600/7033]	lr: 2.000e-04, eta: 10:35:40, time: 0.923, data_time: 0.027, memory: 13462, loss_cls: 0.2191, loss_bbox: 0.3379, d0.loss_cls: 0.2379, d0.loss_bbox: 0.5445, d1.loss_cls: 0.2165, d1.loss_bbox: 0.3517, d2.loss_cls: 0.2101, d2.loss_bbox: 0.3151, d3.loss_cls: 0.2126, d3.loss_bbox: 0.3195, d4.loss_cls: 0.2160, d4.loss_bbox: 0.3237, loss: 3.5045, grad_norm: 27.4137
2025-06-17 03:31:37,620 - mmdet - INFO - Epoch [1][1650/7033]	lr: 2.000e-04, eta: 10:34:32, time: 0.922, data_time: 0.028, memory: 13462, loss_cls: 0.2108, loss_bbox: 0.3202, d0.loss_cls: 0.2313, d0.loss_bbox: 0.5189, d1.loss_cls: 0.2106, d1.loss_bbox: 0.3468, d2.loss_cls: 0.2036, d2.loss_bbox: 0.3086, d3.loss_cls: 0.2061, d3.loss_bbox: 0.3080, d4.loss_cls: 0.2067, d4.loss_bbox: 0.3107, loss: 3.3823, grad_norm: 27.1955
2025-06-17 03:32:23,739 - mmdet - INFO - Epoch [1][1700/7033]	lr: 2.000e-04, eta: 10:33:25, time: 0.922, data_time: 0.028, memory: 13462, loss_cls: 0.2195, loss_bbox: 0.3315, d0.loss_cls: 0.2367, d0.loss_bbox: 0.5385, d1.loss_cls: 0.2175, d1.loss_bbox: 0.3447, d2.loss_cls: 0.2140, d2.loss_bbox: 0.3031, d3.loss_cls: 0.2123, d3.loss_bbox: 0.3101, d4.loss_cls: 0.2164, d4.loss_bbox: 0.3155, loss: 3.4598, grad_norm: 26.4172
2025-06-17 03:33:10,270 - mmdet - INFO - Epoch [1][1750/7033]	lr: 2.000e-04, eta: 10:32:29, time: 0.931, data_time: 0.028, memory: 13491, loss_cls: 0.2028, loss_bbox: 0.3250, d0.loss_cls: 0.2221, d0.loss_bbox: 0.4981, d1.loss_cls: 0.1957, d1.loss_bbox: 0.3329, d2.loss_cls: 0.1944, d2.loss_bbox: 0.2978, d3.loss_cls: 0.1945, d3.loss_bbox: 0.3040, d4.loss_cls: 0.2001, d4.loss_bbox: 0.3112, loss: 3.2787, grad_norm: 25.3396
2025-06-17 03:33:56,159 - mmdet - INFO - Epoch [1][1800/7033]	lr: 2.000e-04, eta: 10:31:20, time: 0.918, data_time: 0.026, memory: 13491, loss_cls: 0.1955, loss_bbox: 0.3208, d0.loss_cls: 0.2218, d0.loss_bbox: 0.4912, d1.loss_cls: 0.1956, d1.loss_bbox: 0.3210, d2.loss_cls: 0.1939, d2.loss_bbox: 0.2899, d3.loss_cls: 0.1911, d3.loss_bbox: 0.2978, d4.loss_cls: 0.1932, d4.loss_bbox: 0.3057, loss: 3.2176, grad_norm: 22.0968
2025-06-17 03:34:42,345 - mmdet - INFO - Epoch [1][1850/7033]	lr: 2.000e-04, eta: 10:30:17, time: 0.924, data_time: 0.026, memory: 13491, loss_cls: 0.1960, loss_bbox: 0.2995, d0.loss_cls: 0.2149, d0.loss_bbox: 0.4691, d1.loss_cls: 0.1915, d1.loss_bbox: 0.3162, d2.loss_cls: 0.1937, d2.loss_bbox: 0.2788, d3.loss_cls: 0.1932, d3.loss_bbox: 0.2831, d4.loss_cls: 0.1921, d4.loss_bbox: 0.2883, loss: 3.1163, grad_norm: 27.2220
2025-06-17 03:35:28,301 - mmdet - INFO - Epoch [1][1900/7033]	lr: 2.000e-04, eta: 10:29:11, time: 0.919, data_time: 0.026, memory: 13491, loss_cls: 0.1974, loss_bbox: 0.3045, d0.loss_cls: 0.2129, d0.loss_bbox: 0.4621, d1.loss_cls: 0.1943, d1.loss_bbox: 0.3166, d2.loss_cls: 0.1919, d2.loss_bbox: 0.2844, d3.loss_cls: 0.1930, d3.loss_bbox: 0.2869, d4.loss_cls: 0.1944, d4.loss_bbox: 0.2926, loss: 3.1312, grad_norm: 26.1436
2025-06-17 03:36:14,622 - mmdet - INFO - Epoch [1][1950/7033]	lr: 2.000e-04, eta: 10:28:14, time: 0.926, data_time: 0.028, memory: 13491, loss_cls: 0.1906, loss_bbox: 0.3076, d0.loss_cls: 0.2164, d0.loss_bbox: 0.4666, d1.loss_cls: 0.1877, d1.loss_bbox: 0.3207, d2.loss_cls: 0.1855, d2.loss_bbox: 0.2850, d3.loss_cls: 0.1852, d3.loss_bbox: 0.2857, d4.loss_cls: 0.1858, d4.loss_bbox: 0.2928, loss: 3.1097, grad_norm: 23.9905
2025-06-17 03:37:00,766 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 03:37:00,767 - mmdet - INFO - Epoch [1][2000/7033]	lr: 2.000e-04, eta: 10:27:13, time: 0.923, data_time: 0.026, memory: 13491, loss_cls: 0.2066, loss_bbox: 0.3216, d0.loss_cls: 0.2289, d0.loss_bbox: 0.4906, d1.loss_cls: 0.2026, d1.loss_bbox: 0.3353, d2.loss_cls: 0.2007, d2.loss_bbox: 0.2937, d3.loss_cls: 0.2006, d3.loss_bbox: 0.2985, d4.loss_cls: 0.2022, d4.loss_bbox: 0.3067, loss: 3.2881, grad_norm: 25.5459
2025-06-17 03:37:46,697 - mmdet - INFO - Epoch [1][2050/7033]	lr: 2.000e-04, eta: 10:26:09, time: 0.919, data_time: 0.027, memory: 13491, loss_cls: 0.1882, loss_bbox: 0.3169, d0.loss_cls: 0.2101, d0.loss_bbox: 0.4580, d1.loss_cls: 0.1855, d1.loss_bbox: 0.3173, d2.loss_cls: 0.1807, d2.loss_bbox: 0.2884, d3.loss_cls: 0.1812, d3.loss_bbox: 0.2942, d4.loss_cls: 0.1838, d4.loss_bbox: 0.3012, loss: 3.1055, grad_norm: 27.7668
2025-06-17 03:38:32,608 - mmdet - INFO - Epoch [1][2100/7033]	lr: 2.000e-04, eta: 10:25:06, time: 0.918, data_time: 0.027, memory: 13491, loss_cls: 0.1927, loss_bbox: 0.3059, d0.loss_cls: 0.2107, d0.loss_bbox: 0.4585, d1.loss_cls: 0.1853, d1.loss_bbox: 0.3139, d2.loss_cls: 0.1869, d2.loss_bbox: 0.2826, d3.loss_cls: 0.1885, d3.loss_bbox: 0.2863, d4.loss_cls: 0.1889, d4.loss_bbox: 0.2923, loss: 3.0926, grad_norm: 24.1843
2025-06-17 03:39:18,791 - mmdet - INFO - Epoch [1][2150/7033]	lr: 2.000e-04, eta: 10:24:08, time: 0.924, data_time: 0.029, memory: 13491, loss_cls: 0.1950, loss_bbox: 0.3011, d0.loss_cls: 0.2176, d0.loss_bbox: 0.4572, d1.loss_cls: 0.1869, d1.loss_bbox: 0.3105, d2.loss_cls: 0.1891, d2.loss_bbox: 0.2784, d3.loss_cls: 0.1874, d3.loss_bbox: 0.2819, d4.loss_cls: 0.1914, d4.loss_bbox: 0.2859, loss: 3.0824, grad_norm: 26.6993
2025-06-17 03:40:04,955 - mmdet - INFO - Epoch [1][2200/7033]	lr: 2.000e-04, eta: 10:23:10, time: 0.923, data_time: 0.030, memory: 13491, loss_cls: 0.1797, loss_bbox: 0.3081, d0.loss_cls: 0.2044, d0.loss_bbox: 0.4490, d1.loss_cls: 0.1815, d1.loss_bbox: 0.3078, d2.loss_cls: 0.1786, d2.loss_bbox: 0.2831, d3.loss_cls: 0.1754, d3.loss_bbox: 0.2882, d4.loss_cls: 0.1770, d4.loss_bbox: 0.2920, loss: 3.0247, grad_norm: 26.6592
2025-06-17 03:40:51,012 - mmdet - INFO - Epoch [1][2250/7033]	lr: 2.000e-04, eta: 10:22:12, time: 0.921, data_time: 0.027, memory: 13491, loss_cls: 0.1846, loss_bbox: 0.2948, d0.loss_cls: 0.2035, d0.loss_bbox: 0.4401, d1.loss_cls: 0.1799, d1.loss_bbox: 0.3079, d2.loss_cls: 0.1794, d2.loss_bbox: 0.2807, d3.loss_cls: 0.1804, d3.loss_bbox: 0.2814, d4.loss_cls: 0.1804, d4.loss_bbox: 0.2852, loss: 2.9983, grad_norm: 23.8444
2025-06-17 03:41:37,015 - mmdet - INFO - Epoch [1][2300/7033]	lr: 2.000e-04, eta: 10:21:12, time: 0.920, data_time: 0.028, memory: 13491, loss_cls: 0.1740, loss_bbox: 0.2896, d0.loss_cls: 0.2081, d0.loss_bbox: 0.4234, d1.loss_cls: 0.1765, d1.loss_bbox: 0.2940, d2.loss_cls: 0.1736, d2.loss_bbox: 0.2692, d3.loss_cls: 0.1729, d3.loss_bbox: 0.2708, d4.loss_cls: 0.1737, d4.loss_bbox: 0.2768, loss: 2.9027, grad_norm: 23.7933
2025-06-17 03:42:23,397 - mmdet - INFO - Epoch [1][2350/7033]	lr: 2.000e-04, eta: 10:20:20, time: 0.928, data_time: 0.030, memory: 13570, loss_cls: 0.1762, loss_bbox: 0.2987, d0.loss_cls: 0.2025, d0.loss_bbox: 0.4398, d1.loss_cls: 0.1730, d1.loss_bbox: 0.3095, d2.loss_cls: 0.1696, d2.loss_bbox: 0.2802, d3.loss_cls: 0.1690, d3.loss_bbox: 0.2827, d4.loss_cls: 0.1726, d4.loss_bbox: 0.2867, loss: 2.9604, grad_norm: 27.9628
2025-06-17 03:43:09,791 - mmdet - INFO - Epoch [1][2400/7033]	lr: 2.000e-04, eta: 10:19:28, time: 0.928, data_time: 0.029, memory: 13570, loss_cls: 0.1758, loss_bbox: 0.2981, d0.loss_cls: 0.1987, d0.loss_bbox: 0.4349, d1.loss_cls: 0.1711, d1.loss_bbox: 0.3090, d2.loss_cls: 0.1732, d2.loss_bbox: 0.2806, d3.loss_cls: 0.1719, d3.loss_bbox: 0.2810, d4.loss_cls: 0.1731, d4.loss_bbox: 0.2853, loss: 2.9527, grad_norm: 26.1486
2025-06-17 03:43:55,925 - mmdet - INFO - Epoch [1][2450/7033]	lr: 2.000e-04, eta: 10:18:32, time: 0.923, data_time: 0.029, memory: 13570, loss_cls: 0.1764, loss_bbox: 0.2944, d0.loss_cls: 0.2070, d0.loss_bbox: 0.4268, d1.loss_cls: 0.1727, d1.loss_bbox: 0.2982, d2.loss_cls: 0.1718, d2.loss_bbox: 0.2746, d3.loss_cls: 0.1709, d3.loss_bbox: 0.2781, d4.loss_cls: 0.1721, d4.loss_bbox: 0.2834, loss: 2.9266, grad_norm: 24.2915
2025-06-17 03:44:41,893 - mmdet - INFO - Epoch [1][2500/7033]	lr: 2.000e-04, eta: 10:17:34, time: 0.919, data_time: 0.026, memory: 13570, loss_cls: 0.1774, loss_bbox: 0.3013, d0.loss_cls: 0.2002, d0.loss_bbox: 0.4392, d1.loss_cls: 0.1722, d1.loss_bbox: 0.3008, d2.loss_cls: 0.1714, d2.loss_bbox: 0.2750, d3.loss_cls: 0.1712, d3.loss_bbox: 0.2775, d4.loss_cls: 0.1734, d4.loss_bbox: 0.2854, loss: 2.9448, grad_norm: 27.0719
2025-06-17 03:45:27,687 - mmdet - INFO - Epoch [1][2550/7033]	lr: 2.000e-04, eta: 10:16:34, time: 0.916, data_time: 0.026, memory: 13570, loss_cls: 0.1730, loss_bbox: 0.2890, d0.loss_cls: 0.1932, d0.loss_bbox: 0.4232, d1.loss_cls: 0.1731, d1.loss_bbox: 0.2932, d2.loss_cls: 0.1697, d2.loss_bbox: 0.2701, d3.loss_cls: 0.1693, d3.loss_bbox: 0.2690, d4.loss_cls: 0.1680, d4.loss_bbox: 0.2797, loss: 2.8704, grad_norm: 37.7428
2025-06-17 03:46:13,755 - mmdet - INFO - Epoch [1][2600/7033]	lr: 2.000e-04, eta: 10:15:38, time: 0.921, data_time: 0.027, memory: 13570, loss_cls: 0.1867, loss_bbox: 0.2953, d0.loss_cls: 0.2092, d0.loss_bbox: 0.4471, d1.loss_cls: 0.1886, d1.loss_bbox: 0.3021, d2.loss_cls: 0.1855, d2.loss_bbox: 0.2712, d3.loss_cls: 0.1840, d3.loss_bbox: 0.2759, d4.loss_cls: 0.1831, d4.loss_bbox: 0.2829, loss: 3.0117, grad_norm: 23.5735
2025-06-17 03:46:59,994 - mmdet - INFO - Epoch [1][2650/7033]	lr: 2.000e-04, eta: 10:14:46, time: 0.925, data_time: 0.029, memory: 13570, loss_cls: 0.1682, loss_bbox: 0.3132, d0.loss_cls: 0.1989, d0.loss_bbox: 0.4353, d1.loss_cls: 0.1673, d1.loss_bbox: 0.3081, d2.loss_cls: 0.1656, d2.loss_bbox: 0.2882, d3.loss_cls: 0.1643, d3.loss_bbox: 0.2944, d4.loss_cls: 0.1654, d4.loss_bbox: 0.3014, loss: 2.9705, grad_norm: 24.9023
2025-06-17 03:47:46,067 - mmdet - INFO - Epoch [1][2700/7033]	lr: 2.000e-04, eta: 10:13:51, time: 0.921, data_time: 0.027, memory: 13570, loss_cls: 0.1703, loss_bbox: 0.2810, d0.loss_cls: 0.1920, d0.loss_bbox: 0.4173, d1.loss_cls: 0.1668, d1.loss_bbox: 0.2905, d2.loss_cls: 0.1669, d2.loss_bbox: 0.2658, d3.loss_cls: 0.1662, d3.loss_bbox: 0.2655, d4.loss_cls: 0.1666, d4.loss_bbox: 0.2720, loss: 2.8209, grad_norm: 27.7881
2025-06-17 03:48:32,060 - mmdet - INFO - Epoch [1][2750/7033]	lr: 2.000e-04, eta: 10:12:55, time: 0.920, data_time: 0.029, memory: 13570, loss_cls: 0.1731, loss_bbox: 0.2918, d0.loss_cls: 0.1977, d0.loss_bbox: 0.4308, d1.loss_cls: 0.1746, d1.loss_bbox: 0.3007, d2.loss_cls: 0.1724, d2.loss_bbox: 0.2789, d3.loss_cls: 0.1705, d3.loss_bbox: 0.2801, d4.loss_cls: 0.1725, d4.loss_bbox: 0.2825, loss: 2.9256, grad_norm: 24.2411
2025-06-17 03:49:18,344 - mmdet - INFO - Epoch [1][2800/7033]	lr: 2.000e-04, eta: 10:12:04, time: 0.926, data_time: 0.030, memory: 13570, loss_cls: 0.1635, loss_bbox: 0.2891, d0.loss_cls: 0.1939, d0.loss_bbox: 0.4151, d1.loss_cls: 0.1654, d1.loss_bbox: 0.2888, d2.loss_cls: 0.1646, d2.loss_bbox: 0.2654, d3.loss_cls: 0.1632, d3.loss_bbox: 0.2679, d4.loss_cls: 0.1612, d4.loss_bbox: 0.2779, loss: 2.8160, grad_norm: 23.1762
2025-06-17 03:50:04,762 - mmdet - INFO - Epoch [1][2850/7033]	lr: 2.000e-04, eta: 10:11:15, time: 0.928, data_time: 0.030, memory: 13570, loss_cls: 0.1683, loss_bbox: 0.2886, d0.loss_cls: 0.1940, d0.loss_bbox: 0.4094, d1.loss_cls: 0.1656, d1.loss_bbox: 0.2873, d2.loss_cls: 0.1649, d2.loss_bbox: 0.2661, d3.loss_cls: 0.1639, d3.loss_bbox: 0.2674, d4.loss_cls: 0.1654, d4.loss_bbox: 0.2755, loss: 2.8165, grad_norm: 22.6832
2025-06-17 03:50:50,850 - mmdet - INFO - Epoch [1][2900/7033]	lr: 2.000e-04, eta: 10:10:21, time: 0.922, data_time: 0.027, memory: 13570, loss_cls: 0.1558, loss_bbox: 0.2723, d0.loss_cls: 0.1885, d0.loss_bbox: 0.3935, d1.loss_cls: 0.1571, d1.loss_bbox: 0.2728, d2.loss_cls: 0.1542, d2.loss_bbox: 0.2553, d3.loss_cls: 0.1514, d3.loss_bbox: 0.2544, d4.loss_cls: 0.1521, d4.loss_bbox: 0.2610, loss: 2.6684, grad_norm: 24.0110
2025-06-17 03:51:36,909 - mmdet - INFO - Epoch [1][2950/7033]	lr: 2.000e-04, eta: 10:09:27, time: 0.921, data_time: 0.027, memory: 13570, loss_cls: 0.1731, loss_bbox: 0.2952, d0.loss_cls: 0.2024, d0.loss_bbox: 0.4194, d1.loss_cls: 0.1734, d1.loss_bbox: 0.2964, d2.loss_cls: 0.1719, d2.loss_bbox: 0.2728, d3.loss_cls: 0.1689, d3.loss_bbox: 0.2753, d4.loss_cls: 0.1690, d4.loss_bbox: 0.2823, loss: 2.9001, grad_norm: 27.7526
2025-06-17 03:52:23,101 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 03:52:23,101 - mmdet - INFO - Epoch [1][3000/7033]	lr: 2.000e-04, eta: 10:08:36, time: 0.924, data_time: 0.031, memory: 13570, loss_cls: 0.1653, loss_bbox: 0.2999, d0.loss_cls: 0.2018, d0.loss_bbox: 0.4239, d1.loss_cls: 0.1747, d1.loss_bbox: 0.2995, d2.loss_cls: 0.1682, d2.loss_bbox: 0.2811, d3.loss_cls: 0.1627, d3.loss_bbox: 0.2842, d4.loss_cls: 0.1630, d4.loss_bbox: 0.2910, loss: 2.9153, grad_norm: 25.2317
2025-06-17 03:53:09,461 - mmdet - INFO - Epoch [1][3050/7033]	lr: 2.000e-04, eta: 10:07:46, time: 0.927, data_time: 0.029, memory: 13570, loss_cls: 0.1596, loss_bbox: 0.2865, d0.loss_cls: 0.1960, d0.loss_bbox: 0.4197, d1.loss_cls: 0.1658, d1.loss_bbox: 0.3008, d2.loss_cls: 0.1636, d2.loss_bbox: 0.2746, d3.loss_cls: 0.1574, d3.loss_bbox: 0.2747, d4.loss_cls: 0.1578, d4.loss_bbox: 0.2768, loss: 2.8333, grad_norm: 29.3016
2025-06-17 03:53:55,666 - mmdet - INFO - Epoch [1][3100/7033]	lr: 2.000e-04, eta: 10:06:55, time: 0.924, data_time: 0.026, memory: 13570, loss_cls: 0.1502, loss_bbox: 0.2902, d0.loss_cls: 0.1915, d0.loss_bbox: 0.4051, d1.loss_cls: 0.1548, d1.loss_bbox: 0.2892, d2.loss_cls: 0.1530, d2.loss_bbox: 0.2672, d3.loss_cls: 0.1476, d3.loss_bbox: 0.2707, d4.loss_cls: 0.1477, d4.loss_bbox: 0.2774, loss: 2.7446, grad_norm: 24.7782
2025-06-17 03:54:42,084 - mmdet - INFO - Epoch [1][3150/7033]	lr: 2.000e-04, eta: 10:06:07, time: 0.928, data_time: 0.029, memory: 13570, loss_cls: 0.1644, loss_bbox: 0.2991, d0.loss_cls: 0.2007, d0.loss_bbox: 0.4341, d1.loss_cls: 0.1720, d1.loss_bbox: 0.3069, d2.loss_cls: 0.1684, d2.loss_bbox: 0.2813, d3.loss_cls: 0.1654, d3.loss_bbox: 0.2819, d4.loss_cls: 0.1643, d4.loss_bbox: 0.2864, loss: 2.9248, grad_norm: 29.3007
2025-06-17 03:55:28,416 - mmdet - INFO - Epoch [1][3200/7033]	lr: 2.000e-04, eta: 10:05:17, time: 0.927, data_time: 0.027, memory: 13570, loss_cls: 0.1616, loss_bbox: 0.2871, d0.loss_cls: 0.1949, d0.loss_bbox: 0.4176, d1.loss_cls: 0.1636, d1.loss_bbox: 0.2942, d2.loss_cls: 0.1644, d2.loss_bbox: 0.2710, d3.loss_cls: 0.1604, d3.loss_bbox: 0.2699, d4.loss_cls: 0.1595, d4.loss_bbox: 0.2752, loss: 2.8193, grad_norm: 22.6390
2025-06-17 03:56:14,461 - mmdet - INFO - Epoch [1][3250/7033]	lr: 2.000e-04, eta: 10:04:24, time: 0.921, data_time: 0.029, memory: 13570, loss_cls: 0.1578, loss_bbox: 0.2797, d0.loss_cls: 0.1893, d0.loss_bbox: 0.4145, d1.loss_cls: 0.1612, d1.loss_bbox: 0.2868, d2.loss_cls: 0.1580, d2.loss_bbox: 0.2638, d3.loss_cls: 0.1530, d3.loss_bbox: 0.2656, d4.loss_cls: 0.1539, d4.loss_bbox: 0.2702, loss: 2.7539, grad_norm: 24.1554
2025-06-17 03:57:00,774 - mmdet - INFO - Epoch [1][3300/7033]	lr: 2.000e-04, eta: 10:03:35, time: 0.926, data_time: 0.028, memory: 13570, loss_cls: 0.1556, loss_bbox: 0.2855, d0.loss_cls: 0.1929, d0.loss_bbox: 0.4201, d1.loss_cls: 0.1640, d1.loss_bbox: 0.2948, d2.loss_cls: 0.1607, d2.loss_bbox: 0.2661, d3.loss_cls: 0.1537, d3.loss_bbox: 0.2688, d4.loss_cls: 0.1541, d4.loss_bbox: 0.2745, loss: 2.7908, grad_norm: 25.8168
2025-06-17 03:57:46,706 - mmdet - INFO - Epoch [1][3350/7033]	lr: 2.000e-04, eta: 10:02:41, time: 0.919, data_time: 0.026, memory: 13570, loss_cls: 0.1589, loss_bbox: 0.2839, d0.loss_cls: 0.1919, d0.loss_bbox: 0.4216, d1.loss_cls: 0.1650, d1.loss_bbox: 0.2956, d2.loss_cls: 0.1615, d2.loss_bbox: 0.2720, d3.loss_cls: 0.1552, d3.loss_bbox: 0.2690, d4.loss_cls: 0.1561, d4.loss_bbox: 0.2727, loss: 2.8033, grad_norm: 28.0554
2025-06-17 03:58:32,981 - mmdet - INFO - Epoch [1][3400/7033]	lr: 2.000e-04, eta: 10:01:52, time: 0.925, data_time: 0.026, memory: 13570, loss_cls: 0.1585, loss_bbox: 0.2907, d0.loss_cls: 0.1936, d0.loss_bbox: 0.4206, d1.loss_cls: 0.1675, d1.loss_bbox: 0.2952, d2.loss_cls: 0.1647, d2.loss_bbox: 0.2731, d3.loss_cls: 0.1581, d3.loss_bbox: 0.2746, d4.loss_cls: 0.1590, d4.loss_bbox: 0.2796, loss: 2.8353, grad_norm: 25.3675
2025-06-17 03:59:19,345 - mmdet - INFO - Epoch [1][3450/7033]	lr: 2.000e-04, eta: 10:01:03, time: 0.927, data_time: 0.030, memory: 13570, loss_cls: 0.1399, loss_bbox: 0.2775, d0.loss_cls: 0.1810, d0.loss_bbox: 0.4117, d1.loss_cls: 0.1536, d1.loss_bbox: 0.2884, d2.loss_cls: 0.1495, d2.loss_bbox: 0.2633, d3.loss_cls: 0.1424, d3.loss_bbox: 0.2635, d4.loss_cls: 0.1387, d4.loss_bbox: 0.2688, loss: 2.6784, grad_norm: 26.4797
2025-06-17 04:00:05,449 - mmdet - INFO - Epoch [1][3500/7033]	lr: 2.000e-04, eta: 10:00:12, time: 0.922, data_time: 0.027, memory: 13570, loss_cls: 0.1527, loss_bbox: 0.2818, d0.loss_cls: 0.1948, d0.loss_bbox: 0.3983, d1.loss_cls: 0.1606, d1.loss_bbox: 0.2800, d2.loss_cls: 0.1585, d2.loss_bbox: 0.2595, d3.loss_cls: 0.1510, d3.loss_bbox: 0.2640, d4.loss_cls: 0.1523, d4.loss_bbox: 0.2702, loss: 2.7238, grad_norm: 24.5377
2025-06-17 04:00:51,494 - mmdet - INFO - Epoch [1][3550/7033]	lr: 2.000e-04, eta: 9:59:20, time: 0.921, data_time: 0.026, memory: 13570, loss_cls: 0.1482, loss_bbox: 0.2769, d0.loss_cls: 0.1918, d0.loss_bbox: 0.3984, d1.loss_cls: 0.1638, d1.loss_bbox: 0.2814, d2.loss_cls: 0.1582, d2.loss_bbox: 0.2596, d3.loss_cls: 0.1483, d3.loss_bbox: 0.2621, d4.loss_cls: 0.1471, d4.loss_bbox: 0.2649, loss: 2.7009, grad_norm: 40.8659
2025-06-17 04:01:37,671 - mmdet - INFO - Epoch [1][3600/7033]	lr: 2.000e-04, eta: 9:58:30, time: 0.923, data_time: 0.027, memory: 13570, loss_cls: 0.1461, loss_bbox: 0.2791, d0.loss_cls: 0.1903, d0.loss_bbox: 0.3909, d1.loss_cls: 0.1612, d1.loss_bbox: 0.2767, d2.loss_cls: 0.1559, d2.loss_bbox: 0.2578, d3.loss_cls: 0.1490, d3.loss_bbox: 0.2593, d4.loss_cls: 0.1461, d4.loss_bbox: 0.2686, loss: 2.6810, grad_norm: 29.2914
2025-06-17 04:02:24,031 - mmdet - INFO - Epoch [1][3650/7033]	lr: 2.000e-04, eta: 9:57:42, time: 0.927, data_time: 0.031, memory: 13570, loss_cls: 0.1536, loss_bbox: 0.2800, d0.loss_cls: 0.1955, d0.loss_bbox: 0.4087, d1.loss_cls: 0.1667, d1.loss_bbox: 0.2870, d2.loss_cls: 0.1609, d2.loss_bbox: 0.2674, d3.loss_cls: 0.1524, d3.loss_bbox: 0.2648, d4.loss_cls: 0.1524, d4.loss_bbox: 0.2681, loss: 2.7574, grad_norm: 33.2316
2025-06-17 04:03:14,249 - mmdet - INFO - Epoch [1][3700/7033]	lr: 2.000e-04, eta: 9:57:34, time: 1.004, data_time: 0.030, memory: 13570, loss_cls: 0.1559, loss_bbox: 0.2776, d0.loss_cls: 0.1924, d0.loss_bbox: 0.4119, d1.loss_cls: 0.1663, d1.loss_bbox: 0.2860, d2.loss_cls: 0.1609, d2.loss_bbox: 0.2629, d3.loss_cls: 0.1532, d3.loss_bbox: 0.2617, d4.loss_cls: 0.1546, d4.loss_bbox: 0.2660, loss: 2.7494, grad_norm: 24.2813
2025-06-17 04:04:00,053 - mmdet - INFO - Epoch [1][3750/7033]	lr: 2.000e-04, eta: 9:56:39, time: 0.916, data_time: 0.027, memory: 13570, loss_cls: 0.1536, loss_bbox: 0.2746, d0.loss_cls: 0.1839, d0.loss_bbox: 0.3928, d1.loss_cls: 0.1625, d1.loss_bbox: 0.2788, d2.loss_cls: 0.1572, d2.loss_bbox: 0.2615, d3.loss_cls: 0.1498, d3.loss_bbox: 0.2608, d4.loss_cls: 0.1498, d4.loss_bbox: 0.2651, loss: 2.6906, grad_norm: 32.9722
2025-06-17 04:04:46,470 - mmdet - INFO - Epoch [1][3800/7033]	lr: 2.000e-04, eta: 9:55:51, time: 0.928, data_time: 0.027, memory: 13570, loss_cls: 0.1454, loss_bbox: 0.2788, d0.loss_cls: 0.1867, d0.loss_bbox: 0.3961, d1.loss_cls: 0.1601, d1.loss_bbox: 0.2798, d2.loss_cls: 0.1559, d2.loss_bbox: 0.2618, d3.loss_cls: 0.1465, d3.loss_bbox: 0.2645, d4.loss_cls: 0.1447, d4.loss_bbox: 0.2708, loss: 2.6912, grad_norm: 27.9203
2025-06-17 04:05:32,890 - mmdet - INFO - Epoch [1][3850/7033]	lr: 2.000e-04, eta: 9:55:03, time: 0.928, data_time: 0.031, memory: 13570, loss_cls: 0.1476, loss_bbox: 0.2760, d0.loss_cls: 0.1907, d0.loss_bbox: 0.4051, d1.loss_cls: 0.1621, d1.loss_bbox: 0.2863, d2.loss_cls: 0.1559, d2.loss_bbox: 0.2619, d3.loss_cls: 0.1463, d3.loss_bbox: 0.2611, d4.loss_cls: 0.1463, d4.loss_bbox: 0.2659, loss: 2.7052, grad_norm: 26.0500
2025-06-17 04:06:19,245 - mmdet - INFO - Epoch [1][3900/7033]	lr: 2.000e-04, eta: 9:54:15, time: 0.927, data_time: 0.030, memory: 13570, loss_cls: 0.1476, loss_bbox: 0.2730, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3857, d1.loss_cls: 0.1559, d1.loss_bbox: 0.2774, d2.loss_cls: 0.1547, d2.loss_bbox: 0.2581, d3.loss_cls: 0.1451, d3.loss_bbox: 0.2598, d4.loss_cls: 0.1459, d4.loss_bbox: 0.2625, loss: 2.6526, grad_norm: 24.4877
2025-06-17 04:07:05,290 - mmdet - INFO - Epoch [1][3950/7033]	lr: 2.000e-04, eta: 9:53:23, time: 0.921, data_time: 0.026, memory: 13570, loss_cls: 0.1439, loss_bbox: 0.2748, d0.loss_cls: 0.1901, d0.loss_bbox: 0.4100, d1.loss_cls: 0.1561, d1.loss_bbox: 0.2853, d2.loss_cls: 0.1515, d2.loss_bbox: 0.2619, d3.loss_cls: 0.1432, d3.loss_bbox: 0.2621, d4.loss_cls: 0.1419, d4.loss_bbox: 0.2666, loss: 2.6874, grad_norm: 23.4207
2025-06-17 04:07:53,446 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 04:07:53,446 - mmdet - INFO - Epoch [1][4000/7033]	lr: 2.000e-04, eta: 9:52:52, time: 0.963, data_time: 0.027, memory: 13570, loss_cls: 0.1458, loss_bbox: 0.2765, d0.loss_cls: 0.1877, d0.loss_bbox: 0.3937, d1.loss_cls: 0.1602, d1.loss_bbox: 0.2789, d2.loss_cls: 0.1532, d2.loss_bbox: 0.2601, d3.loss_cls: 0.1432, d3.loss_bbox: 0.2618, d4.loss_cls: 0.1457, d4.loss_bbox: 0.2647, loss: 2.6714, grad_norm: 30.5194
2025-06-17 04:08:39,430 - mmdet - INFO - Epoch [1][4050/7033]	lr: 2.000e-04, eta: 9:52:00, time: 0.920, data_time: 0.028, memory: 13570, loss_cls: 0.1420, loss_bbox: 0.2673, d0.loss_cls: 0.1925, d0.loss_bbox: 0.3995, d1.loss_cls: 0.1567, d1.loss_bbox: 0.2815, d2.loss_cls: 0.1519, d2.loss_bbox: 0.2597, d3.loss_cls: 0.1423, d3.loss_bbox: 0.2574, d4.loss_cls: 0.1420, d4.loss_bbox: 0.2603, loss: 2.6532, grad_norm: 24.3908
2025-06-17 04:09:25,745 - mmdet - INFO - Epoch [1][4100/7033]	lr: 2.000e-04, eta: 9:51:11, time: 0.926, data_time: 0.030, memory: 13570, loss_cls: 0.1382, loss_bbox: 0.2893, d0.loss_cls: 0.1901, d0.loss_bbox: 0.4045, d1.loss_cls: 0.1530, d1.loss_bbox: 0.2933, d2.loss_cls: 0.1469, d2.loss_bbox: 0.2713, d3.loss_cls: 0.1406, d3.loss_bbox: 0.2729, d4.loss_cls: 0.1371, d4.loss_bbox: 0.2803, loss: 2.7173, grad_norm: 33.2145
2025-06-17 04:10:12,186 - mmdet - INFO - Epoch [1][4150/7033]	lr: 2.000e-04, eta: 9:50:24, time: 0.929, data_time: 0.030, memory: 13570, loss_cls: 0.1294, loss_bbox: 0.2779, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3913, d1.loss_cls: 0.1525, d1.loss_bbox: 0.2766, d2.loss_cls: 0.1423, d2.loss_bbox: 0.2560, d3.loss_cls: 0.1308, d3.loss_bbox: 0.2577, d4.loss_cls: 0.1300, d4.loss_bbox: 0.2649, loss: 2.5906, grad_norm: 29.7436
2025-06-17 04:10:58,333 - mmdet - INFO - Epoch [1][4200/7033]	lr: 2.000e-04, eta: 9:49:34, time: 0.923, data_time: 0.027, memory: 13570, loss_cls: 0.1348, loss_bbox: 0.2806, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3974, d1.loss_cls: 0.1521, d1.loss_bbox: 0.2920, d2.loss_cls: 0.1456, d2.loss_bbox: 0.2671, d3.loss_cls: 0.1354, d3.loss_bbox: 0.2656, d4.loss_cls: 0.1341, d4.loss_bbox: 0.2682, loss: 2.6568, grad_norm: 25.8553
2025-06-17 04:11:44,631 - mmdet - INFO - Epoch [1][4250/7033]	lr: 2.000e-04, eta: 9:48:45, time: 0.926, data_time: 0.029, memory: 13570, loss_cls: 0.1305, loss_bbox: 0.2638, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1513, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1440, d2.loss_bbox: 0.2505, d3.loss_cls: 0.1331, d3.loss_bbox: 0.2507, d4.loss_cls: 0.1313, d4.loss_bbox: 0.2555, loss: 2.5431, grad_norm: 24.5921
2025-06-17 04:12:31,058 - mmdet - INFO - Epoch [1][4300/7033]	lr: 2.000e-04, eta: 9:47:57, time: 0.929, data_time: 0.029, memory: 13570, loss_cls: 0.1351, loss_bbox: 0.2793, d0.loss_cls: 0.1901, d0.loss_bbox: 0.4089, d1.loss_cls: 0.1548, d1.loss_bbox: 0.2878, d2.loss_cls: 0.1482, d2.loss_bbox: 0.2691, d3.loss_cls: 0.1371, d3.loss_bbox: 0.2685, d4.loss_cls: 0.1342, d4.loss_bbox: 0.2712, loss: 2.6844, grad_norm: 25.0793
2025-06-17 04:13:17,565 - mmdet - INFO - Epoch [1][4350/7033]	lr: 2.000e-04, eta: 9:47:10, time: 0.930, data_time: 0.029, memory: 13570, loss_cls: 0.1293, loss_bbox: 0.2842, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3891, d1.loss_cls: 0.1497, d1.loss_bbox: 0.2846, d2.loss_cls: 0.1414, d2.loss_bbox: 0.2661, d3.loss_cls: 0.1316, d3.loss_bbox: 0.2668, d4.loss_cls: 0.1314, d4.loss_bbox: 0.2707, loss: 2.6249, grad_norm: 25.1596
2025-06-17 04:14:03,837 - mmdet - INFO - Epoch [1][4400/7033]	lr: 2.000e-04, eta: 9:46:21, time: 0.925, data_time: 0.026, memory: 13570, loss_cls: 0.1357, loss_bbox: 0.2753, d0.loss_cls: 0.1893, d0.loss_bbox: 0.4022, d1.loss_cls: 0.1549, d1.loss_bbox: 0.2896, d2.loss_cls: 0.1458, d2.loss_bbox: 0.2684, d3.loss_cls: 0.1355, d3.loss_bbox: 0.2656, d4.loss_cls: 0.1367, d4.loss_bbox: 0.2659, loss: 2.6648, grad_norm: 25.2489
2025-06-17 04:14:49,990 - mmdet - INFO - Epoch [1][4450/7033]	lr: 2.000e-04, eta: 9:45:32, time: 0.923, data_time: 0.028, memory: 13570, loss_cls: 0.1287, loss_bbox: 0.2670, d0.loss_cls: 0.1790, d0.loss_bbox: 0.3886, d1.loss_cls: 0.1523, d1.loss_bbox: 0.2746, d2.loss_cls: 0.1414, d2.loss_bbox: 0.2587, d3.loss_cls: 0.1289, d3.loss_bbox: 0.2570, d4.loss_cls: 0.1275, d4.loss_bbox: 0.2598, loss: 2.5633, grad_norm: 26.3191
2025-06-17 04:15:36,403 - mmdet - INFO - Epoch [1][4500/7033]	lr: 2.000e-04, eta: 9:44:44, time: 0.928, data_time: 0.031, memory: 13570, loss_cls: 0.1329, loss_bbox: 0.2763, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3807, d1.loss_cls: 0.1514, d1.loss_bbox: 0.2777, d2.loss_cls: 0.1411, d2.loss_bbox: 0.2601, d3.loss_cls: 0.1294, d3.loss_bbox: 0.2613, d4.loss_cls: 0.1289, d4.loss_bbox: 0.2673, loss: 2.5812, grad_norm: 28.2303
2025-06-17 04:16:22,676 - mmdet - INFO - Epoch [1][4550/7033]	lr: 2.000e-04, eta: 9:43:55, time: 0.925, data_time: 0.029, memory: 13570, loss_cls: 0.1351, loss_bbox: 0.2993, d0.loss_cls: 0.1914, d0.loss_bbox: 0.4138, d1.loss_cls: 0.1608, d1.loss_bbox: 0.3009, d2.loss_cls: 0.1483, d2.loss_bbox: 0.2839, d3.loss_cls: 0.1362, d3.loss_bbox: 0.2857, d4.loss_cls: 0.1328, d4.loss_bbox: 0.2912, loss: 2.7792, grad_norm: 29.7741
2025-06-17 04:17:08,846 - mmdet - INFO - Epoch [1][4600/7033]	lr: 2.000e-04, eta: 9:43:06, time: 0.923, data_time: 0.028, memory: 13570, loss_cls: 0.1363, loss_bbox: 0.2803, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3991, d1.loss_cls: 0.1599, d1.loss_bbox: 0.2914, d2.loss_cls: 0.1491, d2.loss_bbox: 0.2680, d3.loss_cls: 0.1380, d3.loss_bbox: 0.2686, d4.loss_cls: 0.1375, d4.loss_bbox: 0.2707, loss: 2.6836, grad_norm: 29.9096
2025-06-17 04:17:55,183 - mmdet - INFO - Epoch [1][4650/7033]	lr: 2.000e-04, eta: 9:42:18, time: 0.926, data_time: 0.028, memory: 13570, loss_cls: 0.1344, loss_bbox: 0.2699, d0.loss_cls: 0.1884, d0.loss_bbox: 0.3829, d1.loss_cls: 0.1565, d1.loss_bbox: 0.2784, d2.loss_cls: 0.1448, d2.loss_bbox: 0.2596, d3.loss_cls: 0.1358, d3.loss_bbox: 0.2573, d4.loss_cls: 0.1337, d4.loss_bbox: 0.2617, loss: 2.6035, grad_norm: 26.2963
2025-06-17 04:18:41,607 - mmdet - INFO - Epoch [1][4700/7033]	lr: 2.000e-04, eta: 9:41:31, time: 0.929, data_time: 0.031, memory: 13570, loss_cls: 0.1296, loss_bbox: 0.2803, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3909, d1.loss_cls: 0.1519, d1.loss_bbox: 0.2792, d2.loss_cls: 0.1410, d2.loss_bbox: 0.2608, d3.loss_cls: 0.1307, d3.loss_bbox: 0.2601, d4.loss_cls: 0.1296, d4.loss_bbox: 0.2669, loss: 2.5988, grad_norm: 26.1835
2025-06-17 04:19:27,794 - mmdet - INFO - Epoch [1][4750/7033]	lr: 2.000e-04, eta: 9:40:41, time: 0.924, data_time: 0.030, memory: 13570, loss_cls: 0.1291, loss_bbox: 0.2616, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3742, d1.loss_cls: 0.1458, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1387, d2.loss_bbox: 0.2513, d3.loss_cls: 0.1301, d3.loss_bbox: 0.2501, d4.loss_cls: 0.1271, d4.loss_bbox: 0.2542, loss: 2.5082, grad_norm: 24.2560
2025-06-17 04:20:14,118 - mmdet - INFO - Epoch [1][4800/7033]	lr: 2.000e-04, eta: 9:39:53, time: 0.926, data_time: 0.031, memory: 13570, loss_cls: 0.1251, loss_bbox: 0.2635, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3710, d1.loss_cls: 0.1485, d1.loss_bbox: 0.2668, d2.loss_cls: 0.1382, d2.loss_bbox: 0.2473, d3.loss_cls: 0.1282, d3.loss_bbox: 0.2460, d4.loss_cls: 0.1250, d4.loss_bbox: 0.2513, loss: 2.4916, grad_norm: 25.9610
2025-06-17 04:21:00,495 - mmdet - INFO - Epoch [1][4850/7033]	lr: 2.000e-04, eta: 9:39:06, time: 0.928, data_time: 0.029, memory: 13570, loss_cls: 0.1339, loss_bbox: 0.2741, d0.loss_cls: 0.1887, d0.loss_bbox: 0.3862, d1.loss_cls: 0.1575, d1.loss_bbox: 0.2746, d2.loss_cls: 0.1478, d2.loss_bbox: 0.2546, d3.loss_cls: 0.1366, d3.loss_bbox: 0.2533, d4.loss_cls: 0.1323, d4.loss_bbox: 0.2627, loss: 2.6024, grad_norm: 24.8367
2025-06-17 04:21:46,710 - mmdet - INFO - Epoch [1][4900/7033]	lr: 2.000e-04, eta: 9:38:17, time: 0.924, data_time: 0.029, memory: 13570, loss_cls: 0.1367, loss_bbox: 0.2715, d0.loss_cls: 0.1893, d0.loss_bbox: 0.3980, d1.loss_cls: 0.1598, d1.loss_bbox: 0.2842, d2.loss_cls: 0.1495, d2.loss_bbox: 0.2610, d3.loss_cls: 0.1393, d3.loss_bbox: 0.2582, d4.loss_cls: 0.1380, d4.loss_bbox: 0.2611, loss: 2.6465, grad_norm: 30.8527
2025-06-17 04:22:33,025 - mmdet - INFO - Epoch [1][4950/7033]	lr: 2.000e-04, eta: 9:37:29, time: 0.926, data_time: 0.030, memory: 13570, loss_cls: 0.1441, loss_bbox: 0.2689, d0.loss_cls: 0.1894, d0.loss_bbox: 0.3983, d1.loss_cls: 0.1634, d1.loss_bbox: 0.2797, d2.loss_cls: 0.1528, d2.loss_bbox: 0.2568, d3.loss_cls: 0.1438, d3.loss_bbox: 0.2546, d4.loss_cls: 0.1428, d4.loss_bbox: 0.2598, loss: 2.6543, grad_norm: 27.4439
2025-06-17 04:23:19,317 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 04:23:19,317 - mmdet - INFO - Epoch [1][5000/7033]	lr: 2.000e-04, eta: 9:36:41, time: 0.926, data_time: 0.030, memory: 13570, loss_cls: 0.1219, loss_bbox: 0.2648, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3757, d1.loss_cls: 0.1480, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1355, d2.loss_bbox: 0.2534, d3.loss_cls: 0.1217, d3.loss_bbox: 0.2535, d4.loss_cls: 0.1197, d4.loss_bbox: 0.2580, loss: 2.4948, grad_norm: 26.5797
2025-06-17 04:24:05,461 - mmdet - INFO - Epoch [1][5050/7033]	lr: 2.000e-04, eta: 9:35:52, time: 0.923, data_time: 0.027, memory: 13570, loss_cls: 0.1267, loss_bbox: 0.2628, d0.loss_cls: 0.1875, d0.loss_bbox: 0.3761, d1.loss_cls: 0.1564, d1.loss_bbox: 0.2710, d2.loss_cls: 0.1409, d2.loss_bbox: 0.2538, d3.loss_cls: 0.1304, d3.loss_bbox: 0.2511, d4.loss_cls: 0.1275, d4.loss_bbox: 0.2547, loss: 2.5388, grad_norm: 29.7081
2025-06-17 04:24:51,575 - mmdet - INFO - Epoch [1][5100/7033]	lr: 2.000e-04, eta: 9:35:02, time: 0.922, data_time: 0.031, memory: 13570, loss_cls: 0.1244, loss_bbox: 0.2687, d0.loss_cls: 0.1855, d0.loss_bbox: 0.3914, d1.loss_cls: 0.1547, d1.loss_bbox: 0.2858, d2.loss_cls: 0.1425, d2.loss_bbox: 0.2647, d3.loss_cls: 0.1287, d3.loss_bbox: 0.2611, d4.loss_cls: 0.1251, d4.loss_bbox: 0.2618, loss: 2.5945, grad_norm: 25.9277
2025-06-17 04:25:37,769 - mmdet - INFO - Epoch [1][5150/7033]	lr: 2.000e-04, eta: 9:34:13, time: 0.924, data_time: 0.030, memory: 13570, loss_cls: 0.1349, loss_bbox: 0.2781, d0.loss_cls: 0.1853, d0.loss_bbox: 0.3905, d1.loss_cls: 0.1593, d1.loss_bbox: 0.2829, d2.loss_cls: 0.1481, d2.loss_bbox: 0.2664, d3.loss_cls: 0.1389, d3.loss_bbox: 0.2667, d4.loss_cls: 0.1351, d4.loss_bbox: 0.2715, loss: 2.6578, grad_norm: 25.2476
2025-06-17 04:26:24,090 - mmdet - INFO - Epoch [1][5200/7033]	lr: 2.000e-04, eta: 9:33:26, time: 0.926, data_time: 0.029, memory: 13570, loss_cls: 0.1310, loss_bbox: 0.2701, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3812, d1.loss_cls: 0.1530, d1.loss_bbox: 0.2758, d2.loss_cls: 0.1405, d2.loss_bbox: 0.2578, d3.loss_cls: 0.1313, d3.loss_bbox: 0.2561, d4.loss_cls: 0.1301, d4.loss_bbox: 0.2599, loss: 2.5662, grad_norm: 27.2447
2025-06-17 04:27:10,340 - mmdet - INFO - Epoch [1][5250/7033]	lr: 2.000e-04, eta: 9:32:37, time: 0.925, data_time: 0.028, memory: 13570, loss_cls: 0.1259, loss_bbox: 0.2663, d0.loss_cls: 0.1822, d0.loss_bbox: 0.3777, d1.loss_cls: 0.1504, d1.loss_bbox: 0.2707, d2.loss_cls: 0.1383, d2.loss_bbox: 0.2501, d3.loss_cls: 0.1283, d3.loss_bbox: 0.2495, d4.loss_cls: 0.1251, d4.loss_bbox: 0.2562, loss: 2.5207, grad_norm: 33.0834
2025-06-17 04:27:56,407 - mmdet - INFO - Epoch [1][5300/7033]	lr: 2.000e-04, eta: 9:31:48, time: 0.921, data_time: 0.029, memory: 13570, loss_cls: 0.1206, loss_bbox: 0.2622, d0.loss_cls: 0.1737, d0.loss_bbox: 0.3728, d1.loss_cls: 0.1451, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1327, d2.loss_bbox: 0.2526, d3.loss_cls: 0.1231, d3.loss_bbox: 0.2478, d4.loss_cls: 0.1208, d4.loss_bbox: 0.2531, loss: 2.4733, grad_norm: 26.0872
2025-06-17 04:28:42,776 - mmdet - INFO - Epoch [1][5350/7033]	lr: 2.000e-04, eta: 9:31:01, time: 0.927, data_time: 0.030, memory: 13570, loss_cls: 0.1321, loss_bbox: 0.2760, d0.loss_cls: 0.1891, d0.loss_bbox: 0.3925, d1.loss_cls: 0.1604, d1.loss_bbox: 0.2850, d2.loss_cls: 0.1479, d2.loss_bbox: 0.2678, d3.loss_cls: 0.1346, d3.loss_bbox: 0.2661, d4.loss_cls: 0.1336, d4.loss_bbox: 0.2673, loss: 2.6524, grad_norm: 26.4552
2025-06-17 04:29:29,193 - mmdet - INFO - Epoch [1][5400/7033]	lr: 2.000e-04, eta: 9:30:14, time: 0.928, data_time: 0.030, memory: 13570, loss_cls: 0.1304, loss_bbox: 0.2710, d0.loss_cls: 0.1869, d0.loss_bbox: 0.3931, d1.loss_cls: 0.1599, d1.loss_bbox: 0.2768, d2.loss_cls: 0.1443, d2.loss_bbox: 0.2569, d3.loss_cls: 0.1318, d3.loss_bbox: 0.2543, d4.loss_cls: 0.1299, d4.loss_bbox: 0.2596, loss: 2.5948, grad_norm: 30.1127
2025-06-17 04:30:15,469 - mmdet - INFO - Epoch [1][5450/7033]	lr: 2.000e-04, eta: 9:29:26, time: 0.926, data_time: 0.031, memory: 13570, loss_cls: 0.1275, loss_bbox: 0.2736, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3893, d1.loss_cls: 0.1582, d1.loss_bbox: 0.2807, d2.loss_cls: 0.1431, d2.loss_bbox: 0.2639, d3.loss_cls: 0.1292, d3.loss_bbox: 0.2626, d4.loss_cls: 0.1272, d4.loss_bbox: 0.2652, loss: 2.6036, grad_norm: 24.6864
2025-06-17 04:31:01,565 - mmdet - INFO - Epoch [1][5500/7033]	lr: 2.000e-04, eta: 9:28:37, time: 0.922, data_time: 0.029, memory: 13570, loss_cls: 0.1268, loss_bbox: 0.2688, d0.loss_cls: 0.1768, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1500, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1438, d2.loss_bbox: 0.2529, d3.loss_cls: 0.1301, d3.loss_bbox: 0.2534, d4.loss_cls: 0.1265, d4.loss_bbox: 0.2598, loss: 2.5298, grad_norm: 26.0584
2025-06-17 04:31:47,739 - mmdet - INFO - Epoch [1][5550/7033]	lr: 2.000e-04, eta: 9:27:48, time: 0.923, data_time: 0.030, memory: 13570, loss_cls: 0.1224, loss_bbox: 0.2806, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3912, d1.loss_cls: 0.1485, d1.loss_bbox: 0.2845, d2.loss_cls: 0.1374, d2.loss_bbox: 0.2648, d3.loss_cls: 0.1259, d3.loss_bbox: 0.2645, d4.loss_cls: 0.1224, d4.loss_bbox: 0.2701, loss: 2.5939, grad_norm: 28.0365
2025-06-17 04:32:33,916 - mmdet - INFO - Epoch [1][5600/7033]	lr: 2.000e-04, eta: 9:27:00, time: 0.924, data_time: 0.030, memory: 13570, loss_cls: 0.1225, loss_bbox: 0.2778, d0.loss_cls: 0.1786, d0.loss_bbox: 0.3917, d1.loss_cls: 0.1487, d1.loss_bbox: 0.2821, d2.loss_cls: 0.1351, d2.loss_bbox: 0.2650, d3.loss_cls: 0.1238, d3.loss_bbox: 0.2627, d4.loss_cls: 0.1203, d4.loss_bbox: 0.2692, loss: 2.5776, grad_norm: 26.2607
2025-06-17 04:33:20,153 - mmdet - INFO - Epoch [1][5650/7033]	lr: 2.000e-04, eta: 9:26:12, time: 0.925, data_time: 0.031, memory: 13570, loss_cls: 0.1341, loss_bbox: 0.2811, d0.loss_cls: 0.1895, d0.loss_bbox: 0.3864, d1.loss_cls: 0.1636, d1.loss_bbox: 0.2869, d2.loss_cls: 0.1502, d2.loss_bbox: 0.2706, d3.loss_cls: 0.1372, d3.loss_bbox: 0.2673, d4.loss_cls: 0.1349, d4.loss_bbox: 0.2710, loss: 2.6727, grad_norm: 28.2318
2025-06-17 04:34:06,146 - mmdet - INFO - Epoch [1][5700/7033]	lr: 2.000e-04, eta: 9:25:22, time: 0.920, data_time: 0.028, memory: 13570, loss_cls: 0.1180, loss_bbox: 0.2632, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1472, d1.loss_bbox: 0.2723, d2.loss_cls: 0.1330, d2.loss_bbox: 0.2542, d3.loss_cls: 0.1203, d3.loss_bbox: 0.2518, d4.loss_cls: 0.1180, d4.loss_bbox: 0.2561, loss: 2.4779, grad_norm: 25.4998
2025-06-17 04:34:52,257 - mmdet - INFO - Epoch [1][5750/7033]	lr: 2.000e-04, eta: 9:24:33, time: 0.922, data_time: 0.029, memory: 13570, loss_cls: 0.1196, loss_bbox: 0.2605, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3802, d1.loss_cls: 0.1468, d1.loss_bbox: 0.2727, d2.loss_cls: 0.1339, d2.loss_bbox: 0.2546, d3.loss_cls: 0.1221, d3.loss_bbox: 0.2503, d4.loss_cls: 0.1180, d4.loss_bbox: 0.2556, loss: 2.4913, grad_norm: 26.2809
2025-06-17 04:35:38,573 - mmdet - INFO - Epoch [1][5800/7033]	lr: 2.000e-04, eta: 9:23:46, time: 0.926, data_time: 0.029, memory: 13570, loss_cls: 0.1214, loss_bbox: 0.2632, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3849, d1.loss_cls: 0.1500, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1377, d2.loss_bbox: 0.2543, d3.loss_cls: 0.1254, d3.loss_bbox: 0.2516, d4.loss_cls: 0.1218, d4.loss_bbox: 0.2566, loss: 2.5152, grad_norm: 27.1057
2025-06-17 04:36:24,819 - mmdet - INFO - Epoch [1][5850/7033]	lr: 2.000e-04, eta: 9:22:58, time: 0.925, data_time: 0.031, memory: 13570, loss_cls: 0.1218, loss_bbox: 0.2595, d0.loss_cls: 0.1888, d0.loss_bbox: 0.3774, d1.loss_cls: 0.1516, d1.loss_bbox: 0.2740, d2.loss_cls: 0.1393, d2.loss_bbox: 0.2520, d3.loss_cls: 0.1259, d3.loss_bbox: 0.2492, d4.loss_cls: 0.1216, d4.loss_bbox: 0.2527, loss: 2.5137, grad_norm: 28.5036
2025-06-17 04:37:11,125 - mmdet - INFO - Epoch [1][5900/7033]	lr: 2.000e-04, eta: 9:22:11, time: 0.926, data_time: 0.030, memory: 13570, loss_cls: 0.1239, loss_bbox: 0.2754, d0.loss_cls: 0.1824, d0.loss_bbox: 0.3910, d1.loss_cls: 0.1541, d1.loss_bbox: 0.2821, d2.loss_cls: 0.1424, d2.loss_bbox: 0.2602, d3.loss_cls: 0.1256, d3.loss_bbox: 0.2606, d4.loss_cls: 0.1232, d4.loss_bbox: 0.2669, loss: 2.5878, grad_norm: 29.8650
2025-06-17 04:37:57,046 - mmdet - INFO - Epoch [1][5950/7033]	lr: 2.000e-04, eta: 9:21:21, time: 0.918, data_time: 0.028, memory: 13570, loss_cls: 0.1195, loss_bbox: 0.2699, d0.loss_cls: 0.1772, d0.loss_bbox: 0.3807, d1.loss_cls: 0.1489, d1.loss_bbox: 0.2752, d2.loss_cls: 0.1324, d2.loss_bbox: 0.2606, d3.loss_cls: 0.1223, d3.loss_bbox: 0.2569, d4.loss_cls: 0.1199, d4.loss_bbox: 0.2629, loss: 2.5264, grad_norm: 26.7098
2025-06-17 04:38:43,359 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 04:38:43,359 - mmdet - INFO - Epoch [1][6000/7033]	lr: 2.000e-04, eta: 9:20:33, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1260, loss_bbox: 0.2782, d0.loss_cls: 0.1872, d0.loss_bbox: 0.3893, d1.loss_cls: 0.1524, d1.loss_bbox: 0.2853, d2.loss_cls: 0.1414, d2.loss_bbox: 0.2695, d3.loss_cls: 0.1304, d3.loss_bbox: 0.2662, d4.loss_cls: 0.1254, d4.loss_bbox: 0.2706, loss: 2.6221, grad_norm: 32.3304
2025-06-17 04:39:29,425 - mmdet - INFO - Epoch [1][6050/7033]	lr: 2.000e-04, eta: 9:19:45, time: 0.921, data_time: 0.029, memory: 13888, loss_cls: 0.1188, loss_bbox: 0.2740, d0.loss_cls: 0.1715, d0.loss_bbox: 0.3873, d1.loss_cls: 0.1438, d1.loss_bbox: 0.2851, d2.loss_cls: 0.1344, d2.loss_bbox: 0.2644, d3.loss_cls: 0.1221, d3.loss_bbox: 0.2626, d4.loss_cls: 0.1181, d4.loss_bbox: 0.2674, loss: 2.5494, grad_norm: 33.6124
2025-06-17 04:40:15,678 - mmdet - INFO - Epoch [1][6100/7033]	lr: 2.000e-04, eta: 9:18:57, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.1263, loss_bbox: 0.2686, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3709, d1.loss_cls: 0.1586, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1432, d2.loss_bbox: 0.2554, d3.loss_cls: 0.1329, d3.loss_bbox: 0.2525, d4.loss_cls: 0.1304, d4.loss_bbox: 0.2585, loss: 2.5548, grad_norm: 29.3689
2025-06-17 04:41:02,170 - mmdet - INFO - Epoch [1][6150/7033]	lr: 2.000e-04, eta: 9:18:11, time: 0.930, data_time: 0.028, memory: 13888, loss_cls: 0.1195, loss_bbox: 0.2741, d0.loss_cls: 0.1720, d0.loss_bbox: 0.3763, d1.loss_cls: 0.1479, d1.loss_bbox: 0.2779, d2.loss_cls: 0.1342, d2.loss_bbox: 0.2593, d3.loss_cls: 0.1227, d3.loss_bbox: 0.2624, d4.loss_cls: 0.1191, d4.loss_bbox: 0.2661, loss: 2.5317, grad_norm: 27.2660
2025-06-17 04:41:48,375 - mmdet - INFO - Epoch [1][6200/7033]	lr: 2.000e-04, eta: 9:17:23, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.1053, loss_bbox: 0.2722, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1379, d1.loss_bbox: 0.2736, d2.loss_cls: 0.1228, d2.loss_bbox: 0.2561, d3.loss_cls: 0.1107, d3.loss_bbox: 0.2569, d4.loss_cls: 0.1071, d4.loss_bbox: 0.2617, loss: 2.4510, grad_norm: 28.6166
2025-06-17 04:42:34,340 - mmdet - INFO - Epoch [1][6250/7033]	lr: 2.000e-04, eta: 9:16:34, time: 0.919, data_time: 0.027, memory: 13888, loss_cls: 0.1242, loss_bbox: 0.2615, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3863, d1.loss_cls: 0.1540, d1.loss_bbox: 0.2746, d2.loss_cls: 0.1418, d2.loss_bbox: 0.2544, d3.loss_cls: 0.1268, d3.loss_bbox: 0.2521, d4.loss_cls: 0.1241, d4.loss_bbox: 0.2549, loss: 2.5355, grad_norm: 36.0469
2025-06-17 04:43:20,854 - mmdet - INFO - Epoch [1][6300/7033]	lr: 2.000e-04, eta: 9:15:47, time: 0.930, data_time: 0.031, memory: 13888, loss_cls: 0.1232, loss_bbox: 0.2721, d0.loss_cls: 0.1858, d0.loss_bbox: 0.4030, d1.loss_cls: 0.1559, d1.loss_bbox: 0.2870, d2.loss_cls: 0.1382, d2.loss_bbox: 0.2675, d3.loss_cls: 0.1308, d3.loss_bbox: 0.2623, d4.loss_cls: 0.1243, d4.loss_bbox: 0.2652, loss: 2.6154, grad_norm: 34.9573
2025-06-17 04:44:06,928 - mmdet - INFO - Epoch [1][6350/7033]	lr: 2.000e-04, eta: 9:14:59, time: 0.921, data_time: 0.029, memory: 13888, loss_cls: 0.1338, loss_bbox: 0.2686, d0.loss_cls: 0.1902, d0.loss_bbox: 0.3920, d1.loss_cls: 0.1630, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1472, d2.loss_bbox: 0.2605, d3.loss_cls: 0.1388, d3.loss_bbox: 0.2584, d4.loss_cls: 0.1357, d4.loss_bbox: 0.2620, loss: 2.6341, grad_norm: 29.7073
2025-06-17 04:44:53,137 - mmdet - INFO - Epoch [1][6400/7033]	lr: 2.000e-04, eta: 9:14:11, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.1247, loss_bbox: 0.2782, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3928, d1.loss_cls: 0.1582, d1.loss_bbox: 0.2839, d2.loss_cls: 0.1395, d2.loss_bbox: 0.2652, d3.loss_cls: 0.1280, d3.loss_bbox: 0.2631, d4.loss_cls: 0.1259, d4.loss_bbox: 0.2676, loss: 2.6172, grad_norm: 30.2555
2025-06-17 04:45:39,282 - mmdet - INFO - Epoch [1][6450/7033]	lr: 2.000e-04, eta: 9:13:23, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.1128, loss_bbox: 0.2604, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3695, d1.loss_cls: 0.1487, d1.loss_bbox: 0.2669, d2.loss_cls: 0.1331, d2.loss_bbox: 0.2508, d3.loss_cls: 0.1186, d3.loss_bbox: 0.2494, d4.loss_cls: 0.1128, d4.loss_bbox: 0.2545, loss: 2.4539, grad_norm: 25.3623
2025-06-17 04:46:25,410 - mmdet - INFO - Epoch [1][6500/7033]	lr: 2.000e-04, eta: 9:12:35, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.1251, loss_bbox: 0.2761, d0.loss_cls: 0.1892, d0.loss_bbox: 0.3837, d1.loss_cls: 0.1610, d1.loss_bbox: 0.2827, d2.loss_cls: 0.1408, d2.loss_bbox: 0.2637, d3.loss_cls: 0.1276, d3.loss_bbox: 0.2607, d4.loss_cls: 0.1252, d4.loss_bbox: 0.2646, loss: 2.6004, grad_norm: 27.2280
2025-06-17 04:47:11,319 - mmdet - INFO - Epoch [1][6550/7033]	lr: 2.000e-04, eta: 9:11:46, time: 0.918, data_time: 0.028, memory: 13888, loss_cls: 0.1255, loss_bbox: 0.2687, d0.loss_cls: 0.1817, d0.loss_bbox: 0.3799, d1.loss_cls: 0.1549, d1.loss_bbox: 0.2770, d2.loss_cls: 0.1399, d2.loss_bbox: 0.2584, d3.loss_cls: 0.1283, d3.loss_bbox: 0.2563, d4.loss_cls: 0.1266, d4.loss_bbox: 0.2607, loss: 2.5579, grad_norm: 28.9839
2025-06-17 04:47:57,212 - mmdet - INFO - Epoch [1][6600/7033]	lr: 2.000e-04, eta: 9:10:56, time: 0.918, data_time: 0.027, memory: 13888, loss_cls: 0.1145, loss_bbox: 0.2618, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3692, d1.loss_cls: 0.1475, d1.loss_bbox: 0.2653, d2.loss_cls: 0.1268, d2.loss_bbox: 0.2491, d3.loss_cls: 0.1168, d3.loss_bbox: 0.2477, d4.loss_cls: 0.1138, d4.loss_bbox: 0.2514, loss: 2.4380, grad_norm: 26.2191
2025-06-17 04:48:43,447 - mmdet - INFO - Epoch [1][6650/7033]	lr: 2.000e-04, eta: 9:10:09, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1166, loss_bbox: 0.2728, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3860, d1.loss_cls: 0.1474, d1.loss_bbox: 0.2767, d2.loss_cls: 0.1322, d2.loss_bbox: 0.2555, d3.loss_cls: 0.1190, d3.loss_bbox: 0.2555, d4.loss_cls: 0.1159, d4.loss_bbox: 0.2614, loss: 2.5131, grad_norm: 28.6064
2025-06-17 04:49:29,713 - mmdet - INFO - Epoch [1][6700/7033]	lr: 2.000e-04, eta: 9:09:21, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1160, loss_bbox: 0.2634, d0.loss_cls: 0.1792, d0.loss_bbox: 0.3695, d1.loss_cls: 0.1512, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1323, d2.loss_bbox: 0.2483, d3.loss_cls: 0.1189, d3.loss_bbox: 0.2486, d4.loss_cls: 0.1168, d4.loss_bbox: 0.2527, loss: 2.4608, grad_norm: 29.7001
2025-06-17 04:50:15,997 - mmdet - INFO - Epoch [1][6750/7033]	lr: 2.000e-04, eta: 9:08:34, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.1109, loss_bbox: 0.2652, d0.loss_cls: 0.1788, d0.loss_bbox: 0.3821, d1.loss_cls: 0.1451, d1.loss_bbox: 0.2745, d2.loss_cls: 0.1274, d2.loss_bbox: 0.2531, d3.loss_cls: 0.1132, d3.loss_bbox: 0.2488, d4.loss_cls: 0.1115, d4.loss_bbox: 0.2560, loss: 2.4667, grad_norm: 27.5300
2025-06-17 04:51:02,034 - mmdet - INFO - Epoch [1][6800/7033]	lr: 2.000e-04, eta: 9:07:46, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.1185, loss_bbox: 0.2589, d0.loss_cls: 0.1857, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1515, d1.loss_bbox: 0.2726, d2.loss_cls: 0.1359, d2.loss_bbox: 0.2517, d3.loss_cls: 0.1212, d3.loss_bbox: 0.2491, d4.loss_cls: 0.1183, d4.loss_bbox: 0.2508, loss: 2.4876, grad_norm: 28.8354
2025-06-17 04:51:48,012 - mmdet - INFO - Epoch [1][6850/7033]	lr: 2.000e-04, eta: 9:06:57, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.1107, loss_bbox: 0.2555, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3688, d1.loss_cls: 0.1438, d1.loss_bbox: 0.2704, d2.loss_cls: 0.1288, d2.loss_bbox: 0.2500, d3.loss_cls: 0.1168, d3.loss_bbox: 0.2465, d4.loss_cls: 0.1112, d4.loss_bbox: 0.2496, loss: 2.4247, grad_norm: 27.8069
2025-06-17 04:52:34,294 - mmdet - INFO - Epoch [1][6900/7033]	lr: 2.000e-04, eta: 9:06:10, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.1171, loss_bbox: 0.2658, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3773, d1.loss_cls: 0.1505, d1.loss_bbox: 0.2743, d2.loss_cls: 0.1336, d2.loss_bbox: 0.2558, d3.loss_cls: 0.1209, d3.loss_bbox: 0.2515, d4.loss_cls: 0.1177, d4.loss_bbox: 0.2552, loss: 2.4983, grad_norm: 25.3331
2025-06-17 04:53:20,324 - mmdet - INFO - Epoch [1][6950/7033]	lr: 2.000e-04, eta: 9:05:22, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.1121, loss_bbox: 0.2571, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3695, d1.loss_cls: 0.1447, d1.loss_bbox: 0.2666, d2.loss_cls: 0.1302, d2.loss_bbox: 0.2478, d3.loss_cls: 0.1167, d3.loss_bbox: 0.2462, d4.loss_cls: 0.1133, d4.loss_bbox: 0.2497, loss: 2.4244, grad_norm: 26.9465
2025-06-17 04:54:06,619 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 04:54:06,619 - mmdet - INFO - Epoch [1][7000/7033]	lr: 2.000e-04, eta: 9:04:35, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1154, loss_bbox: 0.2656, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3795, d1.loss_cls: 0.1460, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1298, d2.loss_bbox: 0.2569, d3.loss_cls: 0.1193, d3.loss_bbox: 0.2526, d4.loss_cls: 0.1152, d4.loss_bbox: 0.2562, loss: 2.4830, grad_norm: 27.4415
2025-06-17 04:54:39,211 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-06-17 05:15:55,117 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 05:15:55,117 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.5110, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8509, pts_bbox_NuScenes/car_AP_dist_2.0: 0.8926, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9088, pts_bbox_NuScenes/car_trans_err: 0.3760, pts_bbox_NuScenes/car_scale_err: 0.1576, pts_bbox_NuScenes/car_orient_err: 0.0664, pts_bbox_NuScenes/car_vel_err: 0.4375, pts_bbox_NuScenes/car_attr_err: 0.1838, pts_bbox_NuScenes/mATE: 0.4375, pts_bbox_NuScenes/mASE: 0.2786, pts_bbox_NuScenes/mAOE: 0.3252, pts_bbox_NuScenes/mAVE: 0.4064, pts_bbox_NuScenes/mAAE: 0.1852, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.1888, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.5529, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.6851, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7354, pts_bbox_NuScenes/truck_trans_err: 0.5284, pts_bbox_NuScenes/truck_scale_err: 0.2162, pts_bbox_NuScenes/truck_orient_err: 0.0734, pts_bbox_NuScenes/truck_vel_err: 0.3769, pts_bbox_NuScenes/truck_attr_err: 0.2013, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0264, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1691, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3952, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4666, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.7398, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4641, pts_bbox_NuScenes/construction_vehicle_orient_err: 1.0037, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1062, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3119, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.0990, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.6208, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8736, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9109, pts_bbox_NuScenes/bus_trans_err: 0.6134, pts_bbox_NuScenes/bus_scale_err: 0.2298, pts_bbox_NuScenes/bus_orient_err: 0.1110, pts_bbox_NuScenes/bus_vel_err: 0.7277, pts_bbox_NuScenes/bus_attr_err: 0.2744, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.0262, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3221, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5657, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6555, pts_bbox_NuScenes/trailer_trans_err: 0.7148, pts_bbox_NuScenes/trailer_scale_err: 0.2475, pts_bbox_NuScenes/trailer_orient_err: 0.5505, pts_bbox_NuScenes/trailer_vel_err: 0.3690, pts_bbox_NuScenes/trailer_attr_err: 0.1666, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5272, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6351, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.6929, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7216, pts_bbox_NuScenes/barrier_trans_err: 0.2682, pts_bbox_NuScenes/barrier_scale_err: 0.2991, pts_bbox_NuScenes/barrier_orient_err: 0.0746, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.4215, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7195, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7791, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7888, pts_bbox_NuScenes/motorcycle_trans_err: 0.3845, pts_bbox_NuScenes/motorcycle_scale_err: 0.2539, pts_bbox_NuScenes/motorcycle_orient_err: 0.2453, pts_bbox_NuScenes/motorcycle_vel_err: 0.6511, pts_bbox_NuScenes/motorcycle_attr_err: 0.2120, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.4070, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5776, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5888, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5957, pts_bbox_NuScenes/bicycle_trans_err: 0.3268, pts_bbox_NuScenes/bicycle_scale_err: 0.2803, pts_bbox_NuScenes/bicycle_orient_err: 0.4403, pts_bbox_NuScenes/bicycle_vel_err: 0.3259, pts_bbox_NuScenes/bicycle_attr_err: 0.0104, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7682, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8321, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8602, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8759, pts_bbox_NuScenes/pedestrian_trans_err: 0.2392, pts_bbox_NuScenes/pedestrian_scale_err: 0.3009, pts_bbox_NuScenes/pedestrian_orient_err: 0.3613, pts_bbox_NuScenes/pedestrian_vel_err: 0.2572, pts_bbox_NuScenes/pedestrian_attr_err: 0.1214, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6708, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7255, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7541, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.7890, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1839, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3368, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6390, pts_bbox_NuScenes/mAP: 0.6047
2025-06-17 05:16:50,405 - mmdet - INFO - Epoch [2][50/7033]	lr: 1.866e-04, eta: 9:01:07, time: 1.015, data_time: 0.118, memory: 13888, loss_cls: 0.1158, loss_bbox: 0.2564, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3671, d1.loss_cls: 0.1443, d1.loss_bbox: 0.2685, d2.loss_cls: 0.1317, d2.loss_bbox: 0.2504, d3.loss_cls: 0.1199, d3.loss_bbox: 0.2482, d4.loss_cls: 0.1171, d4.loss_bbox: 0.2494, loss: 2.4417, grad_norm: 24.6342
2025-06-17 05:17:36,504 - mmdet - INFO - Epoch [2][100/7033]	lr: 1.866e-04, eta: 9:00:20, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.1218, loss_bbox: 0.2656, d0.loss_cls: 0.1854, d0.loss_bbox: 0.3769, d1.loss_cls: 0.1552, d1.loss_bbox: 0.2795, d2.loss_cls: 0.1364, d2.loss_bbox: 0.2614, d3.loss_cls: 0.1244, d3.loss_bbox: 0.2566, d4.loss_cls: 0.1215, d4.loss_bbox: 0.2595, loss: 2.5442, grad_norm: 29.8880
2025-06-17 05:18:22,861 - mmdet - INFO - Epoch [2][150/7033]	lr: 1.866e-04, eta: 8:59:35, time: 0.927, data_time: 0.029, memory: 13888, loss_cls: 0.1006, loss_bbox: 0.2472, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2594, d2.loss_cls: 0.1179, d2.loss_bbox: 0.2437, d3.loss_cls: 0.1044, d3.loss_bbox: 0.2405, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2429, loss: 2.3280, grad_norm: 26.6474
2025-06-17 05:19:09,104 - mmdet - INFO - Epoch [2][200/7033]	lr: 1.866e-04, eta: 8:58:48, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1096, loss_bbox: 0.2579, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3836, d1.loss_cls: 0.1455, d1.loss_bbox: 0.2725, d2.loss_cls: 0.1290, d2.loss_bbox: 0.2504, d3.loss_cls: 0.1166, d3.loss_bbox: 0.2467, d4.loss_cls: 0.1083, d4.loss_bbox: 0.2528, loss: 2.4472, grad_norm: 28.2969
2025-06-17 05:19:55,253 - mmdet - INFO - Epoch [2][250/7033]	lr: 1.866e-04, eta: 8:58:02, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.1080, loss_bbox: 0.2536, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1442, d1.loss_bbox: 0.2579, d2.loss_cls: 0.1270, d2.loss_bbox: 0.2427, d3.loss_cls: 0.1141, d3.loss_bbox: 0.2423, d4.loss_cls: 0.1085, d4.loss_bbox: 0.2468, loss: 2.3669, grad_norm: 26.8894
2025-06-17 05:20:41,346 - mmdet - INFO - Epoch [2][300/7033]	lr: 1.866e-04, eta: 8:57:15, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.1028, loss_bbox: 0.2578, d0.loss_cls: 0.1680, d0.loss_bbox: 0.3646, d1.loss_cls: 0.1395, d1.loss_bbox: 0.2635, d2.loss_cls: 0.1210, d2.loss_bbox: 0.2452, d3.loss_cls: 0.1087, d3.loss_bbox: 0.2445, d4.loss_cls: 0.1061, d4.loss_bbox: 0.2485, loss: 2.3702, grad_norm: 25.7502
2025-06-17 05:21:27,666 - mmdet - INFO - Epoch [2][350/7033]	lr: 1.866e-04, eta: 8:56:29, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1252, loss_bbox: 0.2581, d0.loss_cls: 0.1864, d0.loss_bbox: 0.3810, d1.loss_cls: 0.1586, d1.loss_bbox: 0.2731, d2.loss_cls: 0.1425, d2.loss_bbox: 0.2534, d3.loss_cls: 0.1302, d3.loss_bbox: 0.2509, d4.loss_cls: 0.1269, d4.loss_bbox: 0.2519, loss: 2.5383, grad_norm: 26.8811
2025-06-17 05:22:14,091 - mmdet - INFO - Epoch [2][400/7033]	lr: 1.866e-04, eta: 8:55:44, time: 0.928, data_time: 0.029, memory: 13888, loss_cls: 0.1034, loss_bbox: 0.2552, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3567, d1.loss_cls: 0.1382, d1.loss_bbox: 0.2588, d2.loss_cls: 0.1218, d2.loss_bbox: 0.2433, d3.loss_cls: 0.1052, d3.loss_bbox: 0.2413, d4.loss_cls: 0.1027, d4.loss_bbox: 0.2479, loss: 2.3384, grad_norm: 31.0197
2025-06-17 05:23:00,458 - mmdet - INFO - Epoch [2][450/7033]	lr: 1.866e-04, eta: 8:54:58, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.1125, loss_bbox: 0.2687, d0.loss_cls: 0.1849, d0.loss_bbox: 0.3844, d1.loss_cls: 0.1554, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1347, d2.loss_bbox: 0.2542, d3.loss_cls: 0.1181, d3.loss_bbox: 0.2533, d4.loss_cls: 0.1136, d4.loss_bbox: 0.2581, loss: 2.5128, grad_norm: 35.8353
2025-06-17 05:23:46,820 - mmdet - INFO - Epoch [2][500/7033]	lr: 1.866e-04, eta: 8:54:13, time: 0.927, data_time: 0.029, memory: 13888, loss_cls: 0.1114, loss_bbox: 0.2499, d0.loss_cls: 0.1795, d0.loss_bbox: 0.3706, d1.loss_cls: 0.1500, d1.loss_bbox: 0.2625, d2.loss_cls: 0.1335, d2.loss_bbox: 0.2411, d3.loss_cls: 0.1170, d3.loss_bbox: 0.2381, d4.loss_cls: 0.1117, d4.loss_bbox: 0.2431, loss: 2.4082, grad_norm: 33.9972
2025-06-17 05:24:32,824 - mmdet - INFO - Epoch [2][550/7033]	lr: 1.866e-04, eta: 8:53:25, time: 0.920, data_time: 0.028, memory: 13888, loss_cls: 0.1108, loss_bbox: 0.2614, d0.loss_cls: 0.1882, d0.loss_bbox: 0.3945, d1.loss_cls: 0.1503, d1.loss_bbox: 0.2779, d2.loss_cls: 0.1311, d2.loss_bbox: 0.2555, d3.loss_cls: 0.1147, d3.loss_bbox: 0.2525, d4.loss_cls: 0.1095, d4.loss_bbox: 0.2564, loss: 2.5030, grad_norm: 29.3559
2025-06-17 05:25:19,188 - mmdet - INFO - Epoch [2][600/7033]	lr: 1.866e-04, eta: 8:52:40, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.1073, loss_bbox: 0.2747, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3743, d1.loss_cls: 0.1475, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1275, d2.loss_bbox: 0.2580, d3.loss_cls: 0.1127, d3.loss_bbox: 0.2577, d4.loss_cls: 0.1080, d4.loss_bbox: 0.2652, loss: 2.4859, grad_norm: 31.4660
2025-06-17 05:26:05,658 - mmdet - INFO - Epoch [2][650/7033]	lr: 1.866e-04, eta: 8:51:54, time: 0.929, data_time: 0.031, memory: 13888, loss_cls: 0.1121, loss_bbox: 0.2690, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3801, d1.loss_cls: 0.1518, d1.loss_bbox: 0.2826, d2.loss_cls: 0.1285, d2.loss_bbox: 0.2666, d3.loss_cls: 0.1145, d3.loss_bbox: 0.2630, d4.loss_cls: 0.1115, d4.loss_bbox: 0.2642, loss: 2.5242, grad_norm: 26.4473
2025-06-17 05:26:51,805 - mmdet - INFO - Epoch [2][700/7033]	lr: 1.866e-04, eta: 8:51:08, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.1208, loss_bbox: 0.2788, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3882, d1.loss_cls: 0.1598, d1.loss_bbox: 0.2864, d2.loss_cls: 0.1412, d2.loss_bbox: 0.2663, d3.loss_cls: 0.1258, d3.loss_bbox: 0.2654, d4.loss_cls: 0.1209, d4.loss_bbox: 0.2715, loss: 2.6088, grad_norm: 29.0218
2025-06-17 05:27:38,255 - mmdet - INFO - Epoch [2][750/7033]	lr: 1.866e-04, eta: 8:50:23, time: 0.929, data_time: 0.027, memory: 13888, loss_cls: 0.1121, loss_bbox: 0.2660, d0.loss_cls: 0.1806, d0.loss_bbox: 0.3793, d1.loss_cls: 0.1525, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1315, d2.loss_bbox: 0.2562, d3.loss_cls: 0.1168, d3.loss_bbox: 0.2538, d4.loss_cls: 0.1113, d4.loss_bbox: 0.2590, loss: 2.4940, grad_norm: 28.9671
2025-06-17 05:28:24,726 - mmdet - INFO - Epoch [2][800/7033]	lr: 1.866e-04, eta: 8:49:37, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.1200, loss_bbox: 0.2582, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3647, d1.loss_cls: 0.1523, d1.loss_bbox: 0.2652, d2.loss_cls: 0.1333, d2.loss_bbox: 0.2497, d3.loss_cls: 0.1250, d3.loss_bbox: 0.2467, d4.loss_cls: 0.1210, d4.loss_bbox: 0.2503, loss: 2.4661, grad_norm: 27.2269
2025-06-17 05:29:10,898 - mmdet - INFO - Epoch [2][850/7033]	lr: 1.866e-04, eta: 8:48:51, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.1104, loss_bbox: 0.2539, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3584, d1.loss_cls: 0.1457, d1.loss_bbox: 0.2648, d2.loss_cls: 0.1276, d2.loss_bbox: 0.2459, d3.loss_cls: 0.1150, d3.loss_bbox: 0.2418, d4.loss_cls: 0.1108, d4.loss_bbox: 0.2462, loss: 2.3952, grad_norm: 28.5918
2025-06-17 05:29:57,299 - mmdet - INFO - Epoch [2][900/7033]	lr: 1.866e-04, eta: 8:48:05, time: 0.928, data_time: 0.032, memory: 13888, loss_cls: 0.1122, loss_bbox: 0.2526, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1473, d1.loss_bbox: 0.2652, d2.loss_cls: 0.1268, d2.loss_bbox: 0.2486, d3.loss_cls: 0.1146, d3.loss_bbox: 0.2448, d4.loss_cls: 0.1112, d4.loss_bbox: 0.2463, loss: 2.4050, grad_norm: 31.3654
2025-06-17 05:30:43,618 - mmdet - INFO - Epoch [2][950/7033]	lr: 1.866e-04, eta: 8:47:19, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1016, loss_bbox: 0.2530, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3498, d1.loss_cls: 0.1425, d1.loss_bbox: 0.2509, d2.loss_cls: 0.1189, d2.loss_bbox: 0.2357, d3.loss_cls: 0.1038, d3.loss_bbox: 0.2356, d4.loss_cls: 0.1017, d4.loss_bbox: 0.2411, loss: 2.3085, grad_norm: 29.1918
2025-06-17 05:31:29,787 - mmdet - INFO - Epoch [2][1000/7033]	lr: 1.866e-04, eta: 8:46:33, time: 0.923, data_time: 0.027, memory: 13888, loss_cls: 0.1032, loss_bbox: 0.2533, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3694, d1.loss_cls: 0.1367, d1.loss_bbox: 0.2642, d2.loss_cls: 0.1214, d2.loss_bbox: 0.2456, d3.loss_cls: 0.1057, d3.loss_bbox: 0.2443, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2470, loss: 2.3611, grad_norm: 27.9087
2025-06-17 05:32:16,460 - mmdet - INFO - Epoch [2][1050/7033]	lr: 1.866e-04, eta: 8:45:49, time: 0.933, data_time: 0.031, memory: 13888, loss_cls: 0.1021, loss_bbox: 0.2554, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3695, d1.loss_cls: 0.1435, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1227, d2.loss_bbox: 0.2510, d3.loss_cls: 0.1083, d3.loss_bbox: 0.2465, d4.loss_cls: 0.1025, d4.loss_bbox: 0.2488, loss: 2.3951, grad_norm: 27.7580
2025-06-17 05:33:02,862 - mmdet - INFO - Epoch [2][1100/7033]	lr: 1.866e-04, eta: 8:45:03, time: 0.928, data_time: 0.031, memory: 13888, loss_cls: 0.1097, loss_bbox: 0.2505, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3635, d1.loss_cls: 0.1520, d1.loss_bbox: 0.2622, d2.loss_cls: 0.1341, d2.loss_bbox: 0.2447, d3.loss_cls: 0.1165, d3.loss_bbox: 0.2415, d4.loss_cls: 0.1110, d4.loss_bbox: 0.2442, loss: 2.4052, grad_norm: 36.1993
2025-06-17 05:33:48,989 - mmdet - INFO - Epoch [2][1150/7033]	lr: 1.866e-04, eta: 8:44:16, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.1025, loss_bbox: 0.2591, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3587, d1.loss_cls: 0.1397, d1.loss_bbox: 0.2647, d2.loss_cls: 0.1230, d2.loss_bbox: 0.2473, d3.loss_cls: 0.1080, d3.loss_bbox: 0.2463, d4.loss_cls: 0.1044, d4.loss_bbox: 0.2508, loss: 2.3709, grad_norm: 32.4303
2025-06-17 05:34:34,767 - mmdet - INFO - Epoch [2][1200/7033]	lr: 1.866e-04, eta: 8:43:28, time: 0.916, data_time: 0.027, memory: 13888, loss_cls: 0.1058, loss_bbox: 0.2464, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1396, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1234, d2.loss_bbox: 0.2404, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2370, d4.loss_cls: 0.1059, d4.loss_bbox: 0.2400, loss: 2.3332, grad_norm: 32.7779
2025-06-17 05:35:21,566 - mmdet - INFO - Epoch [2][1250/7033]	lr: 1.866e-04, eta: 8:42:44, time: 0.936, data_time: 0.031, memory: 13888, loss_cls: 0.1004, loss_bbox: 0.2603, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3657, d1.loss_cls: 0.1402, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1173, d2.loss_bbox: 0.2523, d3.loss_cls: 0.1056, d3.loss_bbox: 0.2500, d4.loss_cls: 0.1017, d4.loss_bbox: 0.2524, loss: 2.3873, grad_norm: 30.9276
2025-06-17 05:36:08,223 - mmdet - INFO - Epoch [2][1300/7033]	lr: 1.866e-04, eta: 8:42:00, time: 0.933, data_time: 0.030, memory: 13888, loss_cls: 0.0995, loss_bbox: 0.2586, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3520, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2584, d2.loss_cls: 0.1162, d2.loss_bbox: 0.2431, d3.loss_cls: 0.1040, d3.loss_bbox: 0.2380, d4.loss_cls: 0.1015, d4.loss_bbox: 0.2447, loss: 2.3288, grad_norm: 30.6240
2025-06-17 05:36:54,545 - mmdet - INFO - Epoch [2][1350/7033]	lr: 1.866e-04, eta: 8:41:14, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.1083, loss_bbox: 0.2488, d0.loss_cls: 0.1789, d0.loss_bbox: 0.3595, d1.loss_cls: 0.1450, d1.loss_bbox: 0.2626, d2.loss_cls: 0.1266, d2.loss_bbox: 0.2436, d3.loss_cls: 0.1133, d3.loss_bbox: 0.2389, d4.loss_cls: 0.1079, d4.loss_bbox: 0.2439, loss: 2.3774, grad_norm: 35.8359
2025-06-17 05:37:40,715 - mmdet - INFO - Epoch [2][1400/7033]	lr: 1.866e-04, eta: 8:40:27, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.1097, loss_bbox: 0.2524, d0.loss_cls: 0.1876, d0.loss_bbox: 0.3600, d1.loss_cls: 0.1502, d1.loss_bbox: 0.2597, d2.loss_cls: 0.1315, d2.loss_bbox: 0.2413, d3.loss_cls: 0.1152, d3.loss_bbox: 0.2378, d4.loss_cls: 0.1087, d4.loss_bbox: 0.2454, loss: 2.3994, grad_norm: 31.7484
2025-06-17 05:38:26,910 - mmdet - INFO - Epoch [2][1450/7033]	lr: 1.866e-04, eta: 8:39:41, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.1136, loss_bbox: 0.2475, d0.loss_cls: 0.1791, d0.loss_bbox: 0.3699, d1.loss_cls: 0.1513, d1.loss_bbox: 0.2589, d2.loss_cls: 0.1346, d2.loss_bbox: 0.2416, d3.loss_cls: 0.1184, d3.loss_bbox: 0.2391, d4.loss_cls: 0.1127, d4.loss_bbox: 0.2427, loss: 2.4094, grad_norm: 27.9894
2025-06-17 05:39:13,576 - mmdet - INFO - Epoch [2][1500/7033]	lr: 1.866e-04, eta: 8:38:56, time: 0.933, data_time: 0.031, memory: 13888, loss_cls: 0.1197, loss_bbox: 0.2524, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3739, d1.loss_cls: 0.1498, d1.loss_bbox: 0.2661, d2.loss_cls: 0.1337, d2.loss_bbox: 0.2484, d3.loss_cls: 0.1232, d3.loss_bbox: 0.2438, d4.loss_cls: 0.1195, d4.loss_bbox: 0.2476, loss: 2.4584, grad_norm: 29.5665
2025-06-17 05:39:59,722 - mmdet - INFO - Epoch [2][1550/7033]	lr: 1.866e-04, eta: 8:38:10, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.1085, loss_bbox: 0.2563, d0.loss_cls: 0.1840, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1513, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1290, d2.loss_bbox: 0.2499, d3.loss_cls: 0.1138, d3.loss_bbox: 0.2467, d4.loss_cls: 0.1098, d4.loss_bbox: 0.2498, loss: 2.4289, grad_norm: 31.2189
2025-06-17 05:40:45,712 - mmdet - INFO - Epoch [2][1600/7033]	lr: 1.866e-04, eta: 8:37:22, time: 0.920, data_time: 0.028, memory: 13888, loss_cls: 0.1050, loss_bbox: 0.2475, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3510, d1.loss_cls: 0.1440, d1.loss_bbox: 0.2553, d2.loss_cls: 0.1259, d2.loss_bbox: 0.2391, d3.loss_cls: 0.1102, d3.loss_bbox: 0.2348, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2395, loss: 2.3350, grad_norm: 31.3969
2025-06-17 05:41:31,957 - mmdet - INFO - Epoch [2][1650/7033]	lr: 1.866e-04, eta: 8:36:36, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1125, loss_bbox: 0.2625, d0.loss_cls: 0.1840, d0.loss_bbox: 0.3731, d1.loss_cls: 0.1518, d1.loss_bbox: 0.2711, d2.loss_cls: 0.1345, d2.loss_bbox: 0.2517, d3.loss_cls: 0.1206, d3.loss_bbox: 0.2474, d4.loss_cls: 0.1139, d4.loss_bbox: 0.2531, loss: 2.4761, grad_norm: 28.9919
2025-06-17 05:42:18,537 - mmdet - INFO - Epoch [2][1700/7033]	lr: 1.866e-04, eta: 8:35:51, time: 0.932, data_time: 0.031, memory: 13888, loss_cls: 0.1050, loss_bbox: 0.2479, d0.loss_cls: 0.1771, d0.loss_bbox: 0.3676, d1.loss_cls: 0.1455, d1.loss_bbox: 0.2604, d2.loss_cls: 0.1247, d2.loss_bbox: 0.2425, d3.loss_cls: 0.1124, d3.loss_bbox: 0.2383, d4.loss_cls: 0.1066, d4.loss_bbox: 0.2414, loss: 2.3693, grad_norm: 31.3562
2025-06-17 05:43:04,957 - mmdet - INFO - Epoch [2][1750/7033]	lr: 1.866e-04, eta: 8:35:06, time: 0.928, data_time: 0.031, memory: 13888, loss_cls: 0.1017, loss_bbox: 0.2556, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3707, d1.loss_cls: 0.1468, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1231, d2.loss_bbox: 0.2486, d3.loss_cls: 0.1089, d3.loss_bbox: 0.2452, d4.loss_cls: 0.1034, d4.loss_bbox: 0.2482, loss: 2.3965, grad_norm: 30.8825
2025-06-17 05:43:50,987 - mmdet - INFO - Epoch [2][1800/7033]	lr: 1.866e-04, eta: 8:34:18, time: 0.921, data_time: 0.027, memory: 13888, loss_cls: 0.1071, loss_bbox: 0.2524, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3650, d1.loss_cls: 0.1469, d1.loss_bbox: 0.2601, d2.loss_cls: 0.1251, d2.loss_bbox: 0.2420, d3.loss_cls: 0.1115, d3.loss_bbox: 0.2396, d4.loss_cls: 0.1070, d4.loss_bbox: 0.2450, loss: 2.3767, grad_norm: 38.9516
2025-06-17 05:44:37,392 - mmdet - INFO - Epoch [2][1850/7033]	lr: 1.866e-04, eta: 8:33:33, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.1050, loss_bbox: 0.2492, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3599, d1.loss_cls: 0.1423, d1.loss_bbox: 0.2605, d2.loss_cls: 0.1234, d2.loss_bbox: 0.2425, d3.loss_cls: 0.1089, d3.loss_bbox: 0.2369, d4.loss_cls: 0.1055, d4.loss_bbox: 0.2417, loss: 2.3560, grad_norm: 33.2892
2025-06-17 05:45:23,728 - mmdet - INFO - Epoch [2][1900/7033]	lr: 1.866e-04, eta: 8:32:47, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.1025, loss_bbox: 0.2491, d0.loss_cls: 0.1804, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1461, d1.loss_bbox: 0.2566, d2.loss_cls: 0.1263, d2.loss_bbox: 0.2397, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2358, d4.loss_cls: 0.1040, d4.loss_bbox: 0.2410, loss: 2.3531, grad_norm: 30.1320
2025-06-17 05:46:09,979 - mmdet - INFO - Epoch [2][1950/7033]	lr: 1.866e-04, eta: 8:32:01, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1037, loss_bbox: 0.2448, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3601, d1.loss_cls: 0.1474, d1.loss_bbox: 0.2577, d2.loss_cls: 0.1273, d2.loss_bbox: 0.2415, d3.loss_cls: 0.1107, d3.loss_bbox: 0.2362, d4.loss_cls: 0.1065, d4.loss_bbox: 0.2383, loss: 2.3572, grad_norm: 24.4009
2025-06-17 05:46:56,362 - mmdet - INFO - Epoch [2][2000/7033]	lr: 1.866e-04, eta: 8:31:15, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.1040, loss_bbox: 0.2522, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3661, d1.loss_cls: 0.1452, d1.loss_bbox: 0.2591, d2.loss_cls: 0.1259, d2.loss_bbox: 0.2396, d3.loss_cls: 0.1086, d3.loss_bbox: 0.2384, d4.loss_cls: 0.1030, d4.loss_bbox: 0.2426, loss: 2.3666, grad_norm: 36.7228
2025-06-17 05:47:42,815 - mmdet - INFO - Epoch [2][2050/7033]	lr: 1.866e-04, eta: 8:30:29, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.1102, loss_bbox: 0.2535, d0.loss_cls: 0.1832, d0.loss_bbox: 0.3716, d1.loss_cls: 0.1496, d1.loss_bbox: 0.2658, d2.loss_cls: 0.1291, d2.loss_bbox: 0.2461, d3.loss_cls: 0.1147, d3.loss_bbox: 0.2438, d4.loss_cls: 0.1086, d4.loss_bbox: 0.2482, loss: 2.4245, grad_norm: 26.5140
2025-06-17 05:48:29,174 - mmdet - INFO - Epoch [2][2100/7033]	lr: 1.866e-04, eta: 8:29:44, time: 0.927, data_time: 0.029, memory: 13888, loss_cls: 0.1085, loss_bbox: 0.2459, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3640, d1.loss_cls: 0.1472, d1.loss_bbox: 0.2556, d2.loss_cls: 0.1283, d2.loss_bbox: 0.2390, d3.loss_cls: 0.1133, d3.loss_bbox: 0.2364, d4.loss_cls: 0.1099, d4.loss_bbox: 0.2384, loss: 2.3615, grad_norm: 31.3949
2025-06-17 05:49:15,465 - mmdet - INFO - Epoch [2][2150/7033]	lr: 1.866e-04, eta: 8:28:57, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.1062, loss_bbox: 0.2476, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3647, d1.loss_cls: 0.1428, d1.loss_bbox: 0.2628, d2.loss_cls: 0.1229, d2.loss_bbox: 0.2451, d3.loss_cls: 0.1128, d3.loss_bbox: 0.2391, d4.loss_cls: 0.1081, d4.loss_bbox: 0.2413, loss: 2.3686, grad_norm: 31.4404
2025-06-17 05:50:01,796 - mmdet - INFO - Epoch [2][2200/7033]	lr: 1.866e-04, eta: 8:28:12, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.1027, loss_bbox: 0.2534, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1442, d1.loss_bbox: 0.2638, d2.loss_cls: 0.1201, d2.loss_bbox: 0.2492, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2425, d4.loss_cls: 0.1028, d4.loss_bbox: 0.2464, loss: 2.3708, grad_norm: 28.3492
2025-06-17 05:50:48,246 - mmdet - INFO - Epoch [2][2250/7033]	lr: 1.866e-04, eta: 8:27:26, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.1042, loss_bbox: 0.2590, d0.loss_cls: 0.1665, d0.loss_bbox: 0.3722, d1.loss_cls: 0.1453, d1.loss_bbox: 0.2712, d2.loss_cls: 0.1234, d2.loss_bbox: 0.2551, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2506, d4.loss_cls: 0.1040, d4.loss_bbox: 0.2533, loss: 2.4119, grad_norm: 28.7419
2025-06-17 05:51:34,299 - mmdet - INFO - Epoch [2][2300/7033]	lr: 1.866e-04, eta: 8:26:39, time: 0.921, data_time: 0.029, memory: 13888, loss_cls: 0.1087, loss_bbox: 0.2507, d0.loss_cls: 0.1780, d0.loss_bbox: 0.3644, d1.loss_cls: 0.1529, d1.loss_bbox: 0.2642, d2.loss_cls: 0.1319, d2.loss_bbox: 0.2471, d3.loss_cls: 0.1162, d3.loss_bbox: 0.2434, d4.loss_cls: 0.1100, d4.loss_bbox: 0.2451, loss: 2.4128, grad_norm: 27.8525
2025-06-17 05:52:20,441 - mmdet - INFO - Epoch [2][2350/7033]	lr: 1.866e-04, eta: 8:25:52, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.1068, loss_bbox: 0.2472, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3588, d1.loss_cls: 0.1467, d1.loss_bbox: 0.2540, d2.loss_cls: 0.1274, d2.loss_bbox: 0.2388, d3.loss_cls: 0.1130, d3.loss_bbox: 0.2354, d4.loss_cls: 0.1074, d4.loss_bbox: 0.2410, loss: 2.3512, grad_norm: 34.6950
2025-06-17 05:53:06,862 - mmdet - INFO - Epoch [2][2400/7033]	lr: 1.866e-04, eta: 8:25:07, time: 0.928, data_time: 0.029, memory: 13888, loss_cls: 0.1161, loss_bbox: 0.2641, d0.loss_cls: 0.1856, d0.loss_bbox: 0.3819, d1.loss_cls: 0.1553, d1.loss_bbox: 0.2713, d2.loss_cls: 0.1354, d2.loss_bbox: 0.2538, d3.loss_cls: 0.1205, d3.loss_bbox: 0.2505, d4.loss_cls: 0.1162, d4.loss_bbox: 0.2557, loss: 2.5063, grad_norm: 32.9937
2025-06-17 05:53:52,895 - mmdet - INFO - Epoch [2][2450/7033]	lr: 1.866e-04, eta: 8:24:20, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.1072, loss_bbox: 0.2470, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3640, d1.loss_cls: 0.1481, d1.loss_bbox: 0.2617, d2.loss_cls: 0.1265, d2.loss_bbox: 0.2438, d3.loss_cls: 0.1145, d3.loss_bbox: 0.2371, d4.loss_cls: 0.1079, d4.loss_bbox: 0.2399, loss: 2.3790, grad_norm: 30.3872
2025-06-17 05:54:39,086 - mmdet - INFO - Epoch [2][2500/7033]	lr: 1.866e-04, eta: 8:23:33, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.1016, loss_bbox: 0.2507, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3635, d1.loss_cls: 0.1426, d1.loss_bbox: 0.2612, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2425, d3.loss_cls: 0.1095, d3.loss_bbox: 0.2408, d4.loss_cls: 0.1041, d4.loss_bbox: 0.2440, loss: 2.3558, grad_norm: 27.8298
2025-06-17 05:55:25,386 - mmdet - INFO - Epoch [2][2550/7033]	lr: 1.866e-04, eta: 8:22:47, time: 0.926, data_time: 0.032, memory: 13888, loss_cls: 0.1079, loss_bbox: 0.2706, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1444, d1.loss_bbox: 0.2692, d2.loss_cls: 0.1236, d2.loss_bbox: 0.2574, d3.loss_cls: 0.1122, d3.loss_bbox: 0.2540, d4.loss_cls: 0.1062, d4.loss_bbox: 0.2616, loss: 2.4497, grad_norm: 29.7914
2025-06-17 05:56:11,721 - mmdet - INFO - Epoch [2][2600/7033]	lr: 1.866e-04, eta: 8:22:01, time: 0.927, data_time: 0.028, memory: 13888, loss_cls: 0.1131, loss_bbox: 0.2584, d0.loss_cls: 0.1824, d0.loss_bbox: 0.3717, d1.loss_cls: 0.1511, d1.loss_bbox: 0.2685, d2.loss_cls: 0.1310, d2.loss_bbox: 0.2505, d3.loss_cls: 0.1197, d3.loss_bbox: 0.2484, d4.loss_cls: 0.1121, d4.loss_bbox: 0.2525, loss: 2.4594, grad_norm: 32.2126
2025-06-17 05:56:58,117 - mmdet - INFO - Epoch [2][2650/7033]	lr: 1.866e-04, eta: 8:21:15, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.1028, loss_bbox: 0.2524, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1476, d1.loss_bbox: 0.2646, d2.loss_cls: 0.1236, d2.loss_bbox: 0.2446, d3.loss_cls: 0.1098, d3.loss_bbox: 0.2398, d4.loss_cls: 0.1056, d4.loss_bbox: 0.2455, loss: 2.3728, grad_norm: 30.2451
2025-06-17 05:57:44,196 - mmdet - INFO - Epoch [2][2700/7033]	lr: 1.866e-04, eta: 8:20:29, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.1081, loss_bbox: 0.2654, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3593, d1.loss_cls: 0.1494, d1.loss_bbox: 0.2629, d2.loss_cls: 0.1272, d2.loss_bbox: 0.2480, d3.loss_cls: 0.1117, d3.loss_bbox: 0.2486, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2547, loss: 2.4186, grad_norm: 33.3290
2025-06-17 05:58:30,548 - mmdet - INFO - Epoch [2][2750/7033]	lr: 1.866e-04, eta: 8:19:43, time: 0.927, data_time: 0.028, memory: 13888, loss_cls: 0.1088, loss_bbox: 0.2510, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3677, d1.loss_cls: 0.1473, d1.loss_bbox: 0.2682, d2.loss_cls: 0.1277, d2.loss_bbox: 0.2473, d3.loss_cls: 0.1142, d3.loss_bbox: 0.2403, d4.loss_cls: 0.1095, d4.loss_bbox: 0.2448, loss: 2.4021, grad_norm: 36.9405
2025-06-17 05:59:16,694 - mmdet - INFO - Epoch [2][2800/7033]	lr: 1.866e-04, eta: 8:18:56, time: 0.923, data_time: 0.027, memory: 13888, loss_cls: 0.1045, loss_bbox: 0.2497, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3638, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1231, d2.loss_bbox: 0.2454, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2390, d4.loss_cls: 0.1028, d4.loss_bbox: 0.2430, loss: 2.3570, grad_norm: 32.7463
2025-06-17 06:00:03,173 - mmdet - INFO - Epoch [2][2850/7033]	lr: 1.866e-04, eta: 8:18:11, time: 0.930, data_time: 0.030, memory: 13888, loss_cls: 0.1124, loss_bbox: 0.2567, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3707, d1.loss_cls: 0.1526, d1.loss_bbox: 0.2693, d2.loss_cls: 0.1312, d2.loss_bbox: 0.2501, d3.loss_cls: 0.1151, d3.loss_bbox: 0.2455, d4.loss_cls: 0.1118, d4.loss_bbox: 0.2492, loss: 2.4402, grad_norm: 32.8763
2025-06-17 06:00:49,282 - mmdet - INFO - Epoch [2][2900/7033]	lr: 1.866e-04, eta: 8:17:24, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.1025, loss_bbox: 0.2528, d0.loss_cls: 0.1770, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1467, d1.loss_bbox: 0.2640, d2.loss_cls: 0.1267, d2.loss_bbox: 0.2453, d3.loss_cls: 0.1086, d3.loss_bbox: 0.2410, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2438, loss: 2.3767, grad_norm: 29.7228
2025-06-17 06:01:35,560 - mmdet - INFO - Epoch [2][2950/7033]	lr: 1.866e-04, eta: 8:16:38, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.1015, loss_bbox: 0.2418, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1420, d1.loss_bbox: 0.2562, d2.loss_cls: 0.1214, d2.loss_bbox: 0.2377, d3.loss_cls: 0.1088, d3.loss_bbox: 0.2338, d4.loss_cls: 0.1018, d4.loss_bbox: 0.2367, loss: 2.3012, grad_norm: 28.7787
2025-06-17 06:02:21,827 - mmdet - INFO - Epoch [2][3000/7033]	lr: 1.866e-04, eta: 8:15:52, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0930, loss_bbox: 0.2456, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2565, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2392, loss: 2.2751, grad_norm: 29.9742
2025-06-17 06:03:08,099 - mmdet - INFO - Epoch [2][3050/7033]	lr: 1.866e-04, eta: 8:15:05, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.1025, loss_bbox: 0.2543, d0.loss_cls: 0.1781, d0.loss_bbox: 0.3635, d1.loss_cls: 0.1448, d1.loss_bbox: 0.2678, d2.loss_cls: 0.1240, d2.loss_bbox: 0.2489, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2462, d4.loss_cls: 0.1041, d4.loss_bbox: 0.2485, loss: 2.3898, grad_norm: 33.2172
2025-06-17 06:03:54,531 - mmdet - INFO - Epoch [2][3100/7033]	lr: 1.866e-04, eta: 8:14:20, time: 0.929, data_time: 0.028, memory: 13888, loss_cls: 0.1058, loss_bbox: 0.2698, d0.loss_cls: 0.1878, d0.loss_bbox: 0.3810, d1.loss_cls: 0.1490, d1.loss_bbox: 0.2799, d2.loss_cls: 0.1269, d2.loss_bbox: 0.2616, d3.loss_cls: 0.1113, d3.loss_bbox: 0.2581, d4.loss_cls: 0.1071, d4.loss_bbox: 0.2622, loss: 2.5005, grad_norm: 31.3357
2025-06-17 06:04:40,632 - mmdet - INFO - Epoch [2][3150/7033]	lr: 1.866e-04, eta: 8:13:33, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.1057, loss_bbox: 0.2665, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3719, d1.loss_cls: 0.1452, d1.loss_bbox: 0.2699, d2.loss_cls: 0.1275, d2.loss_bbox: 0.2523, d3.loss_cls: 0.1102, d3.loss_bbox: 0.2512, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2563, loss: 2.4453, grad_norm: 33.0239
2025-06-17 06:05:26,964 - mmdet - INFO - Epoch [2][3200/7033]	lr: 1.866e-04, eta: 8:12:47, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.1109, loss_bbox: 0.2633, d0.loss_cls: 0.1846, d0.loss_bbox: 0.3669, d1.loss_cls: 0.1464, d1.loss_bbox: 0.2748, d2.loss_cls: 0.1302, d2.loss_bbox: 0.2567, d3.loss_cls: 0.1174, d3.loss_bbox: 0.2553, d4.loss_cls: 0.1114, d4.loss_bbox: 0.2565, loss: 2.4745, grad_norm: 27.8517
2025-06-17 06:06:13,140 - mmdet - INFO - Epoch [2][3250/7033]	lr: 1.866e-04, eta: 8:12:01, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.1012, loss_bbox: 0.2603, d0.loss_cls: 0.1794, d0.loss_bbox: 0.3687, d1.loss_cls: 0.1454, d1.loss_bbox: 0.2689, d2.loss_cls: 0.1221, d2.loss_bbox: 0.2551, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2502, d4.loss_cls: 0.1022, d4.loss_bbox: 0.2533, loss: 2.4140, grad_norm: 33.9733
2025-06-17 06:06:59,366 - mmdet - INFO - Epoch [2][3300/7033]	lr: 1.866e-04, eta: 8:11:14, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0996, loss_bbox: 0.2419, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3535, d1.loss_cls: 0.1382, d1.loss_bbox: 0.2556, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2414, d3.loss_cls: 0.1061, d3.loss_bbox: 0.2345, d4.loss_cls: 0.1016, d4.loss_bbox: 0.2369, loss: 2.2987, grad_norm: 34.5608
2025-06-17 06:07:45,465 - mmdet - INFO - Epoch [2][3350/7033]	lr: 1.866e-04, eta: 8:10:28, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.0959, loss_bbox: 0.2410, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3599, d1.loss_cls: 0.1421, d1.loss_bbox: 0.2546, d2.loss_cls: 0.1218, d2.loss_bbox: 0.2392, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2359, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2366, loss: 2.3019, grad_norm: 29.4549
2025-06-17 06:08:31,568 - mmdet - INFO - Epoch [2][3400/7033]	lr: 1.866e-04, eta: 8:09:41, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.1029, loss_bbox: 0.2517, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3731, d1.loss_cls: 0.1432, d1.loss_bbox: 0.2732, d2.loss_cls: 0.1229, d2.loss_bbox: 0.2555, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2475, d4.loss_cls: 0.1045, d4.loss_bbox: 0.2470, loss: 2.4055, grad_norm: 27.7164
2025-06-17 06:09:17,603 - mmdet - INFO - Epoch [2][3450/7033]	lr: 1.866e-04, eta: 8:08:54, time: 0.921, data_time: 0.031, memory: 13888, loss_cls: 0.1054, loss_bbox: 0.2506, d0.loss_cls: 0.1812, d0.loss_bbox: 0.3599, d1.loss_cls: 0.1487, d1.loss_bbox: 0.2616, d2.loss_cls: 0.1243, d2.loss_bbox: 0.2436, d3.loss_cls: 0.1109, d3.loss_bbox: 0.2376, d4.loss_cls: 0.1061, d4.loss_bbox: 0.2419, loss: 2.3718, grad_norm: 28.5248
2025-06-17 06:10:03,724 - mmdet - INFO - Epoch [2][3500/7033]	lr: 1.866e-04, eta: 8:08:07, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.1070, loss_bbox: 0.2494, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3588, d1.loss_cls: 0.1417, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1224, d2.loss_bbox: 0.2482, d3.loss_cls: 0.1094, d3.loss_bbox: 0.2449, d4.loss_cls: 0.1066, d4.loss_bbox: 0.2447, loss: 2.3722, grad_norm: 30.0560
2025-06-17 06:10:50,127 - mmdet - INFO - Epoch [2][3550/7033]	lr: 1.866e-04, eta: 8:07:22, time: 0.928, data_time: 0.029, memory: 13888, loss_cls: 0.1015, loss_bbox: 0.2632, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3582, d1.loss_cls: 0.1383, d1.loss_bbox: 0.2675, d2.loss_cls: 0.1203, d2.loss_bbox: 0.2489, d3.loss_cls: 0.1066, d3.loss_bbox: 0.2466, d4.loss_cls: 0.1022, d4.loss_bbox: 0.2543, loss: 2.3812, grad_norm: 34.3202
2025-06-17 06:11:36,325 - mmdet - INFO - Epoch [2][3600/7033]	lr: 1.866e-04, eta: 8:06:35, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.1034, loss_bbox: 0.2568, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3607, d1.loss_cls: 0.1435, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2495, d3.loss_cls: 0.1086, d3.loss_bbox: 0.2455, d4.loss_cls: 0.1039, d4.loss_bbox: 0.2497, loss: 2.3835, grad_norm: 32.8768
2025-06-17 06:12:23,008 - mmdet - INFO - Epoch [2][3650/7033]	lr: 1.866e-04, eta: 8:05:50, time: 0.934, data_time: 0.031, memory: 13888, loss_cls: 0.0993, loss_bbox: 0.2541, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3602, d1.loss_cls: 0.1359, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1172, d2.loss_bbox: 0.2455, d3.loss_cls: 0.1066, d3.loss_bbox: 0.2410, d4.loss_cls: 0.1002, d4.loss_bbox: 0.2454, loss: 2.3381, grad_norm: 36.1649
2025-06-17 06:13:09,271 - mmdet - INFO - Epoch [2][3700/7033]	lr: 1.866e-04, eta: 8:05:04, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.1057, loss_bbox: 0.2550, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1461, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1247, d2.loss_bbox: 0.2495, d3.loss_cls: 0.1115, d3.loss_bbox: 0.2453, d4.loss_cls: 0.1056, d4.loss_bbox: 0.2486, loss: 2.3914, grad_norm: 27.2903
2025-06-17 06:13:55,312 - mmdet - INFO - Epoch [2][3750/7033]	lr: 1.866e-04, eta: 8:04:17, time: 0.921, data_time: 0.029, memory: 13888, loss_cls: 0.0973, loss_bbox: 0.2452, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3472, d1.loss_cls: 0.1339, d1.loss_bbox: 0.2595, d2.loss_cls: 0.1154, d2.loss_bbox: 0.2398, d3.loss_cls: 0.1022, d3.loss_bbox: 0.2370, d4.loss_cls: 0.0973, d4.loss_bbox: 0.2391, loss: 2.2791, grad_norm: 33.0270
2025-06-17 06:14:41,446 - mmdet - INFO - Epoch [2][3800/7033]	lr: 1.866e-04, eta: 8:03:31, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.1007, loss_bbox: 0.2487, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3685, d1.loss_cls: 0.1400, d1.loss_bbox: 0.2677, d2.loss_cls: 0.1204, d2.loss_bbox: 0.2488, d3.loss_cls: 0.1046, d3.loss_bbox: 0.2442, d4.loss_cls: 0.1011, d4.loss_bbox: 0.2452, loss: 2.3611, grad_norm: 29.7374
2025-06-17 06:15:27,889 - mmdet - INFO - Epoch [2][3850/7033]	lr: 1.866e-04, eta: 8:02:45, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.0994, loss_bbox: 0.2582, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3624, d1.loss_cls: 0.1431, d1.loss_bbox: 0.2647, d2.loss_cls: 0.1218, d2.loss_bbox: 0.2467, d3.loss_cls: 0.1094, d3.loss_bbox: 0.2435, d4.loss_cls: 0.1021, d4.loss_bbox: 0.2485, loss: 2.3736, grad_norm: 35.5613
2025-06-17 06:16:14,679 - mmdet - INFO - Epoch [2][3900/7033]	lr: 1.866e-04, eta: 8:02:00, time: 0.935, data_time: 0.030, memory: 13888, loss_cls: 0.0963, loss_bbox: 0.2565, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3603, d1.loss_cls: 0.1356, d1.loss_bbox: 0.2602, d2.loss_cls: 0.1167, d2.loss_bbox: 0.2422, d3.loss_cls: 0.1046, d3.loss_bbox: 0.2433, d4.loss_cls: 0.0977, d4.loss_bbox: 0.2490, loss: 2.3321, grad_norm: 38.9367
2025-06-17 06:17:01,050 - mmdet - INFO - Epoch [2][3950/7033]	lr: 1.866e-04, eta: 8:01:14, time: 0.928, data_time: 0.032, memory: 13888, loss_cls: 0.1002, loss_bbox: 0.2547, d0.loss_cls: 0.1783, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1375, d1.loss_bbox: 0.2638, d2.loss_cls: 0.1173, d2.loss_bbox: 0.2483, d3.loss_cls: 0.1068, d3.loss_bbox: 0.2429, d4.loss_cls: 0.1026, d4.loss_bbox: 0.2473, loss: 2.3531, grad_norm: 29.5909
2025-06-17 06:17:47,363 - mmdet - INFO - Epoch [2][4000/7033]	lr: 1.866e-04, eta: 8:00:28, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1082, loss_bbox: 0.2580, d0.loss_cls: 0.1802, d0.loss_bbox: 0.3721, d1.loss_cls: 0.1460, d1.loss_bbox: 0.2717, d2.loss_cls: 0.1276, d2.loss_bbox: 0.2526, d3.loss_cls: 0.1148, d3.loss_bbox: 0.2476, d4.loss_cls: 0.1104, d4.loss_bbox: 0.2509, loss: 2.4401, grad_norm: 29.2629
2025-06-17 06:18:33,796 - mmdet - INFO - Epoch [2][4050/7033]	lr: 1.866e-04, eta: 7:59:42, time: 0.929, data_time: 0.028, memory: 13888, loss_cls: 0.1049, loss_bbox: 0.2510, d0.loss_cls: 0.1836, d0.loss_bbox: 0.3693, d1.loss_cls: 0.1450, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1229, d2.loss_bbox: 0.2514, d3.loss_cls: 0.1087, d3.loss_bbox: 0.2454, d4.loss_cls: 0.1053, d4.loss_bbox: 0.2467, loss: 2.4049, grad_norm: 26.6671
2025-06-17 06:19:20,068 - mmdet - INFO - Epoch [2][4100/7033]	lr: 1.866e-04, eta: 7:58:56, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.1055, loss_bbox: 0.2579, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3720, d1.loss_cls: 0.1413, d1.loss_bbox: 0.2721, d2.loss_cls: 0.1244, d2.loss_bbox: 0.2509, d3.loss_cls: 0.1086, d3.loss_bbox: 0.2470, d4.loss_cls: 0.1042, d4.loss_bbox: 0.2499, loss: 2.4057, grad_norm: 32.2259
2025-06-17 06:20:06,282 - mmdet - INFO - Epoch [2][4150/7033]	lr: 1.866e-04, eta: 7:58:10, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.1017, loss_bbox: 0.2512, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1450, d1.loss_bbox: 0.2680, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2479, d3.loss_cls: 0.1073, d3.loss_bbox: 0.2434, d4.loss_cls: 0.1034, d4.loss_bbox: 0.2451, loss: 2.3903, grad_norm: 27.6799
2025-06-17 06:20:52,504 - mmdet - INFO - Epoch [2][4200/7033]	lr: 1.866e-04, eta: 7:57:24, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.1062, loss_bbox: 0.2577, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3667, d1.loss_cls: 0.1419, d1.loss_bbox: 0.2667, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2482, d3.loss_cls: 0.1131, d3.loss_bbox: 0.2436, d4.loss_cls: 0.1081, d4.loss_bbox: 0.2484, loss: 2.4004, grad_norm: 33.4403
2025-06-17 06:21:38,641 - mmdet - INFO - Epoch [2][4250/7033]	lr: 1.866e-04, eta: 7:56:37, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.1029, loss_bbox: 0.2509, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1426, d1.loss_bbox: 0.2650, d2.loss_cls: 0.1255, d2.loss_bbox: 0.2454, d3.loss_cls: 0.1078, d3.loss_bbox: 0.2413, d4.loss_cls: 0.1046, d4.loss_bbox: 0.2448, loss: 2.3641, grad_norm: 33.1205
2025-06-17 06:22:24,677 - mmdet - INFO - Epoch [2][4300/7033]	lr: 1.866e-04, eta: 7:55:50, time: 0.921, data_time: 0.031, memory: 13888, loss_cls: 0.0985, loss_bbox: 0.2577, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3656, d1.loss_cls: 0.1347, d1.loss_bbox: 0.2698, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2509, d3.loss_cls: 0.1049, d3.loss_bbox: 0.2468, d4.loss_cls: 0.0990, d4.loss_bbox: 0.2505, loss: 2.3667, grad_norm: 34.6722
2025-06-17 06:23:11,014 - mmdet - INFO - Epoch [2][4350/7033]	lr: 1.866e-04, eta: 7:55:04, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.1011, loss_bbox: 0.2611, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3661, d1.loss_cls: 0.1384, d1.loss_bbox: 0.2750, d2.loss_cls: 0.1210, d2.loss_bbox: 0.2527, d3.loss_cls: 0.1069, d3.loss_bbox: 0.2489, d4.loss_cls: 0.1034, d4.loss_bbox: 0.2530, loss: 2.4023, grad_norm: 36.8516
2025-06-17 06:23:57,398 - mmdet - INFO - Epoch [2][4400/7033]	lr: 1.866e-04, eta: 7:54:18, time: 0.928, data_time: 0.029, memory: 13888, loss_cls: 0.0944, loss_bbox: 0.2492, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3559, d1.loss_cls: 0.1284, d1.loss_bbox: 0.2590, d2.loss_cls: 0.1131, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2379, d4.loss_cls: 0.0955, d4.loss_bbox: 0.2413, loss: 2.2780, grad_norm: 31.1432
2025-06-17 06:24:43,611 - mmdet - INFO - Epoch [2][4450/7033]	lr: 1.866e-04, eta: 7:53:32, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0991, loss_bbox: 0.2476, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1406, d1.loss_bbox: 0.2631, d2.loss_cls: 0.1198, d2.loss_bbox: 0.2459, d3.loss_cls: 0.1053, d3.loss_bbox: 0.2403, d4.loss_cls: 0.1007, d4.loss_bbox: 0.2425, loss: 2.3439, grad_norm: 33.5868
2025-06-17 06:25:29,796 - mmdet - INFO - Epoch [2][4500/7033]	lr: 1.866e-04, eta: 7:52:45, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0985, loss_bbox: 0.2551, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1370, d1.loss_bbox: 0.2692, d2.loss_cls: 0.1147, d2.loss_bbox: 0.2475, d3.loss_cls: 0.1037, d3.loss_bbox: 0.2420, d4.loss_cls: 0.0990, d4.loss_bbox: 0.2474, loss: 2.3556, grad_norm: 30.9845
2025-06-17 06:26:17,864 - mmdet - INFO - Epoch [2][4550/7033]	lr: 1.866e-04, eta: 7:52:04, time: 0.961, data_time: 0.031, memory: 13888, loss_cls: 0.1000, loss_bbox: 0.2554, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3620, d1.loss_cls: 0.1404, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2480, d3.loss_cls: 0.1042, d3.loss_bbox: 0.2450, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2499, loss: 2.3627, grad_norm: 34.7908
2025-06-17 06:27:04,180 - mmdet - INFO - Epoch [2][4600/7033]	lr: 1.866e-04, eta: 7:51:18, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.1055, loss_bbox: 0.2485, d0.loss_cls: 0.1732, d0.loss_bbox: 0.3603, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2630, d2.loss_cls: 0.1244, d2.loss_bbox: 0.2470, d3.loss_cls: 0.1116, d3.loss_bbox: 0.2411, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2424, loss: 2.3661, grad_norm: 31.9167
2025-06-17 06:27:50,491 - mmdet - INFO - Epoch [2][4650/7033]	lr: 1.866e-04, eta: 7:50:32, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.1035, loss_bbox: 0.2472, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3616, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2600, d2.loss_cls: 0.1231, d2.loss_bbox: 0.2418, d3.loss_cls: 0.1098, d3.loss_bbox: 0.2367, d4.loss_cls: 0.1039, d4.loss_bbox: 0.2417, loss: 2.3474, grad_norm: 32.3376
2025-06-17 06:28:36,660 - mmdet - INFO - Epoch [2][4700/7033]	lr: 1.866e-04, eta: 7:49:45, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.0999, loss_bbox: 0.2445, d0.loss_cls: 0.1803, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1389, d1.loss_bbox: 0.2606, d2.loss_cls: 0.1159, d2.loss_bbox: 0.2450, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2390, d4.loss_cls: 0.0986, d4.loss_bbox: 0.2409, loss: 2.3189, grad_norm: 35.4850
2025-06-17 06:29:23,015 - mmdet - INFO - Epoch [2][4750/7033]	lr: 1.866e-04, eta: 7:48:59, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0994, loss_bbox: 0.2484, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3648, d1.loss_cls: 0.1385, d1.loss_bbox: 0.2662, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2456, d3.loss_cls: 0.1090, d3.loss_bbox: 0.2395, d4.loss_cls: 0.1012, d4.loss_bbox: 0.2424, loss: 2.3422, grad_norm: 33.3749
2025-06-17 06:30:09,452 - mmdet - INFO - Epoch [2][4800/7033]	lr: 1.866e-04, eta: 7:48:13, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.0982, loss_bbox: 0.2447, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1362, d1.loss_bbox: 0.2595, d2.loss_cls: 0.1196, d2.loss_bbox: 0.2393, d3.loss_cls: 0.1045, d3.loss_bbox: 0.2357, d4.loss_cls: 0.1005, d4.loss_bbox: 0.2385, loss: 2.2990, grad_norm: 29.2961
2025-06-17 06:30:55,574 - mmdet - INFO - Epoch [2][4850/7033]	lr: 1.866e-04, eta: 7:47:27, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.0890, loss_bbox: 0.2420, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3515, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2515, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2348, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2354, loss: 2.2369, grad_norm: 39.6136
2025-06-17 06:31:41,792 - mmdet - INFO - Epoch [2][4900/7033]	lr: 1.866e-04, eta: 7:46:40, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.1071, loss_bbox: 0.2596, d0.loss_cls: 0.1879, d0.loss_bbox: 0.3649, d1.loss_cls: 0.1485, d1.loss_bbox: 0.2706, d2.loss_cls: 0.1273, d2.loss_bbox: 0.2505, d3.loss_cls: 0.1152, d3.loss_bbox: 0.2466, d4.loss_cls: 0.1094, d4.loss_bbox: 0.2525, loss: 2.4402, grad_norm: 40.1994
2025-06-17 06:32:28,348 - mmdet - INFO - Epoch [2][4950/7033]	lr: 1.866e-04, eta: 7:45:55, time: 0.931, data_time: 0.032, memory: 13888, loss_cls: 0.1033, loss_bbox: 0.2509, d0.loss_cls: 0.1814, d0.loss_bbox: 0.3665, d1.loss_cls: 0.1424, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1261, d2.loss_bbox: 0.2445, d3.loss_cls: 0.1107, d3.loss_bbox: 0.2396, d4.loss_cls: 0.1060, d4.loss_bbox: 0.2420, loss: 2.3773, grad_norm: 31.1392
2025-06-17 06:33:14,625 - mmdet - INFO - Epoch [2][5000/7033]	lr: 1.866e-04, eta: 7:45:09, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0991, loss_bbox: 0.2447, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1409, d1.loss_bbox: 0.2572, d2.loss_cls: 0.1226, d2.loss_bbox: 0.2389, d3.loss_cls: 0.1051, d3.loss_bbox: 0.2364, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2400, loss: 2.3128, grad_norm: 31.7757
2025-06-17 06:34:00,687 - mmdet - INFO - Epoch [2][5050/7033]	lr: 1.866e-04, eta: 7:44:22, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.1040, loss_bbox: 0.2433, d0.loss_cls: 0.1863, d0.loss_bbox: 0.3610, d1.loss_cls: 0.1468, d1.loss_bbox: 0.2609, d2.loss_cls: 0.1250, d2.loss_bbox: 0.2419, d3.loss_cls: 0.1089, d3.loss_bbox: 0.2379, d4.loss_cls: 0.1048, d4.loss_bbox: 0.2398, loss: 2.3607, grad_norm: 37.5846
2025-06-17 06:34:46,941 - mmdet - INFO - Epoch [2][5100/7033]	lr: 1.866e-04, eta: 7:43:36, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0928, loss_bbox: 0.2418, d0.loss_cls: 0.1733, d0.loss_bbox: 0.3641, d1.loss_cls: 0.1380, d1.loss_bbox: 0.2574, d2.loss_cls: 0.1156, d2.loss_bbox: 0.2385, d3.loss_cls: 0.1000, d3.loss_bbox: 0.2342, d4.loss_cls: 0.0949, d4.loss_bbox: 0.2378, loss: 2.2883, grad_norm: 28.5371
2025-06-17 06:35:32,915 - mmdet - INFO - Epoch [2][5150/7033]	lr: 1.866e-04, eta: 7:42:49, time: 0.919, data_time: 0.028, memory: 13888, loss_cls: 0.1030, loss_bbox: 0.2579, d0.loss_cls: 0.1751, d0.loss_bbox: 0.3770, d1.loss_cls: 0.1377, d1.loss_bbox: 0.2776, d2.loss_cls: 0.1208, d2.loss_bbox: 0.2551, d3.loss_cls: 0.1085, d3.loss_bbox: 0.2510, d4.loss_cls: 0.1041, d4.loss_bbox: 0.2541, loss: 2.4218, grad_norm: 31.4327
2025-06-17 06:36:19,084 - mmdet - INFO - Epoch [2][5200/7033]	lr: 1.866e-04, eta: 7:42:02, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.1103, loss_bbox: 0.2468, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3610, d1.loss_cls: 0.1431, d1.loss_bbox: 0.2625, d2.loss_cls: 0.1279, d2.loss_bbox: 0.2429, d3.loss_cls: 0.1147, d3.loss_bbox: 0.2398, d4.loss_cls: 0.1115, d4.loss_bbox: 0.2427, loss: 2.3810, grad_norm: 38.4586
2025-06-17 06:37:05,433 - mmdet - INFO - Epoch [2][5250/7033]	lr: 1.866e-04, eta: 7:41:16, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.1007, loss_bbox: 0.2493, d0.loss_cls: 0.1799, d0.loss_bbox: 0.3685, d1.loss_cls: 0.1384, d1.loss_bbox: 0.2688, d2.loss_cls: 0.1213, d2.loss_bbox: 0.2472, d3.loss_cls: 0.1072, d3.loss_bbox: 0.2442, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2442, loss: 2.3728, grad_norm: 32.0314
2025-06-17 06:37:51,929 - mmdet - INFO - Epoch [2][5300/7033]	lr: 1.866e-04, eta: 7:40:31, time: 0.930, data_time: 0.031, memory: 13888, loss_cls: 0.1015, loss_bbox: 0.2672, d0.loss_cls: 0.1777, d0.loss_bbox: 0.3655, d1.loss_cls: 0.1433, d1.loss_bbox: 0.2734, d2.loss_cls: 0.1266, d2.loss_bbox: 0.2573, d3.loss_cls: 0.1082, d3.loss_bbox: 0.2561, d4.loss_cls: 0.1043, d4.loss_bbox: 0.2607, loss: 2.4418, grad_norm: 32.0752
2025-06-17 06:38:38,048 - mmdet - INFO - Epoch [2][5350/7033]	lr: 1.866e-04, eta: 7:39:44, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0991, loss_bbox: 0.2427, d0.loss_cls: 0.1808, d0.loss_bbox: 0.3594, d1.loss_cls: 0.1478, d1.loss_bbox: 0.2582, d2.loss_cls: 0.1237, d2.loss_bbox: 0.2399, d3.loss_cls: 0.1077, d3.loss_bbox: 0.2365, d4.loss_cls: 0.1014, d4.loss_bbox: 0.2398, loss: 2.3370, grad_norm: 31.6839
2025-06-17 06:39:24,150 - mmdet - INFO - Epoch [2][5400/7033]	lr: 1.866e-04, eta: 7:38:57, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.1011, loss_bbox: 0.2435, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3669, d1.loss_cls: 0.1375, d1.loss_bbox: 0.2624, d2.loss_cls: 0.1198, d2.loss_bbox: 0.2418, d3.loss_cls: 0.1070, d3.loss_bbox: 0.2357, d4.loss_cls: 0.1024, d4.loss_bbox: 0.2383, loss: 2.3258, grad_norm: 34.5010
2025-06-17 06:40:10,711 - mmdet - INFO - Epoch [2][5450/7033]	lr: 1.866e-04, eta: 7:38:12, time: 0.931, data_time: 0.031, memory: 13888, loss_cls: 0.0947, loss_bbox: 0.2488, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2647, d2.loss_cls: 0.1110, d2.loss_bbox: 0.2475, d3.loss_cls: 0.1010, d3.loss_bbox: 0.2415, d4.loss_cls: 0.0958, d4.loss_bbox: 0.2458, loss: 2.3160, grad_norm: 40.2159
2025-06-17 06:40:57,070 - mmdet - INFO - Epoch [2][5500/7033]	lr: 1.866e-04, eta: 7:37:26, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0998, loss_bbox: 0.2502, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3631, d1.loss_cls: 0.1362, d1.loss_bbox: 0.2692, d2.loss_cls: 0.1184, d2.loss_bbox: 0.2513, d3.loss_cls: 0.1058, d3.loss_bbox: 0.2464, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2478, loss: 2.3636, grad_norm: 32.7350
2025-06-17 06:41:43,048 - mmdet - INFO - Epoch [2][5550/7033]	lr: 1.866e-04, eta: 7:36:39, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.0989, loss_bbox: 0.2481, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3691, d1.loss_cls: 0.1366, d1.loss_bbox: 0.2693, d2.loss_cls: 0.1186, d2.loss_bbox: 0.2487, d3.loss_cls: 0.1052, d3.loss_bbox: 0.2426, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2435, loss: 2.3500, grad_norm: 28.2557
2025-06-17 06:42:29,144 - mmdet - INFO - Epoch [2][5600/7033]	lr: 1.866e-04, eta: 7:35:52, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0944, loss_bbox: 0.2487, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2624, d2.loss_cls: 0.1154, d2.loss_bbox: 0.2452, d3.loss_cls: 0.1041, d3.loss_bbox: 0.2394, d4.loss_cls: 0.0970, d4.loss_bbox: 0.2430, loss: 2.3139, grad_norm: 38.6590
2025-06-17 06:43:15,407 - mmdet - INFO - Epoch [2][5650/7033]	lr: 1.866e-04, eta: 7:35:06, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0971, loss_bbox: 0.2546, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3733, d1.loss_cls: 0.1400, d1.loss_bbox: 0.2779, d2.loss_cls: 0.1167, d2.loss_bbox: 0.2558, d3.loss_cls: 0.1059, d3.loss_bbox: 0.2504, d4.loss_cls: 0.1005, d4.loss_bbox: 0.2530, loss: 2.4015, grad_norm: 29.2259
2025-06-17 06:44:01,598 - mmdet - INFO - Epoch [2][5700/7033]	lr: 1.866e-04, eta: 7:34:19, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0879, loss_bbox: 0.2362, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3466, d1.loss_cls: 0.1307, d1.loss_bbox: 0.2558, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0961, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0882, d4.loss_bbox: 0.2351, loss: 2.2207, grad_norm: 38.9141
2025-06-17 06:44:47,916 - mmdet - INFO - Epoch [2][5750/7033]	lr: 1.866e-04, eta: 7:33:33, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.1054, loss_bbox: 0.2472, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3582, d1.loss_cls: 0.1427, d1.loss_bbox: 0.2641, d2.loss_cls: 0.1227, d2.loss_bbox: 0.2438, d3.loss_cls: 0.1102, d3.loss_bbox: 0.2396, d4.loss_cls: 0.1068, d4.loss_bbox: 0.2422, loss: 2.3642, grad_norm: 28.8012
2025-06-17 06:45:36,251 - mmdet - INFO - Epoch [2][5800/7033]	lr: 1.866e-04, eta: 7:32:52, time: 0.967, data_time: 0.032, memory: 13888, loss_cls: 0.0916, loss_bbox: 0.2434, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3516, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2567, d2.loss_cls: 0.1141, d2.loss_bbox: 0.2387, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2403, loss: 2.2660, grad_norm: 31.8457
2025-06-17 06:46:22,458 - mmdet - INFO - Epoch [2][5850/7033]	lr: 1.866e-04, eta: 7:32:05, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.1012, loss_bbox: 0.2499, d0.loss_cls: 0.1747, d0.loss_bbox: 0.3541, d1.loss_cls: 0.1399, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1187, d2.loss_bbox: 0.2442, d3.loss_cls: 0.1042, d3.loss_bbox: 0.2433, d4.loss_cls: 0.1009, d4.loss_bbox: 0.2481, loss: 2.3431, grad_norm: 35.9841
2025-06-17 06:47:08,917 - mmdet - INFO - Epoch [2][5900/7033]	lr: 1.866e-04, eta: 7:31:20, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.0962, loss_bbox: 0.2569, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3620, d1.loss_cls: 0.1378, d1.loss_bbox: 0.2635, d2.loss_cls: 0.1175, d2.loss_bbox: 0.2471, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2452, d4.loss_cls: 0.0970, d4.loss_bbox: 0.2510, loss: 2.3490, grad_norm: 36.3036
2025-06-17 06:47:55,308 - mmdet - INFO - Epoch [2][5950/7033]	lr: 1.866e-04, eta: 7:30:34, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.0971, loss_bbox: 0.2526, d0.loss_cls: 0.1787, d0.loss_bbox: 0.3665, d1.loss_cls: 0.1362, d1.loss_bbox: 0.2690, d2.loss_cls: 0.1178, d2.loss_bbox: 0.2497, d3.loss_cls: 0.1031, d3.loss_bbox: 0.2461, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2495, loss: 2.3658, grad_norm: 41.9122
2025-06-17 06:48:41,485 - mmdet - INFO - Epoch [2][6000/7033]	lr: 1.866e-04, eta: 7:29:47, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0913, loss_bbox: 0.2376, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1292, d1.loss_bbox: 0.2532, d2.loss_cls: 0.1129, d2.loss_bbox: 0.2341, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2320, d4.loss_cls: 0.0923, d4.loss_bbox: 0.2335, loss: 2.2306, grad_norm: 37.1561
2025-06-17 06:49:27,687 - mmdet - INFO - Epoch [2][6050/7033]	lr: 1.866e-04, eta: 7:29:01, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.0938, loss_bbox: 0.2554, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3651, d1.loss_cls: 0.1290, d1.loss_bbox: 0.2680, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2504, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2492, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2528, loss: 2.3431, grad_norm: 24.0424
2025-06-17 06:50:13,870 - mmdet - INFO - Epoch [2][6100/7033]	lr: 1.866e-04, eta: 7:28:14, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0944, loss_bbox: 0.2370, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2549, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2361, d3.loss_cls: 0.1011, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2375, loss: 2.2628, grad_norm: 30.3240
2025-06-17 06:50:59,863 - mmdet - INFO - Epoch [2][6150/7033]	lr: 1.866e-04, eta: 7:27:27, time: 0.920, data_time: 0.028, memory: 13888, loss_cls: 0.0984, loss_bbox: 0.2378, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3601, d1.loss_cls: 0.1331, d1.loss_bbox: 0.2607, d2.loss_cls: 0.1184, d2.loss_bbox: 0.2401, d3.loss_cls: 0.1053, d3.loss_bbox: 0.2350, d4.loss_cls: 0.0998, d4.loss_bbox: 0.2356, loss: 2.2945, grad_norm: 28.2124
2025-06-17 06:51:46,169 - mmdet - INFO - Epoch [2][6200/7033]	lr: 1.866e-04, eta: 7:26:41, time: 0.926, data_time: 0.029, memory: 13888, loss_cls: 0.0950, loss_bbox: 0.2494, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1352, d1.loss_bbox: 0.2636, d2.loss_cls: 0.1120, d2.loss_bbox: 0.2429, d3.loss_cls: 0.0987, d3.loss_bbox: 0.2407, d4.loss_cls: 0.0957, d4.loss_bbox: 0.2442, loss: 2.3117, grad_norm: 31.2728
2025-06-17 06:52:32,462 - mmdet - INFO - Epoch [2][6250/7033]	lr: 1.866e-04, eta: 7:25:55, time: 0.926, data_time: 0.029, memory: 13888, loss_cls: 0.1041, loss_bbox: 0.2416, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3683, d1.loss_cls: 0.1355, d1.loss_bbox: 0.2654, d2.loss_cls: 0.1185, d2.loss_bbox: 0.2456, d3.loss_cls: 0.1097, d3.loss_bbox: 0.2399, d4.loss_cls: 0.1039, d4.loss_bbox: 0.2432, loss: 2.3497, grad_norm: 40.6231
2025-06-17 06:53:18,755 - mmdet - INFO - Epoch [2][6300/7033]	lr: 1.866e-04, eta: 7:25:09, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.1021, loss_bbox: 0.2450, d0.loss_cls: 0.1730, d0.loss_bbox: 0.3705, d1.loss_cls: 0.1359, d1.loss_bbox: 0.2659, d2.loss_cls: 0.1207, d2.loss_bbox: 0.2447, d3.loss_cls: 0.1074, d3.loss_bbox: 0.2392, d4.loss_cls: 0.1031, d4.loss_bbox: 0.2432, loss: 2.3505, grad_norm: 30.5498
2025-06-17 06:54:05,160 - mmdet - INFO - Epoch [2][6350/7033]	lr: 1.866e-04, eta: 7:24:23, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.1064, loss_bbox: 0.2452, d0.loss_cls: 0.1838, d0.loss_bbox: 0.3781, d1.loss_cls: 0.1503, d1.loss_bbox: 0.2704, d2.loss_cls: 0.1299, d2.loss_bbox: 0.2466, d3.loss_cls: 0.1127, d3.loss_bbox: 0.2409, d4.loss_cls: 0.1065, d4.loss_bbox: 0.2445, loss: 2.4153, grad_norm: 33.1824
2025-06-17 06:54:51,598 - mmdet - INFO - Epoch [2][6400/7033]	lr: 1.866e-04, eta: 7:23:37, time: 0.929, data_time: 0.029, memory: 13888, loss_cls: 0.1039, loss_bbox: 0.2506, d0.loss_cls: 0.1769, d0.loss_bbox: 0.3686, d1.loss_cls: 0.1435, d1.loss_bbox: 0.2671, d2.loss_cls: 0.1266, d2.loss_bbox: 0.2470, d3.loss_cls: 0.1094, d3.loss_bbox: 0.2446, d4.loss_cls: 0.1040, d4.loss_bbox: 0.2490, loss: 2.3911, grad_norm: 30.3831
2025-06-17 06:55:37,758 - mmdet - INFO - Epoch [2][6450/7033]	lr: 1.866e-04, eta: 7:22:50, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0978, loss_bbox: 0.2453, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3703, d1.loss_cls: 0.1373, d1.loss_bbox: 0.2677, d2.loss_cls: 0.1189, d2.loss_bbox: 0.2487, d3.loss_cls: 0.1055, d3.loss_bbox: 0.2424, d4.loss_cls: 0.0985, d4.loss_bbox: 0.2449, loss: 2.3456, grad_norm: 50.5475
2025-06-17 06:56:24,102 - mmdet - INFO - Epoch [2][6500/7033]	lr: 1.866e-04, eta: 7:22:04, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0984, loss_bbox: 0.2483, d0.loss_cls: 0.1756, d0.loss_bbox: 0.3569, d1.loss_cls: 0.1387, d1.loss_bbox: 0.2593, d2.loss_cls: 0.1188, d2.loss_bbox: 0.2433, d3.loss_cls: 0.1041, d3.loss_bbox: 0.2398, d4.loss_cls: 0.1006, d4.loss_bbox: 0.2429, loss: 2.3266, grad_norm: 30.2437
2025-06-17 06:57:10,277 - mmdet - INFO - Epoch [2][6550/7033]	lr: 1.866e-04, eta: 7:21:18, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0910, loss_bbox: 0.2430, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2554, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2394, loss: 2.2500, grad_norm: 47.2561
2025-06-17 06:57:56,366 - mmdet - INFO - Epoch [2][6600/7033]	lr: 1.866e-04, eta: 7:20:31, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0938, loss_bbox: 0.2322, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3570, d1.loss_cls: 0.1337, d1.loss_bbox: 0.2509, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2336, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2293, d4.loss_cls: 0.0952, d4.loss_bbox: 0.2333, loss: 2.2432, grad_norm: 37.2416
2025-06-17 06:58:42,598 - mmdet - INFO - Epoch [2][6650/7033]	lr: 1.866e-04, eta: 7:19:45, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.1032, loss_bbox: 0.2503, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1381, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1200, d2.loss_bbox: 0.2490, d3.loss_cls: 0.1080, d3.loss_bbox: 0.2451, d4.loss_cls: 0.1016, d4.loss_bbox: 0.2503, loss: 2.3696, grad_norm: 44.0066
2025-06-17 06:59:28,712 - mmdet - INFO - Epoch [2][6700/7033]	lr: 1.866e-04, eta: 7:18:58, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0914, loss_bbox: 0.2379, d0.loss_cls: 0.1689, d0.loss_bbox: 0.3577, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2605, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2410, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2365, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2390, loss: 2.2688, grad_norm: 32.7450
2025-06-17 07:00:15,035 - mmdet - INFO - Epoch [2][6750/7033]	lr: 1.866e-04, eta: 7:18:12, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0960, loss_bbox: 0.2516, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3605, d1.loss_cls: 0.1344, d1.loss_bbox: 0.2639, d2.loss_cls: 0.1182, d2.loss_bbox: 0.2474, d3.loss_cls: 0.1030, d3.loss_bbox: 0.2437, d4.loss_cls: 0.0977, d4.loss_bbox: 0.2485, loss: 2.3340, grad_norm: 28.7907
2025-06-17 07:01:01,072 - mmdet - INFO - Epoch [2][6800/7033]	lr: 1.866e-04, eta: 7:17:25, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0979, loss_bbox: 0.2412, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3633, d1.loss_cls: 0.1359, d1.loss_bbox: 0.2560, d2.loss_cls: 0.1166, d2.loss_bbox: 0.2369, d3.loss_cls: 0.1038, d3.loss_bbox: 0.2369, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2393, loss: 2.3006, grad_norm: 33.9638
2025-06-17 07:01:47,352 - mmdet - INFO - Epoch [2][6850/7033]	lr: 1.866e-04, eta: 7:16:39, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.0996, loss_bbox: 0.2473, d0.loss_cls: 0.1782, d0.loss_bbox: 0.3682, d1.loss_cls: 0.1409, d1.loss_bbox: 0.2663, d2.loss_cls: 0.1183, d2.loss_bbox: 0.2470, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2434, d4.loss_cls: 0.1021, d4.loss_bbox: 0.2464, loss: 2.3624, grad_norm: 38.9492
2025-06-17 07:02:33,833 - mmdet - INFO - Epoch [2][6900/7033]	lr: 1.866e-04, eta: 7:15:53, time: 0.930, data_time: 0.032, memory: 13888, loss_cls: 0.0995, loss_bbox: 0.2602, d0.loss_cls: 0.1819, d0.loss_bbox: 0.3771, d1.loss_cls: 0.1401, d1.loss_bbox: 0.2741, d2.loss_cls: 0.1192, d2.loss_bbox: 0.2538, d3.loss_cls: 0.1053, d3.loss_bbox: 0.2519, d4.loss_cls: 0.1010, d4.loss_bbox: 0.2555, loss: 2.4197, grad_norm: 32.8134
2025-06-17 07:03:20,042 - mmdet - INFO - Epoch [2][6950/7033]	lr: 1.866e-04, eta: 7:15:07, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0892, loss_bbox: 0.2305, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3545, d1.loss_cls: 0.1305, d1.loss_bbox: 0.2511, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2308, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2316, loss: 2.2228, grad_norm: 37.5962
2025-06-17 07:04:06,279 - mmdet - INFO - Epoch [2][7000/7033]	lr: 1.866e-04, eta: 7:14:21, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0961, loss_bbox: 0.2445, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3560, d1.loss_cls: 0.1372, d1.loss_bbox: 0.2612, d2.loss_cls: 0.1181, d2.loss_bbox: 0.2440, d3.loss_cls: 0.1034, d3.loss_bbox: 0.2413, d4.loss_cls: 0.0975, d4.loss_bbox: 0.2443, loss: 2.3188, grad_norm: 46.1422
2025-06-17 07:04:37,101 - mmdet - INFO - Saving checkpoint at 2 epochs
2025-06-17 07:26:14,607 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 07:26:14,607 - mmdet - INFO - Epoch(val) [2][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7652, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8696, pts_bbox_NuScenes/car_AP_dist_2.0: 0.8971, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9127, pts_bbox_NuScenes/car_trans_err: 0.2065, pts_bbox_NuScenes/car_scale_err: 0.1543, pts_bbox_NuScenes/car_orient_err: 0.0494, pts_bbox_NuScenes/car_vel_err: 0.3632, pts_bbox_NuScenes/car_attr_err: 0.1617, pts_bbox_NuScenes/mATE: 0.3154, pts_bbox_NuScenes/mASE: 0.2701, pts_bbox_NuScenes/mAOE: 0.2636, pts_bbox_NuScenes/mAVE: 0.3326, pts_bbox_NuScenes/mAAE: 0.1757, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4129, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6128, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7127, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7544, pts_bbox_NuScenes/truck_trans_err: 0.3506, pts_bbox_NuScenes/truck_scale_err: 0.2040, pts_bbox_NuScenes/truck_orient_err: 0.0559, pts_bbox_NuScenes/truck_vel_err: 0.3021, pts_bbox_NuScenes/truck_attr_err: 0.2052, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0620, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2027, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3979, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4666, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6437, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4424, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8065, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1017, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3079, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4794, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7336, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8860, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9140, pts_bbox_NuScenes/bus_trans_err: 0.3570, pts_bbox_NuScenes/bus_scale_err: 0.2014, pts_bbox_NuScenes/bus_orient_err: 0.0416, pts_bbox_NuScenes/bus_vel_err: 0.6331, pts_bbox_NuScenes/bus_attr_err: 0.2497, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1444, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3904, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5563, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6704, pts_bbox_NuScenes/trailer_trans_err: 0.5147, pts_bbox_NuScenes/trailer_scale_err: 0.2601, pts_bbox_NuScenes/trailer_orient_err: 0.5208, pts_bbox_NuScenes/trailer_vel_err: 0.1988, pts_bbox_NuScenes/trailer_attr_err: 0.1276, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5495, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6548, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7110, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7346, pts_bbox_NuScenes/barrier_trans_err: 0.2540, pts_bbox_NuScenes/barrier_scale_err: 0.2853, pts_bbox_NuScenes/barrier_orient_err: 0.0581, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5950, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7460, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7836, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7898, pts_bbox_NuScenes/motorcycle_trans_err: 0.2413, pts_bbox_NuScenes/motorcycle_scale_err: 0.2470, pts_bbox_NuScenes/motorcycle_orient_err: 0.2221, pts_bbox_NuScenes/motorcycle_vel_err: 0.6021, pts_bbox_NuScenes/motorcycle_attr_err: 0.2319, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5093, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5726, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5836, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5920, pts_bbox_NuScenes/bicycle_trans_err: 0.2010, pts_bbox_NuScenes/bicycle_scale_err: 0.2764, pts_bbox_NuScenes/bicycle_orient_err: 0.2962, pts_bbox_NuScenes/bicycle_vel_err: 0.2303, pts_bbox_NuScenes/bicycle_attr_err: 0.0099, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7808, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8353, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8637, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8786, pts_bbox_NuScenes/pedestrian_trans_err: 0.1969, pts_bbox_NuScenes/pedestrian_scale_err: 0.2920, pts_bbox_NuScenes/pedestrian_orient_err: 0.3214, pts_bbox_NuScenes/pedestrian_vel_err: 0.2293, pts_bbox_NuScenes/pedestrian_attr_err: 0.1114, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6691, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7180, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7530, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.7861, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1882, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3385, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6886, pts_bbox_NuScenes/mAP: 0.6487
2025-06-17 07:27:10,097 - mmdet - INFO - Epoch [3][50/7033]	lr: 1.501e-04, eta: 7:12:12, time: 1.017, data_time: 0.125, memory: 13888, loss_cls: 0.0971, loss_bbox: 0.2414, d0.loss_cls: 0.1722, d0.loss_bbox: 0.3587, d1.loss_cls: 0.1361, d1.loss_bbox: 0.2564, d2.loss_cls: 0.1174, d2.loss_bbox: 0.2383, d3.loss_cls: 0.1037, d3.loss_bbox: 0.2337, d4.loss_cls: 0.0974, d4.loss_bbox: 0.2376, loss: 2.2900, grad_norm: 32.1674
2025-06-17 07:27:56,282 - mmdet - INFO - Epoch [3][100/7033]	lr: 1.501e-04, eta: 7:11:26, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0922, loss_bbox: 0.2476, d0.loss_cls: 0.1728, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1323, d1.loss_bbox: 0.2623, d2.loss_cls: 0.1122, d2.loss_bbox: 0.2455, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2420, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2459, loss: 2.3012, grad_norm: 30.2064
2025-06-17 07:28:42,584 - mmdet - INFO - Epoch [3][150/7033]	lr: 1.501e-04, eta: 7:10:40, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0901, loss_bbox: 0.2323, d0.loss_cls: 0.1632, d0.loss_bbox: 0.3461, d1.loss_cls: 0.1265, d1.loss_bbox: 0.2491, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2316, loss: 2.1937, grad_norm: 38.6851
2025-06-17 07:29:28,746 - mmdet - INFO - Epoch [3][200/7033]	lr: 1.501e-04, eta: 7:09:54, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0888, loss_bbox: 0.2381, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2569, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0975, d3.loss_bbox: 0.2335, d4.loss_cls: 0.0897, d4.loss_bbox: 0.2387, loss: 2.2514, grad_norm: 37.2706
2025-06-17 07:30:15,002 - mmdet - INFO - Epoch [3][250/7033]	lr: 1.501e-04, eta: 7:09:08, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0877, loss_bbox: 0.2293, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3441, d1.loss_cls: 0.1327, d1.loss_bbox: 0.2478, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2287, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2260, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2282, loss: 2.1844, grad_norm: 31.8296
2025-06-17 07:31:01,110 - mmdet - INFO - Epoch [3][300/7033]	lr: 1.501e-04, eta: 7:08:22, time: 0.922, data_time: 0.028, memory: 13888, loss_cls: 0.0918, loss_bbox: 0.2359, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3511, d1.loss_cls: 0.1310, d1.loss_bbox: 0.2577, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2362, d3.loss_cls: 0.0987, d3.loss_bbox: 0.2330, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2347, loss: 2.2478, grad_norm: 40.7404
2025-06-17 07:31:47,187 - mmdet - INFO - Epoch [3][350/7033]	lr: 1.501e-04, eta: 7:07:35, time: 0.921, data_time: 0.027, memory: 13888, loss_cls: 0.0910, loss_bbox: 0.2385, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3528, d1.loss_cls: 0.1252, d1.loss_bbox: 0.2615, d2.loss_cls: 0.1077, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2365, loss: 2.2454, grad_norm: 38.1774
2025-06-17 07:32:33,386 - mmdet - INFO - Epoch [3][400/7033]	lr: 1.501e-04, eta: 7:06:49, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0948, loss_bbox: 0.2271, d0.loss_cls: 0.1778, d0.loss_bbox: 0.3431, d1.loss_cls: 0.1358, d1.loss_bbox: 0.2484, d2.loss_cls: 0.1167, d2.loss_bbox: 0.2311, d3.loss_cls: 0.1019, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0969, d4.loss_bbox: 0.2258, loss: 2.2247, grad_norm: 43.9911
2025-06-17 07:33:19,434 - mmdet - INFO - Epoch [3][450/7033]	lr: 1.501e-04, eta: 7:06:03, time: 0.921, data_time: 0.032, memory: 13888, loss_cls: 0.0948, loss_bbox: 0.2256, d0.loss_cls: 0.1677, d0.loss_bbox: 0.3517, d1.loss_cls: 0.1317, d1.loss_bbox: 0.2555, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2338, d3.loss_cls: 0.1013, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0962, d4.loss_bbox: 0.2275, loss: 2.2297, grad_norm: 29.2077
2025-06-17 07:34:05,559 - mmdet - INFO - Epoch [3][500/7033]	lr: 1.501e-04, eta: 7:05:17, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0918, loss_bbox: 0.2336, d0.loss_cls: 0.1741, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1341, d1.loss_bbox: 0.2545, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2329, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2364, loss: 2.2428, grad_norm: 39.6499
2025-06-17 07:34:51,545 - mmdet - INFO - Epoch [3][550/7033]	lr: 1.501e-04, eta: 7:04:30, time: 0.920, data_time: 0.028, memory: 13888, loss_cls: 0.0874, loss_bbox: 0.2291, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2542, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2340, d3.loss_cls: 0.0975, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0893, d4.loss_bbox: 0.2301, loss: 2.2106, grad_norm: 30.3828
2025-06-17 07:35:37,859 - mmdet - INFO - Epoch [3][600/7033]	lr: 1.501e-04, eta: 7:03:44, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0979, loss_bbox: 0.2339, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1367, d1.loss_bbox: 0.2559, d2.loss_cls: 0.1170, d2.loss_bbox: 0.2383, d3.loss_cls: 0.1069, d3.loss_bbox: 0.2352, d4.loss_cls: 0.0999, d4.loss_bbox: 0.2366, loss: 2.2819, grad_norm: 40.6568
2025-06-17 07:36:24,060 - mmdet - INFO - Epoch [3][650/7033]	lr: 1.501e-04, eta: 7:02:58, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0884, loss_bbox: 0.2279, d0.loss_cls: 0.1685, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1289, d1.loss_bbox: 0.2519, d2.loss_cls: 0.1082, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2276, loss: 2.1934, grad_norm: 44.5726
2025-06-17 07:37:10,247 - mmdet - INFO - Epoch [3][700/7033]	lr: 1.501e-04, eta: 7:02:12, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0982, loss_bbox: 0.2296, d0.loss_cls: 0.1774, d0.loss_bbox: 0.3562, d1.loss_cls: 0.1394, d1.loss_bbox: 0.2586, d2.loss_cls: 0.1168, d2.loss_bbox: 0.2399, d3.loss_cls: 0.1048, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0991, d4.loss_bbox: 0.2321, loss: 2.2848, grad_norm: 44.3711
2025-06-17 07:37:56,221 - mmdet - INFO - Epoch [3][750/7033]	lr: 1.501e-04, eta: 7:01:25, time: 0.919, data_time: 0.027, memory: 13888, loss_cls: 0.0912, loss_bbox: 0.2388, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1316, d1.loss_bbox: 0.2590, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2423, d3.loss_cls: 0.1005, d3.loss_bbox: 0.2376, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2404, loss: 2.2793, grad_norm: 40.9648
2025-06-17 07:38:42,445 - mmdet - INFO - Epoch [3][800/7033]	lr: 1.501e-04, eta: 7:00:39, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0920, loss_bbox: 0.2357, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3538, d1.loss_cls: 0.1346, d1.loss_bbox: 0.2540, d2.loss_cls: 0.1169, d2.loss_bbox: 0.2367, d3.loss_cls: 0.1019, d3.loss_bbox: 0.2327, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2373, loss: 2.2645, grad_norm: 37.5237
2025-06-17 07:39:28,717 - mmdet - INFO - Epoch [3][850/7033]	lr: 1.501e-04, eta: 6:59:53, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0911, loss_bbox: 0.2332, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3493, d1.loss_cls: 0.1317, d1.loss_bbox: 0.2513, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2346, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2325, loss: 2.2195, grad_norm: 31.4151
2025-06-17 07:40:15,332 - mmdet - INFO - Epoch [3][900/7033]	lr: 1.501e-04, eta: 6:59:08, time: 0.932, data_time: 0.031, memory: 13888, loss_cls: 0.0952, loss_bbox: 0.2368, d0.loss_cls: 0.1815, d0.loss_bbox: 0.3680, d1.loss_cls: 0.1357, d1.loss_bbox: 0.2610, d2.loss_cls: 0.1146, d2.loss_bbox: 0.2396, d3.loss_cls: 0.1017, d3.loss_bbox: 0.2353, d4.loss_cls: 0.0966, d4.loss_bbox: 0.2365, loss: 2.3025, grad_norm: 39.5039
2025-06-17 07:41:01,677 - mmdet - INFO - Epoch [3][950/7033]	lr: 1.501e-04, eta: 6:58:22, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0916, loss_bbox: 0.2324, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1304, d1.loss_bbox: 0.2547, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2338, d3.loss_cls: 0.0978, d3.loss_bbox: 0.2290, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2309, loss: 2.2276, grad_norm: 43.7527
2025-06-17 07:41:47,877 - mmdet - INFO - Epoch [3][1000/7033]	lr: 1.501e-04, eta: 6:57:36, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.0866, loss_bbox: 0.2306, d0.loss_cls: 0.1672, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1229, d1.loss_bbox: 0.2470, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2257, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2271, loss: 2.1738, grad_norm: 42.5701
2025-06-17 07:42:33,979 - mmdet - INFO - Epoch [3][1050/7033]	lr: 1.501e-04, eta: 6:56:50, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0863, loss_bbox: 0.2267, d0.loss_cls: 0.1661, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2466, d2.loss_cls: 0.1059, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0866, d4.loss_bbox: 0.2272, loss: 2.1558, grad_norm: 38.0995
2025-06-17 07:43:20,175 - mmdet - INFO - Epoch [3][1100/7033]	lr: 1.501e-04, eta: 6:56:03, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0905, loss_bbox: 0.2278, d0.loss_cls: 0.1642, d0.loss_bbox: 0.3484, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2525, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0951, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2310, loss: 2.1973, grad_norm: 35.9437
2025-06-17 07:44:06,406 - mmdet - INFO - Epoch [3][1150/7033]	lr: 1.501e-04, eta: 6:55:17, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0890, loss_bbox: 0.2428, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2647, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2462, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2407, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2440, loss: 2.2831, grad_norm: 50.3689
2025-06-17 07:44:52,596 - mmdet - INFO - Epoch [3][1200/7033]	lr: 1.501e-04, eta: 6:54:31, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0888, loss_bbox: 0.2298, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1277, d1.loss_bbox: 0.2515, d2.loss_cls: 0.1083, d2.loss_bbox: 0.2333, d3.loss_cls: 0.0949, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0908, d4.loss_bbox: 0.2281, loss: 2.2075, grad_norm: 40.9431
2025-06-17 07:45:38,890 - mmdet - INFO - Epoch [3][1250/7033]	lr: 1.501e-04, eta: 6:53:45, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.0936, loss_bbox: 0.2358, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1301, d1.loss_bbox: 0.2574, d2.loss_cls: 0.1158, d2.loss_bbox: 0.2392, d3.loss_cls: 0.1013, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0956, d4.loss_bbox: 0.2354, loss: 2.2737, grad_norm: 38.5650
2025-06-17 07:46:25,147 - mmdet - INFO - Epoch [3][1300/7033]	lr: 1.501e-04, eta: 6:52:59, time: 0.925, data_time: 0.032, memory: 13888, loss_cls: 0.0918, loss_bbox: 0.2327, d0.loss_cls: 0.1726, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2527, d2.loss_cls: 0.1095, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0972, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2334, loss: 2.2249, grad_norm: 32.0022
2025-06-17 07:47:11,322 - mmdet - INFO - Epoch [3][1350/7033]	lr: 1.501e-04, eta: 6:52:13, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0924, loss_bbox: 0.2303, d0.loss_cls: 0.1718, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1351, d1.loss_bbox: 0.2508, d2.loss_cls: 0.1153, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0989, d3.loss_bbox: 0.2310, d4.loss_cls: 0.0949, d4.loss_bbox: 0.2303, loss: 2.2332, grad_norm: 34.8709
2025-06-17 07:47:57,533 - mmdet - INFO - Epoch [3][1400/7033]	lr: 1.501e-04, eta: 6:51:27, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0897, loss_bbox: 0.2322, d0.loss_cls: 0.1719, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1303, d1.loss_bbox: 0.2486, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0951, d3.loss_bbox: 0.2306, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2320, loss: 2.2157, grad_norm: 36.9271
2025-06-17 07:48:43,944 - mmdet - INFO - Epoch [3][1450/7033]	lr: 1.501e-04, eta: 6:50:41, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.0992, loss_bbox: 0.2368, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3604, d1.loss_cls: 0.1364, d1.loss_bbox: 0.2582, d2.loss_cls: 0.1203, d2.loss_bbox: 0.2391, d3.loss_cls: 0.1049, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0994, d4.loss_bbox: 0.2350, loss: 2.2979, grad_norm: 44.6846
2025-06-17 07:49:30,261 - mmdet - INFO - Epoch [3][1500/7033]	lr: 1.501e-04, eta: 6:49:55, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0915, loss_bbox: 0.2307, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1328, d1.loss_bbox: 0.2523, d2.loss_cls: 0.1120, d2.loss_bbox: 0.2349, d3.loss_cls: 0.1003, d3.loss_bbox: 0.2305, d4.loss_cls: 0.0931, d4.loss_bbox: 0.2318, loss: 2.2309, grad_norm: 42.5165
2025-06-17 07:50:16,480 - mmdet - INFO - Epoch [3][1550/7033]	lr: 1.501e-04, eta: 6:49:09, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0899, loss_bbox: 0.2291, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2541, d2.loss_cls: 0.1114, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0956, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2312, loss: 2.2203, grad_norm: 43.1987
2025-06-17 07:51:02,637 - mmdet - INFO - Epoch [3][1600/7033]	lr: 1.501e-04, eta: 6:48:23, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.0908, loss_bbox: 0.2327, d0.loss_cls: 0.1717, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1271, d1.loss_bbox: 0.2552, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2375, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2331, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2338, loss: 2.2266, grad_norm: 35.9713
2025-06-17 07:51:48,964 - mmdet - INFO - Epoch [3][1650/7033]	lr: 1.501e-04, eta: 6:47:37, time: 0.927, data_time: 0.027, memory: 13888, loss_cls: 0.0880, loss_bbox: 0.2354, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3547, d1.loss_cls: 0.1256, d1.loss_bbox: 0.2577, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2390, loss: 2.2423, grad_norm: 34.3441
2025-06-17 07:52:35,267 - mmdet - INFO - Epoch [3][1700/7033]	lr: 1.501e-04, eta: 6:46:51, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0834, loss_bbox: 0.2201, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3423, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2408, d2.loss_cls: 0.1066, d2.loss_bbox: 0.2225, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2220, loss: 2.1242, grad_norm: 45.5986
2025-06-17 07:53:21,873 - mmdet - INFO - Epoch [3][1750/7033]	lr: 1.501e-04, eta: 6:46:06, time: 0.932, data_time: 0.033, memory: 13888, loss_cls: 0.0870, loss_bbox: 0.2267, d0.loss_cls: 0.1682, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2525, d2.loss_cls: 0.1080, d2.loss_bbox: 0.2325, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0883, d4.loss_bbox: 0.2283, loss: 2.1836, grad_norm: 37.3431
2025-06-17 07:54:07,983 - mmdet - INFO - Epoch [3][1800/7033]	lr: 1.501e-04, eta: 6:45:19, time: 0.922, data_time: 0.027, memory: 13888, loss_cls: 0.0913, loss_bbox: 0.2271, d0.loss_cls: 0.1716, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2489, d2.loss_cls: 0.1113, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0980, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0925, d4.loss_bbox: 0.2290, loss: 2.2010, grad_norm: 35.4085
2025-06-17 07:54:54,223 - mmdet - INFO - Epoch [3][1850/7033]	lr: 1.501e-04, eta: 6:44:33, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0970, loss_bbox: 0.2303, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2520, d2.loss_cls: 0.1158, d2.loss_bbox: 0.2327, d3.loss_cls: 0.1039, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0983, d4.loss_bbox: 0.2295, loss: 2.2362, grad_norm: 46.6448
2025-06-17 07:55:40,871 - mmdet - INFO - Epoch [3][1900/7033]	lr: 1.501e-04, eta: 6:43:48, time: 0.933, data_time: 0.031, memory: 13888, loss_cls: 0.0894, loss_bbox: 0.2367, d0.loss_cls: 0.1740, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1298, d1.loss_bbox: 0.2554, d2.loss_cls: 0.1105, d2.loss_bbox: 0.2400, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2358, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2371, loss: 2.2513, grad_norm: 37.2808
2025-06-17 07:56:27,146 - mmdet - INFO - Epoch [3][1950/7033]	lr: 1.501e-04, eta: 6:43:02, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0878, loss_bbox: 0.2324, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3647, d1.loss_cls: 0.1303, d1.loss_bbox: 0.2589, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2394, d3.loss_cls: 0.0950, d3.loss_bbox: 0.2335, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2337, loss: 2.2474, grad_norm: 37.2683
2025-06-17 07:57:13,021 - mmdet - INFO - Epoch [3][2000/7033]	lr: 1.501e-04, eta: 6:42:15, time: 0.917, data_time: 0.028, memory: 13888, loss_cls: 0.0912, loss_bbox: 0.2313, d0.loss_cls: 0.1796, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1327, d1.loss_bbox: 0.2508, d2.loss_cls: 0.1125, d2.loss_bbox: 0.2338, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0930, d4.loss_bbox: 0.2330, loss: 2.2356, grad_norm: 45.4993
2025-06-17 07:57:59,055 - mmdet - INFO - Epoch [3][2050/7033]	lr: 1.501e-04, eta: 6:41:29, time: 0.921, data_time: 0.027, memory: 13888, loss_cls: 0.0882, loss_bbox: 0.2364, d0.loss_cls: 0.1785, d0.loss_bbox: 0.3597, d1.loss_cls: 0.1324, d1.loss_bbox: 0.2566, d2.loss_cls: 0.1138, d2.loss_bbox: 0.2381, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2359, loss: 2.2631, grad_norm: 38.9491
2025-06-17 07:58:45,149 - mmdet - INFO - Epoch [3][2100/7033]	lr: 1.501e-04, eta: 6:40:42, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0942, loss_bbox: 0.2405, d0.loss_cls: 0.1807, d0.loss_bbox: 0.3697, d1.loss_cls: 0.1374, d1.loss_bbox: 0.2648, d2.loss_cls: 0.1162, d2.loss_bbox: 0.2444, d3.loss_cls: 0.1021, d3.loss_bbox: 0.2402, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2414, loss: 2.3267, grad_norm: 34.4686
2025-06-17 07:59:31,179 - mmdet - INFO - Epoch [3][2150/7033]	lr: 1.501e-04, eta: 6:39:56, time: 0.921, data_time: 0.026, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2346, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3446, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2482, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2295, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2267, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2312, loss: 2.1764, grad_norm: 29.4747
2025-06-17 08:00:17,525 - mmdet - INFO - Epoch [3][2200/7033]	lr: 1.501e-04, eta: 6:39:10, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0915, loss_bbox: 0.2253, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3435, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2511, d2.loss_cls: 0.1104, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2237, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2259, loss: 2.1855, grad_norm: 34.6984
2025-06-17 08:01:03,634 - mmdet - INFO - Epoch [3][2250/7033]	lr: 1.501e-04, eta: 6:38:24, time: 0.922, data_time: 0.027, memory: 13888, loss_cls: 0.0873, loss_bbox: 0.2293, d0.loss_cls: 0.1617, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2536, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2298, loss: 2.1809, grad_norm: 46.3322
2025-06-17 08:01:49,598 - mmdet - INFO - Epoch [3][2300/7033]	lr: 1.501e-04, eta: 6:37:37, time: 0.919, data_time: 0.028, memory: 13888, loss_cls: 0.0962, loss_bbox: 0.2288, d0.loss_cls: 0.1818, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1293, d1.loss_bbox: 0.2513, d2.loss_cls: 0.1159, d2.loss_bbox: 0.2320, d3.loss_cls: 0.1028, d3.loss_bbox: 0.2279, d4.loss_cls: 0.0980, d4.loss_bbox: 0.2302, loss: 2.2403, grad_norm: 34.4155
2025-06-17 08:02:35,921 - mmdet - INFO - Epoch [3][2350/7033]	lr: 1.501e-04, eta: 6:36:51, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0914, loss_bbox: 0.2401, d0.loss_cls: 0.1813, d0.loss_bbox: 0.3614, d1.loss_cls: 0.1354, d1.loss_bbox: 0.2604, d2.loss_cls: 0.1139, d2.loss_bbox: 0.2422, d3.loss_cls: 0.0983, d3.loss_bbox: 0.2397, d4.loss_cls: 0.0935, d4.loss_bbox: 0.2410, loss: 2.2985, grad_norm: 44.9051
2025-06-17 08:03:22,270 - mmdet - INFO - Epoch [3][2400/7033]	lr: 1.501e-04, eta: 6:36:05, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0901, loss_bbox: 0.2369, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3609, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2620, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2424, d3.loss_cls: 0.0948, d3.loss_bbox: 0.2382, d4.loss_cls: 0.0914, d4.loss_bbox: 0.2390, loss: 2.2563, grad_norm: 37.6976
2025-06-17 08:04:08,294 - mmdet - INFO - Epoch [3][2450/7033]	lr: 1.501e-04, eta: 6:35:19, time: 0.920, data_time: 0.027, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2297, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2525, d2.loss_cls: 0.1064, d2.loss_bbox: 0.2345, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2309, loss: 2.1930, grad_norm: 39.8573
2025-06-17 08:04:54,179 - mmdet - INFO - Epoch [3][2500/7033]	lr: 1.501e-04, eta: 6:34:32, time: 0.918, data_time: 0.029, memory: 13888, loss_cls: 0.0850, loss_bbox: 0.2314, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1309, d1.loss_bbox: 0.2514, d2.loss_cls: 0.1092, d2.loss_bbox: 0.2299, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2305, loss: 2.1886, grad_norm: 42.9306
2025-06-17 08:05:40,495 - mmdet - INFO - Epoch [3][2550/7033]	lr: 1.501e-04, eta: 6:33:46, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0865, loss_bbox: 0.2360, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1260, d1.loss_bbox: 0.2613, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2377, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2338, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2364, loss: 2.2240, grad_norm: 33.2257
2025-06-17 08:06:26,596 - mmdet - INFO - Epoch [3][2600/7033]	lr: 1.501e-04, eta: 6:33:00, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0898, loss_bbox: 0.2338, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2550, d2.loss_cls: 0.1103, d2.loss_bbox: 0.2373, d3.loss_cls: 0.0976, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2349, loss: 2.2257, grad_norm: 38.8618
2025-06-17 08:07:12,785 - mmdet - INFO - Epoch [3][2650/7033]	lr: 1.501e-04, eta: 6:32:14, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2350, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2544, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2315, d4.loss_cls: 0.0879, d4.loss_bbox: 0.2332, loss: 2.1975, grad_norm: 51.3706
2025-06-17 08:07:59,002 - mmdet - INFO - Epoch [3][2700/7033]	lr: 1.501e-04, eta: 6:31:28, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0931, loss_bbox: 0.2377, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3563, d1.loss_cls: 0.1298, d1.loss_bbox: 0.2615, d2.loss_cls: 0.1099, d2.loss_bbox: 0.2420, d3.loss_cls: 0.0998, d3.loss_bbox: 0.2385, d4.loss_cls: 0.0949, d4.loss_bbox: 0.2384, loss: 2.2714, grad_norm: 33.6043
2025-06-17 08:08:45,226 - mmdet - INFO - Epoch [3][2750/7033]	lr: 1.501e-04, eta: 6:30:42, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0896, loss_bbox: 0.2297, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3495, d1.loss_cls: 0.1299, d1.loss_bbox: 0.2517, d2.loss_cls: 0.1098, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0988, d3.loss_bbox: 0.2286, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2307, loss: 2.2164, grad_norm: 39.3900
2025-06-17 08:09:31,586 - mmdet - INFO - Epoch [3][2800/7033]	lr: 1.501e-04, eta: 6:29:56, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0956, loss_bbox: 0.2414, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3634, d1.loss_cls: 0.1357, d1.loss_bbox: 0.2616, d2.loss_cls: 0.1175, d2.loss_bbox: 0.2416, d3.loss_cls: 0.1025, d3.loss_bbox: 0.2378, d4.loss_cls: 0.0979, d4.loss_bbox: 0.2411, loss: 2.3098, grad_norm: 34.7475
2025-06-17 08:10:17,697 - mmdet - INFO - Epoch [3][2850/7033]	lr: 1.501e-04, eta: 6:29:09, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0923, loss_bbox: 0.2392, d0.loss_cls: 0.1765, d0.loss_bbox: 0.3610, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2603, d2.loss_cls: 0.1119, d2.loss_bbox: 0.2368, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2367, d4.loss_cls: 0.0944, d4.loss_bbox: 0.2396, loss: 2.2801, grad_norm: 39.0126
2025-06-17 08:11:03,872 - mmdet - INFO - Epoch [3][2900/7033]	lr: 1.501e-04, eta: 6:28:23, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0906, loss_bbox: 0.2251, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3367, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2483, d2.loss_cls: 0.1062, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2260, d4.loss_cls: 0.0924, d4.loss_bbox: 0.2278, loss: 2.1724, grad_norm: 53.6867
2025-06-17 08:11:50,204 - mmdet - INFO - Epoch [3][2950/7033]	lr: 1.501e-04, eta: 6:27:37, time: 0.927, data_time: 0.029, memory: 13888, loss_cls: 0.0826, loss_bbox: 0.2306, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2467, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2299, d3.loss_cls: 0.0898, d3.loss_bbox: 0.2267, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2312, loss: 2.1544, grad_norm: 36.1890
2025-06-17 08:12:36,292 - mmdet - INFO - Epoch [3][3000/7033]	lr: 1.501e-04, eta: 6:26:51, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0861, loss_bbox: 0.2260, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1257, d1.loss_bbox: 0.2540, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0911, d3.loss_bbox: 0.2283, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2294, loss: 2.1901, grad_norm: 35.4456
2025-06-17 08:13:22,714 - mmdet - INFO - Epoch [3][3050/7033]	lr: 1.501e-04, eta: 6:26:05, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.0883, loss_bbox: 0.2280, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3460, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2539, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2334, d3.loss_cls: 0.0924, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2333, loss: 2.1804, grad_norm: 37.5702
2025-06-17 08:14:08,753 - mmdet - INFO - Epoch [3][3100/7033]	lr: 1.501e-04, eta: 6:25:19, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0950, loss_bbox: 0.2286, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3483, d1.loss_cls: 0.1263, d1.loss_bbox: 0.2571, d2.loss_cls: 0.1124, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2337, loss: 2.2297, grad_norm: 35.5888
2025-06-17 08:14:54,955 - mmdet - INFO - Epoch [3][3150/7033]	lr: 1.501e-04, eta: 6:24:33, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0951, loss_bbox: 0.2248, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3433, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2495, d2.loss_cls: 0.1106, d2.loss_bbox: 0.2319, d3.loss_cls: 0.1008, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0949, d4.loss_bbox: 0.2262, loss: 2.1915, grad_norm: 38.3199
2025-06-17 08:15:41,030 - mmdet - INFO - Epoch [3][3200/7033]	lr: 1.501e-04, eta: 6:23:46, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0928, loss_bbox: 0.2349, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3515, d1.loss_cls: 0.1306, d1.loss_bbox: 0.2562, d2.loss_cls: 0.1107, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0994, d3.loss_bbox: 0.2318, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2357, loss: 2.2424, grad_norm: 34.3150
2025-06-17 08:16:27,438 - mmdet - INFO - Epoch [3][3250/7033]	lr: 1.501e-04, eta: 6:23:00, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.0880, loss_bbox: 0.2277, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1279, d1.loss_bbox: 0.2448, d2.loss_cls: 0.1058, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0926, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0909, d4.loss_bbox: 0.2263, loss: 2.1590, grad_norm: 45.1270
2025-06-17 08:17:13,703 - mmdet - INFO - Epoch [3][3300/7033]	lr: 1.501e-04, eta: 6:22:14, time: 0.925, data_time: 0.032, memory: 13888, loss_cls: 0.0902, loss_bbox: 0.2433, d0.loss_cls: 0.1684, d0.loss_bbox: 0.3546, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2547, d2.loss_cls: 0.1105, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0973, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0927, d4.loss_bbox: 0.2405, loss: 2.2588, grad_norm: 39.3543
2025-06-17 08:17:59,932 - mmdet - INFO - Epoch [3][3350/7033]	lr: 1.501e-04, eta: 6:21:28, time: 0.925, data_time: 0.028, memory: 13888, loss_cls: 0.0841, loss_bbox: 0.2305, d0.loss_cls: 0.1694, d0.loss_bbox: 0.3546, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2550, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0899, d3.loss_bbox: 0.2314, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2319, loss: 2.1902, grad_norm: 57.2303
2025-06-17 08:18:46,261 - mmdet - INFO - Epoch [3][3400/7033]	lr: 1.501e-04, eta: 6:20:42, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0865, loss_bbox: 0.2241, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2500, d2.loss_cls: 0.1070, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0924, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0867, d4.loss_bbox: 0.2250, loss: 2.1670, grad_norm: 34.2100
2025-06-17 08:19:32,671 - mmdet - INFO - Epoch [3][3450/7033]	lr: 1.501e-04, eta: 6:19:56, time: 0.928, data_time: 0.030, memory: 13888, loss_cls: 0.0896, loss_bbox: 0.2432, d0.loss_cls: 0.1700, d0.loss_bbox: 0.3606, d1.loss_cls: 0.1288, d1.loss_bbox: 0.2606, d2.loss_cls: 0.1120, d2.loss_bbox: 0.2401, d3.loss_cls: 0.0971, d3.loss_bbox: 0.2372, d4.loss_cls: 0.0923, d4.loss_bbox: 0.2391, loss: 2.2706, grad_norm: 31.6874
2025-06-17 08:20:18,899 - mmdet - INFO - Epoch [3][3500/7033]	lr: 1.501e-04, eta: 6:19:10, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0882, loss_bbox: 0.2320, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3543, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2539, d2.loss_cls: 0.1092, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0876, d4.loss_bbox: 0.2302, loss: 2.2053, grad_norm: 31.5092
2025-06-17 08:21:04,993 - mmdet - INFO - Epoch [3][3550/7033]	lr: 1.501e-04, eta: 6:18:24, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0847, loss_bbox: 0.2266, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3408, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2478, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2282, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2273, loss: 2.1486, grad_norm: 40.3941
2025-06-17 08:21:51,081 - mmdet - INFO - Epoch [3][3600/7033]	lr: 1.501e-04, eta: 6:17:38, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0933, loss_bbox: 0.2228, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1337, d1.loss_bbox: 0.2489, d2.loss_cls: 0.1162, d2.loss_bbox: 0.2278, d3.loss_cls: 0.1015, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2246, loss: 2.2058, grad_norm: 56.1768
2025-06-17 08:22:37,215 - mmdet - INFO - Epoch [3][3650/7033]	lr: 1.501e-04, eta: 6:16:51, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0938, loss_bbox: 0.2299, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3564, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2583, d2.loss_cls: 0.1112, d2.loss_bbox: 0.2361, d3.loss_cls: 0.0991, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0932, d4.loss_bbox: 0.2321, loss: 2.2395, grad_norm: 32.5365
2025-06-17 08:23:23,761 - mmdet - INFO - Epoch [3][3700/7033]	lr: 1.501e-04, eta: 6:16:06, time: 0.931, data_time: 0.030, memory: 13888, loss_cls: 0.1093, loss_bbox: 0.2358, d0.loss_cls: 0.1900, d0.loss_bbox: 0.3513, d1.loss_cls: 0.1388, d1.loss_bbox: 0.2613, d2.loss_cls: 0.1184, d2.loss_bbox: 0.2414, d3.loss_cls: 0.1114, d3.loss_bbox: 0.2379, d4.loss_cls: 0.1097, d4.loss_bbox: 0.2398, loss: 2.3452, grad_norm: 28.1641
2025-06-17 08:24:10,075 - mmdet - INFO - Epoch [3][3750/7033]	lr: 1.501e-04, eta: 6:15:20, time: 0.926, data_time: 0.029, memory: 13888, loss_cls: 0.0943, loss_bbox: 0.2389, d0.loss_cls: 0.1753, d0.loss_bbox: 0.3580, d1.loss_cls: 0.1385, d1.loss_bbox: 0.2579, d2.loss_cls: 0.1126, d2.loss_bbox: 0.2401, d3.loss_cls: 0.1018, d3.loss_bbox: 0.2361, d4.loss_cls: 0.0961, d4.loss_bbox: 0.2407, loss: 2.2903, grad_norm: 39.7418
2025-06-17 08:24:56,994 - mmdet - INFO - Epoch [3][3800/7033]	lr: 1.501e-04, eta: 6:14:35, time: 0.938, data_time: 0.030, memory: 13888, loss_cls: 0.0885, loss_bbox: 0.2347, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1264, d1.loss_bbox: 0.2578, d2.loss_cls: 0.1085, d2.loss_bbox: 0.2398, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2343, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2367, loss: 2.2359, grad_norm: 39.9725
2025-06-17 08:25:43,098 - mmdet - INFO - Epoch [3][3850/7033]	lr: 1.501e-04, eta: 6:13:48, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0961, loss_bbox: 0.2384, d0.loss_cls: 0.1757, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1337, d1.loss_bbox: 0.2610, d2.loss_cls: 0.1180, d2.loss_bbox: 0.2397, d3.loss_cls: 0.1043, d3.loss_bbox: 0.2373, d4.loss_cls: 0.0989, d4.loss_bbox: 0.2387, loss: 2.2968, grad_norm: 32.7122
2025-06-17 08:26:29,272 - mmdet - INFO - Epoch [3][3900/7033]	lr: 1.501e-04, eta: 6:13:02, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0895, loss_bbox: 0.2323, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3591, d1.loss_cls: 0.1354, d1.loss_bbox: 0.2551, d2.loss_cls: 0.1152, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0993, d3.loss_bbox: 0.2297, d4.loss_cls: 0.0926, d4.loss_bbox: 0.2324, loss: 2.2577, grad_norm: 57.8173
2025-06-17 08:27:15,515 - mmdet - INFO - Epoch [3][3950/7033]	lr: 1.501e-04, eta: 6:12:16, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0859, loss_bbox: 0.2280, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3526, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2529, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2337, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2294, loss: 2.1956, grad_norm: 36.1995
2025-06-17 08:28:01,780 - mmdet - INFO - Epoch [3][4000/7033]	lr: 1.501e-04, eta: 6:11:30, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0875, loss_bbox: 0.2340, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3507, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2544, d2.loss_cls: 0.1087, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2309, d4.loss_cls: 0.0881, d4.loss_bbox: 0.2329, loss: 2.2047, grad_norm: 32.2866
2025-06-17 08:28:48,001 - mmdet - INFO - Epoch [3][4050/7033]	lr: 1.501e-04, eta: 6:10:44, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0827, loss_bbox: 0.2196, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2429, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2204, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2203, loss: 2.1183, grad_norm: 29.0345
2025-06-17 08:29:34,402 - mmdet - INFO - Epoch [3][4100/7033]	lr: 1.501e-04, eta: 6:09:58, time: 0.928, data_time: 0.028, memory: 13888, loss_cls: 0.0891, loss_bbox: 0.2377, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3548, d1.loss_cls: 0.1297, d1.loss_bbox: 0.2598, d2.loss_cls: 0.1123, d2.loss_bbox: 0.2374, d3.loss_cls: 0.0958, d3.loss_bbox: 0.2366, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2397, loss: 2.2500, grad_norm: 37.0809
2025-06-17 08:30:20,774 - mmdet - INFO - Epoch [3][4150/7033]	lr: 1.501e-04, eta: 6:09:12, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0800, loss_bbox: 0.2331, d0.loss_cls: 0.1641, d0.loss_bbox: 0.3415, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2500, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2331, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2300, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2329, loss: 2.1501, grad_norm: 47.9875
2025-06-17 08:31:07,074 - mmdet - INFO - Epoch [3][4200/7033]	lr: 1.501e-04, eta: 6:08:26, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0885, loss_bbox: 0.2273, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3471, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2508, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0892, d4.loss_bbox: 0.2275, loss: 2.1823, grad_norm: 33.3657
2025-06-17 08:31:53,271 - mmdet - INFO - Epoch [3][4250/7033]	lr: 1.501e-04, eta: 6:07:40, time: 0.924, data_time: 0.027, memory: 13888, loss_cls: 0.0999, loss_bbox: 0.2344, d0.loss_cls: 0.1800, d0.loss_bbox: 0.3638, d1.loss_cls: 0.1315, d1.loss_bbox: 0.2679, d2.loss_cls: 0.1210, d2.loss_bbox: 0.2424, d3.loss_cls: 0.1056, d3.loss_bbox: 0.2376, d4.loss_cls: 0.1019, d4.loss_bbox: 0.2380, loss: 2.3240, grad_norm: 34.0261
2025-06-17 08:32:39,503 - mmdet - INFO - Epoch [3][4300/7033]	lr: 1.501e-04, eta: 6:06:54, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.0953, loss_bbox: 0.2387, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3770, d1.loss_cls: 0.1327, d1.loss_bbox: 0.2668, d2.loss_cls: 0.1133, d2.loss_bbox: 0.2434, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2412, d4.loss_cls: 0.0964, d4.loss_bbox: 0.2419, loss: 2.3199, grad_norm: 50.6017
2025-06-17 08:33:25,693 - mmdet - INFO - Epoch [3][4350/7033]	lr: 1.501e-04, eta: 6:06:07, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0930, loss_bbox: 0.2341, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3567, d1.loss_cls: 0.1355, d1.loss_bbox: 0.2544, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2379, d3.loss_cls: 0.1000, d3.loss_bbox: 0.2349, d4.loss_cls: 0.0951, d4.loss_bbox: 0.2379, loss: 2.2769, grad_norm: 47.3110
2025-06-17 08:34:12,207 - mmdet - INFO - Epoch [3][4400/7033]	lr: 1.501e-04, eta: 6:05:22, time: 0.930, data_time: 0.032, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2339, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3477, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2529, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2364, d3.loss_cls: 0.0912, d3.loss_bbox: 0.2332, d4.loss_cls: 0.0864, d4.loss_bbox: 0.2357, loss: 2.1968, grad_norm: 32.8641
2025-06-17 08:34:58,410 - mmdet - INFO - Epoch [3][4450/7033]	lr: 1.501e-04, eta: 6:04:36, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0830, loss_bbox: 0.2247, d0.loss_cls: 0.1597, d0.loss_bbox: 0.3377, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2486, d2.loss_cls: 0.1013, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2284, loss: 2.1341, grad_norm: 34.9647
2025-06-17 08:35:44,334 - mmdet - INFO - Epoch [3][4500/7033]	lr: 1.501e-04, eta: 6:03:49, time: 0.918, data_time: 0.028, memory: 13888, loss_cls: 0.0846, loss_bbox: 0.2279, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3473, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2509, d2.loss_cls: 0.1079, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0933, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2289, loss: 2.1727, grad_norm: 30.4623
2025-06-17 08:36:30,677 - mmdet - INFO - Epoch [3][4550/7033]	lr: 1.501e-04, eta: 6:03:03, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0905, loss_bbox: 0.2320, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3492, d1.loss_cls: 0.1295, d1.loss_bbox: 0.2578, d2.loss_cls: 0.1113, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0979, d3.loss_bbox: 0.2333, d4.loss_cls: 0.0919, d4.loss_bbox: 0.2359, loss: 2.2418, grad_norm: 37.6731
2025-06-17 08:37:16,823 - mmdet - INFO - Epoch [3][4600/7033]	lr: 1.501e-04, eta: 6:02:17, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2380, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2578, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2388, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2351, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2392, loss: 2.1960, grad_norm: 43.1094
2025-06-17 08:38:02,955 - mmdet - INFO - Epoch [3][4650/7033]	lr: 1.501e-04, eta: 6:01:31, time: 0.923, data_time: 0.027, memory: 13888, loss_cls: 0.0913, loss_bbox: 0.2366, d0.loss_cls: 0.1638, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1247, d1.loss_bbox: 0.2572, d2.loss_cls: 0.1090, d2.loss_bbox: 0.2375, d3.loss_cls: 0.0963, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0939, d4.loss_bbox: 0.2371, loss: 2.2319, grad_norm: 43.6161
2025-06-17 08:38:49,314 - mmdet - INFO - Epoch [3][4700/7033]	lr: 1.501e-04, eta: 6:00:45, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0884, loss_bbox: 0.2375, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3480, d1.loss_cls: 0.1240, d1.loss_bbox: 0.2637, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2444, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2397, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2408, loss: 2.2392, grad_norm: 39.2885
2025-06-17 08:39:35,585 - mmdet - INFO - Epoch [3][4750/7033]	lr: 1.501e-04, eta: 5:59:58, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0866, loss_bbox: 0.2214, d0.loss_cls: 0.1742, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1253, d1.loss_bbox: 0.2467, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2277, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0877, d4.loss_bbox: 0.2234, loss: 2.1581, grad_norm: 32.4254
2025-06-17 08:40:21,902 - mmdet - INFO - Epoch [3][4800/7033]	lr: 1.501e-04, eta: 5:59:12, time: 0.926, data_time: 0.032, memory: 13888, loss_cls: 0.0896, loss_bbox: 0.2270, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3487, d1.loss_cls: 0.1255, d1.loss_bbox: 0.2530, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0970, d3.loss_bbox: 0.2304, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2317, loss: 2.2059, grad_norm: 42.8363
2025-06-17 08:41:07,857 - mmdet - INFO - Epoch [3][4850/7033]	lr: 1.501e-04, eta: 5:58:26, time: 0.919, data_time: 0.029, memory: 13888, loss_cls: 0.0777, loss_bbox: 0.2180, d0.loss_cls: 0.1561, d0.loss_bbox: 0.3280, d1.loss_cls: 0.1118, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0832, d3.loss_bbox: 0.2200, d4.loss_cls: 0.0782, d4.loss_bbox: 0.2208, loss: 2.0588, grad_norm: 29.7259
2025-06-17 08:41:54,006 - mmdet - INFO - Epoch [3][4900/7033]	lr: 1.501e-04, eta: 5:57:40, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.0803, loss_bbox: 0.2283, d0.loss_cls: 0.1643, d0.loss_bbox: 0.3393, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2475, d2.loss_cls: 0.0989, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2273, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2315, loss: 2.1362, grad_norm: 39.9043
2025-06-17 08:42:40,282 - mmdet - INFO - Epoch [3][4950/7033]	lr: 1.501e-04, eta: 5:56:54, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0787, loss_bbox: 0.2229, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3364, d1.loss_cls: 0.1125, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0931, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2245, loss: 2.0767, grad_norm: 34.0128
2025-06-17 08:43:26,760 - mmdet - INFO - Epoch [3][5000/7033]	lr: 1.501e-04, eta: 5:56:08, time: 0.930, data_time: 0.032, memory: 13888, loss_cls: 0.0866, loss_bbox: 0.2314, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1197, d1.loss_bbox: 0.2499, d2.loss_cls: 0.1039, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0909, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0859, d4.loss_bbox: 0.2387, loss: 2.1776, grad_norm: 36.3934
2025-06-17 08:44:12,757 - mmdet - INFO - Epoch [3][5050/7033]	lr: 1.501e-04, eta: 5:55:21, time: 0.920, data_time: 0.027, memory: 13888, loss_cls: 0.0929, loss_bbox: 0.2330, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3410, d1.loss_cls: 0.1321, d1.loss_bbox: 0.2539, d2.loss_cls: 0.1146, d2.loss_bbox: 0.2353, d3.loss_cls: 0.0984, d3.loss_bbox: 0.2326, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2353, loss: 2.2373, grad_norm: 42.1508
2025-06-17 08:44:58,819 - mmdet - INFO - Epoch [3][5100/7033]	lr: 1.501e-04, eta: 5:54:35, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0764, loss_bbox: 0.2179, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3394, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2422, d2.loss_cls: 0.0965, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0776, d4.loss_bbox: 0.2195, loss: 2.0742, grad_norm: 33.4352
2025-06-17 08:45:45,096 - mmdet - INFO - Epoch [3][5150/7033]	lr: 1.501e-04, eta: 5:53:49, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0842, loss_bbox: 0.2254, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3384, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2514, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2278, loss: 2.1420, grad_norm: 42.7646
2025-06-17 08:46:31,328 - mmdet - INFO - Epoch [3][5200/7033]	lr: 1.501e-04, eta: 5:53:03, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0819, loss_bbox: 0.2275, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3484, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2525, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2313, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2285, d4.loss_cls: 0.0826, d4.loss_bbox: 0.2289, loss: 2.1524, grad_norm: 35.8216
2025-06-17 08:47:18,925 - mmdet - INFO - Epoch [3][5250/7033]	lr: 1.501e-04, eta: 5:52:18, time: 0.952, data_time: 0.028, memory: 13888, loss_cls: 0.0850, loss_bbox: 0.2317, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3399, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2528, d2.loss_cls: 0.1057, d2.loss_bbox: 0.2367, d3.loss_cls: 0.0943, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0886, d4.loss_bbox: 0.2353, loss: 2.2018, grad_norm: 34.1333
2025-06-17 08:48:04,935 - mmdet - INFO - Epoch [3][5300/7033]	lr: 1.501e-04, eta: 5:51:32, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.0869, loss_bbox: 0.2362, d0.loss_cls: 0.1731, d0.loss_bbox: 0.3573, d1.loss_cls: 0.1296, d1.loss_bbox: 0.2546, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2351, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2365, loss: 2.2354, grad_norm: 42.3485
2025-06-17 08:48:51,088 - mmdet - INFO - Epoch [3][5350/7033]	lr: 1.501e-04, eta: 5:50:46, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0782, loss_bbox: 0.2201, d0.loss_cls: 0.1708, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2445, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2246, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2238, loss: 2.1056, grad_norm: 39.8377
2025-06-17 08:49:37,390 - mmdet - INFO - Epoch [3][5400/7033]	lr: 1.501e-04, eta: 5:50:00, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0875, loss_bbox: 0.2358, d0.loss_cls: 0.1679, d0.loss_bbox: 0.3457, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2531, d2.loss_cls: 0.1072, d2.loss_bbox: 0.2370, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2347, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2380, loss: 2.2133, grad_norm: 37.4341
2025-06-17 08:50:23,561 - mmdet - INFO - Epoch [3][5450/7033]	lr: 1.501e-04, eta: 5:49:13, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0943, loss_bbox: 0.2319, d0.loss_cls: 0.1763, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1327, d1.loss_bbox: 0.2527, d2.loss_cls: 0.1130, d2.loss_bbox: 0.2358, d3.loss_cls: 0.1002, d3.loss_bbox: 0.2334, d4.loss_cls: 0.0954, d4.loss_bbox: 0.2354, loss: 2.2464, grad_norm: 40.5724
2025-06-17 08:51:09,503 - mmdet - INFO - Epoch [3][5500/7033]	lr: 1.501e-04, eta: 5:48:27, time: 0.919, data_time: 0.027, memory: 13888, loss_cls: 0.0844, loss_bbox: 0.2355, d0.loss_cls: 0.1657, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2545, d2.loss_cls: 0.1075, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0925, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0871, d4.loss_bbox: 0.2339, loss: 2.2010, grad_norm: 36.3113
2025-06-17 08:51:55,687 - mmdet - INFO - Epoch [3][5550/7033]	lr: 1.501e-04, eta: 5:47:41, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0808, loss_bbox: 0.2217, d0.loss_cls: 0.1598, d0.loss_bbox: 0.3345, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2256, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2225, loss: 2.1029, grad_norm: 39.3993
2025-06-17 08:52:43,800 - mmdet - INFO - Epoch [3][5600/7033]	lr: 1.501e-04, eta: 5:46:57, time: 0.962, data_time: 0.032, memory: 13888, loss_cls: 0.0860, loss_bbox: 0.2267, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3480, d1.loss_cls: 0.1209, d1.loss_bbox: 0.2522, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0935, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2281, loss: 2.1670, grad_norm: 31.6632
2025-06-17 08:53:30,059 - mmdet - INFO - Epoch [3][5650/7033]	lr: 1.501e-04, eta: 5:46:11, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0789, loss_bbox: 0.2272, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3490, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2482, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0854, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2287, loss: 2.1346, grad_norm: 35.9563
2025-06-17 08:54:16,227 - mmdet - INFO - Epoch [3][5700/7033]	lr: 1.501e-04, eta: 5:45:24, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0886, loss_bbox: 0.2241, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3470, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2537, d2.loss_cls: 0.1096, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0965, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0899, d4.loss_bbox: 0.2260, loss: 2.1874, grad_norm: 39.7476
2025-06-17 08:55:02,386 - mmdet - INFO - Epoch [3][5750/7033]	lr: 1.501e-04, eta: 5:44:38, time: 0.923, data_time: 0.028, memory: 13888, loss_cls: 0.0866, loss_bbox: 0.2237, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3523, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2518, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0941, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2245, loss: 2.1813, grad_norm: 26.2478
2025-06-17 08:55:48,638 - mmdet - INFO - Epoch [3][5800/7033]	lr: 1.501e-04, eta: 5:43:52, time: 0.925, data_time: 0.032, memory: 13888, loss_cls: 0.0902, loss_bbox: 0.2318, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3544, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2613, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2393, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2360, d4.loss_cls: 0.0917, d4.loss_bbox: 0.2358, loss: 2.2375, grad_norm: 49.1926
2025-06-17 08:56:34,835 - mmdet - INFO - Epoch [3][5850/7033]	lr: 1.501e-04, eta: 5:43:06, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0870, loss_bbox: 0.2341, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3553, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2634, d2.loss_cls: 0.1073, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2357, d4.loss_cls: 0.0873, d4.loss_bbox: 0.2389, loss: 2.2329, grad_norm: 28.0832
2025-06-17 08:57:21,072 - mmdet - INFO - Epoch [3][5900/7033]	lr: 1.501e-04, eta: 5:42:20, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0845, loss_bbox: 0.2401, d0.loss_cls: 0.1784, d0.loss_bbox: 0.3558, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2631, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2432, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2400, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2408, loss: 2.2534, grad_norm: 40.6001
2025-06-17 08:58:07,329 - mmdet - INFO - Epoch [3][5950/7033]	lr: 1.501e-04, eta: 5:41:34, time: 0.925, data_time: 0.028, memory: 13888, loss_cls: 0.0922, loss_bbox: 0.2365, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3533, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2546, d2.loss_cls: 0.1108, d2.loss_bbox: 0.2330, d3.loss_cls: 0.0986, d3.loss_bbox: 0.2328, d4.loss_cls: 0.0934, d4.loss_bbox: 0.2376, loss: 2.2462, grad_norm: 33.4808
2025-06-17 08:58:53,758 - mmdet - INFO - Epoch [3][6000/7033]	lr: 1.501e-04, eta: 5:40:48, time: 0.929, data_time: 0.029, memory: 13888, loss_cls: 0.0920, loss_bbox: 0.2280, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3502, d1.loss_cls: 0.1282, d1.loss_bbox: 0.2542, d2.loss_cls: 0.1102, d2.loss_bbox: 0.2332, d3.loss_cls: 0.0987, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0928, d4.loss_bbox: 0.2323, loss: 2.2188, grad_norm: 45.9905
2025-06-17 08:59:39,973 - mmdet - INFO - Epoch [3][6050/7033]	lr: 1.501e-04, eta: 5:40:01, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0894, loss_bbox: 0.2241, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3405, d1.loss_cls: 0.1248, d1.loss_bbox: 0.2473, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0936, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2241, loss: 2.1512, grad_norm: 39.7066
2025-06-17 09:00:26,224 - mmdet - INFO - Epoch [3][6100/7033]	lr: 1.501e-04, eta: 5:39:15, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0871, loss_bbox: 0.2299, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3349, d1.loss_cls: 0.1215, d1.loss_bbox: 0.2462, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0917, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0870, d4.loss_bbox: 0.2271, loss: 2.1485, grad_norm: 40.5261
2025-06-17 09:01:12,293 - mmdet - INFO - Epoch [3][6150/7033]	lr: 1.501e-04, eta: 5:38:29, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0920, loss_bbox: 0.2338, d0.loss_cls: 0.1729, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1311, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1122, d2.loss_bbox: 0.2381, d3.loss_cls: 0.0997, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0942, d4.loss_bbox: 0.2344, loss: 2.2510, grad_norm: 42.7556
2025-06-17 09:01:58,401 - mmdet - INFO - Epoch [3][6200/7033]	lr: 1.501e-04, eta: 5:37:43, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0881, loss_bbox: 0.2250, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3418, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2501, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2286, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2241, d4.loss_cls: 0.0895, d4.loss_bbox: 0.2254, loss: 2.1752, grad_norm: 41.5406
2025-06-17 09:02:44,716 - mmdet - INFO - Epoch [3][6250/7033]	lr: 1.501e-04, eta: 5:36:57, time: 0.926, data_time: 0.030, memory: 13888, loss_cls: 0.0897, loss_bbox: 0.2308, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3450, d1.loss_cls: 0.1312, d1.loss_bbox: 0.2563, d2.loss_cls: 0.1065, d2.loss_bbox: 0.2385, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2341, d4.loss_cls: 0.0901, d4.loss_bbox: 0.2352, loss: 2.2275, grad_norm: 30.6631
2025-06-17 09:03:30,806 - mmdet - INFO - Epoch [3][6300/7033]	lr: 1.501e-04, eta: 5:36:10, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0835, loss_bbox: 0.2301, d0.loss_cls: 0.1696, d0.loss_bbox: 0.3422, d1.loss_cls: 0.1262, d1.loss_bbox: 0.2538, d2.loss_cls: 0.1063, d2.loss_bbox: 0.2349, d3.loss_cls: 0.0904, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2324, loss: 2.1852, grad_norm: 34.9746
2025-06-17 09:04:16,973 - mmdet - INFO - Epoch [3][6350/7033]	lr: 1.501e-04, eta: 5:35:24, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0926, loss_bbox: 0.2423, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3636, d1.loss_cls: 0.1366, d1.loss_bbox: 0.2651, d2.loss_cls: 0.1151, d2.loss_bbox: 0.2456, d3.loss_cls: 0.1001, d3.loss_bbox: 0.2437, d4.loss_cls: 0.0929, d4.loss_bbox: 0.2454, loss: 2.3189, grad_norm: 49.0404
2025-06-17 09:05:03,273 - mmdet - INFO - Epoch [3][6400/7033]	lr: 1.501e-04, eta: 5:34:38, time: 0.926, data_time: 0.028, memory: 13888, loss_cls: 0.0830, loss_bbox: 0.2301, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3377, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2493, d2.loss_cls: 0.1014, d2.loss_bbox: 0.2323, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2336, loss: 2.1545, grad_norm: 38.1342
2025-06-17 09:05:49,890 - mmdet - INFO - Epoch [3][6450/7033]	lr: 1.501e-04, eta: 5:33:52, time: 0.932, data_time: 0.034, memory: 13888, loss_cls: 0.0896, loss_bbox: 0.2331, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1269, d1.loss_bbox: 0.2573, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2371, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2336, d4.loss_cls: 0.0907, d4.loss_bbox: 0.2357, loss: 2.2351, grad_norm: 32.5097
2025-06-17 09:06:36,017 - mmdet - INFO - Epoch [3][6500/7033]	lr: 1.501e-04, eta: 5:33:06, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.0883, loss_bbox: 0.2381, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3552, d1.loss_cls: 0.1266, d1.loss_bbox: 0.2603, d2.loss_cls: 0.1101, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0962, d3.loss_bbox: 0.2371, d4.loss_cls: 0.0900, d4.loss_bbox: 0.2377, loss: 2.2520, grad_norm: 41.6906
2025-06-17 09:07:22,218 - mmdet - INFO - Epoch [3][6550/7033]	lr: 1.501e-04, eta: 5:32:20, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0904, loss_bbox: 0.2421, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3598, d1.loss_cls: 0.1274, d1.loss_bbox: 0.2622, d2.loss_cls: 0.1105, d2.loss_bbox: 0.2414, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2393, d4.loss_cls: 0.0906, d4.loss_bbox: 0.2413, loss: 2.2688, grad_norm: 47.8806
2025-06-17 09:08:08,573 - mmdet - INFO - Epoch [3][6600/7033]	lr: 1.501e-04, eta: 5:31:34, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0836, loss_bbox: 0.2357, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2581, d2.loss_cls: 0.1027, d2.loss_bbox: 0.2395, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2340, d4.loss_cls: 0.0852, d4.loss_bbox: 0.2339, loss: 2.2065, grad_norm: 38.4504
2025-06-17 09:08:55,067 - mmdet - INFO - Epoch [3][6650/7033]	lr: 1.501e-04, eta: 5:30:48, time: 0.930, data_time: 0.034, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2196, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3545, d1.loss_cls: 0.1243, d1.loss_bbox: 0.2482, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2204, d4.loss_cls: 0.0875, d4.loss_bbox: 0.2211, loss: 2.1545, grad_norm: 38.3493
2025-06-17 09:09:41,315 - mmdet - INFO - Epoch [3][6700/7033]	lr: 1.501e-04, eta: 5:30:02, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0854, loss_bbox: 0.2303, d0.loss_cls: 0.1628, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1227, d1.loss_bbox: 0.2489, d2.loss_cls: 0.1040, d2.loss_bbox: 0.2329, d3.loss_cls: 0.0922, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2324, loss: 2.1634, grad_norm: 63.2112
2025-06-17 09:10:27,392 - mmdet - INFO - Epoch [3][6750/7033]	lr: 1.501e-04, eta: 5:29:15, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.0900, loss_bbox: 0.2284, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3465, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2545, d2.loss_cls: 0.1067, d2.loss_bbox: 0.2379, d3.loss_cls: 0.0946, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0916, d4.loss_bbox: 0.2312, loss: 2.2150, grad_norm: 32.7315
2025-06-17 09:11:13,622 - mmdet - INFO - Epoch [3][6800/7033]	lr: 1.501e-04, eta: 5:28:29, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0882, loss_bbox: 0.2280, d0.loss_cls: 0.1750, d0.loss_bbox: 0.3438, d1.loss_cls: 0.1286, d1.loss_bbox: 0.2508, d2.loss_cls: 0.1106, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0952, d3.loss_bbox: 0.2288, d4.loss_cls: 0.0902, d4.loss_bbox: 0.2312, loss: 2.2022, grad_norm: 41.6651
2025-06-17 09:11:59,987 - mmdet - INFO - Epoch [3][6850/7033]	lr: 1.501e-04, eta: 5:27:43, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0934, loss_bbox: 0.2419, d0.loss_cls: 0.1793, d0.loss_bbox: 0.3615, d1.loss_cls: 0.1335, d1.loss_bbox: 0.2649, d2.loss_cls: 0.1143, d2.loss_bbox: 0.2462, d3.loss_cls: 0.0996, d3.loss_bbox: 0.2452, d4.loss_cls: 0.0958, d4.loss_bbox: 0.2473, loss: 2.3229, grad_norm: 60.4012
2025-06-17 09:12:46,275 - mmdet - INFO - Epoch [3][6900/7033]	lr: 1.501e-04, eta: 5:26:57, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0934, loss_bbox: 0.2354, d0.loss_cls: 0.1816, d0.loss_bbox: 0.3521, d1.loss_cls: 0.1353, d1.loss_bbox: 0.2575, d2.loss_cls: 0.1154, d2.loss_bbox: 0.2378, d3.loss_cls: 0.1013, d3.loss_bbox: 0.2347, d4.loss_cls: 0.0946, d4.loss_bbox: 0.2384, loss: 2.2773, grad_norm: 42.1498
2025-06-17 09:13:32,522 - mmdet - INFO - Epoch [3][6950/7033]	lr: 1.501e-04, eta: 5:26:11, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0910, loss_bbox: 0.2288, d0.loss_cls: 0.1779, d0.loss_bbox: 0.3575, d1.loss_cls: 0.1364, d1.loss_bbox: 0.2527, d2.loss_cls: 0.1161, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0990, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0933, d4.loss_bbox: 0.2311, loss: 2.2461, grad_norm: 36.5745
2025-06-17 09:14:18,892 - mmdet - INFO - Epoch [3][7000/7033]	lr: 1.501e-04, eta: 5:25:25, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0886, loss_bbox: 0.2232, d0.loss_cls: 0.1833, d0.loss_bbox: 0.3556, d1.loss_cls: 0.1349, d1.loss_bbox: 0.2468, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2272, d3.loss_cls: 0.0953, d3.loss_bbox: 0.2240, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2251, loss: 2.2037, grad_norm: 36.7141
2025-06-17 09:14:49,675 - mmdet - INFO - Saving checkpoint at 3 epochs
2025-06-17 09:37:29,945 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 09:37:29,945 - mmdet - INFO - Epoch(val) [3][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7840, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8762, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9037, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9181, pts_bbox_NuScenes/car_trans_err: 0.1898, pts_bbox_NuScenes/car_scale_err: 0.1538, pts_bbox_NuScenes/car_orient_err: 0.0487, pts_bbox_NuScenes/car_vel_err: 0.3316, pts_bbox_NuScenes/car_attr_err: 0.1685, pts_bbox_NuScenes/mATE: 0.3041, pts_bbox_NuScenes/mASE: 0.2671, pts_bbox_NuScenes/mAOE: 0.2622, pts_bbox_NuScenes/mAVE: 0.3139, pts_bbox_NuScenes/mAAE: 0.1831, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4264, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6099, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7037, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7579, pts_bbox_NuScenes/truck_trans_err: 0.3235, pts_bbox_NuScenes/truck_scale_err: 0.1998, pts_bbox_NuScenes/truck_orient_err: 0.0560, pts_bbox_NuScenes/truck_vel_err: 0.2942, pts_bbox_NuScenes/truck_attr_err: 0.1835, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0478, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1897, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3889, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4532, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6761, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4448, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8083, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1074, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3029, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4976, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7360, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8826, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9105, pts_bbox_NuScenes/bus_trans_err: 0.3420, pts_bbox_NuScenes/bus_scale_err: 0.1949, pts_bbox_NuScenes/bus_orient_err: 0.0524, pts_bbox_NuScenes/bus_vel_err: 0.6014, pts_bbox_NuScenes/bus_attr_err: 0.3158, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1635, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4338, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5693, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6470, pts_bbox_NuScenes/trailer_trans_err: 0.5088, pts_bbox_NuScenes/trailer_scale_err: 0.2311, pts_bbox_NuScenes/trailer_orient_err: 0.4908, pts_bbox_NuScenes/trailer_vel_err: 0.2524, pts_bbox_NuScenes/trailer_attr_err: 0.1568, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5683, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6676, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7266, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7491, pts_bbox_NuScenes/barrier_trans_err: 0.2454, pts_bbox_NuScenes/barrier_scale_err: 0.2927, pts_bbox_NuScenes/barrier_orient_err: 0.0489, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6173, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7460, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7876, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8006, pts_bbox_NuScenes/motorcycle_trans_err: 0.2333, pts_bbox_NuScenes/motorcycle_scale_err: 0.2554, pts_bbox_NuScenes/motorcycle_orient_err: 0.2215, pts_bbox_NuScenes/motorcycle_vel_err: 0.4613, pts_bbox_NuScenes/motorcycle_attr_err: 0.2162, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5155, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5745, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.5876, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.5984, pts_bbox_NuScenes/bicycle_trans_err: 0.1933, pts_bbox_NuScenes/bicycle_scale_err: 0.2752, pts_bbox_NuScenes/bicycle_orient_err: 0.3024, pts_bbox_NuScenes/bicycle_vel_err: 0.2306, pts_bbox_NuScenes/bicycle_attr_err: 0.0070, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7822, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8346, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8632, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8788, pts_bbox_NuScenes/pedestrian_trans_err: 0.1715, pts_bbox_NuScenes/pedestrian_scale_err: 0.2974, pts_bbox_NuScenes/pedestrian_orient_err: 0.3307, pts_bbox_NuScenes/pedestrian_vel_err: 0.2321, pts_bbox_NuScenes/pedestrian_attr_err: 0.1143, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6955, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7370, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7645, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.7933, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1568, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3262, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6943, pts_bbox_NuScenes/mAP: 0.6547
2025-06-17 09:38:25,493 - mmdet - INFO - Epoch [4][50/7033]	lr: 1.001e-04, eta: 5:23:42, time: 1.007, data_time: 0.114, memory: 13888, loss_cls: 0.0906, loss_bbox: 0.2195, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3620, d1.loss_cls: 0.1336, d1.loss_bbox: 0.2534, d2.loss_cls: 0.1148, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0985, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0928, d4.loss_bbox: 0.2225, loss: 2.2135, grad_norm: 60.8216
2025-06-17 09:39:11,518 - mmdet - INFO - Epoch [4][100/7033]	lr: 1.001e-04, eta: 5:22:56, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.0831, loss_bbox: 0.2175, d0.loss_cls: 0.1761, d0.loss_bbox: 0.3412, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2438, d2.loss_cls: 0.1041, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0880, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2193, loss: 2.1274, grad_norm: 43.0115
2025-06-17 09:39:57,701 - mmdet - INFO - Epoch [4][150/7033]	lr: 1.001e-04, eta: 5:22:10, time: 0.924, data_time: 0.029, memory: 13888, loss_cls: 0.0782, loss_bbox: 0.2237, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3478, d1.loss_cls: 0.1208, d1.loss_bbox: 0.2425, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0827, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0788, d4.loss_bbox: 0.2237, loss: 2.1080, grad_norm: 37.1445
2025-06-17 09:41:23,128 - mmdet - INFO - Epoch [4][200/7033]	lr: 1.001e-04, eta: 5:22:02, time: 1.709, data_time: 0.031, memory: 13888, loss_cls: 0.0819, loss_bbox: 0.2258, d0.loss_cls: 0.1649, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2511, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2328, d3.loss_cls: 0.0928, d3.loss_bbox: 0.2283, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2289, loss: 2.1596, grad_norm: 33.4780
2025-06-17 09:42:26,201 - mmdet - INFO - Epoch [4][250/7033]	lr: 1.001e-04, eta: 5:21:32, time: 1.261, data_time: 0.029, memory: 13888, loss_cls: 0.0809, loss_bbox: 0.2158, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3388, d1.loss_cls: 0.1170, d1.loss_bbox: 0.2449, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0822, d4.loss_bbox: 0.2196, loss: 2.0984, grad_norm: 37.3323
2025-06-17 09:43:13,738 - mmdet - INFO - Epoch [4][300/7033]	lr: 1.001e-04, eta: 5:20:47, time: 0.951, data_time: 0.028, memory: 13888, loss_cls: 0.0855, loss_bbox: 0.2220, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3573, d1.loss_cls: 0.1220, d1.loss_bbox: 0.2525, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2310, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2263, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2259, loss: 2.1774, grad_norm: 43.4499
2025-06-17 09:44:00,109 - mmdet - INFO - Epoch [4][350/7033]	lr: 1.001e-04, eta: 5:20:01, time: 0.927, data_time: 0.028, memory: 13888, loss_cls: 0.0844, loss_bbox: 0.2239, d0.loss_cls: 0.1691, d0.loss_bbox: 0.3496, d1.loss_cls: 0.1235, d1.loss_bbox: 0.2529, d2.loss_cls: 0.1072, d2.loss_bbox: 0.2308, d3.loss_cls: 0.0927, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0862, d4.loss_bbox: 0.2283, loss: 2.1763, grad_norm: 38.4428
2025-06-17 09:44:46,602 - mmdet - INFO - Epoch [4][400/7033]	lr: 1.001e-04, eta: 5:19:15, time: 0.930, data_time: 0.030, memory: 13888, loss_cls: 0.0911, loss_bbox: 0.2202, d0.loss_cls: 0.1844, d0.loss_bbox: 0.3608, d1.loss_cls: 0.1374, d1.loss_bbox: 0.2491, d2.loss_cls: 0.1149, d2.loss_bbox: 0.2268, d3.loss_cls: 0.1015, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0940, d4.loss_bbox: 0.2225, loss: 2.2245, grad_norm: 55.5580
2025-06-17 09:45:33,099 - mmdet - INFO - Epoch [4][450/7033]	lr: 1.001e-04, eta: 5:18:29, time: 0.930, data_time: 0.030, memory: 13888, loss_cls: 0.0801, loss_bbox: 0.2221, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3475, d1.loss_cls: 0.1204, d1.loss_bbox: 0.2479, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2296, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2241, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2254, loss: 2.1314, grad_norm: 46.4196
2025-06-17 09:46:19,562 - mmdet - INFO - Epoch [4][500/7033]	lr: 1.001e-04, eta: 5:17:43, time: 0.929, data_time: 0.033, memory: 13888, loss_cls: 0.0864, loss_bbox: 0.2221, d0.loss_cls: 0.1797, d0.loss_bbox: 0.3513, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2495, d2.loss_cls: 0.1071, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0929, d3.loss_bbox: 0.2240, d4.loss_cls: 0.0884, d4.loss_bbox: 0.2255, loss: 2.1841, grad_norm: 62.1371
2025-06-17 09:47:05,575 - mmdet - INFO - Epoch [4][550/7033]	lr: 1.001e-04, eta: 5:16:56, time: 0.920, data_time: 0.031, memory: 13888, loss_cls: 0.0811, loss_bbox: 0.2186, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3550, d1.loss_cls: 0.1249, d1.loss_bbox: 0.2490, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2273, d3.loss_cls: 0.0895, d3.loss_bbox: 0.2229, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2217, loss: 2.1454, grad_norm: 33.2950
2025-06-17 09:47:51,734 - mmdet - INFO - Epoch [4][600/7033]	lr: 1.001e-04, eta: 5:16:10, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0886, loss_bbox: 0.2099, d0.loss_cls: 0.1704, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1273, d1.loss_bbox: 0.2374, d2.loss_cls: 0.1076, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0960, d3.loss_bbox: 0.2133, d4.loss_cls: 0.0891, d4.loss_bbox: 0.2135, loss: 2.1060, grad_norm: 33.7892
2025-06-17 09:48:38,203 - mmdet - INFO - Epoch [4][650/7033]	lr: 1.001e-04, eta: 5:15:24, time: 0.929, data_time: 0.031, memory: 13888, loss_cls: 0.0896, loss_bbox: 0.2200, d0.loss_cls: 0.1762, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1276, d1.loss_bbox: 0.2484, d2.loss_cls: 0.1093, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0936, d3.loss_bbox: 0.2236, d4.loss_cls: 0.0904, d4.loss_bbox: 0.2246, loss: 2.1815, grad_norm: 36.1012
2025-06-17 09:49:24,664 - mmdet - INFO - Epoch [4][700/7033]	lr: 1.001e-04, eta: 5:14:38, time: 0.929, data_time: 0.033, memory: 13888, loss_cls: 0.0837, loss_bbox: 0.2247, d0.loss_cls: 0.1724, d0.loss_bbox: 0.3517, d1.loss_cls: 0.1226, d1.loss_bbox: 0.2471, d2.loss_cls: 0.1016, d2.loss_bbox: 0.2305, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0843, d4.loss_bbox: 0.2282, loss: 2.1639, grad_norm: 42.2037
2025-06-17 09:50:10,776 - mmdet - INFO - Epoch [4][750/7033]	lr: 1.001e-04, eta: 5:13:51, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0824, loss_bbox: 0.2154, d0.loss_cls: 0.1647, d0.loss_bbox: 0.3368, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2427, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2169, loss: 2.0972, grad_norm: 34.8143
2025-06-17 09:50:57,194 - mmdet - INFO - Epoch [4][800/7033]	lr: 1.001e-04, eta: 5:13:05, time: 0.928, data_time: 0.033, memory: 13888, loss_cls: 0.0820, loss_bbox: 0.2182, d0.loss_cls: 0.1706, d0.loss_bbox: 0.3450, d1.loss_cls: 0.1233, d1.loss_bbox: 0.2458, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0891, d3.loss_bbox: 0.2225, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2219, loss: 2.1309, grad_norm: 33.2028
2025-06-17 09:51:43,408 - mmdet - INFO - Epoch [4][850/7033]	lr: 1.001e-04, eta: 5:12:19, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0835, loss_bbox: 0.2250, d0.loss_cls: 0.1721, d0.loss_bbox: 0.3501, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2516, d2.loss_cls: 0.1054, d2.loss_bbox: 0.2336, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0850, d4.loss_bbox: 0.2291, loss: 2.1822, grad_norm: 43.3312
2025-06-17 09:52:29,652 - mmdet - INFO - Epoch [4][900/7033]	lr: 1.001e-04, eta: 5:11:33, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.0780, loss_bbox: 0.2109, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3365, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2400, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0856, d3.loss_bbox: 0.2170, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2169, loss: 2.0692, grad_norm: 62.0697
2025-06-17 09:53:15,933 - mmdet - INFO - Epoch [4][950/7033]	lr: 1.001e-04, eta: 5:10:46, time: 0.926, data_time: 0.033, memory: 13888, loss_cls: 0.0846, loss_bbox: 0.2241, d0.loss_cls: 0.1711, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1238, d1.loss_bbox: 0.2523, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0918, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2262, loss: 2.1712, grad_norm: 45.8205
2025-06-17 09:54:02,038 - mmdet - INFO - Epoch [4][1000/7033]	lr: 1.001e-04, eta: 5:10:00, time: 0.922, data_time: 0.032, memory: 13888, loss_cls: 0.0816, loss_bbox: 0.2226, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3391, d1.loss_cls: 0.1197, d1.loss_bbox: 0.2498, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2301, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0831, d4.loss_bbox: 0.2245, loss: 2.1334, grad_norm: 38.1212
2025-06-17 09:54:48,168 - mmdet - INFO - Epoch [4][1050/7033]	lr: 1.001e-04, eta: 5:09:14, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0724, loss_bbox: 0.2164, d0.loss_cls: 0.1634, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1155, d1.loss_bbox: 0.2330, d2.loss_cls: 0.0910, d2.loss_bbox: 0.2189, d3.loss_cls: 0.0782, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2162, loss: 2.0236, grad_norm: 40.6741
2025-06-17 09:55:34,256 - mmdet - INFO - Epoch [4][1100/7033]	lr: 1.001e-04, eta: 5:08:27, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0792, loss_bbox: 0.2268, d0.loss_cls: 0.1709, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2477, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2285, loss: 2.1563, grad_norm: 34.0706
2025-06-17 09:56:20,593 - mmdet - INFO - Epoch [4][1150/7033]	lr: 1.001e-04, eta: 5:07:41, time: 0.927, data_time: 0.034, memory: 13888, loss_cls: 0.0833, loss_bbox: 0.2210, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3503, d1.loss_cls: 0.1242, d1.loss_bbox: 0.2496, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2302, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2238, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2232, loss: 2.1618, grad_norm: 41.7709
2025-06-17 09:57:07,034 - mmdet - INFO - Epoch [4][1200/7033]	lr: 1.001e-04, eta: 5:06:55, time: 0.929, data_time: 0.032, memory: 13888, loss_cls: 0.0811, loss_bbox: 0.2155, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3467, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2417, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0877, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2185, loss: 2.1033, grad_norm: 33.1557
2025-06-17 09:57:53,204 - mmdet - INFO - Epoch [4][1250/7033]	lr: 1.001e-04, eta: 5:06:08, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.0904, loss_bbox: 0.2223, d0.loss_cls: 0.1801, d0.loss_bbox: 0.3619, d1.loss_cls: 0.1372, d1.loss_bbox: 0.2506, d2.loss_cls: 0.1129, d2.loss_bbox: 0.2315, d3.loss_cls: 0.0992, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0915, d4.loss_bbox: 0.2274, loss: 2.2312, grad_norm: 45.7957
2025-06-17 09:58:39,353 - mmdet - INFO - Epoch [4][1300/7033]	lr: 1.001e-04, eta: 5:05:22, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0843, loss_bbox: 0.2211, d0.loss_cls: 0.1735, d0.loss_bbox: 0.3525, d1.loss_cls: 0.1285, d1.loss_bbox: 0.2479, d2.loss_cls: 0.1086, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0947, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0863, d4.loss_bbox: 0.2258, loss: 2.1758, grad_norm: 28.9543
2025-06-17 09:59:25,579 - mmdet - INFO - Epoch [4][1350/7033]	lr: 1.001e-04, eta: 5:04:36, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0809, loss_bbox: 0.2165, d0.loss_cls: 0.1623, d0.loss_bbox: 0.3430, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2443, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2188, loss: 2.0999, grad_norm: 37.8483
2025-06-17 10:00:11,716 - mmdet - INFO - Epoch [4][1400/7033]	lr: 1.001e-04, eta: 5:03:49, time: 0.923, data_time: 0.033, memory: 13888, loss_cls: 0.0855, loss_bbox: 0.2179, d0.loss_cls: 0.1745, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2475, d2.loss_cls: 0.1084, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0942, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2219, loss: 2.1595, grad_norm: 38.1708
2025-06-17 10:00:57,914 - mmdet - INFO - Epoch [4][1450/7033]	lr: 1.001e-04, eta: 5:03:03, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2175, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3532, d1.loss_cls: 0.1291, d1.loss_bbox: 0.2428, d2.loss_cls: 0.1051, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2197, d4.loss_cls: 0.0837, d4.loss_bbox: 0.2207, loss: 2.1466, grad_norm: 57.3106
2025-06-17 10:01:44,006 - mmdet - INFO - Epoch [4][1500/7033]	lr: 1.001e-04, eta: 5:02:17, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0834, loss_bbox: 0.2162, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3512, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2502, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2279, d3.loss_cls: 0.0900, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0846, d4.loss_bbox: 0.2201, loss: 2.1393, grad_norm: 36.2038
2025-06-17 10:02:30,346 - mmdet - INFO - Epoch [4][1550/7033]	lr: 1.001e-04, eta: 5:01:30, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0791, loss_bbox: 0.2200, d0.loss_cls: 0.1670, d0.loss_bbox: 0.3359, d1.loss_cls: 0.1224, d1.loss_bbox: 0.2453, d2.loss_cls: 0.1060, d2.loss_bbox: 0.2248, d3.loss_cls: 0.0889, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2225, loss: 2.1158, grad_norm: 36.3187
2025-06-17 10:03:16,492 - mmdet - INFO - Epoch [4][1600/7033]	lr: 1.001e-04, eta: 5:00:44, time: 0.923, data_time: 0.032, memory: 13888, loss_cls: 0.0832, loss_bbox: 0.2172, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3519, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2478, d2.loss_cls: 0.1018, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2206, loss: 2.1349, grad_norm: 39.6183
2025-06-17 10:04:02,768 - mmdet - INFO - Epoch [4][1650/7033]	lr: 1.001e-04, eta: 4:59:58, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0805, loss_bbox: 0.2167, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3514, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2467, d2.loss_cls: 0.0994, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0862, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0812, d4.loss_bbox: 0.2210, loss: 2.1274, grad_norm: 47.3947
2025-06-17 10:04:48,668 - mmdet - INFO - Epoch [4][1700/7033]	lr: 1.001e-04, eta: 4:59:11, time: 0.918, data_time: 0.030, memory: 13888, loss_cls: 0.0846, loss_bbox: 0.2233, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3565, d1.loss_cls: 0.1275, d1.loss_bbox: 0.2537, d2.loss_cls: 0.1074, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0913, d3.loss_bbox: 0.2261, d4.loss_cls: 0.0855, d4.loss_bbox: 0.2260, loss: 2.1881, grad_norm: 33.5546
2025-06-17 10:05:34,736 - mmdet - INFO - Epoch [4][1750/7033]	lr: 1.001e-04, eta: 4:58:25, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0854, loss_bbox: 0.2146, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2416, d2.loss_cls: 0.1022, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2193, loss: 2.1244, grad_norm: 40.0474
2025-06-17 10:06:21,197 - mmdet - INFO - Epoch [4][1800/7033]	lr: 1.001e-04, eta: 4:57:39, time: 0.929, data_time: 0.041, memory: 13888, loss_cls: 0.0789, loss_bbox: 0.2142, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3520, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2402, d2.loss_cls: 0.1004, d2.loss_bbox: 0.2216, d3.loss_cls: 0.0860, d3.loss_bbox: 0.2172, d4.loss_cls: 0.0809, d4.loss_bbox: 0.2173, loss: 2.1019, grad_norm: 36.1377
2025-06-17 10:07:07,213 - mmdet - INFO - Epoch [4][1850/7033]	lr: 1.001e-04, eta: 4:56:52, time: 0.920, data_time: 0.030, memory: 13888, loss_cls: 0.0876, loss_bbox: 0.2177, d0.loss_cls: 0.1773, d0.loss_bbox: 0.3409, d1.loss_cls: 0.1272, d1.loss_bbox: 0.2419, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2229, d3.loss_cls: 0.0966, d3.loss_bbox: 0.2195, d4.loss_cls: 0.0910, d4.loss_bbox: 0.2196, loss: 2.1521, grad_norm: 40.6891
2025-06-17 10:07:53,483 - mmdet - INFO - Epoch [4][1900/7033]	lr: 1.001e-04, eta: 4:56:06, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0838, loss_bbox: 0.2184, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3411, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2417, d2.loss_cls: 0.1037, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0840, d4.loss_bbox: 0.2210, loss: 2.1227, grad_norm: 38.5595
2025-06-17 10:08:39,274 - mmdet - INFO - Epoch [4][1950/7033]	lr: 1.001e-04, eta: 4:55:19, time: 0.916, data_time: 0.029, memory: 13888, loss_cls: 0.0868, loss_bbox: 0.2217, d0.loss_cls: 0.1738, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1280, d1.loss_bbox: 0.2477, d2.loss_cls: 0.1048, d2.loss_bbox: 0.2260, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2244, d4.loss_cls: 0.0890, d4.loss_bbox: 0.2250, loss: 2.1707, grad_norm: 39.4626
2025-06-17 10:09:25,388 - mmdet - INFO - Epoch [4][2000/7033]	lr: 1.001e-04, eta: 4:54:33, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0800, loss_bbox: 0.2283, d0.loss_cls: 0.1692, d0.loss_bbox: 0.3534, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2487, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2278, loss: 2.1536, grad_norm: 37.4122
2025-06-17 10:10:11,367 - mmdet - INFO - Epoch [4][2050/7033]	lr: 1.001e-04, eta: 4:53:46, time: 0.920, data_time: 0.031, memory: 13888, loss_cls: 0.0805, loss_bbox: 0.2190, d0.loss_cls: 0.1663, d0.loss_bbox: 0.3509, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2456, d2.loss_cls: 0.1019, d2.loss_bbox: 0.2258, d3.loss_cls: 0.0872, d3.loss_bbox: 0.2227, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2216, loss: 2.1268, grad_norm: 33.9381
2025-06-17 10:10:57,726 - mmdet - INFO - Epoch [4][2100/7033]	lr: 1.001e-04, eta: 4:53:00, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0798, loss_bbox: 0.2148, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3382, d1.loss_cls: 0.1220, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0997, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2156, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2168, loss: 2.0817, grad_norm: 43.3802
2025-06-17 10:11:43,755 - mmdet - INFO - Epoch [4][2150/7033]	lr: 1.001e-04, eta: 4:52:14, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.0907, loss_bbox: 0.2261, d0.loss_cls: 0.1746, d0.loss_bbox: 0.3467, d1.loss_cls: 0.1278, d1.loss_bbox: 0.2505, d2.loss_cls: 0.1088, d2.loss_bbox: 0.2327, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0922, d4.loss_bbox: 0.2296, loss: 2.2050, grad_norm: 48.6949
2025-06-17 10:12:30,093 - mmdet - INFO - Epoch [4][2200/7033]	lr: 1.001e-04, eta: 4:51:27, time: 0.927, data_time: 0.032, memory: 13888, loss_cls: 0.0827, loss_bbox: 0.2187, d0.loss_cls: 0.1646, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1211, d1.loss_bbox: 0.2469, d2.loss_cls: 0.1015, d2.loss_bbox: 0.2270, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0845, d4.loss_bbox: 0.2202, loss: 2.1297, grad_norm: 32.4398
2025-06-17 10:13:16,427 - mmdet - INFO - Epoch [4][2250/7033]	lr: 1.001e-04, eta: 4:50:41, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0758, loss_bbox: 0.2107, d0.loss_cls: 0.1652, d0.loss_bbox: 0.3262, d1.loss_cls: 0.1129, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0923, d2.loss_bbox: 0.2160, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2117, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2133, loss: 2.0192, grad_norm: 53.4908
2025-06-17 10:14:02,628 - mmdet - INFO - Epoch [4][2300/7033]	lr: 1.001e-04, eta: 4:49:55, time: 0.924, data_time: 0.032, memory: 13888, loss_cls: 0.0838, loss_bbox: 0.2285, d0.loss_cls: 0.1710, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1232, d1.loss_bbox: 0.2497, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0914, d3.loss_bbox: 0.2254, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2263, loss: 2.1570, grad_norm: 35.1092
2025-06-17 10:14:48,694 - mmdet - INFO - Epoch [4][2350/7033]	lr: 1.001e-04, eta: 4:49:08, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.0750, loss_bbox: 0.2162, d0.loss_cls: 0.1601, d0.loss_bbox: 0.3307, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2365, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0803, d3.loss_bbox: 0.2174, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2185, loss: 2.0379, grad_norm: 32.8881
2025-06-17 10:15:34,908 - mmdet - INFO - Epoch [4][2400/7033]	lr: 1.001e-04, eta: 4:48:22, time: 0.924, data_time: 0.033, memory: 13888, loss_cls: 0.0813, loss_bbox: 0.2296, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3463, d1.loss_cls: 0.1183, d1.loss_bbox: 0.2524, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2303, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2302, loss: 2.1615, grad_norm: 78.8698
2025-06-17 10:16:21,351 - mmdet - INFO - Epoch [4][2450/7033]	lr: 1.001e-04, eta: 4:47:36, time: 0.928, data_time: 0.034, memory: 13888, loss_cls: 0.0834, loss_bbox: 0.2237, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2464, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2252, d4.loss_cls: 0.0849, d4.loss_bbox: 0.2252, loss: 2.1437, grad_norm: 34.4516
2025-06-17 10:17:07,542 - mmdet - INFO - Epoch [4][2500/7033]	lr: 1.001e-04, eta: 4:46:50, time: 0.924, data_time: 0.032, memory: 13888, loss_cls: 0.0818, loss_bbox: 0.2278, d0.loss_cls: 0.1766, d0.loss_bbox: 0.3458, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2529, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2321, d3.loss_cls: 0.0887, d3.loss_bbox: 0.2301, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2293, loss: 2.1759, grad_norm: 60.6414
2025-06-17 10:17:53,624 - mmdet - INFO - Epoch [4][2550/7033]	lr: 1.001e-04, eta: 4:46:03, time: 0.922, data_time: 0.033, memory: 13888, loss_cls: 0.0873, loss_bbox: 0.2221, d0.loss_cls: 0.1725, d0.loss_bbox: 0.3452, d1.loss_cls: 0.1248, d1.loss_bbox: 0.2504, d2.loss_cls: 0.1042, d2.loss_bbox: 0.2314, d3.loss_cls: 0.0915, d3.loss_bbox: 0.2274, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2271, loss: 2.1713, grad_norm: 48.7187
2025-06-17 10:18:39,746 - mmdet - INFO - Epoch [4][2600/7033]	lr: 1.001e-04, eta: 4:45:17, time: 0.922, data_time: 0.032, memory: 13888, loss_cls: 0.0790, loss_bbox: 0.2243, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3442, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2477, d2.loss_cls: 0.1000, d2.loss_bbox: 0.2304, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2264, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2267, loss: 2.1341, grad_norm: 51.4637
2025-06-17 10:19:25,909 - mmdet - INFO - Epoch [4][2650/7033]	lr: 1.001e-04, eta: 4:44:31, time: 0.923, data_time: 0.033, memory: 13888, loss_cls: 0.0764, loss_bbox: 0.2222, d0.loss_cls: 0.1651, d0.loss_bbox: 0.3464, d1.loss_cls: 0.1137, d1.loss_bbox: 0.2499, d2.loss_cls: 0.0962, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0838, d3.loss_bbox: 0.2255, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2252, loss: 2.1118, grad_norm: 45.7962
2025-06-17 10:20:12,353 - mmdet - INFO - Epoch [4][2700/7033]	lr: 1.001e-04, eta: 4:43:44, time: 0.929, data_time: 0.033, memory: 13888, loss_cls: 0.0803, loss_bbox: 0.2199, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3308, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2407, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2231, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2224, loss: 2.0856, grad_norm: 34.6613
2025-06-17 10:20:58,520 - mmdet - INFO - Epoch [4][2750/7033]	lr: 1.001e-04, eta: 4:42:58, time: 0.923, data_time: 0.031, memory: 13888, loss_cls: 0.0822, loss_bbox: 0.2187, d0.loss_cls: 0.1667, d0.loss_bbox: 0.3469, d1.loss_cls: 0.1194, d1.loss_bbox: 0.2467, d2.loss_cls: 0.1021, d2.loss_bbox: 0.2268, d3.loss_cls: 0.0923, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2209, loss: 2.1289, grad_norm: 35.3392
2025-06-17 10:21:44,751 - mmdet - INFO - Epoch [4][2800/7033]	lr: 1.001e-04, eta: 4:42:12, time: 0.925, data_time: 0.032, memory: 13888, loss_cls: 0.0803, loss_bbox: 0.2184, d0.loss_cls: 0.1622, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2441, d2.loss_cls: 0.1020, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0888, d3.loss_bbox: 0.2211, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2216, loss: 2.1116, grad_norm: 75.4941
2025-06-17 10:22:31,046 - mmdet - INFO - Epoch [4][2850/7033]	lr: 1.001e-04, eta: 4:41:26, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0825, loss_bbox: 0.2277, d0.loss_cls: 0.1727, d0.loss_bbox: 0.3536, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2515, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2309, d3.loss_cls: 0.0893, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0856, d4.loss_bbox: 0.2296, loss: 2.1772, grad_norm: 48.5904
2025-06-17 10:23:17,424 - mmdet - INFO - Epoch [4][2900/7033]	lr: 1.001e-04, eta: 4:40:39, time: 0.928, data_time: 0.031, memory: 13888, loss_cls: 0.0770, loss_bbox: 0.2175, d0.loss_cls: 0.1598, d0.loss_bbox: 0.3406, d1.loss_cls: 0.1142, d1.loss_bbox: 0.2454, d2.loss_cls: 0.0974, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2216, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2220, loss: 2.0831, grad_norm: 37.1869
2025-06-17 10:24:03,512 - mmdet - INFO - Epoch [4][2950/7033]	lr: 1.001e-04, eta: 4:39:53, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0755, loss_bbox: 0.2153, d0.loss_cls: 0.1676, d0.loss_bbox: 0.3381, d1.loss_cls: 0.1147, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0942, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2183, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2181, loss: 2.0703, grad_norm: 39.9433
2025-06-17 10:24:49,579 - mmdet - INFO - Epoch [4][3000/7033]	lr: 1.001e-04, eta: 4:39:07, time: 0.921, data_time: 0.031, memory: 13888, loss_cls: 0.0775, loss_bbox: 0.2266, d0.loss_cls: 0.1654, d0.loss_bbox: 0.3506, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2493, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2341, d3.loss_cls: 0.0839, d3.loss_bbox: 0.2298, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2292, loss: 2.1422, grad_norm: 31.8989
2025-06-17 10:25:35,541 - mmdet - INFO - Epoch [4][3050/7033]	lr: 1.001e-04, eta: 4:38:20, time: 0.919, data_time: 0.030, memory: 13888, loss_cls: 0.0871, loss_bbox: 0.2255, d0.loss_cls: 0.1805, d0.loss_bbox: 0.3445, d1.loss_cls: 0.1307, d1.loss_bbox: 0.2520, d2.loss_cls: 0.1078, d2.loss_bbox: 0.2350, d3.loss_cls: 0.0954, d3.loss_bbox: 0.2299, d4.loss_cls: 0.0887, d4.loss_bbox: 0.2293, loss: 2.2065, grad_norm: 61.0818
2025-06-17 10:26:21,621 - mmdet - INFO - Epoch [4][3100/7033]	lr: 1.001e-04, eta: 4:37:34, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0794, loss_bbox: 0.2200, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3480, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2446, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2266, d3.loss_cls: 0.0845, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0810, d4.loss_bbox: 0.2236, loss: 2.1159, grad_norm: 51.1262
2025-06-17 10:27:08,182 - mmdet - INFO - Epoch [4][3150/7033]	lr: 1.001e-04, eta: 4:36:48, time: 0.931, data_time: 0.032, memory: 13888, loss_cls: 0.0823, loss_bbox: 0.2169, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3449, d1.loss_cls: 0.1186, d1.loss_bbox: 0.2448, d2.loss_cls: 0.0983, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0827, d4.loss_bbox: 0.2218, loss: 2.1100, grad_norm: 42.5065
2025-06-17 10:27:54,308 - mmdet - INFO - Epoch [4][3200/7033]	lr: 1.001e-04, eta: 4:36:01, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0815, loss_bbox: 0.2141, d0.loss_cls: 0.1678, d0.loss_bbox: 0.3355, d1.loss_cls: 0.1188, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0863, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0819, d4.loss_bbox: 0.2186, loss: 2.0849, grad_norm: 55.0652
2025-06-17 10:28:40,544 - mmdet - INFO - Epoch [4][3250/7033]	lr: 1.001e-04, eta: 4:35:15, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.0791, loss_bbox: 0.2242, d0.loss_cls: 0.1749, d0.loss_bbox: 0.3524, d1.loss_cls: 0.1202, d1.loss_bbox: 0.2507, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2324, d3.loss_cls: 0.0852, d3.loss_bbox: 0.2265, d4.loss_cls: 0.0803, d4.loss_bbox: 0.2260, loss: 2.1504, grad_norm: 34.2425
2025-06-17 10:29:26,853 - mmdet - INFO - Epoch [4][3300/7033]	lr: 1.001e-04, eta: 4:34:29, time: 0.926, data_time: 0.035, memory: 13888, loss_cls: 0.0753, loss_bbox: 0.2084, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1103, d1.loss_bbox: 0.2328, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2162, d3.loss_cls: 0.0813, d3.loss_bbox: 0.2122, d4.loss_cls: 0.0773, d4.loss_bbox: 0.2105, loss: 2.0074, grad_norm: 52.4703
2025-06-17 10:30:13,092 - mmdet - INFO - Epoch [4][3350/7033]	lr: 1.001e-04, eta: 4:33:42, time: 0.925, data_time: 0.032, memory: 13888, loss_cls: 0.0822, loss_bbox: 0.2231, d0.loss_cls: 0.1723, d0.loss_bbox: 0.3415, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2443, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2215, d4.loss_cls: 0.0843, d4.loss_bbox: 0.2229, loss: 2.1315, grad_norm: 44.6187
2025-06-17 10:30:59,154 - mmdet - INFO - Epoch [4][3400/7033]	lr: 1.001e-04, eta: 4:32:56, time: 0.921, data_time: 0.031, memory: 13888, loss_cls: 0.0767, loss_bbox: 0.2198, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3431, d1.loss_cls: 0.1164, d1.loss_bbox: 0.2427, d2.loss_cls: 0.0960, d2.loss_bbox: 0.2253, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2217, d4.loss_cls: 0.0770, d4.loss_bbox: 0.2214, loss: 2.0891, grad_norm: 38.8104
2025-06-17 10:31:45,041 - mmdet - INFO - Epoch [4][3450/7033]	lr: 1.001e-04, eta: 4:32:09, time: 0.918, data_time: 0.027, memory: 13888, loss_cls: 0.0821, loss_bbox: 0.2263, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3433, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2495, d2.loss_cls: 0.1024, d2.loss_bbox: 0.2318, d3.loss_cls: 0.0910, d3.loss_bbox: 0.2271, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2286, loss: 2.1556, grad_norm: 36.9681
2025-06-17 10:32:31,188 - mmdet - INFO - Epoch [4][3500/7033]	lr: 1.001e-04, eta: 4:31:23, time: 0.923, data_time: 0.026, memory: 13888, loss_cls: 0.0814, loss_bbox: 0.2135, d0.loss_cls: 0.1703, d0.loss_bbox: 0.3348, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2397, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2199, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2180, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2184, loss: 2.0905, grad_norm: 64.5996
2025-06-17 10:33:17,357 - mmdet - INFO - Epoch [4][3550/7033]	lr: 1.001e-04, eta: 4:30:37, time: 0.923, data_time: 0.025, memory: 13888, loss_cls: 0.0853, loss_bbox: 0.2238, d0.loss_cls: 0.1690, d0.loss_bbox: 0.3494, d1.loss_cls: 0.1222, d1.loss_bbox: 0.2495, d2.loss_cls: 0.1049, d2.loss_bbox: 0.2307, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0846, d4.loss_bbox: 0.2270, loss: 2.1630, grad_norm: 44.2854
2025-06-17 10:34:03,187 - mmdet - INFO - Epoch [4][3600/7033]	lr: 1.001e-04, eta: 4:29:50, time: 0.917, data_time: 0.028, memory: 13888, loss_cls: 0.0899, loss_bbox: 0.2233, d0.loss_cls: 0.1841, d0.loss_bbox: 0.3401, d1.loss_cls: 0.1330, d1.loss_bbox: 0.2470, d2.loss_cls: 0.1132, d2.loss_bbox: 0.2300, d3.loss_cls: 0.0976, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2259, loss: 2.2057, grad_norm: 33.6993
2025-06-17 10:34:49,104 - mmdet - INFO - Epoch [4][3650/7033]	lr: 1.001e-04, eta: 4:29:04, time: 0.918, data_time: 0.026, memory: 13888, loss_cls: 0.0826, loss_bbox: 0.2144, d0.loss_cls: 0.1713, d0.loss_bbox: 0.3427, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2420, d2.loss_cls: 0.1034, d2.loss_bbox: 0.2209, d3.loss_cls: 0.0890, d3.loss_bbox: 0.2184, d4.loss_cls: 0.0833, d4.loss_bbox: 0.2169, loss: 2.1065, grad_norm: 39.1799
2025-06-17 10:35:34,936 - mmdet - INFO - Epoch [4][3700/7033]	lr: 1.001e-04, eta: 4:28:17, time: 0.917, data_time: 0.025, memory: 13888, loss_cls: 0.0828, loss_bbox: 0.2225, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3426, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2471, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2293, d3.loss_cls: 0.0884, d3.loss_bbox: 0.2270, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2266, loss: 2.1452, grad_norm: 58.8982
2025-06-17 10:36:20,771 - mmdet - INFO - Epoch [4][3750/7033]	lr: 1.001e-04, eta: 4:27:31, time: 0.917, data_time: 0.027, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2167, d0.loss_cls: 0.1734, d0.loss_bbox: 0.3392, d1.loss_cls: 0.1219, d1.loss_bbox: 0.2443, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2243, d3.loss_cls: 0.0902, d3.loss_bbox: 0.2182, d4.loss_cls: 0.0838, d4.loss_bbox: 0.2181, loss: 2.1135, grad_norm: 44.5273
2025-06-17 10:37:06,690 - mmdet - INFO - Epoch [4][3800/7033]	lr: 1.001e-04, eta: 4:26:44, time: 0.918, data_time: 0.026, memory: 13888, loss_cls: 0.0855, loss_bbox: 0.2177, d0.loss_cls: 0.1666, d0.loss_bbox: 0.3354, d1.loss_cls: 0.1207, d1.loss_bbox: 0.2452, d2.loss_cls: 0.1010, d2.loss_bbox: 0.2294, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2223, d4.loss_cls: 0.0860, d4.loss_bbox: 0.2217, loss: 2.1219, grad_norm: 34.5024
2025-06-17 10:37:52,578 - mmdet - INFO - Epoch [4][3850/7033]	lr: 1.001e-04, eta: 4:25:58, time: 0.918, data_time: 0.025, memory: 13888, loss_cls: 0.0743, loss_bbox: 0.2087, d0.loss_cls: 0.1581, d0.loss_bbox: 0.3186, d1.loss_cls: 0.1113, d1.loss_bbox: 0.2322, d2.loss_cls: 0.0941, d2.loss_bbox: 0.2115, d3.loss_cls: 0.0805, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0749, d4.loss_bbox: 0.2108, loss: 1.9857, grad_norm: 49.1594
2025-06-17 10:38:38,663 - mmdet - INFO - Epoch [4][3900/7033]	lr: 1.001e-04, eta: 4:25:11, time: 0.922, data_time: 0.025, memory: 13888, loss_cls: 0.0759, loss_bbox: 0.2096, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3338, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2341, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2171, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2136, d4.loss_cls: 0.0769, d4.loss_bbox: 0.2122, loss: 2.0307, grad_norm: 34.3288
2025-06-17 10:39:24,726 - mmdet - INFO - Epoch [4][3950/7033]	lr: 1.001e-04, eta: 4:24:25, time: 0.921, data_time: 0.027, memory: 13888, loss_cls: 0.0813, loss_bbox: 0.2203, d0.loss_cls: 0.1688, d0.loss_bbox: 0.3358, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2415, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2233, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2225, loss: 2.1086, grad_norm: 40.6733
2025-06-17 10:40:10,570 - mmdet - INFO - Epoch [4][4000/7033]	lr: 1.001e-04, eta: 4:23:38, time: 0.917, data_time: 0.026, memory: 13888, loss_cls: 0.0851, loss_bbox: 0.2227, d0.loss_cls: 0.1759, d0.loss_bbox: 0.3500, d1.loss_cls: 0.1264, d1.loss_bbox: 0.2465, d2.loss_cls: 0.1025, d2.loss_bbox: 0.2275, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2258, loss: 2.1635, grad_norm: 39.9669
2025-06-17 10:40:56,396 - mmdet - INFO - Epoch [4][4050/7033]	lr: 1.001e-04, eta: 4:22:52, time: 0.917, data_time: 0.026, memory: 13888, loss_cls: 0.0763, loss_bbox: 0.2188, d0.loss_cls: 0.1608, d0.loss_bbox: 0.3427, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2450, d2.loss_cls: 0.0977, d2.loss_bbox: 0.2241, d3.loss_cls: 0.0848, d3.loss_bbox: 0.2208, d4.loss_cls: 0.0783, d4.loss_bbox: 0.2201, loss: 2.0840, grad_norm: 61.8726
2025-06-17 10:41:42,173 - mmdet - INFO - Epoch [4][4100/7033]	lr: 1.001e-04, eta: 4:22:05, time: 0.916, data_time: 0.028, memory: 13888, loss_cls: 0.0880, loss_bbox: 0.2250, d0.loss_cls: 0.1697, d0.loss_bbox: 0.3418, d1.loss_cls: 0.1223, d1.loss_bbox: 0.2510, d2.loss_cls: 0.1068, d2.loss_bbox: 0.2316, d3.loss_cls: 0.0957, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0898, d4.loss_bbox: 0.2287, loss: 2.1789, grad_norm: 38.8226
2025-06-17 10:42:28,356 - mmdet - INFO - Epoch [4][4150/7033]	lr: 1.001e-04, eta: 4:21:19, time: 0.924, data_time: 0.030, memory: 13888, loss_cls: 0.0844, loss_bbox: 0.2170, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3294, d1.loss_cls: 0.1270, d1.loss_bbox: 0.2408, d2.loss_cls: 0.1047, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0896, d3.loss_bbox: 0.2197, d4.loss_cls: 0.0853, d4.loss_bbox: 0.2207, loss: 2.1153, grad_norm: 46.3778
2025-06-17 10:43:14,331 - mmdet - INFO - Epoch [4][4200/7033]	lr: 1.001e-04, eta: 4:20:32, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.0827, loss_bbox: 0.2250, d0.loss_cls: 0.1669, d0.loss_bbox: 0.3551, d1.loss_cls: 0.1244, d1.loss_bbox: 0.2542, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2352, d3.loss_cls: 0.0876, d3.loss_bbox: 0.2302, d4.loss_cls: 0.0844, d4.loss_bbox: 0.2285, loss: 2.1770, grad_norm: 44.9626
2025-06-17 10:44:00,478 - mmdet - INFO - Epoch [4][4250/7033]	lr: 1.001e-04, eta: 4:19:46, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0837, loss_bbox: 0.2213, d0.loss_cls: 0.1739, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2468, d2.loss_cls: 0.1029, d2.loss_bbox: 0.2280, d3.loss_cls: 0.0894, d3.loss_bbox: 0.2251, d4.loss_cls: 0.0842, d4.loss_bbox: 0.2261, loss: 2.1501, grad_norm: 35.5496
2025-06-17 10:44:46,459 - mmdet - INFO - Epoch [4][4300/7033]	lr: 1.001e-04, eta: 4:19:00, time: 0.920, data_time: 0.030, memory: 13888, loss_cls: 0.0920, loss_bbox: 0.2285, d0.loss_cls: 0.1760, d0.loss_bbox: 0.3480, d1.loss_cls: 0.1248, d1.loss_bbox: 0.2542, d2.loss_cls: 0.1069, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0967, d3.loss_bbox: 0.2317, d4.loss_cls: 0.0936, d4.loss_bbox: 0.2306, loss: 2.2188, grad_norm: 43.0551
2025-06-17 10:45:32,489 - mmdet - INFO - Epoch [4][4350/7033]	lr: 1.001e-04, eta: 4:18:13, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0799, loss_bbox: 0.2264, d0.loss_cls: 0.1687, d0.loss_bbox: 0.3411, d1.loss_cls: 0.1127, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2311, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2296, loss: 2.1342, grad_norm: 51.6898
2025-06-17 10:46:18,815 - mmdet - INFO - Epoch [4][4400/7033]	lr: 1.001e-04, eta: 4:17:27, time: 0.927, data_time: 0.028, memory: 13888, loss_cls: 0.0849, loss_bbox: 0.2298, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3517, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2549, d2.loss_cls: 0.1033, d2.loss_bbox: 0.2344, d3.loss_cls: 0.0930, d3.loss_bbox: 0.2296, d4.loss_cls: 0.0874, d4.loss_bbox: 0.2317, loss: 2.1980, grad_norm: 40.8601
2025-06-17 10:47:04,728 - mmdet - INFO - Epoch [4][4450/7033]	lr: 1.001e-04, eta: 4:16:41, time: 0.918, data_time: 0.028, memory: 13888, loss_cls: 0.0735, loss_bbox: 0.2145, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3285, d1.loss_cls: 0.1100, d1.loss_bbox: 0.2363, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2184, d3.loss_cls: 0.0790, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0747, d4.loss_bbox: 0.2158, loss: 2.0237, grad_norm: 36.3736
2025-06-17 10:47:57,587 - mmdet - INFO - Epoch [4][4500/7033]	lr: 1.001e-04, eta: 4:15:59, time: 1.057, data_time: 0.026, memory: 13888, loss_cls: 0.0763, loss_bbox: 0.2197, d0.loss_cls: 0.1662, d0.loss_bbox: 0.3330, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0818, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0775, d4.loss_bbox: 0.2217, loss: 2.0656, grad_norm: 45.8257
2025-06-17 10:48:43,523 - mmdet - INFO - Epoch [4][4550/7033]	lr: 1.001e-04, eta: 4:15:12, time: 0.919, data_time: 0.028, memory: 13888, loss_cls: 0.0772, loss_bbox: 0.2258, d0.loss_cls: 0.1736, d0.loss_bbox: 0.3451, d1.loss_cls: 0.1159, d1.loss_bbox: 0.2513, d2.loss_cls: 0.0968, d2.loss_bbox: 0.2326, d3.loss_cls: 0.0824, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2273, loss: 2.1336, grad_norm: 37.3890
2025-06-17 10:49:29,526 - mmdet - INFO - Epoch [4][4600/7033]	lr: 1.001e-04, eta: 4:14:26, time: 0.920, data_time: 0.029, memory: 13888, loss_cls: 0.0810, loss_bbox: 0.2328, d0.loss_cls: 0.1686, d0.loss_bbox: 0.3479, d1.loss_cls: 0.1201, d1.loss_bbox: 0.2565, d2.loss_cls: 0.0990, d2.loss_bbox: 0.2396, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2362, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2370, loss: 2.1858, grad_norm: 34.3941
2025-06-17 10:50:15,276 - mmdet - INFO - Epoch [4][4650/7033]	lr: 1.001e-04, eta: 4:13:39, time: 0.915, data_time: 0.027, memory: 13888, loss_cls: 0.0787, loss_bbox: 0.2189, d0.loss_cls: 0.1659, d0.loss_bbox: 0.3331, d1.loss_cls: 0.1146, d1.loss_bbox: 0.2461, d2.loss_cls: 0.0955, d2.loss_bbox: 0.2284, d3.loss_cls: 0.0841, d3.loss_bbox: 0.2242, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2219, loss: 2.0911, grad_norm: 57.4374
2025-06-17 10:51:01,101 - mmdet - INFO - Epoch [4][4700/7033]	lr: 1.001e-04, eta: 4:12:53, time: 0.916, data_time: 0.026, memory: 13888, loss_cls: 0.0805, loss_bbox: 0.2245, d0.loss_cls: 0.1775, d0.loss_bbox: 0.3482, d1.loss_cls: 0.1212, d1.loss_bbox: 0.2503, d2.loss_cls: 0.0986, d2.loss_bbox: 0.2347, d3.loss_cls: 0.0869, d3.loss_bbox: 0.2284, d4.loss_cls: 0.0825, d4.loss_bbox: 0.2276, loss: 2.1609, grad_norm: 91.5048
2025-06-17 10:51:47,056 - mmdet - INFO - Epoch [4][4750/7033]	lr: 1.001e-04, eta: 4:12:06, time: 0.919, data_time: 0.026, memory: 13888, loss_cls: 0.0843, loss_bbox: 0.2234, d0.loss_cls: 0.1714, d0.loss_bbox: 0.3429, d1.loss_cls: 0.1189, d1.loss_bbox: 0.2494, d2.loss_cls: 0.0998, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0906, d3.loss_bbox: 0.2262, d4.loss_cls: 0.0851, d4.loss_bbox: 0.2264, loss: 2.1490, grad_norm: 37.5241
2025-06-17 10:52:33,142 - mmdet - INFO - Epoch [4][4800/7033]	lr: 1.001e-04, eta: 4:11:20, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0828, loss_bbox: 0.2238, d0.loss_cls: 0.1748, d0.loss_bbox: 0.3325, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2436, d2.loss_cls: 0.0966, d2.loss_bbox: 0.2285, d3.loss_cls: 0.0875, d3.loss_bbox: 0.2258, d4.loss_cls: 0.0830, d4.loss_bbox: 0.2260, loss: 2.1230, grad_norm: 33.3297
2025-06-17 10:53:19,193 - mmdet - INFO - Epoch [4][4850/7033]	lr: 1.001e-04, eta: 4:10:33, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0788, loss_bbox: 0.2189, d0.loss_cls: 0.1639, d0.loss_bbox: 0.3332, d1.loss_cls: 0.1155, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0958, d2.loss_bbox: 0.2235, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2202, d4.loss_cls: 0.0800, d4.loss_bbox: 0.2209, loss: 2.0757, grad_norm: 57.9271
2025-06-17 10:54:05,201 - mmdet - INFO - Epoch [4][4900/7033]	lr: 1.001e-04, eta: 4:09:47, time: 0.920, data_time: 0.028, memory: 13888, loss_cls: 0.0957, loss_bbox: 0.2213, d0.loss_cls: 0.1780, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1259, d1.loss_bbox: 0.2485, d2.loss_cls: 0.1100, d2.loss_bbox: 0.2311, d3.loss_cls: 0.1001, d3.loss_bbox: 0.2253, d4.loss_cls: 0.0947, d4.loss_bbox: 0.2249, loss: 2.1998, grad_norm: 59.1770
2025-06-17 10:54:51,283 - mmdet - INFO - Epoch [4][4950/7033]	lr: 1.001e-04, eta: 4:09:01, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0779, loss_bbox: 0.2209, d0.loss_cls: 0.1674, d0.loss_bbox: 0.3488, d1.loss_cls: 0.1218, d1.loss_bbox: 0.2454, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2278, d3.loss_cls: 0.0866, d3.loss_bbox: 0.2246, d4.loss_cls: 0.0802, d4.loss_bbox: 0.2238, loss: 2.1254, grad_norm: 36.9196
2025-06-17 10:55:37,181 - mmdet - INFO - Epoch [4][5000/7033]	lr: 1.001e-04, eta: 4:08:14, time: 0.918, data_time: 0.030, memory: 13888, loss_cls: 0.0857, loss_bbox: 0.2255, d0.loss_cls: 0.1776, d0.loss_bbox: 0.3394, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2459, d2.loss_cls: 0.1032, d2.loss_bbox: 0.2289, d3.loss_cls: 0.0903, d3.loss_bbox: 0.2280, d4.loss_cls: 0.0854, d4.loss_bbox: 0.2281, loss: 2.1615, grad_norm: 46.8917
2025-06-17 10:56:23,285 - mmdet - INFO - Epoch [4][5050/7033]	lr: 1.001e-04, eta: 4:07:28, time: 0.922, data_time: 0.032, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2164, d0.loss_cls: 0.1650, d0.loss_bbox: 0.3297, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2424, d2.loss_cls: 0.0999, d2.loss_bbox: 0.2261, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2203, loss: 2.0915, grad_norm: 40.5125
2025-06-17 10:57:09,545 - mmdet - INFO - Epoch [4][5100/7033]	lr: 1.001e-04, eta: 4:06:41, time: 0.925, data_time: 0.030, memory: 13888, loss_cls: 0.0882, loss_bbox: 0.2312, d0.loss_cls: 0.1810, d0.loss_bbox: 0.3557, d1.loss_cls: 0.1288, d1.loss_bbox: 0.2585, d2.loss_cls: 0.1097, d2.loss_bbox: 0.2391, d3.loss_cls: 0.0955, d3.loss_bbox: 0.2344, d4.loss_cls: 0.0896, d4.loss_bbox: 0.2337, loss: 2.2453, grad_norm: 55.7530
2025-06-17 10:57:57,276 - mmdet - INFO - Epoch [4][5150/7033]	lr: 1.001e-04, eta: 4:05:56, time: 0.955, data_time: 0.030, memory: 13888, loss_cls: 0.0800, loss_bbox: 0.2147, d0.loss_cls: 0.1743, d0.loss_bbox: 0.3379, d1.loss_cls: 0.1203, d1.loss_bbox: 0.2431, d2.loss_cls: 0.1011, d2.loss_bbox: 0.2249, d3.loss_cls: 0.0883, d3.loss_bbox: 0.2189, d4.loss_cls: 0.0804, d4.loss_bbox: 0.2176, loss: 2.1016, grad_norm: 63.9649
2025-06-17 10:58:43,244 - mmdet - INFO - Epoch [4][5200/7033]	lr: 1.001e-04, eta: 4:05:10, time: 0.919, data_time: 0.031, memory: 13888, loss_cls: 0.0751, loss_bbox: 0.2134, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0965, d2.loss_bbox: 0.2197, d3.loss_cls: 0.0821, d3.loss_bbox: 0.2164, d4.loss_cls: 0.0774, d4.loss_bbox: 0.2155, loss: 2.0469, grad_norm: 64.8943
2025-06-17 10:59:29,209 - mmdet - INFO - Epoch [4][5250/7033]	lr: 1.001e-04, eta: 4:04:23, time: 0.919, data_time: 0.030, memory: 13888, loss_cls: 0.0777, loss_bbox: 0.2092, d0.loss_cls: 0.1653, d0.loss_bbox: 0.3213, d1.loss_cls: 0.1149, d1.loss_bbox: 0.2348, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2126, d3.loss_cls: 0.0829, d3.loss_bbox: 0.2107, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2116, loss: 2.0169, grad_norm: 42.4906
2025-06-17 11:00:15,369 - mmdet - INFO - Epoch [4][5300/7033]	lr: 1.001e-04, eta: 4:03:37, time: 0.923, data_time: 0.029, memory: 13888, loss_cls: 0.0818, loss_bbox: 0.2192, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3369, d1.loss_cls: 0.1210, d1.loss_bbox: 0.2430, d2.loss_cls: 0.1001, d2.loss_bbox: 0.2246, d3.loss_cls: 0.0873, d3.loss_bbox: 0.2214, d4.loss_cls: 0.0836, d4.loss_bbox: 0.2212, loss: 2.1040, grad_norm: 34.0357
2025-06-17 11:01:01,424 - mmdet - INFO - Epoch [4][5350/7033]	lr: 1.001e-04, eta: 4:02:51, time: 0.921, data_time: 0.031, memory: 13888, loss_cls: 0.0793, loss_bbox: 0.2129, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3339, d1.loss_cls: 0.1206, d1.loss_bbox: 0.2367, d2.loss_cls: 0.0967, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0799, d4.loss_bbox: 0.2154, loss: 2.0619, grad_norm: 48.3359
2025-06-17 11:01:47,478 - mmdet - INFO - Epoch [4][5400/7033]	lr: 1.001e-04, eta: 4:02:04, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.0806, loss_bbox: 0.2211, d0.loss_cls: 0.1645, d0.loss_bbox: 0.3385, d1.loss_cls: 0.1199, d1.loss_bbox: 0.2417, d2.loss_cls: 0.1007, d2.loss_bbox: 0.2242, d3.loss_cls: 0.0879, d3.loss_bbox: 0.2220, d4.loss_cls: 0.0820, d4.loss_bbox: 0.2242, loss: 2.1073, grad_norm: 34.3042
2025-06-17 11:02:33,536 - mmdet - INFO - Epoch [4][5450/7033]	lr: 1.001e-04, eta: 4:01:18, time: 0.921, data_time: 0.028, memory: 13888, loss_cls: 0.0763, loss_bbox: 0.2077, d0.loss_cls: 0.1580, d0.loss_bbox: 0.3200, d1.loss_cls: 0.1099, d1.loss_bbox: 0.2326, d2.loss_cls: 0.0927, d2.loss_bbox: 0.2134, d3.loss_cls: 0.0820, d3.loss_bbox: 0.2097, d4.loss_cls: 0.0766, d4.loss_bbox: 0.2101, loss: 1.9888, grad_norm: 39.4245
2025-06-17 11:03:19,663 - mmdet - INFO - Epoch [4][5500/7033]	lr: 1.001e-04, eta: 4:00:31, time: 0.923, data_time: 0.027, memory: 13888, loss_cls: 0.0788, loss_bbox: 0.2150, d0.loss_cls: 0.1627, d0.loss_bbox: 0.3352, d1.loss_cls: 0.1176, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0981, d2.loss_bbox: 0.2211, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2173, loss: 2.0658, grad_norm: 34.6721
2025-06-17 11:04:05,579 - mmdet - INFO - Epoch [4][5550/7033]	lr: 1.001e-04, eta: 3:59:45, time: 0.918, data_time: 0.029, memory: 13888, loss_cls: 0.0825, loss_bbox: 0.2092, d0.loss_cls: 0.1683, d0.loss_bbox: 0.3324, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2369, d2.loss_cls: 0.1017, d2.loss_bbox: 0.2191, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2149, d4.loss_cls: 0.0843, d4.loss_bbox: 0.2132, loss: 2.0761, grad_norm: 43.0772
2025-06-17 11:04:51,839 - mmdet - INFO - Epoch [4][5600/7033]	lr: 1.001e-04, eta: 3:58:59, time: 0.925, data_time: 0.031, memory: 13888, loss_cls: 0.0753, loss_bbox: 0.2081, d0.loss_cls: 0.1553, d0.loss_bbox: 0.3200, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0903, d2.loss_bbox: 0.2152, d3.loss_cls: 0.0796, d3.loss_bbox: 0.2118, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2113, loss: 1.9818, grad_norm: 73.0660
2025-06-17 11:05:38,071 - mmdet - INFO - Epoch [4][5650/7033]	lr: 1.001e-04, eta: 3:58:12, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.0886, loss_bbox: 0.2201, d0.loss_cls: 0.1673, d0.loss_bbox: 0.3440, d1.loss_cls: 0.1261, d1.loss_bbox: 0.2449, d2.loss_cls: 0.1035, d2.loss_bbox: 0.2306, d3.loss_cls: 0.0938, d3.loss_bbox: 0.2256, d4.loss_cls: 0.0894, d4.loss_bbox: 0.2247, loss: 2.1586, grad_norm: 49.8115
2025-06-17 11:06:24,227 - mmdet - INFO - Epoch [4][5700/7033]	lr: 1.001e-04, eta: 3:57:26, time: 0.923, data_time: 0.032, memory: 13888, loss_cls: 0.0908, loss_bbox: 0.2215, d0.loss_cls: 0.1702, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1236, d1.loss_bbox: 0.2508, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2303, d3.loss_cls: 0.0940, d3.loss_bbox: 0.2249, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2243, loss: 2.1706, grad_norm: 43.4139
2025-06-17 11:07:10,401 - mmdet - INFO - Epoch [4][5750/7033]	lr: 1.001e-04, eta: 3:56:40, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0822, loss_bbox: 0.2238, d0.loss_cls: 0.1615, d0.loss_bbox: 0.3413, d1.loss_cls: 0.1158, d1.loss_bbox: 0.2512, d2.loss_cls: 0.0975, d2.loss_bbox: 0.2356, d3.loss_cls: 0.0878, d3.loss_bbox: 0.2285, d4.loss_cls: 0.0835, d4.loss_bbox: 0.2278, loss: 2.1364, grad_norm: 70.2002
2025-06-17 11:07:56,468 - mmdet - INFO - Epoch [4][5800/7033]	lr: 1.001e-04, eta: 3:55:53, time: 0.921, data_time: 0.033, memory: 13888, loss_cls: 0.0791, loss_bbox: 0.2179, d0.loss_cls: 0.1626, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1157, d1.loss_bbox: 0.2423, d2.loss_cls: 0.0961, d2.loss_bbox: 0.2259, d3.loss_cls: 0.0842, d3.loss_bbox: 0.2226, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2202, loss: 2.0816, grad_norm: 37.2416
2025-06-17 11:08:42,807 - mmdet - INFO - Epoch [4][5850/7033]	lr: 1.001e-04, eta: 3:55:07, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0837, loss_bbox: 0.2299, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3447, d1.loss_cls: 0.1217, d1.loss_bbox: 0.2534, d2.loss_cls: 0.1053, d2.loss_bbox: 0.2354, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2324, d4.loss_cls: 0.0873, d4.loss_bbox: 0.2309, loss: 2.1792, grad_norm: 49.7739
2025-06-17 11:09:29,000 - mmdet - INFO - Epoch [4][5900/7033]	lr: 1.001e-04, eta: 3:54:21, time: 0.924, data_time: 0.031, memory: 13888, loss_cls: 0.0801, loss_bbox: 0.2172, d0.loss_cls: 0.1605, d0.loss_bbox: 0.3355, d1.loss_cls: 0.1171, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0995, d2.loss_bbox: 0.2239, d3.loss_cls: 0.0859, d3.loss_bbox: 0.2206, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2192, loss: 2.0832, grad_norm: 83.2988
2025-06-17 11:10:15,099 - mmdet - INFO - Epoch [4][5950/7033]	lr: 1.001e-04, eta: 3:53:35, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0780, loss_bbox: 0.2150, d0.loss_cls: 0.1590, d0.loss_bbox: 0.3336, d1.loss_cls: 0.1117, d1.loss_bbox: 0.2409, d2.loss_cls: 0.0928, d2.loss_bbox: 0.2226, d3.loss_cls: 0.0828, d3.loss_bbox: 0.2168, d4.loss_cls: 0.0790, d4.loss_bbox: 0.2164, loss: 2.0485, grad_norm: 66.0756
2025-06-17 11:11:01,223 - mmdet - INFO - Epoch [4][6000/7033]	lr: 1.001e-04, eta: 3:52:48, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0742, loss_bbox: 0.2170, d0.loss_cls: 0.1744, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1155, d1.loss_bbox: 0.2410, d2.loss_cls: 0.0918, d2.loss_bbox: 0.2252, d3.loss_cls: 0.0804, d3.loss_bbox: 0.2209, d4.loss_cls: 0.0759, d4.loss_bbox: 0.2191, loss: 2.0680, grad_norm: 32.9622
2025-06-17 11:11:47,182 - mmdet - INFO - Epoch [4][6050/7033]	lr: 1.001e-04, eta: 3:52:02, time: 0.919, data_time: 0.028, memory: 13888, loss_cls: 0.0797, loss_bbox: 0.2179, d0.loss_cls: 0.1655, d0.loss_bbox: 0.3372, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2440, d2.loss_cls: 0.0980, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0853, d3.loss_bbox: 0.2212, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2207, loss: 2.0916, grad_norm: 37.7773
2025-06-17 11:12:33,419 - mmdet - INFO - Epoch [4][6100/7033]	lr: 1.001e-04, eta: 3:51:16, time: 0.925, data_time: 0.033, memory: 13888, loss_cls: 0.0766, loss_bbox: 0.2137, d0.loss_cls: 0.1589, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1081, d1.loss_bbox: 0.2413, d2.loss_cls: 0.0916, d2.loss_bbox: 0.2213, d3.loss_cls: 0.0800, d3.loss_bbox: 0.2179, d4.loss_cls: 0.0787, d4.loss_bbox: 0.2161, loss: 2.0383, grad_norm: 40.8398
2025-06-17 11:13:19,592 - mmdet - INFO - Epoch [4][6150/7033]	lr: 1.001e-04, eta: 3:50:29, time: 0.923, data_time: 0.030, memory: 13888, loss_cls: 0.0852, loss_bbox: 0.2227, d0.loss_cls: 0.1699, d0.loss_bbox: 0.3486, d1.loss_cls: 0.1245, d1.loss_bbox: 0.2510, d2.loss_cls: 0.1056, d2.loss_bbox: 0.2320, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2294, d4.loss_cls: 0.0861, d4.loss_bbox: 0.2263, loss: 2.1721, grad_norm: 44.8740
2025-06-17 11:14:06,082 - mmdet - INFO - Epoch [4][6200/7033]	lr: 1.001e-04, eta: 3:49:43, time: 0.930, data_time: 0.034, memory: 13888, loss_cls: 0.0801, loss_bbox: 0.2140, d0.loss_cls: 0.1621, d0.loss_bbox: 0.3322, d1.loss_cls: 0.1175, d1.loss_bbox: 0.2419, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2230, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0808, d4.loss_bbox: 0.2165, loss: 2.0694, grad_norm: 34.1750
2025-06-17 11:14:52,208 - mmdet - INFO - Epoch [4][6250/7033]	lr: 1.001e-04, eta: 3:48:57, time: 0.922, data_time: 0.031, memory: 13888, loss_cls: 0.0805, loss_bbox: 0.2099, d0.loss_cls: 0.1636, d0.loss_bbox: 0.3317, d1.loss_cls: 0.1160, d1.loss_bbox: 0.2377, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2190, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2128, d4.loss_cls: 0.0824, d4.loss_bbox: 0.2128, loss: 2.0504, grad_norm: 37.3525
2025-06-17 11:15:38,092 - mmdet - INFO - Epoch [4][6300/7033]	lr: 1.001e-04, eta: 3:48:10, time: 0.918, data_time: 0.029, memory: 13888, loss_cls: 0.0847, loss_bbox: 0.2202, d0.loss_cls: 0.1671, d0.loss_bbox: 0.3443, d1.loss_cls: 0.1182, d1.loss_bbox: 0.2467, d2.loss_cls: 0.1009, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0908, d3.loss_bbox: 0.2224, d4.loss_cls: 0.0858, d4.loss_bbox: 0.2212, loss: 2.1292, grad_norm: 40.9886
2025-06-17 11:16:24,387 - mmdet - INFO - Epoch [4][6350/7033]	lr: 1.001e-04, eta: 3:47:24, time: 0.926, data_time: 0.029, memory: 13888, loss_cls: 0.0796, loss_bbox: 0.2226, d0.loss_cls: 0.1705, d0.loss_bbox: 0.3444, d1.loss_cls: 0.1192, d1.loss_bbox: 0.2504, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2335, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2277, d4.loss_cls: 0.0805, d4.loss_bbox: 0.2270, loss: 2.1398, grad_norm: 37.0034
2025-06-17 11:17:10,658 - mmdet - INFO - Epoch [4][6400/7033]	lr: 1.001e-04, eta: 3:46:38, time: 0.925, data_time: 0.029, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2180, d0.loss_cls: 0.1675, d0.loss_bbox: 0.3332, d1.loss_cls: 0.1216, d1.loss_bbox: 0.2409, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2208, d3.loss_cls: 0.0886, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0829, d4.loss_bbox: 0.2198, loss: 2.0945, grad_norm: 58.1706
2025-06-17 11:17:56,624 - mmdet - INFO - Epoch [4][6450/7033]	lr: 1.001e-04, eta: 3:45:51, time: 0.919, data_time: 0.027, memory: 13888, loss_cls: 0.0771, loss_bbox: 0.2125, d0.loss_cls: 0.1635, d0.loss_bbox: 0.3289, d1.loss_cls: 0.1174, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0964, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0844, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0798, d4.loss_bbox: 0.2155, loss: 2.0486, grad_norm: 61.0805
2025-06-17 11:18:42,749 - mmdet - INFO - Epoch [4][6500/7033]	lr: 1.001e-04, eta: 3:45:05, time: 0.922, data_time: 0.029, memory: 13888, loss_cls: 0.0810, loss_bbox: 0.2224, d0.loss_cls: 0.1693, d0.loss_bbox: 0.3456, d1.loss_cls: 0.1230, d1.loss_bbox: 0.2499, d2.loss_cls: 0.1045, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0882, d3.loss_bbox: 0.2243, d4.loss_cls: 0.0817, d4.loss_bbox: 0.2247, loss: 2.1434, grad_norm: 47.9939
2025-06-17 11:19:29,081 - mmdet - INFO - Epoch [4][6550/7033]	lr: 1.001e-04, eta: 3:44:19, time: 0.927, data_time: 0.031, memory: 13888, loss_cls: 0.0815, loss_bbox: 0.2174, d0.loss_cls: 0.1577, d0.loss_bbox: 0.3303, d1.loss_cls: 0.1140, d1.loss_bbox: 0.2394, d2.loss_cls: 0.0984, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0865, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0814, d4.loss_bbox: 0.2193, loss: 2.0687, grad_norm: 105.1092
2025-06-17 11:20:15,354 - mmdet - INFO - Epoch [4][6600/7033]	lr: 1.001e-04, eta: 3:43:33, time: 0.925, data_time: 0.033, memory: 13888, loss_cls: 0.0835, loss_bbox: 0.2175, d0.loss_cls: 0.1610, d0.loss_bbox: 0.3281, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2415, d2.loss_cls: 0.0993, d2.loss_bbox: 0.2254, d3.loss_cls: 0.0892, d3.loss_bbox: 0.2201, d4.loss_cls: 0.0847, d4.loss_bbox: 0.2199, loss: 2.0880, grad_norm: 52.4344
2025-06-17 11:21:01,628 - mmdet - INFO - Epoch [4][6650/7033]	lr: 1.001e-04, eta: 3:42:46, time: 0.925, data_time: 0.028, memory: 13888, loss_cls: 0.0796, loss_bbox: 0.2159, d0.loss_cls: 0.1629, d0.loss_bbox: 0.3292, d1.loss_cls: 0.1184, d1.loss_bbox: 0.2396, d2.loss_cls: 0.0992, d2.loss_bbox: 0.2214, d3.loss_cls: 0.0870, d3.loss_bbox: 0.2185, d4.loss_cls: 0.0811, d4.loss_bbox: 0.2176, loss: 2.0705, grad_norm: 41.5631
2025-06-17 11:21:47,380 - mmdet - INFO - Epoch [4][6700/7033]	lr: 1.001e-04, eta: 3:42:00, time: 0.915, data_time: 0.029, memory: 13888, loss_cls: 0.0911, loss_bbox: 0.2194, d0.loss_cls: 0.1752, d0.loss_bbox: 0.3353, d1.loss_cls: 0.1250, d1.loss_bbox: 0.2424, d2.loss_cls: 0.1061, d2.loss_bbox: 0.2237, d3.loss_cls: 0.0959, d3.loss_bbox: 0.2213, d4.loss_cls: 0.0913, d4.loss_bbox: 0.2217, loss: 2.1483, grad_norm: 75.7149
2025-06-17 11:22:33,495 - mmdet - INFO - Epoch [4][6750/7033]	lr: 1.001e-04, eta: 3:41:14, time: 0.922, data_time: 0.030, memory: 13888, loss_cls: 0.0866, loss_bbox: 0.2268, d0.loss_cls: 0.1758, d0.loss_bbox: 0.3476, d1.loss_cls: 0.1239, d1.loss_bbox: 0.2552, d2.loss_cls: 0.1028, d2.loss_bbox: 0.2358, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2316, d4.loss_cls: 0.0872, d4.loss_bbox: 0.2321, loss: 2.1974, grad_norm: 37.5512
2025-06-17 11:23:19,497 - mmdet - INFO - Epoch [4][6800/7033]	lr: 1.001e-04, eta: 3:40:27, time: 0.920, data_time: 0.031, memory: 13888, loss_cls: 0.0729, loss_bbox: 0.2098, d0.loss_cls: 0.1637, d0.loss_bbox: 0.3264, d1.loss_cls: 0.1085, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0924, d2.loss_bbox: 0.2181, d3.loss_cls: 0.0794, d3.loss_bbox: 0.2140, d4.loss_cls: 0.0743, d4.loss_bbox: 0.2124, loss: 2.0115, grad_norm: 54.2889
2025-06-17 11:24:05,406 - mmdet - INFO - Epoch [4][6850/7033]	lr: 1.001e-04, eta: 3:39:41, time: 0.918, data_time: 0.032, memory: 13888, loss_cls: 0.0688, loss_bbox: 0.2130, d0.loss_cls: 0.1568, d0.loss_bbox: 0.3306, d1.loss_cls: 0.1106, d1.loss_bbox: 0.2390, d2.loss_cls: 0.0899, d2.loss_bbox: 0.2206, d3.loss_cls: 0.0767, d3.loss_bbox: 0.2148, d4.loss_cls: 0.0711, d4.loss_bbox: 0.2144, loss: 2.0063, grad_norm: 49.4004
2025-06-17 11:24:51,444 - mmdet - INFO - Epoch [4][6900/7033]	lr: 1.001e-04, eta: 3:38:54, time: 0.921, data_time: 0.030, memory: 13888, loss_cls: 0.0799, loss_bbox: 0.2214, d0.loss_cls: 0.1640, d0.loss_bbox: 0.3504, d1.loss_cls: 0.1161, d1.loss_bbox: 0.2474, d2.loss_cls: 0.0971, d2.loss_bbox: 0.2290, d3.loss_cls: 0.0861, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0821, d4.loss_bbox: 0.2235, loss: 2.1219, grad_norm: 41.1948
2025-06-17 11:25:37,277 - mmdet - INFO - Epoch [4][6950/7033]	lr: 1.001e-04, eta: 3:38:08, time: 0.917, data_time: 0.030, memory: 13888, loss_cls: 0.0804, loss_bbox: 0.2219, d0.loss_cls: 0.1587, d0.loss_bbox: 0.3357, d1.loss_cls: 0.1180, d1.loss_bbox: 0.2453, d2.loss_cls: 0.1002, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0874, d3.loss_bbox: 0.2248, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2247, loss: 2.1082, grad_norm: 37.4144
2025-06-17 11:26:23,633 - mmdet - INFO - Epoch [4][7000/7033]	lr: 1.001e-04, eta: 3:37:22, time: 0.927, data_time: 0.030, memory: 13888, loss_cls: 0.0792, loss_bbox: 0.2172, d0.loss_cls: 0.1604, d0.loss_bbox: 0.3293, d1.loss_cls: 0.1148, d1.loss_bbox: 0.2401, d2.loss_cls: 0.0979, d2.loss_bbox: 0.2207, d3.loss_cls: 0.0849, d3.loss_bbox: 0.2194, d4.loss_cls: 0.0816, d4.loss_bbox: 0.2197, loss: 2.0653, grad_norm: 35.3031
2025-06-17 11:26:54,252 - mmdet - INFO - Saving checkpoint at 4 epochs
2025-06-17 11:49:48,251 - mmdet - INFO - Exp name: lidar_0075v_cam_res_2x2_hednetmiddleencoder_hednetbackbone4_dss0511_dp03_hugeep2_num2_morton_conv_xy_rope_bs2_no_dss.py
2025-06-17 11:49:48,252 - mmdet - INFO - Epoch(val) [4][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7868, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8789, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9063, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9200, pts_bbox_NuScenes/car_trans_err: 0.1865, pts_bbox_NuScenes/car_scale_err: 0.1525, pts_bbox_NuScenes/car_orient_err: 0.0446, pts_bbox_NuScenes/car_vel_err: 0.3189, pts_bbox_NuScenes/car_attr_err: 0.1813, pts_bbox_NuScenes/mATE: 0.3018, pts_bbox_NuScenes/mASE: 0.2625, pts_bbox_NuScenes/mAOE: 0.2431, pts_bbox_NuScenes/mAVE: 0.2940, pts_bbox_NuScenes/mAAE: 0.1884, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.4353, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6222, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7243, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7666, pts_bbox_NuScenes/truck_trans_err: 0.3370, pts_bbox_NuScenes/truck_scale_err: 0.2005, pts_bbox_NuScenes/truck_orient_err: 0.0502, pts_bbox_NuScenes/truck_vel_err: 0.2929, pts_bbox_NuScenes/truck_attr_err: 0.1954, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0677, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.2160, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.3993, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.4720, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.6195, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4351, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.7533, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1063, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3138, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.5112, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7363, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8986, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9253, pts_bbox_NuScenes/bus_trans_err: 0.3500, pts_bbox_NuScenes/bus_scale_err: 0.1863, pts_bbox_NuScenes/bus_orient_err: 0.0410, pts_bbox_NuScenes/bus_vel_err: 0.5273, pts_bbox_NuScenes/bus_attr_err: 0.2738, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1463, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.4148, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5887, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6724, pts_bbox_NuScenes/trailer_trans_err: 0.5099, pts_bbox_NuScenes/trailer_scale_err: 0.2243, pts_bbox_NuScenes/trailer_orient_err: 0.3933, pts_bbox_NuScenes/trailer_vel_err: 0.2647, pts_bbox_NuScenes/trailer_attr_err: 0.1629, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5642, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6785, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7345, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7587, pts_bbox_NuScenes/barrier_trans_err: 0.2552, pts_bbox_NuScenes/barrier_scale_err: 0.2844, pts_bbox_NuScenes/barrier_orient_err: 0.0495, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.6185, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7468, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.7857, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.7947, pts_bbox_NuScenes/motorcycle_trans_err: 0.2193, pts_bbox_NuScenes/motorcycle_scale_err: 0.2481, pts_bbox_NuScenes/motorcycle_orient_err: 0.2068, pts_bbox_NuScenes/motorcycle_vel_err: 0.4051, pts_bbox_NuScenes/motorcycle_attr_err: 0.2495, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5173, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.5862, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6005, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6105, pts_bbox_NuScenes/bicycle_trans_err: 0.1936, pts_bbox_NuScenes/bicycle_scale_err: 0.2728, pts_bbox_NuScenes/bicycle_orient_err: 0.3035, pts_bbox_NuScenes/bicycle_vel_err: 0.2099, pts_bbox_NuScenes/bicycle_attr_err: 0.0080, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7925, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8418, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8680, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8825, pts_bbox_NuScenes/pedestrian_trans_err: 0.1695, pts_bbox_NuScenes/pedestrian_scale_err: 0.2954, pts_bbox_NuScenes/pedestrian_orient_err: 0.3457, pts_bbox_NuScenes/pedestrian_vel_err: 0.2267, pts_bbox_NuScenes/pedestrian_attr_err: 0.1224, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6964, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7465, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7797, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8088, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1772, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3254, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.7023, pts_bbox_NuScenes/mAP: 0.6625
2025-06-17 11:50:46,205 - mmdet - INFO - Epoch [5][50/7033]	lr: 5.015e-05, eta: 3:35:53, time: 1.071, data_time: 0.130, memory: 13888, loss_cls: 0.0785, loss_bbox: 0.2162, d0.loss_cls: 0.1695, d0.loss_bbox: 0.3454, d1.loss_cls: 0.1169, d1.loss_bbox: 0.2486, d2.loss_cls: 0.0978, d2.loss_bbox: 0.2255, d3.loss_cls: 0.0868, d3.loss_bbox: 0.2207, d4.loss_cls: 0.0818, d4.loss_bbox: 0.2192, loss: 2.1070, grad_norm: 34.5899
2025-06-17 11:51:33,861 - mmdet - INFO - Epoch [5][100/7033]	lr: 5.015e-05, eta: 3:35:08, time: 0.953, data_time: 0.048, memory: 13888, loss_cls: 0.0779, loss_bbox: 0.2055, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3271, d1.loss_cls: 0.1133, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0954, d2.loss_bbox: 0.2173, d3.loss_cls: 0.0858, d3.loss_bbox: 0.2103, d4.loss_cls: 0.0807, d4.loss_bbox: 0.2085, loss: 2.0147, grad_norm: 74.2833
2025-06-17 11:52:25,213 - mmdet - INFO - Epoch [5][150/7033]	lr: 5.015e-05, eta: 3:34:24, time: 1.027, data_time: 0.034, memory: 13888, loss_cls: 0.0842, loss_bbox: 0.2184, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3508, d1.loss_cls: 0.1214, d1.loss_bbox: 0.2491, d2.loss_cls: 0.1023, d2.loss_bbox: 0.2288, d3.loss_cls: 0.0919, d3.loss_bbox: 0.2235, d4.loss_cls: 0.0865, d4.loss_bbox: 0.2212, loss: 2.1483, grad_norm: 34.6933
2025-06-17 11:53:12,361 - mmdet - INFO - Epoch [5][200/7033]	lr: 5.015e-05, eta: 3:33:38, time: 0.943, data_time: 0.033, memory: 13888, loss_cls: 0.0756, loss_bbox: 0.2120, d0.loss_cls: 0.1585, d0.loss_bbox: 0.3240, d1.loss_cls: 0.1110, d1.loss_bbox: 0.2384, d2.loss_cls: 0.0919, d2.loss_bbox: 0.2193, d3.loss_cls: 0.0807, d3.loss_bbox: 0.2173, d4.loss_cls: 0.0768, d4.loss_bbox: 0.2141, loss: 2.0196, grad_norm: 36.2624
2025-06-17 11:53:58,916 - mmdet - INFO - Epoch [5][250/7033]	lr: 5.015e-05, eta: 3:32:52, time: 0.931, data_time: 0.034, memory: 13888, loss_cls: 0.0768, loss_bbox: 0.2122, d0.loss_cls: 0.1579, d0.loss_bbox: 0.3295, d1.loss_cls: 0.1145, d1.loss_bbox: 0.2395, d2.loss_cls: 0.0953, d2.loss_bbox: 0.2222, d3.loss_cls: 0.0819, d3.loss_bbox: 0.2171, d4.loss_cls: 0.0771, d4.loss_bbox: 0.2151, loss: 2.0391, grad_norm: 30.0679
2025-06-17 11:54:45,376 - mmdet - INFO - Epoch [5][300/7033]	lr: 5.015e-05, eta: 3:32:06, time: 0.929, data_time: 0.030, memory: 13888, loss_cls: 0.0784, loss_bbox: 0.2142, d0.loss_cls: 0.1611, d0.loss_bbox: 0.3349, d1.loss_cls: 0.1151, d1.loss_bbox: 0.2438, d2.loss_cls: 0.0969, d2.loss_bbox: 0.2242, d3.loss_cls: 0.0846, d3.loss_bbox: 0.2181, d4.loss_cls: 0.0806, d4.loss_bbox: 0.2172, loss: 2.0690, grad_norm: 51.0274
2025-06-17 11:55:31,746 - mmdet - INFO - Epoch [5][350/7033]	lr: 5.015e-05, eta: 3:31:20, time: 0.927, data_time: 0.029, memory: 13888, loss_cls: 0.0735, loss_bbox: 0.2124, d0.loss_cls: 0.1630, d0.loss_bbox: 0.3310, d1.loss_cls: 0.1122, d1.loss_bbox: 0.2405, d2.loss_cls: 0.0921, d2.loss_bbox: 0.2228, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2172, d4.loss_cls: 0.0750, d4.loss_bbox: 0.2153, loss: 2.0360, grad_norm: 39.1960
2025-06-17 11:56:18,297 - mmdet - INFO - Epoch [5][400/7033]	lr: 5.015e-05, eta: 3:30:34, time: 0.931, data_time: 0.029, memory: 13888, loss_cls: 0.0656, loss_bbox: 0.2002, d0.loss_cls: 0.1551, d0.loss_bbox: 0.3253, d1.loss_cls: 0.0998, d1.loss_bbox: 0.2295, d2.loss_cls: 0.0845, d2.loss_bbox: 0.2102, d3.loss_cls: 0.0739, d3.loss_bbox: 0.2046, d4.loss_cls: 0.0680, d4.loss_bbox: 0.2040, loss: 1.9206, grad_norm: 38.7806
2025-06-17 11:57:04,936 - mmdet - INFO - Epoch [5][450/7033]	lr: 5.015e-05, eta: 3:29:48, time: 0.933, data_time: 0.030, memory: 13888, loss_cls: 0.0817, loss_bbox: 0.2156, d0.loss_cls: 0.1620, d0.loss_bbox: 0.3258, d1.loss_cls: 0.1178, d1.loss_bbox: 0.2420, d2.loss_cls: 0.1003, d2.loss_bbox: 0.2227, d3.loss_cls: 0.0881, d3.loss_bbox: 0.2188, d4.loss_cls: 0.0823, d4.loss_bbox: 0.2171, loss: 2.0741, grad_norm: 40.0496
2025-06-17 11:57:52,018 - mmdet - INFO - Epoch [5][500/7033]	lr: 5.015e-05, eta: 3:29:02, time: 0.942, data_time: 0.034, memory: 13888, loss_cls: 0.0844, loss_bbox: 0.2188, d0.loss_cls: 0.1701, d0.loss_bbox: 0.3420, d1.loss_cls: 0.1246, d1.loss_bbox: 0.2497, d2.loss_cls: 0.1030, d2.loss_bbox: 0.2298, d3.loss_cls: 0.0905, d3.loss_bbox: 0.2240, d4.loss_cls: 0.0848, d4.loss_bbox: 0.2222, loss: 2.1439, grad_norm: 63.9402
2025-06-17 11:58:39,310 - mmdet - INFO - Epoch [5][550/7033]	lr: 5.015e-05, eta: 3:28:16, time: 0.946, data_time: 0.034, memory: 13888, loss_cls: 0.0761, loss_bbox: 0.2088, d0.loss_cls: 0.1593, d0.loss_bbox: 0.3209, d1.loss_cls: 0.1138, d1.loss_bbox: 0.2372, d2.loss_cls: 0.0949, d2.loss_bbox: 0.2201, d3.loss_cls: 0.0825, d3.loss_bbox: 0.2152, d4.loss_cls: 0.0780, d4.loss_bbox: 0.2119, loss: 2.0186, grad_norm: 75.8655
2025-06-17 11:59:25,772 - mmdet - INFO - Epoch [5][600/7033]	lr: 5.015e-05, eta: 3:27:30, time: 0.929, data_time: 0.031, memory: 13888, loss_cls: 0.0771, loss_bbox: 0.2141, d0.loss_cls: 0.1591, d0.loss_bbox: 0.3340, d1.loss_cls: 0.1153, d1.loss_bbox: 0.2434, d2.loss_cls: 0.0972, d2.loss_bbox: 0.2236, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2203, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2181, loss: 2.0633, grad_norm: 34.9805
2025-06-17 12:00:12,415 - mmdet - INFO - Epoch [5][650/7033]	lr: 5.015e-05, eta: 3:26:44, time: 0.933, data_time: 0.032, memory: 13888, loss_cls: 0.0772, loss_bbox: 0.2180, d0.loss_cls: 0.1600, d0.loss_bbox: 0.3303, d1.loss_cls: 0.1115, d1.loss_bbox: 0.2443, d2.loss_cls: 0.0956, d2.loss_bbox: 0.2269, d3.loss_cls: 0.0834, d3.loss_bbox: 0.2219, d4.loss_cls: 0.0786, d4.loss_bbox: 0.2208, loss: 2.0685, grad_norm: 48.8204
2025-06-17 12:00:59,115 - mmdet - INFO - Epoch [5][700/7033]	lr: 5.015e-05, eta: 3:25:58, time: 0.934, data_time: 0.031, memory: 13888, loss_cls: 0.0697, loss_bbox: 0.2093, d0.loss_cls: 0.1548, d0.loss_bbox: 0.3338, d1.loss_cls: 0.1047, d1.loss_bbox: 0.2386, d2.loss_cls: 0.0860, d2.loss_bbox: 0.2187, d3.loss_cls: 0.0744, d3.loss_bbox: 0.2153, d4.loss_cls: 0.0720, d4.loss_bbox: 0.2108, loss: 1.9882, grad_norm: 33.8852
2025-06-17 12:01:45,614 - mmdet - INFO - Epoch [5][750/7033]	lr: 5.015e-05, eta: 3:25:12, time: 0.930, data_time: 0.030, memory: 13888, loss_cls: 0.0771, loss_bbox: 0.2013, d0.loss_cls: 0.1596, d0.loss_bbox: 0.3256, d1.loss_cls: 0.1106, d1.loss_bbox: 0.2347, d2.loss_cls: 0.0943, d2.loss_bbox: 0.2122, d3.loss_cls: 0.0835, d3.loss_bbox: 0.2070, d4.loss_cls: 0.0791, d4.loss_bbox: 0.2050, loss: 1.9901, grad_norm: 53.8788
2025-06-17 12:02:32,632 - mmdet - INFO - Epoch [5][800/7033]	lr: 5.015e-05, eta: 3:24:26, time: 0.940, data_time: 0.038, memory: 13888, loss_cls: 0.0746, loss_bbox: 0.2108, d0.loss_cls: 0.1537, d0.loss_bbox: 0.3327, d1.loss_cls: 0.1083, d1.loss_bbox: 0.2392, d2.loss_cls: 0.0885, d2.loss_bbox: 0.2212, d3.loss_cls: 0.0806, d3.loss_bbox: 0.2162, d4.loss_cls: 0.0764, d4.loss_bbox: 0.2129, loss: 2.0151, grad_norm: 82.7326
2025-06-17 12:03:18,849 - mmdet - INFO - Epoch [5][850/7033]	lr: 5.015e-05, eta: 3:23:40, time: 0.924, data_time: 0.028, memory: 13888, loss_cls: 0.0734, loss_bbox: 0.2160, d0.loss_cls: 0.1574, d0.loss_bbox: 0.3377, d1.loss_cls: 0.1097, d1.loss_bbox: 0.2468, d2.loss_cls: 0.0913, d2.loss_bbox: 0.2274, d3.loss_cls: 0.0812, d3.loss_bbox: 0.2221, d4.loss_cls: 0.0755, d4.loss_bbox: 0.2186, loss: 2.0570, grad_norm: 38.2373
2025-06-17 12:04:05,173 - mmdet - INFO - Epoch [5][900/7033]	lr: 5.015e-05, eta: 3:22:54, time: 0.926, data_time: 0.031, memory: 13888, loss_cls: 0.0721, loss_bbox: 0.2119, d0.loss_cls: 0.1582, d0.loss_bbox: 0.3247, d1.loss_cls: 0.1077, d1.loss_bbox: 0.2387, d2.loss_cls: 0.0898, d2.loss_bbox: 0.2200, d3.loss_cls: 0.0792, d3.loss_bbox: 0.2158, d4.loss_cls: 0.0758, d4.loss_bbox: 0.2136, loss: 2.0075, grad_norm: 49.9477
2025-06-17 12:04:51,315 - mmdet - INFO - Epoch [5][950/7033]	lr: 5.015e-05, eta: 3:22:08, time: 0.923, data_time: 0.032, memory: 13888, loss_cls: 0.0612, loss_bbox: 0.1997, d0.loss_cls: 0.1488, d0.loss_bbox: 0.3204, d1.loss_cls: 0.0948, d1.loss_bbox: 0.2276, d2.loss_cls: 0.0786, d2.loss_bbox: 0.2108, d3.loss_cls: 0.0657, d3.loss_bbox: 0.2059, d4.loss_cls: 0.0626, d4.loss_bbox: 0.2034, loss: 1.8796, grad_norm: 56.6789
2025-06-17 12:05:37,895 - mmdet - INFO - Epoch [5][1000/7033]	lr: 5.015e-05, eta: 3:21:22, time: 0.932, data_time: 0.041, memory: 13888, loss_cls: 0.0769, loss_bbox: 0.2134, d0.loss_cls: 0.1556, d0.loss_bbox: 0.3262, d1.loss_cls: 0.1109, d1.loss_bbox: 0.2397, d2.loss_cls: 0.0914, d2.loss_bbox: 0.2238, d3.loss_cls: 0.0817, d3.loss_bbox: 0.2186, d4.loss_cls: 0.0777, d4.loss_bbox: 0.2171, loss: 2.0331, grad_norm: 35.2400
