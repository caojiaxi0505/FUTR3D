2025-04-16 17:33:08,323 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.20 | packaged by conda-forge | (default, Sep 30 2024, 17:52:49) [GCC 13.3.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 4090 D
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
PyTorch: 1.13.0+cu116
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.14.0+cu116
OpenCV: 4.11.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.27.0
MMSegmentation: 0.30.0
MMDetection3D: 1.0.0rc6+9130d71
spconv2.0: True
------------------------------------------------------------

2025-04-16 17:33:09,019 - mmdet - INFO - 分布式训练: True
2025-04-16 17:33:09,636 - mmdet - INFO - 配置:
point_cloud_range = [-54, -54, -5.0, 54, 54, 3.0]
class_names = [
    'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
    'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
]
dataset_type = 'NuScenesDataset'
data_root = 'data/nuscenes/'
input_modality = dict(
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=False)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(type='LoadAnnotations3D', with_bbox_3d=True, with_label_3d=True),
    dict(
        type='PointsRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectRangeFilter',
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    dict(
        type='ObjectNameFilter',
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(type='PointShuffle'),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ]),
    dict(
        type='Collect3D',
        keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFiles', to_float32=True),
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='NormalizeMultiviewImage',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='GlobalRotScaleTrans',
                rot_range=[0, 0],
                scale_ratio_range=[1.0, 1.0],
                translation_std=[0, 0, 0]),
            dict(type='RandomFlip3D'),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ],
                with_label=False),
            dict(type='Collect3D', keys=['points', 'img'])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=9,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk'),
        pad_empty_sweeps=True,
        remove_close=True),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    train=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True),
            dict(
                type='PointsRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectRangeFilter',
                point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
            dict(
                type='ObjectNameFilter',
                classes=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(type='PointShuffle'),
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
                    'barrier', 'motorcycle', 'bicycle', 'pedestrian',
                    'traffic_cone'
                ]),
            dict(
                type='Collect3D',
                keys=['points', 'img', 'gt_bboxes_3d', 'gt_labels_3d'])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=False,
        box_type_3d='LiDAR',
        use_valid_flag=True),
    val=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'),
    test=dict(
        type='NuScenesDataset',
        data_root='data/nuscenes/',
        ann_file='data/nuscenes/nuscenes_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFiles', to_float32=True),
            dict(
                type='LoadPointsFromFile',
                coord_type='LIDAR',
                load_dim=5,
                use_dim=5,
                file_client_args=dict(backend='disk')),
            dict(
                type='LoadPointsFromMultiSweeps',
                sweeps_num=9,
                use_dim=[0, 1, 2, 3, 4],
                file_client_args=dict(backend='disk'),
                pad_empty_sweeps=True,
                remove_close=True),
            dict(
                type='NormalizeMultiviewImage',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='GlobalRotScaleTrans',
                        rot_range=[0, 0],
                        scale_ratio_range=[1.0, 1.0],
                        translation_std=[0, 0, 0]),
                    dict(type='RandomFlip3D'),
                    dict(
                        type='PointsRangeFilter',
                        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
                    dict(
                        type='DefaultFormatBundle3D',
                        class_names=[
                            'car', 'truck', 'construction_vehicle', 'bus',
                            'trailer', 'barrier', 'motorcycle', 'bicycle',
                            'pedestrian', 'traffic_cone'
                        ],
                        with_label=False),
                    dict(type='Collect3D', keys=['points', 'img'])
                ])
        ],
        classes=[
            'car', 'truck', 'construction_vehicle', 'bus', 'trailer',
            'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
        ],
        modality=dict(
            use_lidar=True,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=False),
        test_mode=True,
        box_type_3d='LiDAR'))
evaluation = dict(
    interval=1,
    pipeline=[
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            load_dim=5,
            use_dim=5,
            file_client_args=dict(backend='disk')),
        dict(
            type='LoadPointsFromMultiSweeps',
            sweeps_num=10,
            file_client_args=dict(backend='disk')),
        dict(
            type='DefaultFormatBundle3D',
            class_names=[
                'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
                'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone',
                'barrier'
            ],
            with_label=False),
        dict(type='Collect3D', keys=['points'])
    ])
checkpoint_config = dict(interval=1, max_keep_ckpts=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/lidar_0075v_cam_res101/default'
load_from = 'checkpoints/fuse.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
plugin = 'plugin/futr3d'
voxel_size = [0.075, 0.075, 0.2]
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
center_head = dict(
    type='CenterHead',
    in_channels=512,
    tasks=[
        dict(num_class=1, class_names=['car']),
        dict(num_class=2, class_names=['truck', 'construction_vehicle']),
        dict(num_class=2, class_names=['bus', 'trailer']),
        dict(num_class=1, class_names=['barrier']),
        dict(num_class=2, class_names=['motorcycle', 'bicycle']),
        dict(num_class=2, class_names=['pedestrian', 'traffic_cone'])
    ],
    common_heads=dict(
        reg=(2, 2), height=(1, 2), dim=(3, 2), rot=(2, 2), vel=(2, 2)),
    share_conv_channel=64,
    bbox_coder=dict(
        type='CenterPointBBoxCoder',
        pc_range=[-54, -54],
        post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
        max_num=500,
        score_threshold=0.1,
        out_size_factor=8,
        voxel_size=[0.075, 0.075],
        code_size=9),
    separate_head=dict(type='SeparateHead', init_bias=-2.19, final_kernel=3),
    loss_cls=dict(type='GaussianFocalLoss', reduction='mean'),
    loss_bbox=dict(type='L1Loss', reduction='mean', loss_weight=0.25),
    norm_bbox=True)
model = dict(
    type='FUTR3D',
    use_lidar=True,
    use_camera=True,
    use_radar=False,
    use_grid_mask=True,
    freeze_backbone=True,
    img_backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN2d', requires_grad=False),
        norm_eval=True,
        style='caffe',
        dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
        stage_with_dcn=(False, False, True, True)),
    img_neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_output',
        num_outs=4,
        relu_before_extra_convs=True),
    pts_voxel_layer=dict(
        max_num_points=10,
        voxel_size=[0.075, 0.075, 0.2],
        max_voxels=(120000, 160000),
        point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0]),
    pts_voxel_encoder=dict(type='HardSimpleVFE', num_features=5),
    pts_middle_encoder=dict(
        type='SparseEncoder',
        in_channels=5,
        sparse_shape=[41, 1440, 1440],
        output_channels=128,
        order=('conv', 'norm', 'act'),
        encoder_channels=((16, 16, 32), (32, 32, 64), (64, 64, 128), (128,
                                                                      128)),
        encoder_paddings=((0, 0, 1), (0, 0, 1), (0, 0, [0, 1, 1]), (0, 0)),
        block_type='basicblock'),
    pts_backbone=dict(
        type='SECOND',
        in_channels=256,
        out_channels=[128, 256],
        layer_nums=[5, 5],
        layer_strides=[1, 2],
        norm_cfg=dict(type='BN', eps=0.001, momentum=0.01),
        conv_cfg=dict(type='Conv2d', bias=False)),
    pts_neck=dict(
        type='FPN',
        norm_cfg=dict(type='BN2d', eps=0.001, momentum=0.01),
        act_cfg=dict(type='ReLU', inplace=False),
        in_channels=[128, 256],
        out_channels=256,
        start_level=0,
        add_extra_convs=True,
        num_outs=4,
        relu_before_extra_convs=True),
    pts_bbox_head=dict(
        type='FUTR3DHead',
        use_dab=True,
        anchor_size=3,
        num_query=900,
        num_classes=10,
        in_channels=256,
        pc_range=[-54, -54, -5.0, 54, 54, 3.0],
        sync_cls_avg_factor=True,
        with_box_refine=True,
        as_two_stage=False,
        code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
        transformer=dict(
            type='FUTR3DTransformer',
            use_dab=True,
            decoder=dict(
                type='FUTR3DTransformerDecoder',
                num_layers=6,
                use_dab=True,
                anchor_size=3,
                return_intermediate=True,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1),
                        dict(
                            type='FUTR3DAttention',
                            use_lidar=True,
                            use_camera=True,
                            use_radar=False,
                            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
                            embed_dims=256)
                    ],
                    feedforward_channels=1024,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding',
            num_feats=128,
            normalize=True,
            offset=-0.5),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0)),
    train_cfg=dict(
        pts=dict(
            point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0],
            pc_range=[-54, -54, -5.0, 54, 54, 3.0],
            grid_size=[1440, 1440, 40],
            voxel_size=[0.075, 0.075, 0.2],
            out_size_factor=8,
            dense_reg=1,
            gaussian_overlap=0.1,
            max_objs=500,
            min_radius=2,
            code_weights=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2],
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0)))),
    test_cfg=dict(
        pts=dict(
            pc_range=[-54, -54],
            post_center_limit_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            max_per_img=500,
            max_pool_nms=False,
            min_radius=[4, 12, 10, 1, 0.85, 0.175],
            out_size_factor=8,
            voxel_size=[0.075, 0.075],
            nms_type='circle',
            pre_max_size=1000,
            post_max_size=83,
            nms_thr=0.2,
            max_num=300,
            score_threshold=0,
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0])))
db_sampler = dict(
    data_root='data/nuscenes/',
    info_path='data/nuscenes/nuscenes_dbinfos_train.pkl',
    rate=1.0,
    prepare=dict(
        filter_by_difficulty=[-1],
        filter_by_min_points=dict(
            car=5,
            truck=5,
            bus=5,
            trailer=5,
            construction_vehicle=5,
            traffic_cone=5,
            barrier=5,
            motorcycle=5,
            bicycle=5,
            pedestrian=5)),
    classes=[
        'car', 'truck', 'construction_vehicle', 'bus', 'trailer', 'barrier',
        'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone'
    ],
    sample_groups=dict(
        car=2,
        truck=3,
        construction_vehicle=7,
        bus=4,
        trailer=6,
        barrier=2,
        motorcycle=6,
        bicycle=6,
        pedestrian=2,
        traffic_cone=2),
    points_loader=dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=[0, 1, 2, 3, 4],
        file_client_args=dict(backend='disk')))
find_unused_parameters = True
runner = dict(type='EpochBasedRunner', max_epochs=6)
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(
            img_backbone=dict(lr_mult=0.1),
            img_neck=dict(lr_mult=0.1),
            pts_middle_encoder=dict(lr_mult=0.1),
            pts_backbone=dict(lr_mult=0.1),
            pts_neck=dict(lr_mult=0.1))),
    weight_decay=0.01)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
gpu_ids = range(0, 2)

2025-04-16 17:33:09,636 - mmdet - INFO - 设置随机种子为 0, deterministic: False
2025-04-16 17:33:09,939 - mmdet - INFO - initialize SECOND with init_cfg {'type': 'Kaiming', 'layer': 'Conv2d'}
2025-04-16 17:33:09,950 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2025-04-16 17:33:10,001 - mmdet - INFO - initialize ResNet with init_cfg [{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2025-04-16 17:33:10,073 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,073 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,107 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,140 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,155 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,207 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,238 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,249 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,291 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,365 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,398 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,407 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,416 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,440 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,477 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,542 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,565 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,583 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,597 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,619 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,640 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,695 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,711 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,730 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,748 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,771 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,798 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,885 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,892 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,906 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,914 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,922 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,929 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2025-04-16 17:33:10,973 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
Name of parameter - Initialization information

pts_middle_encoder.conv_input.0.weight - torch.Size([16, 3, 3, 3, 5]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_input.1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_input.1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.0.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.conv1.weight - torch.Size([16, 3, 3, 3, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn1.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn1.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.conv2.weight - torch.Size([16, 3, 3, 3, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn2.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.1.bn2.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.0.weight - torch.Size([32, 3, 3, 3, 16]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer1.2.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.0.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.conv1.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.conv2.weight - torch.Size([32, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn2.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.1.bn2.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.0.weight - torch.Size([64, 3, 3, 3, 32]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer2.2.1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.conv1.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.conv2.weight - torch.Size([64, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.0.weight - torch.Size([128, 3, 3, 3, 64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer3.2.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.conv1.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.conv2.weight - torch.Size([128, 3, 3, 3, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.encoder_layers.encoder_layer4.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.0.weight - torch.Size([128, 3, 1, 1, 128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_middle_encoder.conv_out.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.0.weight - torch.Size([128, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.3.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.4.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.4.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.6.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.7.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.7.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.9.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.10.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.10.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.12.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.13.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.13.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.15.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.0.16.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.0.16.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.0.weight - torch.Size([256, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.3.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.6.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.7.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.7.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.9.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.10.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.10.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.12.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.13.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.13.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.15.weight - torch.Size([256, 256, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

pts_backbone.blocks.1.16.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_backbone.blocks.1.16.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.conv.weight - torch.Size([256, 128, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.1.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.lateral_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.lateral_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.0.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.0.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.1.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.1.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.2.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.2.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

pts_neck.fpn_convs.3.bn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_neck.fpn_convs.3.bn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.level_embeds - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.0.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.1.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.2.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.3.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.4.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.0.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.weight - torch.Size([128, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight - torch.Size([24, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias - torch.Size([24]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.weight - torch.Size([256, 3]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.position_encoder.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.layers.5.ffns.0.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.0.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.query_scale.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.query_scale.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.weight - torch.Size([256, 384]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.transformer.decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.0.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.1.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.1.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.2.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.2.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.3.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.3.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.4.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.cls_branches.5.weight - torch.Size([10, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.cls_branches.5.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.0.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.0.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.1.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.1.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.2.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.2.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.3.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.3.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.4.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.4.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.reg_branches.5.4.weight - torch.Size([10, 256]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.reg_branches.5.4.bias - torch.Size([10]): 
Initialized by user-defined `init_weights` in FUTR3DHead  

pts_bbox_head.tgt_embed.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

pts_bbox_head.refpoint_embed.weight - torch.Size([900, 3]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.0.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.1.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.1.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn1.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn1.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn2.weight - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.bn2.bias - torch.Size([64]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer1.2.bn3.weight - torch.Size([256]): 
ConstantInit: val=0, bias=0 

img_backbone.layer1.2.bn3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.0.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.1.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.1.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.2.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.2.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn1.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn1.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn2.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.bn2.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer2.3.bn3.weight - torch.Size([512]): 
ConstantInit: val=0, bias=0 

img_backbone.layer2.3.bn3.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.0.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.1.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.1.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.2.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.2.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.3.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.3.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.3.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.4.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.4.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.4.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.5.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.5.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.5.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.6.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.6.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.6.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.6.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.7.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.7.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.7.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.7.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.8.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.8.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.8.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.8.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.9.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.9.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.9.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.9.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.10.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.10.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.10.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.10.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.11.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.11.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.11.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.11.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.12.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.12.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.12.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.12.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.13.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.13.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.13.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.13.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.14.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.14.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.14.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.14.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.15.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.15.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.15.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.15.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.16.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.16.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.16.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.16.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.17.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.17.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.17.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.17.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.18.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.18.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.18.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.18.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.19.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.19.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.19.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.19.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.20.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.20.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.20.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.20.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.21.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.21.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.21.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.21.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv1.weight - torch.Size([256, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv2.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.weight - torch.Size([27, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer3.22.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.bn2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer3.22.conv3.weight - torch.Size([1024, 256, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer3.22.bn3.weight - torch.Size([1024]): 
ConstantInit: val=0, bias=0 

img_backbone.layer3.22.bn3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.0.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.0.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.1.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.1.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.1.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.weight - torch.Size([27, 512, 3, 3]): 
Initialized by user-defined `init_weights` in ModulatedDeformConv2dPack  

img_backbone.layer4.2.conv2.conv_offset.bias - torch.Size([27]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.bn2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 

img_backbone.layer4.2.bn3.weight - torch.Size([2048]): 
ConstantInit: val=0, bias=0 

img_backbone.layer4.2.bn3.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  

img_neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

img_neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of FUTR3D  
2025-04-16 17:33:11,025 - mmdet - INFO - Model:
FUTR3D(
  (grid_mask): GridMask()
  (pts_voxel_layer): Voxelization(voxel_size=[0.075, 0.075, 0.2], point_cloud_range=[-54, -54, -5.0, 54, 54, 3.0], max_num_points=10, max_voxels=(120000, 160000), deterministic=True)
  (pts_voxel_encoder): HardSimpleVFE()
  (pts_middle_encoder): SparseEncoder(
    (conv_input): SparseSequential(
      (0): SubMConv3d(5, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (encoder_layers): SparseSequential(
      (encoder_layer1): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(16, 16, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d(16, 32, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer2): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(32, 32, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d(32, 64, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer3): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): SparseSequential(
          (0): SparseConv3d(64, 128, kernel_size=[3, 3, 3], stride=[2, 2, 2], padding=[0, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
        )
      )
      (encoder_layer4): SparseSequential(
        (0): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (1): SparseBasicBlock(
          (conv1): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (conv2): SubMConv3d(128, 128, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
          (bn2): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (conv_out): SparseSequential(
      (0): SparseConv3d(128, 128, kernel_size=[3, 1, 1], stride=[2, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.MaskImplicitGemm)
      (1): BatchNorm1d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (pts_backbone): SECOND(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (10): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (11): ReLU(inplace=True)
        (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (13): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (14): ReLU(inplace=True)
        (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (16): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (17): ReLU(inplace=True)
      )
      (1): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (8): ReLU(inplace=True)
        (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (10): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (11): ReLU(inplace=True)
        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (13): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (14): ReLU(inplace=True)
        (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (16): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (17): ReLU(inplace=True)
      )
    )
  )
  init_cfg={'type': 'Kaiming', 'layer': 'Conv2d'}
  (pts_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (activate): ReLU()
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
  (pts_bbox_head): FUTR3DHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (loss_iou): GIoULoss()
    (activate): ReLU(inplace=True)
    (positional_encoding): SinePositionalEncoding(num_feats=128, temperature=10000, normalize=True, scale=6.283185307179586, eps=1e-06)
    (transformer): FUTR3DTransformer(
      (decoder): FUTR3DTransformerDecoder(
        (layers): ModuleList(
          (0): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (4): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
          (5): DetrTransformerDecoderLayer(
            (attentions): ModuleList(
              (0): MultiheadAttention(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.0, inplace=False)
                (dropout_layer): Dropout(p=0.1, inplace=False)
              )
              (1): FUTR3DAttention(
                (dropout): Dropout(p=0.1, inplace=False)
                (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
                (attention_weights): Linear(in_features=256, out_features=128, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
                (img_attention_weights): Linear(in_features=256, out_features=24, bias=True)
                (img_output_proj): Linear(in_features=256, out_features=256, bias=True)
                (position_encoder): Sequential(
                  (0): Linear(in_features=3, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU(inplace=True)
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (5): ReLU(inplace=True)
                )
                (weight_dropout): Dropout(p=0.0, inplace=False)
                (modality_fusion_layer): Sequential(
                  (0): Linear(in_features=512, out_features=256, bias=True)
                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (2): ReLU()
                  (3): Linear(in_features=256, out_features=256, bias=True)
                  (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
            (ffns): ModuleList(
              (0): FFN(
                (activate): ReLU(inplace=True)
                (layers): Sequential(
                  (0): Sequential(
                    (0): Linear(in_features=256, out_features=1024, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.1, inplace=False)
                  )
                  (1): Linear(in_features=1024, out_features=256, bias=True)
                  (2): Dropout(p=0.1, inplace=False)
                )
                (dropout_layer): Identity()
              )
            )
            (norms): ModuleList(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (query_scale): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
      )
    )
    (cls_branches): ModuleList(
      (0): Linear(in_features=256, out_features=10, bias=True)
      (1): Linear(in_features=256, out_features=10, bias=True)
      (2): Linear(in_features=256, out_features=10, bias=True)
      (3): Linear(in_features=256, out_features=10, bias=True)
      (4): Linear(in_features=256, out_features=10, bias=True)
      (5): Linear(in_features=256, out_features=10, bias=True)
    )
    (reg_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (4): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
      (5): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (tgt_embed): Embedding(900, 256)
    (refpoint_embed): Embedding(900, 3)
  )
  (img_backbone): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer2): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer3): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
    (layer4): ResLayer(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): ModulatedDeformConv2dPack(
          (conv_offset): Conv2d(512, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      init_cfg={'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
    )
  )
  init_cfg=[{'type': 'Kaiming', 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (img_neck): FPN(
    (lateral_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (fpn_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
2025-04-16 17:33:23,454 - mmdet - INFO - load checkpoint from local path: checkpoints/fuse.pth
2025-04-16 17:33:26,267 - mmdet - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: pts_bbox_head.code_weights, pts_bbox_head.query_embedding.weight, pts_bbox_head.aux_head.shared_conv.conv.weight, pts_bbox_head.aux_head.shared_conv.bn.weight, pts_bbox_head.aux_head.shared_conv.bn.bias, pts_bbox_head.aux_head.shared_conv.bn.running_mean, pts_bbox_head.aux_head.shared_conv.bn.running_var, pts_bbox_head.aux_head.shared_conv.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.reg.1.weight, pts_bbox_head.aux_head.task_heads.0.reg.1.bias, pts_bbox_head.aux_head.task_heads.0.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.height.1.weight, pts_bbox_head.aux_head.task_heads.0.height.1.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.dim.1.weight, pts_bbox_head.aux_head.task_heads.0.dim.1.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.rot.1.weight, pts_bbox_head.aux_head.task_heads.0.rot.1.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.vel.1.weight, pts_bbox_head.aux_head.task_heads.0.vel.1.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.0.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.0.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.0.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.reg.1.weight, pts_bbox_head.aux_head.task_heads.1.reg.1.bias, pts_bbox_head.aux_head.task_heads.1.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.height.1.weight, pts_bbox_head.aux_head.task_heads.1.height.1.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.dim.1.weight, pts_bbox_head.aux_head.task_heads.1.dim.1.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.rot.1.weight, pts_bbox_head.aux_head.task_heads.1.rot.1.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.vel.1.weight, pts_bbox_head.aux_head.task_heads.1.vel.1.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.1.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.1.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.1.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.reg.1.weight, pts_bbox_head.aux_head.task_heads.2.reg.1.bias, pts_bbox_head.aux_head.task_heads.2.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.height.1.weight, pts_bbox_head.aux_head.task_heads.2.height.1.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.dim.1.weight, pts_bbox_head.aux_head.task_heads.2.dim.1.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.rot.1.weight, pts_bbox_head.aux_head.task_heads.2.rot.1.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.vel.1.weight, pts_bbox_head.aux_head.task_heads.2.vel.1.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.2.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.2.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.2.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.reg.1.weight, pts_bbox_head.aux_head.task_heads.3.reg.1.bias, pts_bbox_head.aux_head.task_heads.3.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.height.1.weight, pts_bbox_head.aux_head.task_heads.3.height.1.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.dim.1.weight, pts_bbox_head.aux_head.task_heads.3.dim.1.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.rot.1.weight, pts_bbox_head.aux_head.task_heads.3.rot.1.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.vel.1.weight, pts_bbox_head.aux_head.task_heads.3.vel.1.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.3.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.3.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.3.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.reg.1.weight, pts_bbox_head.aux_head.task_heads.4.reg.1.bias, pts_bbox_head.aux_head.task_heads.4.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.height.1.weight, pts_bbox_head.aux_head.task_heads.4.height.1.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.dim.1.weight, pts_bbox_head.aux_head.task_heads.4.dim.1.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.rot.1.weight, pts_bbox_head.aux_head.task_heads.4.rot.1.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.vel.1.weight, pts_bbox_head.aux_head.task_heads.4.vel.1.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.4.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.4.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.4.heatmap.1.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.reg.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.reg.1.weight, pts_bbox_head.aux_head.task_heads.5.reg.1.bias, pts_bbox_head.aux_head.task_heads.5.height.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.height.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.height.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.height.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.height.1.weight, pts_bbox_head.aux_head.task_heads.5.height.1.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.dim.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.dim.1.weight, pts_bbox_head.aux_head.task_heads.5.dim.1.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.rot.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.rot.1.weight, pts_bbox_head.aux_head.task_heads.5.rot.1.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.vel.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.vel.1.weight, pts_bbox_head.aux_head.task_heads.5.vel.1.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.conv.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.bias, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_mean, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.running_var, pts_bbox_head.aux_head.task_heads.5.heatmap.0.bn.num_batches_tracked, pts_bbox_head.aux_head.task_heads.5.heatmap.1.weight, pts_bbox_head.aux_head.task_heads.5.heatmap.1.bias, pts_bbox_head.transformer.reference_points.weight, pts_bbox_head.transformer.reference_points.bias, pts_bbox_head.cls_branches.0.0.weight, pts_bbox_head.cls_branches.0.0.bias, pts_bbox_head.cls_branches.0.1.weight, pts_bbox_head.cls_branches.0.1.bias, pts_bbox_head.cls_branches.0.3.weight, pts_bbox_head.cls_branches.0.3.bias, pts_bbox_head.cls_branches.0.4.weight, pts_bbox_head.cls_branches.0.4.bias, pts_bbox_head.cls_branches.0.6.weight, pts_bbox_head.cls_branches.0.6.bias, pts_bbox_head.cls_branches.1.0.weight, pts_bbox_head.cls_branches.1.0.bias, pts_bbox_head.cls_branches.1.1.weight, pts_bbox_head.cls_branches.1.1.bias, pts_bbox_head.cls_branches.1.3.weight, pts_bbox_head.cls_branches.1.3.bias, pts_bbox_head.cls_branches.1.4.weight, pts_bbox_head.cls_branches.1.4.bias, pts_bbox_head.cls_branches.1.6.weight, pts_bbox_head.cls_branches.1.6.bias, pts_bbox_head.cls_branches.2.0.weight, pts_bbox_head.cls_branches.2.0.bias, pts_bbox_head.cls_branches.2.1.weight, pts_bbox_head.cls_branches.2.1.bias, pts_bbox_head.cls_branches.2.3.weight, pts_bbox_head.cls_branches.2.3.bias, pts_bbox_head.cls_branches.2.4.weight, pts_bbox_head.cls_branches.2.4.bias, pts_bbox_head.cls_branches.2.6.weight, pts_bbox_head.cls_branches.2.6.bias, pts_bbox_head.cls_branches.3.0.weight, pts_bbox_head.cls_branches.3.0.bias, pts_bbox_head.cls_branches.3.1.weight, pts_bbox_head.cls_branches.3.1.bias, pts_bbox_head.cls_branches.3.3.weight, pts_bbox_head.cls_branches.3.3.bias, pts_bbox_head.cls_branches.3.4.weight, pts_bbox_head.cls_branches.3.4.bias, pts_bbox_head.cls_branches.3.6.weight, pts_bbox_head.cls_branches.3.6.bias, pts_bbox_head.cls_branches.4.0.weight, pts_bbox_head.cls_branches.4.0.bias, pts_bbox_head.cls_branches.4.1.weight, pts_bbox_head.cls_branches.4.1.bias, pts_bbox_head.cls_branches.4.3.weight, pts_bbox_head.cls_branches.4.3.bias, pts_bbox_head.cls_branches.4.4.weight, pts_bbox_head.cls_branches.4.4.bias, pts_bbox_head.cls_branches.4.6.weight, pts_bbox_head.cls_branches.4.6.bias, pts_bbox_head.cls_branches.5.0.weight, pts_bbox_head.cls_branches.5.0.bias, pts_bbox_head.cls_branches.5.1.weight, pts_bbox_head.cls_branches.5.1.bias, pts_bbox_head.cls_branches.5.3.weight, pts_bbox_head.cls_branches.5.3.bias, pts_bbox_head.cls_branches.5.4.weight, pts_bbox_head.cls_branches.5.4.bias, pts_bbox_head.cls_branches.5.6.weight, pts_bbox_head.cls_branches.5.6.bias

missing keys in source state_dict: pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.0.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.1.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.2.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.3.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.4.attentions.1.modality_fusion_layer.4.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_attention_weights.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.img_output_proj.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.0.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.1.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.3.bias, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.weight, pts_bbox_head.transformer.decoder.layers.5.attentions.1.modality_fusion_layer.4.bias

2025-04-16 17:33:26,272 - mmdet - INFO - Start running, host: ubuntu@ubuntu, work_dir: /home/ubuntu/jxcao/hdd/jxc/FUTR3D/work_dirs/lidar_0075v_cam_res101/default
2025-04-16 17:33:26,294 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2025-04-16 17:33:26,346 - mmdet - INFO - workflow: [('train', 1)], max: 6 epochs
2025-04-16 17:33:26,372 - mmdet - INFO - Checkpoints will be saved to /home/ubuntu/jxcao/hdd/jxc/FUTR3D/work_dirs/lidar_0075v_cam_res101/default by HardDiskBackend.
2025-04-16 17:34:23,940 - mmdet - INFO - Epoch [1][50/7033]	lr: 3.987e-05, eta: 13:25:40, time: 1.147, data_time: 0.108, memory: 4350, loss_cls: 1.2281, loss_bbox: 1.6048, d0.loss_cls: 1.2861, d0.loss_bbox: 1.7250, d1.loss_cls: 1.2458, d1.loss_bbox: 1.6974, d2.loss_cls: 1.1683, d2.loss_bbox: 1.6006, d3.loss_cls: 1.1791, d3.loss_bbox: 1.6171, d4.loss_cls: 1.1492, d4.loss_bbox: 1.5599, loss: 17.0617, grad_norm: 16.7517
2025-04-16 17:35:13,883 - mmdet - INFO - Epoch [1][100/7033]	lr: 4.653e-05, eta: 12:32:45, time: 0.999, data_time: 0.023, memory: 4350, loss_cls: 0.9176, loss_bbox: 1.3058, d0.loss_cls: 1.0097, d0.loss_bbox: 1.4379, d1.loss_cls: 1.0088, d1.loss_bbox: 1.3837, d2.loss_cls: 0.9905, d2.loss_bbox: 1.3417, d3.loss_cls: 0.9419, d3.loss_bbox: 1.3344, d4.loss_cls: 0.8966, d4.loss_bbox: 1.2980, loss: 13.8665, grad_norm: 11.5862
2025-04-16 17:36:06,143 - mmdet - INFO - Epoch [1][150/7033]	lr: 5.320e-05, eta: 12:25:24, time: 1.045, data_time: 0.022, memory: 4350, loss_cls: 0.8200, loss_bbox: 1.2277, d0.loss_cls: 0.9223, d0.loss_bbox: 1.3464, d1.loss_cls: 0.8869, d1.loss_bbox: 1.2696, d2.loss_cls: 0.8888, d2.loss_bbox: 1.2572, d3.loss_cls: 0.8549, d3.loss_bbox: 1.2385, d4.loss_cls: 0.8220, d4.loss_bbox: 1.2339, loss: 12.7683, grad_norm: 10.3063
2025-04-16 17:36:57,660 - mmdet - INFO - Epoch [1][200/7033]	lr: 5.987e-05, eta: 12:18:41, time: 1.030, data_time: 0.025, memory: 4350, loss_cls: 0.7629, loss_bbox: 1.1604, d0.loss_cls: 0.8707, d0.loss_bbox: 1.2689, d1.loss_cls: 0.7991, d1.loss_bbox: 1.2004, d2.loss_cls: 0.7990, d2.loss_bbox: 1.1845, d3.loss_cls: 0.7976, d3.loss_bbox: 1.1714, d4.loss_cls: 0.7719, d4.loss_bbox: 1.1773, loss: 11.9640, grad_norm: 8.9948
2025-04-16 17:37:48,802 - mmdet - INFO - Epoch [1][250/7033]	lr: 6.653e-05, eta: 12:13:15, time: 1.023, data_time: 0.026, memory: 4350, loss_cls: 0.7366, loss_bbox: 1.0967, d0.loss_cls: 0.8332, d0.loss_bbox: 1.2057, d1.loss_cls: 0.7590, d1.loss_bbox: 1.1329, d2.loss_cls: 0.7568, d2.loss_bbox: 1.1185, d3.loss_cls: 0.7558, d3.loss_bbox: 1.1067, d4.loss_cls: 0.7523, d4.loss_bbox: 1.1099, loss: 11.3642, grad_norm: 8.8638
2025-04-16 17:38:39,864 - mmdet - INFO - Epoch [1][300/7033]	lr: 7.320e-05, eta: 12:09:10, time: 1.021, data_time: 0.029, memory: 4350, loss_cls: 0.7058, loss_bbox: 1.0371, d0.loss_cls: 0.7992, d0.loss_bbox: 1.1448, d1.loss_cls: 0.7082, d1.loss_bbox: 1.0936, d2.loss_cls: 0.7101, d2.loss_bbox: 1.0646, d3.loss_cls: 0.7183, d3.loss_bbox: 1.0559, d4.loss_cls: 0.7135, d4.loss_bbox: 1.0546, loss: 10.8057, grad_norm: 8.4209
2025-04-16 17:39:30,784 - mmdet - INFO - Epoch [1][350/7033]	lr: 7.987e-05, eta: 12:05:44, time: 1.018, data_time: 0.024, memory: 4350, loss_cls: 0.6887, loss_bbox: 1.0277, d0.loss_cls: 0.7665, d0.loss_bbox: 1.1343, d1.loss_cls: 0.6729, d1.loss_bbox: 1.0885, d2.loss_cls: 0.6773, d2.loss_bbox: 1.0586, d3.loss_cls: 0.6885, d3.loss_bbox: 1.0488, d4.loss_cls: 0.6905, d4.loss_bbox: 1.0480, loss: 10.5903, grad_norm: 8.3127
2025-04-16 17:40:21,417 - mmdet - INFO - Epoch [1][400/7033]	lr: 8.653e-05, eta: 12:02:26, time: 1.013, data_time: 0.024, memory: 4350, loss_cls: 0.6440, loss_bbox: 0.9951, d0.loss_cls: 0.7010, d0.loss_bbox: 1.1035, d1.loss_cls: 0.6152, d1.loss_bbox: 1.0590, d2.loss_cls: 0.6213, d2.loss_bbox: 1.0254, d3.loss_cls: 0.6285, d3.loss_bbox: 1.0211, d4.loss_cls: 0.6411, d4.loss_bbox: 1.0211, loss: 10.0762, grad_norm: 8.4972
2025-04-16 17:41:02,472 - mmdet - INFO - Epoch [1][450/7033]	lr: 9.320e-05, eta: 11:44:53, time: 0.821, data_time: 0.025, memory: 4350, loss_cls: 0.6234, loss_bbox: 0.9748, d0.loss_cls: 0.5969, d0.loss_bbox: 1.0795, d1.loss_cls: 0.5772, d1.loss_bbox: 1.0391, d2.loss_cls: 0.5785, d2.loss_bbox: 1.0114, d3.loss_cls: 0.5959, d3.loss_bbox: 1.0018, d4.loss_cls: 0.6087, d4.loss_bbox: 1.0030, loss: 9.6902, grad_norm: 8.0837
2025-04-16 17:41:41,759 - mmdet - INFO - Epoch [1][500/7033]	lr: 9.987e-05, eta: 11:28:14, time: 0.786, data_time: 0.025, memory: 4350, loss_cls: 0.5512, loss_bbox: 0.9406, d0.loss_cls: 0.4794, d0.loss_bbox: 1.0658, d1.loss_cls: 0.4624, d1.loss_bbox: 1.0190, d2.loss_cls: 0.4600, d2.loss_bbox: 0.9919, d3.loss_cls: 0.4810, d3.loss_bbox: 0.9895, d4.loss_cls: 0.5121, d4.loss_bbox: 0.9794, loss: 8.9324, grad_norm: 9.5989
2025-04-16 17:42:21,033 - mmdet - INFO - Epoch [1][550/7033]	lr: 1.000e-04, eta: 11:14:29, time: 0.785, data_time: 0.023, memory: 4350, loss_cls: 0.4311, loss_bbox: 0.9111, d0.loss_cls: 0.4108, d0.loss_bbox: 1.0169, d1.loss_cls: 0.3765, d1.loss_bbox: 0.9835, d2.loss_cls: 0.3809, d2.loss_bbox: 0.9553, d3.loss_cls: 0.3851, d3.loss_bbox: 0.9612, d4.loss_cls: 0.4025, d4.loss_bbox: 0.9476, loss: 8.1627, grad_norm: 10.4433
2025-04-16 17:43:00,217 - mmdet - INFO - Epoch [1][600/7033]	lr: 1.000e-04, eta: 11:02:49, time: 0.784, data_time: 0.022, memory: 4350, loss_cls: 0.3687, loss_bbox: 0.8952, d0.loss_cls: 0.3802, d0.loss_bbox: 0.9839, d1.loss_cls: 0.3419, d1.loss_bbox: 0.9662, d2.loss_cls: 0.3387, d2.loss_bbox: 0.9277, d3.loss_cls: 0.3387, d3.loss_bbox: 0.9314, d4.loss_cls: 0.3485, d4.loss_bbox: 0.9192, loss: 7.7402, grad_norm: 11.3543
2025-04-16 17:43:39,280 - mmdet - INFO - Epoch [1][650/7033]	lr: 1.000e-04, eta: 10:52:42, time: 0.781, data_time: 0.023, memory: 4350, loss_cls: 0.3319, loss_bbox: 0.8461, d0.loss_cls: 0.3545, d0.loss_bbox: 0.9378, d1.loss_cls: 0.3128, d1.loss_bbox: 0.9178, d2.loss_cls: 0.3124, d2.loss_bbox: 0.8773, d3.loss_cls: 0.3172, d3.loss_bbox: 0.8757, d4.loss_cls: 0.3165, d4.loss_bbox: 0.8691, loss: 7.2692, grad_norm: 11.9579
2025-04-16 17:44:18,577 - mmdet - INFO - Epoch [1][700/7033]	lr: 1.000e-04, eta: 10:44:11, time: 0.786, data_time: 0.024, memory: 4350, loss_cls: 0.3163, loss_bbox: 0.7839, d0.loss_cls: 0.3347, d0.loss_bbox: 0.8769, d1.loss_cls: 0.3019, d1.loss_bbox: 0.8566, d2.loss_cls: 0.2991, d2.loss_bbox: 0.8096, d3.loss_cls: 0.3032, d3.loss_bbox: 0.8059, d4.loss_cls: 0.3099, d4.loss_bbox: 0.7995, loss: 6.7975, grad_norm: 12.0077
2025-04-16 17:44:57,779 - mmdet - INFO - Epoch [1][750/7033]	lr: 1.000e-04, eta: 10:36:37, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.3033, loss_bbox: 0.7540, d0.loss_cls: 0.3347, d0.loss_bbox: 0.8419, d1.loss_cls: 0.2943, d1.loss_bbox: 0.8174, d2.loss_cls: 0.2903, d2.loss_bbox: 0.7755, d3.loss_cls: 0.2956, d3.loss_bbox: 0.7701, d4.loss_cls: 0.2959, d4.loss_bbox: 0.7678, loss: 6.5409, grad_norm: 12.2943
2025-04-16 17:45:36,989 - mmdet - INFO - Epoch [1][800/7033]	lr: 1.000e-04, eta: 10:29:55, time: 0.784, data_time: 0.022, memory: 4350, loss_cls: 0.3009, loss_bbox: 0.6888, d0.loss_cls: 0.3350, d0.loss_bbox: 0.7787, d1.loss_cls: 0.2914, d1.loss_bbox: 0.7446, d2.loss_cls: 0.2893, d2.loss_bbox: 0.6985, d3.loss_cls: 0.2932, d3.loss_bbox: 0.6955, d4.loss_cls: 0.2928, d4.loss_bbox: 0.7040, loss: 6.1127, grad_norm: 12.9126
2025-04-16 17:46:16,147 - mmdet - INFO - Epoch [1][850/7033]	lr: 1.000e-04, eta: 10:23:54, time: 0.783, data_time: 0.022, memory: 4350, loss_cls: 0.2817, loss_bbox: 0.6850, d0.loss_cls: 0.3213, d0.loss_bbox: 0.7686, d1.loss_cls: 0.2732, d1.loss_bbox: 0.7320, d2.loss_cls: 0.2677, d2.loss_bbox: 0.6936, d3.loss_cls: 0.2707, d3.loss_bbox: 0.6842, d4.loss_cls: 0.2711, d4.loss_bbox: 0.6967, loss: 5.9457, grad_norm: 14.7584
2025-04-16 17:46:55,305 - mmdet - INFO - Epoch [1][900/7033]	lr: 1.000e-04, eta: 10:18:28, time: 0.783, data_time: 0.023, memory: 4350, loss_cls: 0.2789, loss_bbox: 0.6590, d0.loss_cls: 0.3104, d0.loss_bbox: 0.7408, d1.loss_cls: 0.2673, d1.loss_bbox: 0.7112, d2.loss_cls: 0.2693, d2.loss_bbox: 0.6693, d3.loss_cls: 0.2697, d3.loss_bbox: 0.6618, d4.loss_cls: 0.2709, d4.loss_bbox: 0.6657, loss: 5.7742, grad_norm: 13.8121
2025-04-16 17:47:34,513 - mmdet - INFO - Epoch [1][950/7033]	lr: 1.000e-04, eta: 10:13:35, time: 0.784, data_time: 0.022, memory: 4350, loss_cls: 0.2706, loss_bbox: 0.6198, d0.loss_cls: 0.3003, d0.loss_bbox: 0.7203, d1.loss_cls: 0.2580, d1.loss_bbox: 0.6796, d2.loss_cls: 0.2592, d2.loss_bbox: 0.6395, d3.loss_cls: 0.2659, d3.loss_bbox: 0.6192, d4.loss_cls: 0.2665, d4.loss_bbox: 0.6257, loss: 5.5246, grad_norm: 14.9144
2025-04-16 17:48:13,626 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 17:48:13,627 - mmdet - INFO - Epoch [1][1000/7033]	lr: 1.000e-04, eta: 10:09:03, time: 0.782, data_time: 0.023, memory: 4350, loss_cls: 0.2606, loss_bbox: 0.6417, d0.loss_cls: 0.2890, d0.loss_bbox: 0.7289, d1.loss_cls: 0.2448, d1.loss_bbox: 0.6977, d2.loss_cls: 0.2446, d2.loss_bbox: 0.6516, d3.loss_cls: 0.2470, d3.loss_bbox: 0.6401, d4.loss_cls: 0.2517, d4.loss_bbox: 0.6503, loss: 5.5480, grad_norm: 17.8784
2025-04-16 17:48:52,772 - mmdet - INFO - Epoch [1][1050/7033]	lr: 1.000e-04, eta: 10:04:55, time: 0.783, data_time: 0.023, memory: 4350, loss_cls: 0.2673, loss_bbox: 0.6397, d0.loss_cls: 0.3012, d0.loss_bbox: 0.7442, d1.loss_cls: 0.2563, d1.loss_bbox: 0.7036, d2.loss_cls: 0.2583, d2.loss_bbox: 0.6594, d3.loss_cls: 0.2608, d3.loss_bbox: 0.6423, d4.loss_cls: 0.2635, d4.loss_bbox: 0.6470, loss: 5.6434, grad_norm: 19.2842
2025-04-16 17:49:31,863 - mmdet - INFO - Epoch [1][1100/7033]	lr: 1.000e-04, eta: 10:01:03, time: 0.782, data_time: 0.020, memory: 4350, loss_cls: 0.2648, loss_bbox: 0.5953, d0.loss_cls: 0.2949, d0.loss_bbox: 0.6853, d1.loss_cls: 0.2513, d1.loss_bbox: 0.6581, d2.loss_cls: 0.2524, d2.loss_bbox: 0.6065, d3.loss_cls: 0.2545, d3.loss_bbox: 0.5919, d4.loss_cls: 0.2602, d4.loss_bbox: 0.5961, loss: 5.3114, grad_norm: 18.2493
2025-04-16 17:50:11,058 - mmdet - INFO - Epoch [1][1150/7033]	lr: 1.000e-04, eta: 9:57:33, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.2577, loss_bbox: 0.5529, d0.loss_cls: 0.2961, d0.loss_bbox: 0.6503, d1.loss_cls: 0.2457, d1.loss_bbox: 0.6093, d2.loss_cls: 0.2502, d2.loss_bbox: 0.5635, d3.loss_cls: 0.2533, d3.loss_bbox: 0.5492, d4.loss_cls: 0.2568, d4.loss_bbox: 0.5530, loss: 5.0378, grad_norm: 16.5936
2025-04-16 17:50:50,256 - mmdet - INFO - Epoch [1][1200/7033]	lr: 1.000e-04, eta: 9:54:16, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.2515, loss_bbox: 0.5573, d0.loss_cls: 0.2903, d0.loss_bbox: 0.6567, d1.loss_cls: 0.2448, d1.loss_bbox: 0.6169, d2.loss_cls: 0.2422, d2.loss_bbox: 0.5700, d3.loss_cls: 0.2455, d3.loss_bbox: 0.5533, d4.loss_cls: 0.2465, d4.loss_bbox: 0.5591, loss: 5.0341, grad_norm: 13.8293
2025-04-16 17:51:29,290 - mmdet - INFO - Epoch [1][1250/7033]	lr: 1.000e-04, eta: 9:51:07, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.2487, loss_bbox: 0.5349, d0.loss_cls: 0.2936, d0.loss_bbox: 0.6292, d1.loss_cls: 0.2403, d1.loss_bbox: 0.5949, d2.loss_cls: 0.2405, d2.loss_bbox: 0.5535, d3.loss_cls: 0.2454, d3.loss_bbox: 0.5357, d4.loss_cls: 0.2459, d4.loss_bbox: 0.5384, loss: 4.9011, grad_norm: 13.7985
2025-04-16 17:52:08,295 - mmdet - INFO - Epoch [1][1300/7033]	lr: 1.000e-04, eta: 9:48:08, time: 0.780, data_time: 0.020, memory: 4350, loss_cls: 0.2336, loss_bbox: 0.5178, d0.loss_cls: 0.2722, d0.loss_bbox: 0.5947, d1.loss_cls: 0.2306, d1.loss_bbox: 0.5644, d2.loss_cls: 0.2300, d2.loss_bbox: 0.5233, d3.loss_cls: 0.2336, d3.loss_bbox: 0.5073, d4.loss_cls: 0.2326, d4.loss_bbox: 0.5133, loss: 4.6535, grad_norm: 14.3077
2025-04-16 17:52:47,221 - mmdet - INFO - Epoch [1][1350/7033]	lr: 1.000e-04, eta: 9:45:17, time: 0.779, data_time: 0.020, memory: 4350, loss_cls: 0.2287, loss_bbox: 0.4994, d0.loss_cls: 0.2684, d0.loss_bbox: 0.5740, d1.loss_cls: 0.2262, d1.loss_bbox: 0.5382, d2.loss_cls: 0.2228, d2.loss_bbox: 0.5065, d3.loss_cls: 0.2298, d3.loss_bbox: 0.4942, d4.loss_cls: 0.2260, d4.loss_bbox: 0.5001, loss: 4.5142, grad_norm: 14.0000
2025-04-16 17:53:26,330 - mmdet - INFO - Epoch [1][1400/7033]	lr: 1.000e-04, eta: 9:42:41, time: 0.782, data_time: 0.021, memory: 4350, loss_cls: 0.2249, loss_bbox: 0.4977, d0.loss_cls: 0.2788, d0.loss_bbox: 0.5545, d1.loss_cls: 0.2272, d1.loss_bbox: 0.5254, d2.loss_cls: 0.2220, d2.loss_bbox: 0.5067, d3.loss_cls: 0.2241, d3.loss_bbox: 0.4889, d4.loss_cls: 0.2231, d4.loss_bbox: 0.4945, loss: 4.4678, grad_norm: 16.2548
2025-04-16 17:54:05,588 - mmdet - INFO - Epoch [1][1450/7033]	lr: 1.000e-04, eta: 9:40:18, time: 0.785, data_time: 0.022, memory: 4350, loss_cls: 0.2214, loss_bbox: 0.4901, d0.loss_cls: 0.2728, d0.loss_bbox: 0.5567, d1.loss_cls: 0.2267, d1.loss_bbox: 0.5297, d2.loss_cls: 0.2211, d2.loss_bbox: 0.4984, d3.loss_cls: 0.2214, d3.loss_bbox: 0.4869, d4.loss_cls: 0.2213, d4.loss_bbox: 0.4903, loss: 4.4369, grad_norm: 13.9197
2025-04-16 17:54:44,511 - mmdet - INFO - Epoch [1][1500/7033]	lr: 1.000e-04, eta: 9:37:52, time: 0.778, data_time: 0.022, memory: 4350, loss_cls: 0.2235, loss_bbox: 0.4914, d0.loss_cls: 0.2724, d0.loss_bbox: 0.5608, d1.loss_cls: 0.2303, d1.loss_bbox: 0.5228, d2.loss_cls: 0.2229, d2.loss_bbox: 0.4994, d3.loss_cls: 0.2232, d3.loss_bbox: 0.4864, d4.loss_cls: 0.2228, d4.loss_bbox: 0.4892, loss: 4.4450, grad_norm: 14.0035
2025-04-16 17:55:23,789 - mmdet - INFO - Epoch [1][1550/7033]	lr: 1.000e-04, eta: 9:35:42, time: 0.786, data_time: 0.022, memory: 4350, loss_cls: 0.2260, loss_bbox: 0.4808, d0.loss_cls: 0.2800, d0.loss_bbox: 0.5449, d1.loss_cls: 0.2275, d1.loss_bbox: 0.5091, d2.loss_cls: 0.2227, d2.loss_bbox: 0.4871, d3.loss_cls: 0.2225, d3.loss_bbox: 0.4765, d4.loss_cls: 0.2208, d4.loss_bbox: 0.4778, loss: 4.3757, grad_norm: 15.5678
2025-04-16 17:56:02,974 - mmdet - INFO - Epoch [1][1600/7033]	lr: 1.000e-04, eta: 9:33:36, time: 0.784, data_time: 0.020, memory: 4350, loss_cls: 0.2255, loss_bbox: 0.4830, d0.loss_cls: 0.2694, d0.loss_bbox: 0.5438, d1.loss_cls: 0.2263, d1.loss_bbox: 0.5023, d2.loss_cls: 0.2248, d2.loss_bbox: 0.4928, d3.loss_cls: 0.2233, d3.loss_bbox: 0.4794, d4.loss_cls: 0.2231, d4.loss_bbox: 0.4832, loss: 4.3769, grad_norm: 14.8506
2025-04-16 17:56:42,002 - mmdet - INFO - Epoch [1][1650/7033]	lr: 1.000e-04, eta: 9:31:31, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.2108, loss_bbox: 0.4739, d0.loss_cls: 0.2508, d0.loss_bbox: 0.5341, d1.loss_cls: 0.2098, d1.loss_bbox: 0.4897, d2.loss_cls: 0.2063, d2.loss_bbox: 0.4777, d3.loss_cls: 0.2072, d3.loss_bbox: 0.4686, d4.loss_cls: 0.2100, d4.loss_bbox: 0.4684, loss: 4.2073, grad_norm: 16.1474
2025-04-16 17:57:20,875 - mmdet - INFO - Epoch [1][1700/7033]	lr: 1.000e-04, eta: 9:29:27, time: 0.777, data_time: 0.020, memory: 4350, loss_cls: 0.2156, loss_bbox: 0.4919, d0.loss_cls: 0.2737, d0.loss_bbox: 0.5491, d1.loss_cls: 0.2212, d1.loss_bbox: 0.5047, d2.loss_cls: 0.2147, d2.loss_bbox: 0.4952, d3.loss_cls: 0.2151, d3.loss_bbox: 0.4878, d4.loss_cls: 0.2147, d4.loss_bbox: 0.4927, loss: 4.3766, grad_norm: 15.6108
2025-04-16 17:58:00,166 - mmdet - INFO - Epoch [1][1750/7033]	lr: 1.000e-04, eta: 9:27:38, time: 0.786, data_time: 0.021, memory: 4350, loss_cls: 0.2103, loss_bbox: 0.4714, d0.loss_cls: 0.2757, d0.loss_bbox: 0.5270, d1.loss_cls: 0.2219, d1.loss_bbox: 0.4823, d2.loss_cls: 0.2115, d2.loss_bbox: 0.4705, d3.loss_cls: 0.2135, d3.loss_bbox: 0.4647, d4.loss_cls: 0.2082, d4.loss_bbox: 0.4702, loss: 4.2271, grad_norm: 17.7457
2025-04-16 17:58:39,187 - mmdet - INFO - Epoch [1][1800/7033]	lr: 1.000e-04, eta: 9:25:47, time: 0.780, data_time: 0.019, memory: 4350, loss_cls: 0.2086, loss_bbox: 0.4720, d0.loss_cls: 0.2672, d0.loss_bbox: 0.5326, d1.loss_cls: 0.2135, d1.loss_bbox: 0.4853, d2.loss_cls: 0.2084, d2.loss_bbox: 0.4724, d3.loss_cls: 0.2060, d3.loss_bbox: 0.4692, d4.loss_cls: 0.2067, d4.loss_bbox: 0.4690, loss: 4.2109, grad_norm: 17.6801
2025-04-16 17:59:18,255 - mmdet - INFO - Epoch [1][1850/7033]	lr: 1.000e-04, eta: 9:24:01, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.2058, loss_bbox: 0.4497, d0.loss_cls: 0.2670, d0.loss_bbox: 0.5105, d1.loss_cls: 0.2129, d1.loss_bbox: 0.4540, d2.loss_cls: 0.2055, d2.loss_bbox: 0.4470, d3.loss_cls: 0.2059, d3.loss_bbox: 0.4426, d4.loss_cls: 0.2053, d4.loss_bbox: 0.4441, loss: 4.0503, grad_norm: 18.6381
2025-04-16 17:59:57,365 - mmdet - INFO - Epoch [1][1900/7033]	lr: 1.000e-04, eta: 9:22:19, time: 0.782, data_time: 0.020, memory: 4350, loss_cls: 0.2081, loss_bbox: 0.4497, d0.loss_cls: 0.2710, d0.loss_bbox: 0.5005, d1.loss_cls: 0.2165, d1.loss_bbox: 0.4538, d2.loss_cls: 0.2104, d2.loss_bbox: 0.4456, d3.loss_cls: 0.2079, d3.loss_bbox: 0.4447, d4.loss_cls: 0.2075, d4.loss_bbox: 0.4470, loss: 4.0626, grad_norm: 16.4643
2025-04-16 18:00:36,582 - mmdet - INFO - Epoch [1][1950/7033]	lr: 1.000e-04, eta: 9:20:42, time: 0.784, data_time: 0.020, memory: 4350, loss_cls: 0.2004, loss_bbox: 0.4665, d0.loss_cls: 0.2665, d0.loss_bbox: 0.5200, d1.loss_cls: 0.2119, d1.loss_bbox: 0.4727, d2.loss_cls: 0.2035, d2.loss_bbox: 0.4604, d3.loss_cls: 0.2017, d3.loss_bbox: 0.4574, d4.loss_cls: 0.2003, d4.loss_bbox: 0.4633, loss: 4.1245, grad_norm: 16.2996
2025-04-16 18:01:15,600 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 18:01:15,601 - mmdet - INFO - Epoch [1][2000/7033]	lr: 1.000e-04, eta: 9:19:05, time: 0.780, data_time: 0.021, memory: 4350, loss_cls: 0.2154, loss_bbox: 0.4742, d0.loss_cls: 0.2846, d0.loss_bbox: 0.5348, d1.loss_cls: 0.2253, d1.loss_bbox: 0.4851, d2.loss_cls: 0.2143, d2.loss_bbox: 0.4722, d3.loss_cls: 0.2144, d3.loss_bbox: 0.4698, d4.loss_cls: 0.2146, d4.loss_bbox: 0.4707, loss: 4.2754, grad_norm: 16.5576
2025-04-16 18:01:54,935 - mmdet - INFO - Epoch [1][2050/7033]	lr: 1.000e-04, eta: 9:17:36, time: 0.787, data_time: 0.021, memory: 4350, loss_cls: 0.1951, loss_bbox: 0.4462, d0.loss_cls: 0.2566, d0.loss_bbox: 0.4953, d1.loss_cls: 0.2068, d1.loss_bbox: 0.4442, d2.loss_cls: 0.1960, d2.loss_bbox: 0.4383, d3.loss_cls: 0.1966, d3.loss_bbox: 0.4387, d4.loss_cls: 0.1937, d4.loss_bbox: 0.4444, loss: 3.9516, grad_norm: 17.2271
2025-04-16 18:02:34,288 - mmdet - INFO - Epoch [1][2100/7033]	lr: 1.000e-04, eta: 9:16:10, time: 0.787, data_time: 0.021, memory: 4350, loss_cls: 0.1992, loss_bbox: 0.4500, d0.loss_cls: 0.2640, d0.loss_bbox: 0.5130, d1.loss_cls: 0.2130, d1.loss_bbox: 0.4552, d2.loss_cls: 0.2045, d2.loss_bbox: 0.4428, d3.loss_cls: 0.2006, d3.loss_bbox: 0.4402, d4.loss_cls: 0.1983, d4.loss_bbox: 0.4460, loss: 4.0268, grad_norm: 18.4583
2025-04-16 18:03:13,449 - mmdet - INFO - Epoch [1][2150/7033]	lr: 1.000e-04, eta: 9:14:43, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1995, loss_bbox: 0.4598, d0.loss_cls: 0.2703, d0.loss_bbox: 0.5119, d1.loss_cls: 0.2115, d1.loss_bbox: 0.4597, d2.loss_cls: 0.2007, d2.loss_bbox: 0.4499, d3.loss_cls: 0.2009, d3.loss_bbox: 0.4465, d4.loss_cls: 0.1989, d4.loss_bbox: 0.4513, loss: 4.0608, grad_norm: 18.4691
2025-04-16 18:03:52,648 - mmdet - INFO - Epoch [1][2200/7033]	lr: 1.000e-04, eta: 9:13:19, time: 0.784, data_time: 0.020, memory: 4350, loss_cls: 0.1889, loss_bbox: 0.4438, d0.loss_cls: 0.2555, d0.loss_bbox: 0.4989, d1.loss_cls: 0.2025, d1.loss_bbox: 0.4447, d2.loss_cls: 0.1911, d2.loss_bbox: 0.4338, d3.loss_cls: 0.1913, d3.loss_bbox: 0.4339, d4.loss_cls: 0.1903, d4.loss_bbox: 0.4382, loss: 3.9129, grad_norm: 16.4637
2025-04-16 18:04:31,777 - mmdet - INFO - Epoch [1][2250/7033]	lr: 1.000e-04, eta: 9:11:55, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1942, loss_bbox: 0.4370, d0.loss_cls: 0.2618, d0.loss_bbox: 0.4983, d1.loss_cls: 0.2079, d1.loss_bbox: 0.4387, d2.loss_cls: 0.1987, d2.loss_bbox: 0.4298, d3.loss_cls: 0.1951, d3.loss_bbox: 0.4290, d4.loss_cls: 0.1941, d4.loss_bbox: 0.4339, loss: 3.9186, grad_norm: 16.7568
2025-04-16 18:05:10,963 - mmdet - INFO - Epoch [1][2300/7033]	lr: 1.000e-04, eta: 9:10:34, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.1919, loss_bbox: 0.4334, d0.loss_cls: 0.2645, d0.loss_bbox: 0.4877, d1.loss_cls: 0.2115, d1.loss_bbox: 0.4306, d2.loss_cls: 0.1967, d2.loss_bbox: 0.4228, d3.loss_cls: 0.1920, d3.loss_bbox: 0.4261, d4.loss_cls: 0.1921, d4.loss_bbox: 0.4281, loss: 3.8775, grad_norm: 15.4984
2025-04-16 18:05:50,134 - mmdet - INFO - Epoch [1][2350/7033]	lr: 1.000e-04, eta: 9:09:15, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1905, loss_bbox: 0.4447, d0.loss_cls: 0.2656, d0.loss_bbox: 0.5045, d1.loss_cls: 0.2071, d1.loss_bbox: 0.4464, d2.loss_cls: 0.1920, d2.loss_bbox: 0.4403, d3.loss_cls: 0.1910, d3.loss_bbox: 0.4398, d4.loss_cls: 0.1881, d4.loss_bbox: 0.4441, loss: 3.9541, grad_norm: 16.1764
2025-04-16 18:06:29,335 - mmdet - INFO - Epoch [1][2400/7033]	lr: 1.000e-04, eta: 9:07:58, time: 0.784, data_time: 0.020, memory: 4350, loss_cls: 0.1846, loss_bbox: 0.4332, d0.loss_cls: 0.2552, d0.loss_bbox: 0.4903, d1.loss_cls: 0.2003, d1.loss_bbox: 0.4339, d2.loss_cls: 0.1888, d2.loss_bbox: 0.4211, d3.loss_cls: 0.1876, d3.loss_bbox: 0.4216, d4.loss_cls: 0.1876, d4.loss_bbox: 0.4252, loss: 3.8294, grad_norm: 23.1583
2025-04-16 18:07:18,008 - mmdet - INFO - Epoch [1][2450/7033]	lr: 1.000e-04, eta: 9:09:16, time: 0.973, data_time: 0.021, memory: 4350, loss_cls: 0.1837, loss_bbox: 0.4338, d0.loss_cls: 0.2616, d0.loss_bbox: 0.4926, d1.loss_cls: 0.2029, d1.loss_bbox: 0.4332, d2.loss_cls: 0.1905, d2.loss_bbox: 0.4249, d3.loss_cls: 0.1871, d3.loss_bbox: 0.4232, d4.loss_cls: 0.1843, d4.loss_bbox: 0.4276, loss: 3.8453, grad_norm: 17.7280
2025-04-16 18:08:09,736 - mmdet - INFO - Epoch [1][2500/7033]	lr: 1.000e-04, eta: 9:11:18, time: 1.035, data_time: 0.022, memory: 4350, loss_cls: 0.1829, loss_bbox: 0.4300, d0.loss_cls: 0.2532, d0.loss_bbox: 0.4956, d1.loss_cls: 0.1987, d1.loss_bbox: 0.4290, d2.loss_cls: 0.1880, d2.loss_bbox: 0.4217, d3.loss_cls: 0.1838, d3.loss_bbox: 0.4218, d4.loss_cls: 0.1810, d4.loss_bbox: 0.4258, loss: 3.8116, grad_norm: 17.1887
2025-04-16 18:08:59,709 - mmdet - INFO - Epoch [1][2550/7033]	lr: 1.000e-04, eta: 9:12:45, time: 0.999, data_time: 0.022, memory: 4350, loss_cls: 0.1793, loss_bbox: 0.4235, d0.loss_cls: 0.2521, d0.loss_bbox: 0.4798, d1.loss_cls: 0.1914, d1.loss_bbox: 0.4230, d2.loss_cls: 0.1832, d2.loss_bbox: 0.4112, d3.loss_cls: 0.1818, d3.loss_bbox: 0.4129, d4.loss_cls: 0.1797, d4.loss_bbox: 0.4162, loss: 3.7341, grad_norm: 18.9511
2025-04-16 18:09:50,808 - mmdet - INFO - Epoch [1][2600/7033]	lr: 1.000e-04, eta: 9:14:25, time: 1.022, data_time: 0.023, memory: 4350, loss_cls: 0.1832, loss_bbox: 0.4190, d0.loss_cls: 0.2559, d0.loss_bbox: 0.4875, d1.loss_cls: 0.2039, d1.loss_bbox: 0.4246, d2.loss_cls: 0.1907, d2.loss_bbox: 0.4156, d3.loss_cls: 0.1849, d3.loss_bbox: 0.4157, d4.loss_cls: 0.1821, d4.loss_bbox: 0.4176, loss: 3.7808, grad_norm: 24.9826
2025-04-16 18:10:40,795 - mmdet - INFO - Epoch [1][2650/7033]	lr: 1.000e-04, eta: 9:15:42, time: 1.000, data_time: 0.021, memory: 4350, loss_cls: 0.1862, loss_bbox: 0.4277, d0.loss_cls: 0.2578, d0.loss_bbox: 0.4882, d1.loss_cls: 0.2043, d1.loss_bbox: 0.4290, d2.loss_cls: 0.1921, d2.loss_bbox: 0.4165, d3.loss_cls: 0.1870, d3.loss_bbox: 0.4209, d4.loss_cls: 0.1847, d4.loss_bbox: 0.4242, loss: 3.8186, grad_norm: 21.6495
2025-04-16 18:11:30,503 - mmdet - INFO - Epoch [1][2700/7033]	lr: 1.000e-04, eta: 9:16:50, time: 0.994, data_time: 0.022, memory: 4350, loss_cls: 0.1887, loss_bbox: 0.4081, d0.loss_cls: 0.2639, d0.loss_bbox: 0.4719, d1.loss_cls: 0.2079, d1.loss_bbox: 0.4070, d2.loss_cls: 0.1977, d2.loss_bbox: 0.3951, d3.loss_cls: 0.1912, d3.loss_bbox: 0.3958, d4.loss_cls: 0.1892, d4.loss_bbox: 0.4018, loss: 3.7182, grad_norm: 20.1274
2025-04-16 18:12:21,011 - mmdet - INFO - Epoch [1][2750/7033]	lr: 1.000e-04, eta: 9:18:06, time: 1.010, data_time: 0.022, memory: 4350, loss_cls: 0.1874, loss_bbox: 0.4262, d0.loss_cls: 0.2686, d0.loss_bbox: 0.4863, d1.loss_cls: 0.2066, d1.loss_bbox: 0.4251, d2.loss_cls: 0.1940, d2.loss_bbox: 0.4117, d3.loss_cls: 0.1899, d3.loss_bbox: 0.4122, d4.loss_cls: 0.1862, d4.loss_bbox: 0.4192, loss: 3.8134, grad_norm: 19.7734
2025-04-16 18:13:10,869 - mmdet - INFO - Epoch [1][2800/7033]	lr: 1.000e-04, eta: 9:19:08, time: 0.997, data_time: 0.022, memory: 4350, loss_cls: 0.1784, loss_bbox: 0.4103, d0.loss_cls: 0.2572, d0.loss_bbox: 0.4736, d1.loss_cls: 0.1985, d1.loss_bbox: 0.4096, d2.loss_cls: 0.1844, d2.loss_bbox: 0.3973, d3.loss_cls: 0.1795, d3.loss_bbox: 0.3981, d4.loss_cls: 0.1785, d4.loss_bbox: 0.4037, loss: 3.6691, grad_norm: 18.4236
2025-04-16 18:14:00,248 - mmdet - INFO - Epoch [1][2850/7033]	lr: 1.000e-04, eta: 9:19:59, time: 0.988, data_time: 0.022, memory: 4350, loss_cls: 0.1799, loss_bbox: 0.4030, d0.loss_cls: 0.2532, d0.loss_bbox: 0.4673, d1.loss_cls: 0.1941, d1.loss_bbox: 0.4010, d2.loss_cls: 0.1839, d2.loss_bbox: 0.3911, d3.loss_cls: 0.1789, d3.loss_bbox: 0.3942, d4.loss_cls: 0.1792, d4.loss_bbox: 0.3965, loss: 3.6222, grad_norm: 22.8725
2025-04-16 18:14:50,520 - mmdet - INFO - Epoch [1][2900/7033]	lr: 1.000e-04, eta: 9:20:59, time: 1.005, data_time: 0.022, memory: 4350, loss_cls: 0.1732, loss_bbox: 0.3875, d0.loss_cls: 0.2535, d0.loss_bbox: 0.4539, d1.loss_cls: 0.1910, d1.loss_bbox: 0.3881, d2.loss_cls: 0.1789, d2.loss_bbox: 0.3779, d3.loss_cls: 0.1772, d3.loss_bbox: 0.3780, d4.loss_cls: 0.1757, d4.loss_bbox: 0.3810, loss: 3.5160, grad_norm: 20.5634
2025-04-16 18:15:40,843 - mmdet - INFO - Epoch [1][2950/7033]	lr: 1.000e-04, eta: 9:21:56, time: 1.006, data_time: 0.022, memory: 4350, loss_cls: 0.1781, loss_bbox: 0.4076, d0.loss_cls: 0.2586, d0.loss_bbox: 0.4748, d1.loss_cls: 0.2013, d1.loss_bbox: 0.4085, d2.loss_cls: 0.1867, d2.loss_bbox: 0.3963, d3.loss_cls: 0.1806, d3.loss_bbox: 0.3994, d4.loss_cls: 0.1781, d4.loss_bbox: 0.4030, loss: 3.6730, grad_norm: 18.8589
2025-04-16 18:16:32,271 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 18:16:32,271 - mmdet - INFO - Epoch [1][3000/7033]	lr: 1.000e-04, eta: 9:23:04, time: 1.029, data_time: 0.022, memory: 4350, loss_cls: 0.1736, loss_bbox: 0.4145, d0.loss_cls: 0.2583, d0.loss_bbox: 0.4832, d1.loss_cls: 0.2000, d1.loss_bbox: 0.4106, d2.loss_cls: 0.1840, d2.loss_bbox: 0.3972, d3.loss_cls: 0.1770, d3.loss_bbox: 0.4024, d4.loss_cls: 0.1755, d4.loss_bbox: 0.4067, loss: 3.6830, grad_norm: 18.6718
2025-04-16 18:17:22,523 - mmdet - INFO - Epoch [1][3050/7033]	lr: 1.000e-04, eta: 9:23:53, time: 1.005, data_time: 0.024, memory: 4350, loss_cls: 0.1781, loss_bbox: 0.3936, d0.loss_cls: 0.2575, d0.loss_bbox: 0.4741, d1.loss_cls: 0.1979, d1.loss_bbox: 0.4020, d2.loss_cls: 0.1836, d2.loss_bbox: 0.3873, d3.loss_cls: 0.1808, d3.loss_bbox: 0.3854, d4.loss_cls: 0.1778, d4.loss_bbox: 0.3900, loss: 3.6082, grad_norm: 20.9895
2025-04-16 18:18:11,009 - mmdet - INFO - Epoch [1][3100/7033]	lr: 1.000e-04, eta: 9:24:16, time: 0.970, data_time: 0.022, memory: 4350, loss_cls: 0.1735, loss_bbox: 0.3918, d0.loss_cls: 0.2518, d0.loss_bbox: 0.4757, d1.loss_cls: 0.1969, d1.loss_bbox: 0.4043, d2.loss_cls: 0.1822, d2.loss_bbox: 0.3900, d3.loss_cls: 0.1753, d3.loss_bbox: 0.3864, d4.loss_cls: 0.1738, d4.loss_bbox: 0.3879, loss: 3.5898, grad_norm: 23.4347
2025-04-16 18:19:02,807 - mmdet - INFO - Epoch [1][3150/7033]	lr: 1.000e-04, eta: 9:25:18, time: 1.036, data_time: 0.022, memory: 4350, loss_cls: 0.1876, loss_bbox: 0.4260, d0.loss_cls: 0.2642, d0.loss_bbox: 0.5004, d1.loss_cls: 0.2117, d1.loss_bbox: 0.4211, d2.loss_cls: 0.1938, d2.loss_bbox: 0.4128, d3.loss_cls: 0.1938, d3.loss_bbox: 0.4117, d4.loss_cls: 0.1894, d4.loss_bbox: 0.4186, loss: 3.8311, grad_norm: 26.1223
2025-04-16 18:19:53,310 - mmdet - INFO - Epoch [1][3200/7033]	lr: 1.000e-04, eta: 9:26:01, time: 1.010, data_time: 0.023, memory: 4350, loss_cls: 0.1829, loss_bbox: 0.4037, d0.loss_cls: 0.2691, d0.loss_bbox: 0.4864, d1.loss_cls: 0.2062, d1.loss_bbox: 0.4089, d2.loss_cls: 0.1906, d2.loss_bbox: 0.3924, d3.loss_cls: 0.1876, d3.loss_bbox: 0.3935, d4.loss_cls: 0.1841, d4.loss_bbox: 0.3983, loss: 3.7037, grad_norm: 21.4798
2025-04-16 18:20:43,812 - mmdet - INFO - Epoch [1][3250/7033]	lr: 1.000e-04, eta: 9:26:40, time: 1.010, data_time: 0.023, memory: 4350, loss_cls: 0.1708, loss_bbox: 0.3914, d0.loss_cls: 0.2505, d0.loss_bbox: 0.4742, d1.loss_cls: 0.1908, d1.loss_bbox: 0.3903, d2.loss_cls: 0.1779, d2.loss_bbox: 0.3776, d3.loss_cls: 0.1739, d3.loss_bbox: 0.3778, d4.loss_cls: 0.1701, d4.loss_bbox: 0.3859, loss: 3.5312, grad_norm: 24.2859
2025-04-16 18:21:34,401 - mmdet - INFO - Epoch [1][3300/7033]	lr: 1.000e-04, eta: 9:27:19, time: 1.012, data_time: 0.022, memory: 4350, loss_cls: 0.1705, loss_bbox: 0.3893, d0.loss_cls: 0.2514, d0.loss_bbox: 0.4740, d1.loss_cls: 0.1915, d1.loss_bbox: 0.3948, d2.loss_cls: 0.1725, d2.loss_bbox: 0.3806, d3.loss_cls: 0.1739, d3.loss_bbox: 0.3795, d4.loss_cls: 0.1714, d4.loss_bbox: 0.3842, loss: 3.5338, grad_norm: 25.6289
2025-04-16 18:22:24,245 - mmdet - INFO - Epoch [1][3350/7033]	lr: 1.000e-04, eta: 9:27:45, time: 0.997, data_time: 0.024, memory: 4350, loss_cls: 0.1712, loss_bbox: 0.4004, d0.loss_cls: 0.2572, d0.loss_bbox: 0.4754, d1.loss_cls: 0.1936, d1.loss_bbox: 0.3971, d2.loss_cls: 0.1818, d2.loss_bbox: 0.3862, d3.loss_cls: 0.1768, d3.loss_bbox: 0.3863, d4.loss_cls: 0.1719, d4.loss_bbox: 0.3927, loss: 3.5905, grad_norm: 27.2056
2025-04-16 18:23:13,697 - mmdet - INFO - Epoch [1][3400/7033]	lr: 1.000e-04, eta: 9:28:06, time: 0.989, data_time: 0.023, memory: 4350, loss_cls: 0.1777, loss_bbox: 0.3862, d0.loss_cls: 0.2636, d0.loss_bbox: 0.4743, d1.loss_cls: 0.2021, d1.loss_bbox: 0.3931, d2.loss_cls: 0.1856, d2.loss_bbox: 0.3798, d3.loss_cls: 0.1799, d3.loss_bbox: 0.3801, d4.loss_cls: 0.1786, d4.loss_bbox: 0.3811, loss: 3.5820, grad_norm: 22.7098
2025-04-16 18:24:04,499 - mmdet - INFO - Epoch [1][3450/7033]	lr: 1.000e-04, eta: 9:28:39, time: 1.016, data_time: 0.023, memory: 4350, loss_cls: 0.1702, loss_bbox: 0.3749, d0.loss_cls: 0.2421, d0.loss_bbox: 0.4560, d1.loss_cls: 0.1855, d1.loss_bbox: 0.3803, d2.loss_cls: 0.1738, d2.loss_bbox: 0.3664, d3.loss_cls: 0.1725, d3.loss_bbox: 0.3655, d4.loss_cls: 0.1688, d4.loss_bbox: 0.3703, loss: 3.4264, grad_norm: 20.8387
2025-04-16 18:24:55,860 - mmdet - INFO - Epoch [1][3500/7033]	lr: 1.000e-04, eta: 9:29:16, time: 1.027, data_time: 0.022, memory: 4350, loss_cls: 0.1783, loss_bbox: 0.3702, d0.loss_cls: 0.2619, d0.loss_bbox: 0.4557, d1.loss_cls: 0.1983, d1.loss_bbox: 0.3773, d2.loss_cls: 0.1821, d2.loss_bbox: 0.3635, d3.loss_cls: 0.1778, d3.loss_bbox: 0.3618, d4.loss_cls: 0.1786, d4.loss_bbox: 0.3649, loss: 3.4705, grad_norm: 25.9279
2025-04-16 18:25:44,985 - mmdet - INFO - Epoch [1][3550/7033]	lr: 1.000e-04, eta: 9:29:26, time: 0.982, data_time: 0.022, memory: 4350, loss_cls: 0.1720, loss_bbox: 0.3642, d0.loss_cls: 0.2663, d0.loss_bbox: 0.4511, d1.loss_cls: 0.2002, d1.loss_bbox: 0.3666, d2.loss_cls: 0.1820, d2.loss_bbox: 0.3538, d3.loss_cls: 0.1758, d3.loss_bbox: 0.3544, d4.loss_cls: 0.1717, d4.loss_bbox: 0.3590, loss: 3.4169, grad_norm: 20.6550
2025-04-16 18:26:35,571 - mmdet - INFO - Epoch [1][3600/7033]	lr: 1.000e-04, eta: 9:29:50, time: 1.012, data_time: 0.023, memory: 4350, loss_cls: 0.1692, loss_bbox: 0.3715, d0.loss_cls: 0.2492, d0.loss_bbox: 0.4543, d1.loss_cls: 0.1860, d1.loss_bbox: 0.3785, d2.loss_cls: 0.1710, d2.loss_bbox: 0.3687, d3.loss_cls: 0.1695, d3.loss_bbox: 0.3652, d4.loss_cls: 0.1671, d4.loss_bbox: 0.3685, loss: 3.4185, grad_norm: 23.6316
2025-04-16 18:27:27,360 - mmdet - INFO - Epoch [1][3650/7033]	lr: 1.000e-04, eta: 9:30:25, time: 1.035, data_time: 0.021, memory: 4350, loss_cls: 0.1702, loss_bbox: 0.3663, d0.loss_cls: 0.2619, d0.loss_bbox: 0.4654, d1.loss_cls: 0.1985, d1.loss_bbox: 0.3774, d2.loss_cls: 0.1804, d2.loss_bbox: 0.3607, d3.loss_cls: 0.1756, d3.loss_bbox: 0.3596, d4.loss_cls: 0.1709, d4.loss_bbox: 0.3639, loss: 3.4508, grad_norm: 31.5436
2025-04-16 18:28:17,657 - mmdet - INFO - Epoch [1][3700/7033]	lr: 1.000e-04, eta: 9:30:42, time: 1.006, data_time: 0.023, memory: 4350, loss_cls: 0.1659, loss_bbox: 0.3772, d0.loss_cls: 0.2468, d0.loss_bbox: 0.4670, d1.loss_cls: 0.1887, d1.loss_bbox: 0.3803, d2.loss_cls: 0.1724, d2.loss_bbox: 0.3681, d3.loss_cls: 0.1673, d3.loss_bbox: 0.3681, d4.loss_cls: 0.1659, d4.loss_bbox: 0.3714, loss: 3.4390, grad_norm: 22.1802
2025-04-16 18:29:07,473 - mmdet - INFO - Epoch [1][3750/7033]	lr: 1.000e-04, eta: 9:30:53, time: 0.996, data_time: 0.022, memory: 4350, loss_cls: 0.1709, loss_bbox: 0.3651, d0.loss_cls: 0.2533, d0.loss_bbox: 0.4480, d1.loss_cls: 0.1959, d1.loss_bbox: 0.3689, d2.loss_cls: 0.1781, d2.loss_bbox: 0.3549, d3.loss_cls: 0.1738, d3.loss_bbox: 0.3551, d4.loss_cls: 0.1722, d4.loss_bbox: 0.3566, loss: 3.3928, grad_norm: 19.3543
2025-04-16 18:29:57,929 - mmdet - INFO - Epoch [1][3800/7033]	lr: 1.000e-04, eta: 9:31:08, time: 1.009, data_time: 0.024, memory: 4350, loss_cls: 0.1686, loss_bbox: 0.3624, d0.loss_cls: 0.2542, d0.loss_bbox: 0.4455, d1.loss_cls: 0.1928, d1.loss_bbox: 0.3631, d2.loss_cls: 0.1749, d2.loss_bbox: 0.3512, d3.loss_cls: 0.1714, d3.loss_bbox: 0.3525, d4.loss_cls: 0.1685, d4.loss_bbox: 0.3564, loss: 3.3615, grad_norm: 22.0906
2025-04-16 18:30:48,923 - mmdet - INFO - Epoch [1][3850/7033]	lr: 1.000e-04, eta: 9:31:27, time: 1.020, data_time: 0.033, memory: 4350, loss_cls: 0.1597, loss_bbox: 0.3659, d0.loss_cls: 0.2419, d0.loss_bbox: 0.4527, d1.loss_cls: 0.1831, d1.loss_bbox: 0.3751, d2.loss_cls: 0.1681, d2.loss_bbox: 0.3636, d3.loss_cls: 0.1621, d3.loss_bbox: 0.3615, d4.loss_cls: 0.1597, d4.loss_bbox: 0.3617, loss: 3.3552, grad_norm: 32.3761
2025-04-16 18:31:39,772 - mmdet - INFO - Epoch [1][3900/7033]	lr: 1.000e-04, eta: 9:31:42, time: 1.017, data_time: 0.024, memory: 4350, loss_cls: 0.1671, loss_bbox: 0.3616, d0.loss_cls: 0.2535, d0.loss_bbox: 0.4497, d1.loss_cls: 0.1890, d1.loss_bbox: 0.3715, d2.loss_cls: 0.1740, d2.loss_bbox: 0.3539, d3.loss_cls: 0.1684, d3.loss_bbox: 0.3545, d4.loss_cls: 0.1692, d4.loss_bbox: 0.3559, loss: 3.3684, grad_norm: 24.6321
2025-04-16 18:32:29,109 - mmdet - INFO - Epoch [1][3950/7033]	lr: 1.000e-04, eta: 9:31:42, time: 0.987, data_time: 0.023, memory: 4350, loss_cls: 0.1617, loss_bbox: 0.3739, d0.loss_cls: 0.2538, d0.loss_bbox: 0.4633, d1.loss_cls: 0.1832, d1.loss_bbox: 0.3841, d2.loss_cls: 0.1694, d2.loss_bbox: 0.3681, d3.loss_cls: 0.1640, d3.loss_bbox: 0.3655, d4.loss_cls: 0.1611, d4.loss_bbox: 0.3690, loss: 3.4172, grad_norm: 25.6293
2025-04-16 18:33:08,465 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 18:33:08,465 - mmdet - INFO - Epoch [1][4000/7033]	lr: 1.000e-04, eta: 9:30:04, time: 0.787, data_time: 0.025, memory: 4350, loss_cls: 0.1652, loss_bbox: 0.3512, d0.loss_cls: 0.2513, d0.loss_bbox: 0.4472, d1.loss_cls: 0.1904, d1.loss_bbox: 0.3612, d2.loss_cls: 0.1708, d2.loss_bbox: 0.3460, d3.loss_cls: 0.1673, d3.loss_bbox: 0.3449, d4.loss_cls: 0.1648, d4.loss_bbox: 0.3471, loss: 3.3074, grad_norm: 19.8271
2025-04-16 18:33:47,588 - mmdet - INFO - Epoch [1][4050/7033]	lr: 1.000e-04, eta: 9:28:26, time: 0.782, data_time: 0.025, memory: 4350, loss_cls: 0.1741, loss_bbox: 0.3559, d0.loss_cls: 0.2514, d0.loss_bbox: 0.4525, d1.loss_cls: 0.1988, d1.loss_bbox: 0.3654, d2.loss_cls: 0.1820, d2.loss_bbox: 0.3490, d3.loss_cls: 0.1778, d3.loss_bbox: 0.3462, d4.loss_cls: 0.1746, d4.loss_bbox: 0.3505, loss: 3.3783, grad_norm: 22.0678
2025-04-16 18:34:26,711 - mmdet - INFO - Epoch [1][4100/7033]	lr: 1.000e-04, eta: 9:26:50, time: 0.782, data_time: 0.020, memory: 4350, loss_cls: 0.1682, loss_bbox: 0.3709, d0.loss_cls: 0.2532, d0.loss_bbox: 0.4638, d1.loss_cls: 0.1900, d1.loss_bbox: 0.3840, d2.loss_cls: 0.1756, d2.loss_bbox: 0.3630, d3.loss_cls: 0.1724, d3.loss_bbox: 0.3628, d4.loss_cls: 0.1691, d4.loss_bbox: 0.3667, loss: 3.4398, grad_norm: 23.2262
2025-04-16 18:35:05,828 - mmdet - INFO - Epoch [1][4150/7033]	lr: 1.000e-04, eta: 9:25:15, time: 0.782, data_time: 0.019, memory: 4350, loss_cls: 0.1615, loss_bbox: 0.3540, d0.loss_cls: 0.2455, d0.loss_bbox: 0.4379, d1.loss_cls: 0.1870, d1.loss_bbox: 0.3619, d2.loss_cls: 0.1704, d2.loss_bbox: 0.3463, d3.loss_cls: 0.1646, d3.loss_bbox: 0.3464, d4.loss_cls: 0.1611, d4.loss_bbox: 0.3497, loss: 3.2865, grad_norm: 20.2381
2025-04-16 18:35:44,766 - mmdet - INFO - Epoch [1][4200/7033]	lr: 1.000e-04, eta: 9:23:39, time: 0.779, data_time: 0.020, memory: 4350, loss_cls: 0.1617, loss_bbox: 0.3680, d0.loss_cls: 0.2529, d0.loss_bbox: 0.4489, d1.loss_cls: 0.1884, d1.loss_bbox: 0.3675, d2.loss_cls: 0.1698, d2.loss_bbox: 0.3554, d3.loss_cls: 0.1646, d3.loss_bbox: 0.3557, d4.loss_cls: 0.1626, d4.loss_bbox: 0.3606, loss: 3.3561, grad_norm: 19.6999
2025-04-16 18:36:23,856 - mmdet - INFO - Epoch [1][4250/7033]	lr: 1.000e-04, eta: 9:22:06, time: 0.782, data_time: 0.021, memory: 4350, loss_cls: 0.1578, loss_bbox: 0.3484, d0.loss_cls: 0.2406, d0.loss_bbox: 0.4405, d1.loss_cls: 0.1860, d1.loss_bbox: 0.3539, d2.loss_cls: 0.1650, d2.loss_bbox: 0.3391, d3.loss_cls: 0.1615, d3.loss_bbox: 0.3389, d4.loss_cls: 0.1579, d4.loss_bbox: 0.3427, loss: 3.2322, grad_norm: 33.1898
2025-04-16 18:37:02,961 - mmdet - INFO - Epoch [1][4300/7033]	lr: 1.000e-04, eta: 9:20:35, time: 0.782, data_time: 0.023, memory: 4350, loss_cls: 0.1599, loss_bbox: 0.3593, d0.loss_cls: 0.2577, d0.loss_bbox: 0.4527, d1.loss_cls: 0.1926, d1.loss_bbox: 0.3709, d2.loss_cls: 0.1717, d2.loss_bbox: 0.3527, d3.loss_cls: 0.1666, d3.loss_bbox: 0.3513, d4.loss_cls: 0.1615, d4.loss_bbox: 0.3555, loss: 3.3524, grad_norm: 16.1249
2025-04-16 18:37:42,044 - mmdet - INFO - Epoch [1][4350/7033]	lr: 1.000e-04, eta: 9:19:04, time: 0.782, data_time: 0.020, memory: 4350, loss_cls: 0.1631, loss_bbox: 0.3549, d0.loss_cls: 0.2471, d0.loss_bbox: 0.4445, d1.loss_cls: 0.1887, d1.loss_bbox: 0.3625, d2.loss_cls: 0.1704, d2.loss_bbox: 0.3453, d3.loss_cls: 0.1638, d3.loss_bbox: 0.3461, d4.loss_cls: 0.1622, d4.loss_bbox: 0.3501, loss: 3.2987, grad_norm: 21.1239
2025-04-16 18:38:20,946 - mmdet - INFO - Epoch [1][4400/7033]	lr: 1.000e-04, eta: 9:17:34, time: 0.778, data_time: 0.019, memory: 4350, loss_cls: 0.1614, loss_bbox: 0.3595, d0.loss_cls: 0.2527, d0.loss_bbox: 0.4543, d1.loss_cls: 0.1887, d1.loss_bbox: 0.3645, d2.loss_cls: 0.1691, d2.loss_bbox: 0.3499, d3.loss_cls: 0.1661, d3.loss_bbox: 0.3482, d4.loss_cls: 0.1619, d4.loss_bbox: 0.3523, loss: 3.3284, grad_norm: 21.0838
2025-04-16 18:39:01,529 - mmdet - INFO - Epoch [1][4450/7033]	lr: 1.000e-04, eta: 9:16:18, time: 0.812, data_time: 0.020, memory: 4350, loss_cls: 0.1618, loss_bbox: 0.3491, d0.loss_cls: 0.2449, d0.loss_bbox: 0.4447, d1.loss_cls: 0.1909, d1.loss_bbox: 0.3620, d2.loss_cls: 0.1723, d2.loss_bbox: 0.3470, d3.loss_cls: 0.1653, d3.loss_bbox: 0.3450, d4.loss_cls: 0.1625, d4.loss_bbox: 0.3442, loss: 3.2897, grad_norm: 17.0103
2025-04-16 18:39:50,234 - mmdet - INFO - Epoch [1][4500/7033]	lr: 1.000e-04, eta: 9:16:12, time: 0.974, data_time: 0.021, memory: 4350, loss_cls: 0.1643, loss_bbox: 0.3448, d0.loss_cls: 0.2459, d0.loss_bbox: 0.4404, d1.loss_cls: 0.1888, d1.loss_bbox: 0.3563, d2.loss_cls: 0.1692, d2.loss_bbox: 0.3399, d3.loss_cls: 0.1659, d3.loss_bbox: 0.3386, d4.loss_cls: 0.1629, d4.loss_bbox: 0.3430, loss: 3.2601, grad_norm: 24.2409
2025-04-16 18:40:30,380 - mmdet - INFO - Epoch [1][4550/7033]	lr: 1.000e-04, eta: 9:14:53, time: 0.803, data_time: 0.021, memory: 4350, loss_cls: 0.1607, loss_bbox: 0.3520, d0.loss_cls: 0.2578, d0.loss_bbox: 0.4552, d1.loss_cls: 0.1914, d1.loss_bbox: 0.3675, d2.loss_cls: 0.1707, d2.loss_bbox: 0.3489, d3.loss_cls: 0.1637, d3.loss_bbox: 0.3477, d4.loss_cls: 0.1601, d4.loss_bbox: 0.3472, loss: 3.3229, grad_norm: 32.4292
2025-04-16 18:41:13,893 - mmdet - INFO - Epoch [1][4600/7033]	lr: 1.000e-04, eta: 9:14:03, time: 0.870, data_time: 0.021, memory: 4350, loss_cls: 0.1599, loss_bbox: 0.3571, d0.loss_cls: 0.2429, d0.loss_bbox: 0.4458, d1.loss_cls: 0.1847, d1.loss_bbox: 0.3639, d2.loss_cls: 0.1683, d2.loss_bbox: 0.3462, d3.loss_cls: 0.1617, d3.loss_bbox: 0.3482, d4.loss_cls: 0.1608, d4.loss_bbox: 0.3513, loss: 3.2909, grad_norm: 21.0726
2025-04-16 18:42:04,990 - mmdet - INFO - Epoch [1][4650/7033]	lr: 1.000e-04, eta: 9:14:15, time: 1.022, data_time: 0.021, memory: 4350, loss_cls: 0.1581, loss_bbox: 0.3405, d0.loss_cls: 0.2420, d0.loss_bbox: 0.4388, d1.loss_cls: 0.1805, d1.loss_bbox: 0.3499, d2.loss_cls: 0.1676, d2.loss_bbox: 0.3345, d3.loss_cls: 0.1612, d3.loss_bbox: 0.3332, d4.loss_cls: 0.1594, d4.loss_bbox: 0.3341, loss: 3.1998, grad_norm: 23.8682
2025-04-16 18:42:55,798 - mmdet - INFO - Epoch [1][4700/7033]	lr: 1.000e-04, eta: 9:14:22, time: 1.016, data_time: 0.023, memory: 4350, loss_cls: 0.1591, loss_bbox: 0.3498, d0.loss_cls: 0.2462, d0.loss_bbox: 0.4490, d1.loss_cls: 0.1850, d1.loss_bbox: 0.3625, d2.loss_cls: 0.1689, d2.loss_bbox: 0.3431, d3.loss_cls: 0.1632, d3.loss_bbox: 0.3416, d4.loss_cls: 0.1590, d4.loss_bbox: 0.3458, loss: 3.2731, grad_norm: 19.3804
2025-04-16 18:43:44,921 - mmdet - INFO - Epoch [1][4750/7033]	lr: 1.000e-04, eta: 9:14:16, time: 0.983, data_time: 0.022, memory: 4350, loss_cls: 0.1450, loss_bbox: 0.3284, d0.loss_cls: 0.2293, d0.loss_bbox: 0.4244, d1.loss_cls: 0.1724, d1.loss_bbox: 0.3407, d2.loss_cls: 0.1572, d2.loss_bbox: 0.3222, d3.loss_cls: 0.1506, d3.loss_bbox: 0.3218, d4.loss_cls: 0.1466, d4.loss_bbox: 0.3249, loss: 3.0635, grad_norm: 19.5065
2025-04-16 18:44:35,685 - mmdet - INFO - Epoch [1][4800/7033]	lr: 1.000e-04, eta: 9:14:21, time: 1.015, data_time: 0.022, memory: 4350, loss_cls: 0.1499, loss_bbox: 0.3359, d0.loss_cls: 0.2476, d0.loss_bbox: 0.4284, d1.loss_cls: 0.1806, d1.loss_bbox: 0.3432, d2.loss_cls: 0.1594, d2.loss_bbox: 0.3236, d3.loss_cls: 0.1546, d3.loss_bbox: 0.3224, d4.loss_cls: 0.1509, d4.loss_bbox: 0.3268, loss: 3.1233, grad_norm: 19.1092
2025-04-16 18:45:27,065 - mmdet - INFO - Epoch [1][4850/7033]	lr: 1.000e-04, eta: 9:14:30, time: 1.028, data_time: 0.022, memory: 4350, loss_cls: 0.1661, loss_bbox: 0.3465, d0.loss_cls: 0.2533, d0.loss_bbox: 0.4368, d1.loss_cls: 0.1911, d1.loss_bbox: 0.3535, d2.loss_cls: 0.1728, d2.loss_bbox: 0.3352, d3.loss_cls: 0.1681, d3.loss_bbox: 0.3359, d4.loss_cls: 0.1663, d4.loss_bbox: 0.3410, loss: 3.2666, grad_norm: 21.2010
2025-04-16 18:46:18,601 - mmdet - INFO - Epoch [1][4900/7033]	lr: 1.000e-04, eta: 9:14:38, time: 1.031, data_time: 0.022, memory: 4350, loss_cls: 0.1597, loss_bbox: 0.3462, d0.loss_cls: 0.2405, d0.loss_bbox: 0.4452, d1.loss_cls: 0.1822, d1.loss_bbox: 0.3540, d2.loss_cls: 0.1647, d2.loss_bbox: 0.3391, d3.loss_cls: 0.1611, d3.loss_bbox: 0.3373, d4.loss_cls: 0.1588, d4.loss_bbox: 0.3415, loss: 3.2304, grad_norm: 25.2836
2025-04-16 18:47:09,854 - mmdet - INFO - Epoch [1][4950/7033]	lr: 1.000e-04, eta: 9:14:44, time: 1.025, data_time: 0.023, memory: 4350, loss_cls: 0.1710, loss_bbox: 0.3558, d0.loss_cls: 0.2566, d0.loss_bbox: 0.4608, d1.loss_cls: 0.1962, d1.loss_bbox: 0.3648, d2.loss_cls: 0.1780, d2.loss_bbox: 0.3473, d3.loss_cls: 0.1744, d3.loss_bbox: 0.3462, d4.loss_cls: 0.1707, d4.loss_bbox: 0.3499, loss: 3.3716, grad_norm: 25.3842
2025-04-16 18:47:59,303 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 18:47:59,303 - mmdet - INFO - Epoch [1][5000/7033]	lr: 1.000e-04, eta: 9:14:34, time: 0.989, data_time: 0.022, memory: 4350, loss_cls: 0.1476, loss_bbox: 0.3330, d0.loss_cls: 0.2328, d0.loss_bbox: 0.4280, d1.loss_cls: 0.1756, d1.loss_bbox: 0.3363, d2.loss_cls: 0.1577, d2.loss_bbox: 0.3190, d3.loss_cls: 0.1521, d3.loss_bbox: 0.3179, d4.loss_cls: 0.1476, d4.loss_bbox: 0.3262, loss: 3.0737, grad_norm: 24.1508
2025-04-16 18:48:49,252 - mmdet - INFO - Epoch [1][5050/7033]	lr: 1.000e-04, eta: 9:14:28, time: 0.999, data_time: 0.020, memory: 4350, loss_cls: 0.1600, loss_bbox: 0.3311, d0.loss_cls: 0.2536, d0.loss_bbox: 0.4317, d1.loss_cls: 0.1888, d1.loss_bbox: 0.3420, d2.loss_cls: 0.1723, d2.loss_bbox: 0.3269, d3.loss_cls: 0.1660, d3.loss_bbox: 0.3252, d4.loss_cls: 0.1614, d4.loss_bbox: 0.3270, loss: 3.1860, grad_norm: 30.1330
2025-04-16 18:49:40,194 - mmdet - INFO - Epoch [1][5100/7033]	lr: 1.000e-04, eta: 9:14:28, time: 1.019, data_time: 0.022, memory: 4350, loss_cls: 0.1647, loss_bbox: 0.3451, d0.loss_cls: 0.2538, d0.loss_bbox: 0.4542, d1.loss_cls: 0.1950, d1.loss_bbox: 0.3532, d2.loss_cls: 0.1747, d2.loss_bbox: 0.3362, d3.loss_cls: 0.1684, d3.loss_bbox: 0.3341, d4.loss_cls: 0.1668, d4.loss_bbox: 0.3401, loss: 3.2865, grad_norm: 23.9787
2025-04-16 18:50:32,010 - mmdet - INFO - Epoch [1][5150/7033]	lr: 1.000e-04, eta: 9:14:34, time: 1.036, data_time: 0.023, memory: 4350, loss_cls: 0.1677, loss_bbox: 0.3354, d0.loss_cls: 0.2554, d0.loss_bbox: 0.4479, d1.loss_cls: 0.1964, d1.loss_bbox: 0.3499, d2.loss_cls: 0.1754, d2.loss_bbox: 0.3329, d3.loss_cls: 0.1702, d3.loss_bbox: 0.3293, d4.loss_cls: 0.1656, d4.loss_bbox: 0.3329, loss: 3.2590, grad_norm: 19.5209
2025-04-16 18:51:23,909 - mmdet - INFO - Epoch [1][5200/7033]	lr: 1.000e-04, eta: 9:14:38, time: 1.038, data_time: 0.022, memory: 4350, loss_cls: 0.1661, loss_bbox: 0.3414, d0.loss_cls: 0.2503, d0.loss_bbox: 0.4521, d1.loss_cls: 0.1887, d1.loss_bbox: 0.3600, d2.loss_cls: 0.1755, d2.loss_bbox: 0.3372, d3.loss_cls: 0.1681, d3.loss_bbox: 0.3359, d4.loss_cls: 0.1660, d4.loss_bbox: 0.3377, loss: 3.2790, grad_norm: 17.8302
2025-04-16 18:52:15,042 - mmdet - INFO - Epoch [1][5250/7033]	lr: 1.000e-04, eta: 9:14:37, time: 1.023, data_time: 0.023, memory: 4350, loss_cls: 0.1538, loss_bbox: 0.3293, d0.loss_cls: 0.2541, d0.loss_bbox: 0.4275, d1.loss_cls: 0.1843, d1.loss_bbox: 0.3364, d2.loss_cls: 0.1637, d2.loss_bbox: 0.3188, d3.loss_cls: 0.1566, d3.loss_bbox: 0.3188, d4.loss_cls: 0.1533, d4.loss_bbox: 0.3236, loss: 3.1204, grad_norm: 23.5678
2025-04-16 18:53:05,756 - mmdet - INFO - Epoch [1][5300/7033]	lr: 1.000e-04, eta: 9:14:31, time: 1.014, data_time: 0.023, memory: 4350, loss_cls: 0.1533, loss_bbox: 0.3293, d0.loss_cls: 0.2482, d0.loss_bbox: 0.4384, d1.loss_cls: 0.1800, d1.loss_bbox: 0.3438, d2.loss_cls: 0.1614, d2.loss_bbox: 0.3245, d3.loss_cls: 0.1590, d3.loss_bbox: 0.3219, d4.loss_cls: 0.1553, d4.loss_bbox: 0.3233, loss: 3.1385, grad_norm: 25.1917
2025-04-16 18:53:56,805 - mmdet - INFO - Epoch [1][5350/7033]	lr: 1.000e-04, eta: 9:14:27, time: 1.021, data_time: 0.023, memory: 4350, loss_cls: 0.1658, loss_bbox: 0.3532, d0.loss_cls: 0.2521, d0.loss_bbox: 0.4586, d1.loss_cls: 0.1889, d1.loss_bbox: 0.3693, d2.loss_cls: 0.1731, d2.loss_bbox: 0.3513, d3.loss_cls: 0.1667, d3.loss_bbox: 0.3478, d4.loss_cls: 0.1654, d4.loss_bbox: 0.3507, loss: 3.3429, grad_norm: 15.7131
2025-04-16 18:54:46,899 - mmdet - INFO - Epoch [1][5400/7033]	lr: 1.000e-04, eta: 9:14:16, time: 1.002, data_time: 0.022, memory: 4350, loss_cls: 0.1549, loss_bbox: 0.3346, d0.loss_cls: 0.2427, d0.loss_bbox: 0.4423, d1.loss_cls: 0.1802, d1.loss_bbox: 0.3515, d2.loss_cls: 0.1667, d2.loss_bbox: 0.3290, d3.loss_cls: 0.1600, d3.loss_bbox: 0.3270, d4.loss_cls: 0.1555, d4.loss_bbox: 0.3311, loss: 3.1755, grad_norm: 24.2964
2025-04-16 18:55:37,733 - mmdet - INFO - Epoch [1][5450/7033]	lr: 1.000e-04, eta: 9:14:09, time: 1.017, data_time: 0.024, memory: 4350, loss_cls: 0.1551, loss_bbox: 0.3404, d0.loss_cls: 0.2569, d0.loss_bbox: 0.4480, d1.loss_cls: 0.1885, d1.loss_bbox: 0.3529, d2.loss_cls: 0.1649, d2.loss_bbox: 0.3377, d3.loss_cls: 0.1597, d3.loss_bbox: 0.3359, d4.loss_cls: 0.1541, d4.loss_bbox: 0.3371, loss: 3.2312, grad_norm: 20.5412
2025-04-16 18:56:28,402 - mmdet - INFO - Epoch [1][5500/7033]	lr: 1.000e-04, eta: 9:14:00, time: 1.013, data_time: 0.022, memory: 4350, loss_cls: 0.1572, loss_bbox: 0.3228, d0.loss_cls: 0.2430, d0.loss_bbox: 0.4223, d1.loss_cls: 0.1887, d1.loss_bbox: 0.3303, d2.loss_cls: 0.1691, d2.loss_bbox: 0.3140, d3.loss_cls: 0.1618, d3.loss_bbox: 0.3137, d4.loss_cls: 0.1582, d4.loss_bbox: 0.3176, loss: 3.0988, grad_norm: 26.3732
2025-04-16 18:57:17,980 - mmdet - INFO - Epoch [1][5550/7033]	lr: 1.000e-04, eta: 9:13:43, time: 0.992, data_time: 0.022, memory: 4350, loss_cls: 0.1529, loss_bbox: 0.3303, d0.loss_cls: 0.2562, d0.loss_bbox: 0.4423, d1.loss_cls: 0.1894, d1.loss_bbox: 0.3409, d2.loss_cls: 0.1676, d2.loss_bbox: 0.3237, d3.loss_cls: 0.1596, d3.loss_bbox: 0.3238, d4.loss_cls: 0.1549, d4.loss_bbox: 0.3253, loss: 3.1669, grad_norm: 49.6586
2025-04-16 18:58:08,103 - mmdet - INFO - Epoch [1][5600/7033]	lr: 1.000e-04, eta: 9:13:29, time: 1.002, data_time: 0.022, memory: 4350, loss_cls: 0.1596, loss_bbox: 0.3342, d0.loss_cls: 0.2463, d0.loss_bbox: 0.4445, d1.loss_cls: 0.1885, d1.loss_bbox: 0.3495, d2.loss_cls: 0.1704, d2.loss_bbox: 0.3325, d3.loss_cls: 0.1636, d3.loss_bbox: 0.3288, d4.loss_cls: 0.1601, d4.loss_bbox: 0.3311, loss: 3.2090, grad_norm: 24.8277
2025-04-16 18:58:59,338 - mmdet - INFO - Epoch [1][5650/7033]	lr: 1.000e-04, eta: 9:13:21, time: 1.025, data_time: 0.024, memory: 4350, loss_cls: 0.1620, loss_bbox: 0.3419, d0.loss_cls: 0.2510, d0.loss_bbox: 0.4398, d1.loss_cls: 0.1932, d1.loss_bbox: 0.3511, d2.loss_cls: 0.1734, d2.loss_bbox: 0.3332, d3.loss_cls: 0.1693, d3.loss_bbox: 0.3304, d4.loss_cls: 0.1631, d4.loss_bbox: 0.3350, loss: 3.2435, grad_norm: 32.6826
2025-04-16 18:59:50,022 - mmdet - INFO - Epoch [1][5700/7033]	lr: 1.000e-04, eta: 9:13:10, time: 1.014, data_time: 0.023, memory: 4350, loss_cls: 0.1505, loss_bbox: 0.3266, d0.loss_cls: 0.2404, d0.loss_bbox: 0.4297, d1.loss_cls: 0.1800, d1.loss_bbox: 0.3406, d2.loss_cls: 0.1610, d2.loss_bbox: 0.3215, d3.loss_cls: 0.1551, d3.loss_bbox: 0.3196, d4.loss_cls: 0.1512, d4.loss_bbox: 0.3217, loss: 3.0978, grad_norm: 28.7803
2025-04-16 19:00:40,595 - mmdet - INFO - Epoch [1][5750/7033]	lr: 1.000e-04, eta: 9:12:56, time: 1.011, data_time: 0.024, memory: 4350, loss_cls: 0.1428, loss_bbox: 0.3283, d0.loss_cls: 0.2373, d0.loss_bbox: 0.4327, d1.loss_cls: 0.1716, d1.loss_bbox: 0.3395, d2.loss_cls: 0.1547, d2.loss_bbox: 0.3189, d3.loss_cls: 0.1476, d3.loss_bbox: 0.3196, d4.loss_cls: 0.1430, d4.loss_bbox: 0.3246, loss: 3.0606, grad_norm: 17.3588
2025-04-16 19:01:31,006 - mmdet - INFO - Epoch [1][5800/7033]	lr: 1.000e-04, eta: 9:12:42, time: 1.008, data_time: 0.022, memory: 4350, loss_cls: 0.1512, loss_bbox: 0.3238, d0.loss_cls: 0.2452, d0.loss_bbox: 0.4375, d1.loss_cls: 0.1823, d1.loss_bbox: 0.3392, d2.loss_cls: 0.1629, d2.loss_bbox: 0.3204, d3.loss_cls: 0.1562, d3.loss_bbox: 0.3171, d4.loss_cls: 0.1519, d4.loss_bbox: 0.3193, loss: 3.1071, grad_norm: 31.0144
2025-04-16 19:02:21,536 - mmdet - INFO - Epoch [1][5850/7033]	lr: 1.000e-04, eta: 9:12:27, time: 1.011, data_time: 0.023, memory: 4350, loss_cls: 0.1520, loss_bbox: 0.3202, d0.loss_cls: 0.2518, d0.loss_bbox: 0.4343, d1.loss_cls: 0.1888, d1.loss_bbox: 0.3414, d2.loss_cls: 0.1663, d2.loss_bbox: 0.3197, d3.loss_cls: 0.1590, d3.loss_bbox: 0.3162, d4.loss_cls: 0.1521, d4.loss_bbox: 0.3177, loss: 3.1193, grad_norm: 20.5427
2025-04-16 19:03:12,940 - mmdet - INFO - Epoch [1][5900/7033]	lr: 1.000e-04, eta: 9:12:17, time: 1.028, data_time: 0.022, memory: 4350, loss_cls: 0.1568, loss_bbox: 0.3307, d0.loss_cls: 0.2568, d0.loss_bbox: 0.4450, d1.loss_cls: 0.1903, d1.loss_bbox: 0.3436, d2.loss_cls: 0.1693, d2.loss_bbox: 0.3263, d3.loss_cls: 0.1631, d3.loss_bbox: 0.3221, d4.loss_cls: 0.1584, d4.loss_bbox: 0.3264, loss: 3.1887, grad_norm: 32.3396
2025-04-16 19:04:03,482 - mmdet - INFO - Epoch [1][5950/7033]	lr: 1.000e-04, eta: 9:12:01, time: 1.011, data_time: 0.022, memory: 4350, loss_cls: 0.1516, loss_bbox: 0.3246, d0.loss_cls: 0.2421, d0.loss_bbox: 0.4347, d1.loss_cls: 0.1808, d1.loss_bbox: 0.3412, d2.loss_cls: 0.1658, d2.loss_bbox: 0.3193, d3.loss_cls: 0.1564, d3.loss_bbox: 0.3179, d4.loss_cls: 0.1534, d4.loss_bbox: 0.3195, loss: 3.1074, grad_norm: 24.9063
2025-04-16 19:04:54,855 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 19:04:54,855 - mmdet - INFO - Epoch [1][6000/7033]	lr: 1.000e-04, eta: 9:11:50, time: 1.028, data_time: 0.024, memory: 4350, loss_cls: 0.1535, loss_bbox: 0.3324, d0.loss_cls: 0.2482, d0.loss_bbox: 0.4343, d1.loss_cls: 0.1816, d1.loss_bbox: 0.3483, d2.loss_cls: 0.1620, d2.loss_bbox: 0.3270, d3.loss_cls: 0.1581, d3.loss_bbox: 0.3232, d4.loss_cls: 0.1557, d4.loss_bbox: 0.3269, loss: 3.1512, grad_norm: 30.0918
2025-04-16 19:05:44,604 - mmdet - INFO - Epoch [1][6050/7033]	lr: 1.000e-04, eta: 9:11:28, time: 0.995, data_time: 0.023, memory: 4350, loss_cls: 0.1491, loss_bbox: 0.3088, d0.loss_cls: 0.2399, d0.loss_bbox: 0.4248, d1.loss_cls: 0.1786, d1.loss_bbox: 0.3338, d2.loss_cls: 0.1606, d2.loss_bbox: 0.3119, d3.loss_cls: 0.1511, d3.loss_bbox: 0.3082, d4.loss_cls: 0.1488, d4.loss_bbox: 0.3080, loss: 3.0236, grad_norm: 116.5045
2025-04-16 19:06:35,918 - mmdet - INFO - Epoch [1][6100/7033]	lr: 1.000e-04, eta: 9:11:15, time: 1.026, data_time: 0.024, memory: 4350, loss_cls: 0.1566, loss_bbox: 0.3161, d0.loss_cls: 0.2461, d0.loss_bbox: 0.4239, d1.loss_cls: 0.1879, d1.loss_bbox: 0.3294, d2.loss_cls: 0.1684, d2.loss_bbox: 0.3110, d3.loss_cls: 0.1602, d3.loss_bbox: 0.3102, d4.loss_cls: 0.1577, d4.loss_bbox: 0.3114, loss: 3.0788, grad_norm: 32.0313
2025-04-16 19:07:26,362 - mmdet - INFO - Epoch [1][6150/7033]	lr: 1.000e-04, eta: 9:10:57, time: 1.009, data_time: 0.022, memory: 4350, loss_cls: 0.1500, loss_bbox: 0.3300, d0.loss_cls: 0.2465, d0.loss_bbox: 0.4346, d1.loss_cls: 0.1831, d1.loss_bbox: 0.3414, d2.loss_cls: 0.1582, d2.loss_bbox: 0.3248, d3.loss_cls: 0.1533, d3.loss_bbox: 0.3229, d4.loss_cls: 0.1505, d4.loss_bbox: 0.3263, loss: 3.1218, grad_norm: 21.9680
2025-04-16 19:08:05,694 - mmdet - INFO - Epoch [1][6200/7033]	lr: 1.000e-04, eta: 9:09:33, time: 0.787, data_time: 0.024, memory: 4350, loss_cls: 0.1480, loss_bbox: 0.3146, d0.loss_cls: 0.2445, d0.loss_bbox: 0.4296, d1.loss_cls: 0.1812, d1.loss_bbox: 0.3342, d2.loss_cls: 0.1613, d2.loss_bbox: 0.3131, d3.loss_cls: 0.1541, d3.loss_bbox: 0.3079, d4.loss_cls: 0.1492, d4.loss_bbox: 0.3111, loss: 3.0488, grad_norm: 28.7778
2025-04-16 19:08:44,969 - mmdet - INFO - Epoch [1][6250/7033]	lr: 1.000e-04, eta: 9:08:10, time: 0.786, data_time: 0.023, memory: 4350, loss_cls: 0.1462, loss_bbox: 0.3192, d0.loss_cls: 0.2480, d0.loss_bbox: 0.4300, d1.loss_cls: 0.1804, d1.loss_bbox: 0.3362, d2.loss_cls: 0.1572, d2.loss_bbox: 0.3171, d3.loss_cls: 0.1495, d3.loss_bbox: 0.3156, d4.loss_cls: 0.1459, d4.loss_bbox: 0.3174, loss: 3.0627, grad_norm: 34.5700
2025-04-16 19:09:24,106 - mmdet - INFO - Epoch [1][6300/7033]	lr: 1.000e-04, eta: 9:06:46, time: 0.783, data_time: 0.022, memory: 4350, loss_cls: 0.1519, loss_bbox: 0.3368, d0.loss_cls: 0.2412, d0.loss_bbox: 0.4506, d1.loss_cls: 0.1825, d1.loss_bbox: 0.3513, d2.loss_cls: 0.1594, d2.loss_bbox: 0.3313, d3.loss_cls: 0.1550, d3.loss_bbox: 0.3292, d4.loss_cls: 0.1526, d4.loss_bbox: 0.3327, loss: 3.1744, grad_norm: 20.2223
2025-04-16 19:10:03,028 - mmdet - INFO - Epoch [1][6350/7033]	lr: 1.000e-04, eta: 9:05:22, time: 0.778, data_time: 0.020, memory: 4350, loss_cls: 0.1583, loss_bbox: 0.3178, d0.loss_cls: 0.2510, d0.loss_bbox: 0.4367, d1.loss_cls: 0.1925, d1.loss_bbox: 0.3355, d2.loss_cls: 0.1689, d2.loss_bbox: 0.3177, d3.loss_cls: 0.1621, d3.loss_bbox: 0.3164, d4.loss_cls: 0.1594, d4.loss_bbox: 0.3145, loss: 3.1306, grad_norm: 23.0683
2025-04-16 19:10:42,083 - mmdet - INFO - Epoch [1][6400/7033]	lr: 1.000e-04, eta: 9:04:00, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.1638, loss_bbox: 0.3285, d0.loss_cls: 0.2571, d0.loss_bbox: 0.4382, d1.loss_cls: 0.1989, d1.loss_bbox: 0.3458, d2.loss_cls: 0.1758, d2.loss_bbox: 0.3264, d3.loss_cls: 0.1691, d3.loss_bbox: 0.3220, d4.loss_cls: 0.1642, d4.loss_bbox: 0.3252, loss: 3.2151, grad_norm: 34.5718
2025-04-16 19:11:21,241 - mmdet - INFO - Epoch [1][6450/7033]	lr: 1.000e-04, eta: 9:02:39, time: 0.783, data_time: 0.023, memory: 4350, loss_cls: 0.1501, loss_bbox: 0.3128, d0.loss_cls: 0.2359, d0.loss_bbox: 0.4211, d1.loss_cls: 0.1809, d1.loss_bbox: 0.3265, d2.loss_cls: 0.1588, d2.loss_bbox: 0.3091, d3.loss_cls: 0.1536, d3.loss_bbox: 0.3082, d4.loss_cls: 0.1500, d4.loss_bbox: 0.3096, loss: 3.0165, grad_norm: 23.4773
2025-04-16 19:12:00,197 - mmdet - INFO - Epoch [1][6500/7033]	lr: 1.000e-04, eta: 9:01:17, time: 0.779, data_time: 0.020, memory: 4350, loss_cls: 0.1633, loss_bbox: 0.3185, d0.loss_cls: 0.2567, d0.loss_bbox: 0.4378, d1.loss_cls: 0.1969, d1.loss_bbox: 0.3357, d2.loss_cls: 0.1765, d2.loss_bbox: 0.3164, d3.loss_cls: 0.1677, d3.loss_bbox: 0.3146, d4.loss_cls: 0.1643, d4.loss_bbox: 0.3168, loss: 3.1651, grad_norm: 22.4803
2025-04-16 19:12:39,216 - mmdet - INFO - Epoch [1][6550/7033]	lr: 1.000e-04, eta: 8:59:56, time: 0.780, data_time: 0.020, memory: 4350, loss_cls: 0.1530, loss_bbox: 0.3290, d0.loss_cls: 0.2463, d0.loss_bbox: 0.4421, d1.loss_cls: 0.1832, d1.loss_bbox: 0.3396, d2.loss_cls: 0.1659, d2.loss_bbox: 0.3219, d3.loss_cls: 0.1599, d3.loss_bbox: 0.3177, d4.loss_cls: 0.1531, d4.loss_bbox: 0.3236, loss: 3.1353, grad_norm: 26.7015
2025-04-16 19:13:18,157 - mmdet - INFO - Epoch [1][6600/7033]	lr: 1.000e-04, eta: 8:58:36, time: 0.779, data_time: 0.024, memory: 4350, loss_cls: 0.1473, loss_bbox: 0.3112, d0.loss_cls: 0.2395, d0.loss_bbox: 0.4173, d1.loss_cls: 0.1790, d1.loss_bbox: 0.3260, d2.loss_cls: 0.1564, d2.loss_bbox: 0.3059, d3.loss_cls: 0.1508, d3.loss_bbox: 0.3050, d4.loss_cls: 0.1486, d4.loss_bbox: 0.3082, loss: 2.9951, grad_norm: 20.4986
2025-04-16 19:13:57,135 - mmdet - INFO - Epoch [1][6650/7033]	lr: 1.000e-04, eta: 8:57:16, time: 0.780, data_time: 0.019, memory: 4350, loss_cls: 0.1464, loss_bbox: 0.3293, d0.loss_cls: 0.2402, d0.loss_bbox: 0.4339, d1.loss_cls: 0.1731, d1.loss_bbox: 0.3404, d2.loss_cls: 0.1564, d2.loss_bbox: 0.3217, d3.loss_cls: 0.1496, d3.loss_bbox: 0.3210, d4.loss_cls: 0.1450, d4.loss_bbox: 0.3252, loss: 3.0823, grad_norm: 16.8604
2025-04-16 19:14:35,925 - mmdet - INFO - Epoch [1][6700/7033]	lr: 1.000e-04, eta: 8:55:56, time: 0.776, data_time: 0.019, memory: 4350, loss_cls: 0.1452, loss_bbox: 0.3160, d0.loss_cls: 0.2458, d0.loss_bbox: 0.4180, d1.loss_cls: 0.1759, d1.loss_bbox: 0.3334, d2.loss_cls: 0.1563, d2.loss_bbox: 0.3108, d3.loss_cls: 0.1507, d3.loss_bbox: 0.3097, d4.loss_cls: 0.1466, d4.loss_bbox: 0.3126, loss: 3.0210, grad_norm: 21.4686
2025-04-16 19:15:15,277 - mmdet - INFO - Epoch [1][6750/7033]	lr: 1.000e-04, eta: 8:54:39, time: 0.787, data_time: 0.023, memory: 4350, loss_cls: 0.1511, loss_bbox: 0.3158, d0.loss_cls: 0.2446, d0.loss_bbox: 0.4318, d1.loss_cls: 0.1821, d1.loss_bbox: 0.3379, d2.loss_cls: 0.1633, d2.loss_bbox: 0.3158, d3.loss_cls: 0.1566, d3.loss_bbox: 0.3125, d4.loss_cls: 0.1511, d4.loss_bbox: 0.3134, loss: 3.0759, grad_norm: 22.3632
2025-04-16 19:15:54,357 - mmdet - INFO - Epoch [1][6800/7033]	lr: 1.000e-04, eta: 8:53:22, time: 0.782, data_time: 0.022, memory: 4350, loss_cls: 0.1560, loss_bbox: 0.3215, d0.loss_cls: 0.2520, d0.loss_bbox: 0.4299, d1.loss_cls: 0.1865, d1.loss_bbox: 0.3350, d2.loss_cls: 0.1652, d2.loss_bbox: 0.3187, d3.loss_cls: 0.1601, d3.loss_bbox: 0.3146, d4.loss_cls: 0.1573, d4.loss_bbox: 0.3166, loss: 3.1135, grad_norm: 23.8352
2025-04-16 19:16:33,427 - mmdet - INFO - Epoch [1][6850/7033]	lr: 1.000e-04, eta: 8:52:05, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.1423, loss_bbox: 0.3090, d0.loss_cls: 0.2317, d0.loss_bbox: 0.4187, d1.loss_cls: 0.1693, d1.loss_bbox: 0.3263, d2.loss_cls: 0.1519, d2.loss_bbox: 0.3072, d3.loss_cls: 0.1472, d3.loss_bbox: 0.3050, d4.loss_cls: 0.1426, d4.loss_bbox: 0.3077, loss: 2.9588, grad_norm: 30.0259
2025-04-16 19:17:12,326 - mmdet - INFO - Epoch [1][6900/7033]	lr: 1.000e-04, eta: 8:50:48, time: 0.778, data_time: 0.022, memory: 4350, loss_cls: 0.1486, loss_bbox: 0.3227, d0.loss_cls: 0.2400, d0.loss_bbox: 0.4435, d1.loss_cls: 0.1773, d1.loss_bbox: 0.3471, d2.loss_cls: 0.1576, d2.loss_bbox: 0.3260, d3.loss_cls: 0.1518, d3.loss_bbox: 0.3217, d4.loss_cls: 0.1484, d4.loss_bbox: 0.3214, loss: 3.1059, grad_norm: 25.4600
2025-04-16 19:17:51,557 - mmdet - INFO - Epoch [1][6950/7033]	lr: 1.000e-04, eta: 8:49:33, time: 0.785, data_time: 0.020, memory: 4350, loss_cls: 0.1402, loss_bbox: 0.3098, d0.loss_cls: 0.2348, d0.loss_bbox: 0.4270, d1.loss_cls: 0.1730, d1.loss_bbox: 0.3308, d2.loss_cls: 0.1527, d2.loss_bbox: 0.3121, d3.loss_cls: 0.1445, d3.loss_bbox: 0.3072, d4.loss_cls: 0.1418, d4.loss_bbox: 0.3087, loss: 2.9827, grad_norm: 20.8103
2025-04-16 19:18:30,725 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 19:18:30,726 - mmdet - INFO - Epoch [1][7000/7033]	lr: 1.000e-04, eta: 8:48:18, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1479, loss_bbox: 0.3133, d0.loss_cls: 0.2441, d0.loss_bbox: 0.4281, d1.loss_cls: 0.1796, d1.loss_bbox: 0.3303, d2.loss_cls: 0.1589, d2.loss_bbox: 0.3122, d3.loss_cls: 0.1519, d3.loss_bbox: 0.3101, d4.loss_cls: 0.1481, d4.loss_bbox: 0.3124, loss: 3.0369, grad_norm: 31.5139
2025-04-16 19:18:56,494 - mmdet - INFO - Saving checkpoint at 1 epochs
2025-04-16 19:40:33,590 - mmdet - INFO - Exp name: lidar_0075v_cam_res101.py
2025-04-16 19:40:33,590 - mmdet - INFO - Epoch(val) [1][3010]	pts_bbox_NuScenes/car_AP_dist_0.5: 0.7417, pts_bbox_NuScenes/car_AP_dist_1.0: 0.8590, pts_bbox_NuScenes/car_AP_dist_2.0: 0.9010, pts_bbox_NuScenes/car_AP_dist_4.0: 0.9213, pts_bbox_NuScenes/car_trans_err: 0.2080, pts_bbox_NuScenes/car_scale_err: 0.1492, pts_bbox_NuScenes/car_orient_err: 0.0588, pts_bbox_NuScenes/car_vel_err: 0.3884, pts_bbox_NuScenes/car_attr_err: 0.1911, pts_bbox_NuScenes/mATE: 0.3607, pts_bbox_NuScenes/mASE: 0.2676, pts_bbox_NuScenes/mAOE: 0.2957, pts_bbox_NuScenes/mAVE: 0.4157, pts_bbox_NuScenes/mAAE: 0.1802, pts_bbox_NuScenes/truck_AP_dist_0.5: 0.3646, pts_bbox_NuScenes/truck_AP_dist_1.0: 0.6100, pts_bbox_NuScenes/truck_AP_dist_2.0: 0.7120, pts_bbox_NuScenes/truck_AP_dist_4.0: 0.7617, pts_bbox_NuScenes/truck_trans_err: 0.3801, pts_bbox_NuScenes/truck_scale_err: 0.2177, pts_bbox_NuScenes/truck_orient_err: 0.0743, pts_bbox_NuScenes/truck_vel_err: 0.4098, pts_bbox_NuScenes/truck_attr_err: 0.2347, pts_bbox_NuScenes/construction_vehicle_AP_dist_0.5: 0.0147, pts_bbox_NuScenes/construction_vehicle_AP_dist_1.0: 0.1599, pts_bbox_NuScenes/construction_vehicle_AP_dist_2.0: 0.4566, pts_bbox_NuScenes/construction_vehicle_AP_dist_4.0: 0.5274, pts_bbox_NuScenes/construction_vehicle_trans_err: 0.8262, pts_bbox_NuScenes/construction_vehicle_scale_err: 0.4809, pts_bbox_NuScenes/construction_vehicle_orient_err: 0.8635, pts_bbox_NuScenes/construction_vehicle_vel_err: 0.1189, pts_bbox_NuScenes/construction_vehicle_attr_err: 0.3106, pts_bbox_NuScenes/bus_AP_dist_0.5: 0.4222, pts_bbox_NuScenes/bus_AP_dist_1.0: 0.7170, pts_bbox_NuScenes/bus_AP_dist_2.0: 0.8621, pts_bbox_NuScenes/bus_AP_dist_4.0: 0.9008, pts_bbox_NuScenes/bus_trans_err: 0.3931, pts_bbox_NuScenes/bus_scale_err: 0.1981, pts_bbox_NuScenes/bus_orient_err: 0.0548, pts_bbox_NuScenes/bus_vel_err: 0.7520, pts_bbox_NuScenes/bus_attr_err: 0.2383, pts_bbox_NuScenes/trailer_AP_dist_0.5: 0.1059, pts_bbox_NuScenes/trailer_AP_dist_1.0: 0.3260, pts_bbox_NuScenes/trailer_AP_dist_2.0: 0.5269, pts_bbox_NuScenes/trailer_AP_dist_4.0: 0.6684, pts_bbox_NuScenes/trailer_trans_err: 0.6211, pts_bbox_NuScenes/trailer_scale_err: 0.2328, pts_bbox_NuScenes/trailer_orient_err: 0.4870, pts_bbox_NuScenes/trailer_vel_err: 0.2892, pts_bbox_NuScenes/trailer_attr_err: 0.1543, pts_bbox_NuScenes/barrier_AP_dist_0.5: 0.5061, pts_bbox_NuScenes/barrier_AP_dist_1.0: 0.6488, pts_bbox_NuScenes/barrier_AP_dist_2.0: 0.7304, pts_bbox_NuScenes/barrier_AP_dist_4.0: 0.7553, pts_bbox_NuScenes/barrier_trans_err: 0.3120, pts_bbox_NuScenes/barrier_scale_err: 0.2831, pts_bbox_NuScenes/barrier_orient_err: 0.1443, pts_bbox_NuScenes/barrier_vel_err: nan, pts_bbox_NuScenes/barrier_attr_err: nan, pts_bbox_NuScenes/motorcycle_AP_dist_0.5: 0.5843, pts_bbox_NuScenes/motorcycle_AP_dist_1.0: 0.7565, pts_bbox_NuScenes/motorcycle_AP_dist_2.0: 0.8102, pts_bbox_NuScenes/motorcycle_AP_dist_4.0: 0.8221, pts_bbox_NuScenes/motorcycle_trans_err: 0.2749, pts_bbox_NuScenes/motorcycle_scale_err: 0.2458, pts_bbox_NuScenes/motorcycle_orient_err: 0.2697, pts_bbox_NuScenes/motorcycle_vel_err: 0.8536, pts_bbox_NuScenes/motorcycle_attr_err: 0.1801, pts_bbox_NuScenes/bicycle_AP_dist_0.5: 0.5900, pts_bbox_NuScenes/bicycle_AP_dist_1.0: 0.6584, pts_bbox_NuScenes/bicycle_AP_dist_2.0: 0.6772, pts_bbox_NuScenes/bicycle_AP_dist_4.0: 0.6876, pts_bbox_NuScenes/bicycle_trans_err: 0.2055, pts_bbox_NuScenes/bicycle_scale_err: 0.2672, pts_bbox_NuScenes/bicycle_orient_err: 0.3848, pts_bbox_NuScenes/bicycle_vel_err: 0.2544, pts_bbox_NuScenes/bicycle_attr_err: 0.0122, pts_bbox_NuScenes/pedestrian_AP_dist_0.5: 0.7616, pts_bbox_NuScenes/pedestrian_AP_dist_1.0: 0.8358, pts_bbox_NuScenes/pedestrian_AP_dist_2.0: 0.8702, pts_bbox_NuScenes/pedestrian_AP_dist_4.0: 0.8875, pts_bbox_NuScenes/pedestrian_trans_err: 0.1963, pts_bbox_NuScenes/pedestrian_scale_err: 0.2912, pts_bbox_NuScenes/pedestrian_orient_err: 0.3242, pts_bbox_NuScenes/pedestrian_vel_err: 0.2594, pts_bbox_NuScenes/pedestrian_attr_err: 0.1206, pts_bbox_NuScenes/traffic_cone_AP_dist_0.5: 0.6814, pts_bbox_NuScenes/traffic_cone_AP_dist_1.0: 0.7404, pts_bbox_NuScenes/traffic_cone_AP_dist_2.0: 0.7819, pts_bbox_NuScenes/traffic_cone_AP_dist_4.0: 0.8161, pts_bbox_NuScenes/traffic_cone_trans_err: 0.1901, pts_bbox_NuScenes/traffic_cone_scale_err: 0.3103, pts_bbox_NuScenes/traffic_cone_orient_err: nan, pts_bbox_NuScenes/traffic_cone_vel_err: nan, pts_bbox_NuScenes/traffic_cone_attr_err: nan, pts_bbox_NuScenes/NDS: 0.6750, pts_bbox_NuScenes/mAP: 0.6540
2025-04-16 19:41:18,056 - mmdet - INFO - Epoch [2][50/7033]	lr: 9.331e-05, eta: 8:44:29, time: 0.873, data_time: 0.113, memory: 4350, loss_cls: 0.1447, loss_bbox: 0.3099, d0.loss_cls: 0.2348, d0.loss_bbox: 0.4212, d1.loss_cls: 0.1751, d1.loss_bbox: 0.3325, d2.loss_cls: 0.1556, d2.loss_bbox: 0.3112, d3.loss_cls: 0.1489, d3.loss_bbox: 0.3086, d4.loss_cls: 0.1452, d4.loss_bbox: 0.3081, loss: 2.9958, grad_norm: 30.9585
2025-04-16 19:41:57,139 - mmdet - INFO - Epoch [2][100/7033]	lr: 9.331e-05, eta: 8:43:16, time: 0.782, data_time: 0.022, memory: 4350, loss_cls: 0.1489, loss_bbox: 0.3175, d0.loss_cls: 0.2512, d0.loss_bbox: 0.4299, d1.loss_cls: 0.1825, d1.loss_bbox: 0.3358, d2.loss_cls: 0.1617, d2.loss_bbox: 0.3166, d3.loss_cls: 0.1561, d3.loss_bbox: 0.3118, d4.loss_cls: 0.1492, d4.loss_bbox: 0.3150, loss: 3.0762, grad_norm: 34.2931
2025-04-16 19:42:36,377 - mmdet - INFO - Epoch [2][150/7033]	lr: 9.331e-05, eta: 8:42:05, time: 0.785, data_time: 0.021, memory: 4350, loss_cls: 0.1360, loss_bbox: 0.3061, d0.loss_cls: 0.2278, d0.loss_bbox: 0.4200, d1.loss_cls: 0.1675, d1.loss_bbox: 0.3247, d2.loss_cls: 0.1504, d2.loss_bbox: 0.3077, d3.loss_cls: 0.1444, d3.loss_bbox: 0.3030, d4.loss_cls: 0.1371, d4.loss_bbox: 0.3060, loss: 2.9309, grad_norm: 27.0588
2025-04-16 19:43:15,439 - mmdet - INFO - Epoch [2][200/7033]	lr: 9.331e-05, eta: 8:40:52, time: 0.781, data_time: 0.022, memory: 4350, loss_cls: 0.1538, loss_bbox: 0.3221, d0.loss_cls: 0.2455, d0.loss_bbox: 0.4449, d1.loss_cls: 0.1897, d1.loss_bbox: 0.3461, d2.loss_cls: 0.1675, d2.loss_bbox: 0.3256, d3.loss_cls: 0.1597, d3.loss_bbox: 0.3210, d4.loss_cls: 0.1548, d4.loss_bbox: 0.3222, loss: 3.1530, grad_norm: 20.5143
2025-04-16 19:43:54,446 - mmdet - INFO - Epoch [2][250/7033]	lr: 9.331e-05, eta: 8:39:41, time: 0.780, data_time: 0.022, memory: 4350, loss_cls: 0.1467, loss_bbox: 0.3093, d0.loss_cls: 0.2374, d0.loss_bbox: 0.4175, d1.loss_cls: 0.1797, d1.loss_bbox: 0.3275, d2.loss_cls: 0.1617, d2.loss_bbox: 0.3087, d3.loss_cls: 0.1521, d3.loss_bbox: 0.3070, d4.loss_cls: 0.1485, d4.loss_bbox: 0.3085, loss: 3.0045, grad_norm: 25.6997
2025-04-16 19:44:33,712 - mmdet - INFO - Epoch [2][300/7033]	lr: 9.331e-05, eta: 8:38:30, time: 0.785, data_time: 0.021, memory: 4350, loss_cls: 0.1326, loss_bbox: 0.3085, d0.loss_cls: 0.2339, d0.loss_bbox: 0.4199, d1.loss_cls: 0.1683, d1.loss_bbox: 0.3286, d2.loss_cls: 0.1454, d2.loss_bbox: 0.3107, d3.loss_cls: 0.1371, d3.loss_bbox: 0.3099, d4.loss_cls: 0.1341, d4.loss_bbox: 0.3082, loss: 2.9373, grad_norm: 28.1408
2025-04-16 19:45:12,903 - mmdet - INFO - Epoch [2][350/7033]	lr: 9.331e-05, eta: 8:37:20, time: 0.784, data_time: 0.022, memory: 4350, loss_cls: 0.1450, loss_bbox: 0.3230, d0.loss_cls: 0.2437, d0.loss_bbox: 0.4369, d1.loss_cls: 0.1800, d1.loss_bbox: 0.3415, d2.loss_cls: 0.1604, d2.loss_bbox: 0.3224, d3.loss_cls: 0.1521, d3.loss_bbox: 0.3201, d4.loss_cls: 0.1474, d4.loss_bbox: 0.3214, loss: 3.0938, grad_norm: 16.2363
2025-04-16 19:45:52,117 - mmdet - INFO - Epoch [2][400/7033]	lr: 9.331e-05, eta: 8:36:10, time: 0.785, data_time: 0.022, memory: 4350, loss_cls: 0.1431, loss_bbox: 0.3052, d0.loss_cls: 0.2380, d0.loss_bbox: 0.4134, d1.loss_cls: 0.1805, d1.loss_bbox: 0.3284, d2.loss_cls: 0.1571, d2.loss_bbox: 0.3068, d3.loss_cls: 0.1476, d3.loss_bbox: 0.3045, d4.loss_cls: 0.1449, d4.loss_bbox: 0.3044, loss: 2.9738, grad_norm: 35.9562
2025-04-16 19:46:31,369 - mmdet - INFO - Epoch [2][450/7033]	lr: 9.331e-05, eta: 8:35:01, time: 0.785, data_time: 0.022, memory: 4350, loss_cls: 0.1501, loss_bbox: 0.3201, d0.loss_cls: 0.2500, d0.loss_bbox: 0.4376, d1.loss_cls: 0.1838, d1.loss_bbox: 0.3434, d2.loss_cls: 0.1614, d2.loss_bbox: 0.3221, d3.loss_cls: 0.1540, d3.loss_bbox: 0.3187, d4.loss_cls: 0.1509, d4.loss_bbox: 0.3203, loss: 3.1126, grad_norm: 21.5787
2025-04-16 19:47:10,630 - mmdet - INFO - Epoch [2][500/7033]	lr: 9.331e-05, eta: 8:33:53, time: 0.785, data_time: 0.022, memory: 4350, loss_cls: 0.1438, loss_bbox: 0.3068, d0.loss_cls: 0.2430, d0.loss_bbox: 0.4166, d1.loss_cls: 0.1759, d1.loss_bbox: 0.3247, d2.loss_cls: 0.1556, d2.loss_bbox: 0.3038, d3.loss_cls: 0.1486, d3.loss_bbox: 0.3018, d4.loss_cls: 0.1453, d4.loss_bbox: 0.3054, loss: 2.9713, grad_norm: 46.7484
2025-04-16 19:47:49,660 - mmdet - INFO - Epoch [2][550/7033]	lr: 9.331e-05, eta: 8:32:43, time: 0.781, data_time: 0.022, memory: 4350, loss_cls: 0.1481, loss_bbox: 0.3146, d0.loss_cls: 0.2505, d0.loss_bbox: 0.4397, d1.loss_cls: 0.1787, d1.loss_bbox: 0.3418, d2.loss_cls: 0.1620, d2.loss_bbox: 0.3172, d3.loss_cls: 0.1540, d3.loss_bbox: 0.3139, d4.loss_cls: 0.1492, d4.loss_bbox: 0.3156, loss: 3.0852, grad_norm: 33.3351
2025-04-16 19:48:28,786 - mmdet - INFO - Epoch [2][600/7033]	lr: 9.331e-05, eta: 8:31:35, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1420, loss_bbox: 0.3143, d0.loss_cls: 0.2409, d0.loss_bbox: 0.4308, d1.loss_cls: 0.1743, d1.loss_bbox: 0.3348, d2.loss_cls: 0.1534, d2.loss_bbox: 0.3177, d3.loss_cls: 0.1455, d3.loss_bbox: 0.3147, d4.loss_cls: 0.1430, d4.loss_bbox: 0.3151, loss: 3.0266, grad_norm: 27.9080
2025-04-16 19:49:08,144 - mmdet - INFO - Epoch [2][650/7033]	lr: 9.331e-05, eta: 8:30:28, time: 0.787, data_time: 0.022, memory: 4350, loss_cls: 0.1538, loss_bbox: 0.3217, d0.loss_cls: 0.2401, d0.loss_bbox: 0.4415, d1.loss_cls: 0.1798, d1.loss_bbox: 0.3442, d2.loss_cls: 0.1633, d2.loss_bbox: 0.3233, d3.loss_cls: 0.1567, d3.loss_bbox: 0.3214, d4.loss_cls: 0.1538, d4.loss_bbox: 0.3231, loss: 3.1228, grad_norm: 26.1418
2025-04-16 19:49:47,159 - mmdet - INFO - Epoch [2][700/7033]	lr: 9.331e-05, eta: 8:29:20, time: 0.780, data_time: 0.022, memory: 4350, loss_cls: 0.1524, loss_bbox: 0.3249, d0.loss_cls: 0.2471, d0.loss_bbox: 0.4401, d1.loss_cls: 0.1852, d1.loss_bbox: 0.3442, d2.loss_cls: 0.1651, d2.loss_bbox: 0.3255, d3.loss_cls: 0.1560, d3.loss_bbox: 0.3236, d4.loss_cls: 0.1528, d4.loss_bbox: 0.3247, loss: 3.1418, grad_norm: 21.3639
2025-04-16 19:50:26,418 - mmdet - INFO - Epoch [2][750/7033]	lr: 9.331e-05, eta: 8:28:13, time: 0.785, data_time: 0.025, memory: 4350, loss_cls: 0.1484, loss_bbox: 0.3123, d0.loss_cls: 0.2487, d0.loss_bbox: 0.4356, d1.loss_cls: 0.1784, d1.loss_bbox: 0.3386, d2.loss_cls: 0.1609, d2.loss_bbox: 0.3167, d3.loss_cls: 0.1527, d3.loss_bbox: 0.3123, d4.loss_cls: 0.1491, d4.loss_bbox: 0.3140, loss: 3.0678, grad_norm: 24.2509
2025-04-16 19:51:05,703 - mmdet - INFO - Epoch [2][800/7033]	lr: 9.331e-05, eta: 8:27:06, time: 0.786, data_time: 0.021, memory: 4350, loss_cls: 0.1428, loss_bbox: 0.3014, d0.loss_cls: 0.2386, d0.loss_bbox: 0.4093, d1.loss_cls: 0.1771, d1.loss_bbox: 0.3256, d2.loss_cls: 0.1547, d2.loss_bbox: 0.3049, d3.loss_cls: 0.1477, d3.loss_bbox: 0.3011, d4.loss_cls: 0.1429, d4.loss_bbox: 0.3027, loss: 2.9488, grad_norm: 36.7009
2025-04-16 19:51:44,826 - mmdet - INFO - Epoch [2][850/7033]	lr: 9.331e-05, eta: 8:26:00, time: 0.782, data_time: 0.021, memory: 4350, loss_cls: 0.1470, loss_bbox: 0.3031, d0.loss_cls: 0.2439, d0.loss_bbox: 0.4129, d1.loss_cls: 0.1806, d1.loss_bbox: 0.3280, d2.loss_cls: 0.1617, d2.loss_bbox: 0.3078, d3.loss_cls: 0.1534, d3.loss_bbox: 0.3026, d4.loss_cls: 0.1478, d4.loss_bbox: 0.3045, loss: 2.9932, grad_norm: 25.2609
2025-04-16 19:52:23,968 - mmdet - INFO - Epoch [2][900/7033]	lr: 9.331e-05, eta: 8:24:54, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1427, loss_bbox: 0.3060, d0.loss_cls: 0.2384, d0.loss_bbox: 0.4218, d1.loss_cls: 0.1779, d1.loss_bbox: 0.3309, d2.loss_cls: 0.1576, d2.loss_bbox: 0.3067, d3.loss_cls: 0.1491, d3.loss_bbox: 0.3050, d4.loss_cls: 0.1441, d4.loss_bbox: 0.3080, loss: 2.9884, grad_norm: 19.5537
2025-04-16 19:53:03,266 - mmdet - INFO - Epoch [2][950/7033]	lr: 9.331e-05, eta: 8:23:48, time: 0.786, data_time: 0.021, memory: 4350, loss_cls: 0.1380, loss_bbox: 0.2911, d0.loss_cls: 0.2383, d0.loss_bbox: 0.4119, d1.loss_cls: 0.1727, d1.loss_bbox: 0.3181, d2.loss_cls: 0.1510, d2.loss_bbox: 0.2963, d3.loss_cls: 0.1404, d3.loss_bbox: 0.2932, d4.loss_cls: 0.1379, d4.loss_bbox: 0.2946, loss: 2.8835, grad_norm: 30.0225
2025-04-16 19:53:42,379 - mmdet - INFO - Epoch [2][1000/7033]	lr: 9.331e-05, eta: 8:22:43, time: 0.782, data_time: 0.021, memory: 4350, loss_cls: 0.1433, loss_bbox: 0.2963, d0.loss_cls: 0.2340, d0.loss_bbox: 0.4207, d1.loss_cls: 0.1730, d1.loss_bbox: 0.3259, d2.loss_cls: 0.1563, d2.loss_bbox: 0.3039, d3.loss_cls: 0.1462, d3.loss_bbox: 0.2996, d4.loss_cls: 0.1445, d4.loss_bbox: 0.2998, loss: 2.9434, grad_norm: 32.5737
2025-04-16 19:54:21,618 - mmdet - INFO - Epoch [2][1050/7033]	lr: 9.331e-05, eta: 8:21:38, time: 0.785, data_time: 0.021, memory: 4350, loss_cls: 0.1345, loss_bbox: 0.2939, d0.loss_cls: 0.2387, d0.loss_bbox: 0.4163, d1.loss_cls: 0.1738, d1.loss_bbox: 0.3230, d2.loss_cls: 0.1517, d2.loss_bbox: 0.3002, d3.loss_cls: 0.1399, d3.loss_bbox: 0.2976, d4.loss_cls: 0.1359, d4.loss_bbox: 0.2969, loss: 2.9024, grad_norm: 23.1596
2025-04-16 19:55:00,863 - mmdet - INFO - Epoch [2][1100/7033]	lr: 9.331e-05, eta: 8:20:33, time: 0.785, data_time: 0.022, memory: 4350, loss_cls: 0.1541, loss_bbox: 0.2973, d0.loss_cls: 0.2388, d0.loss_bbox: 0.4199, d1.loss_cls: 0.1839, d1.loss_bbox: 0.3227, d2.loss_cls: 0.1634, d2.loss_bbox: 0.3011, d3.loss_cls: 0.1592, d3.loss_bbox: 0.2974, d4.loss_cls: 0.1548, d4.loss_bbox: 0.2998, loss: 2.9927, grad_norm: 23.4463
2025-04-16 19:55:40,128 - mmdet - INFO - Epoch [2][1150/7033]	lr: 9.331e-05, eta: 8:19:29, time: 0.785, data_time: 0.020, memory: 4350, loss_cls: 0.1460, loss_bbox: 0.2963, d0.loss_cls: 0.2355, d0.loss_bbox: 0.4154, d1.loss_cls: 0.1724, d1.loss_bbox: 0.3251, d2.loss_cls: 0.1565, d2.loss_bbox: 0.3027, d3.loss_cls: 0.1500, d3.loss_bbox: 0.2988, d4.loss_cls: 0.1469, d4.loss_bbox: 0.2986, loss: 2.9443, grad_norm: 43.3039
2025-04-16 19:56:19,257 - mmdet - INFO - Epoch [2][1200/7033]	lr: 9.331e-05, eta: 8:18:25, time: 0.783, data_time: 0.021, memory: 4350, loss_cls: 0.1452, loss_bbox: 0.2933, d0.loss_cls: 0.2351, d0.loss_bbox: 0.4231, d1.loss_cls: 0.1782, d1.loss_bbox: 0.3230, d2.loss_cls: 0.1574, d2.loss_bbox: 0.2983, d3.loss_cls: 0.1500, d3.loss_bbox: 0.2928, d4.loss_cls: 0.1479, d4.loss_bbox: 0.2931, loss: 2.9374, grad_norm: 26.2433
2025-04-16 19:56:58,446 - mmdet - INFO - Epoch [2][1250/7033]	lr: 9.331e-05, eta: 8:17:21, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.1325, loss_bbox: 0.2956, d0.loss_cls: 0.2343, d0.loss_bbox: 0.4114, d1.loss_cls: 0.1699, d1.loss_bbox: 0.3183, d2.loss_cls: 0.1451, d2.loss_bbox: 0.2985, d3.loss_cls: 0.1381, d3.loss_bbox: 0.2949, d4.loss_cls: 0.1329, d4.loss_bbox: 0.2986, loss: 2.8700, grad_norm: 36.5994
2025-04-16 19:57:37,812 - mmdet - INFO - Epoch [2][1300/7033]	lr: 9.331e-05, eta: 8:16:18, time: 0.787, data_time: 0.023, memory: 4350, loss_cls: 0.1387, loss_bbox: 0.2935, d0.loss_cls: 0.2258, d0.loss_bbox: 0.4140, d1.loss_cls: 0.1727, d1.loss_bbox: 0.3151, d2.loss_cls: 0.1513, d2.loss_bbox: 0.2947, d3.loss_cls: 0.1436, d3.loss_bbox: 0.2911, d4.loss_cls: 0.1390, d4.loss_bbox: 0.2953, loss: 2.8747, grad_norm: 40.4385
2025-04-16 19:58:17,029 - mmdet - INFO - Epoch [2][1350/7033]	lr: 9.331e-05, eta: 8:15:15, time: 0.784, data_time: 0.021, memory: 4350, loss_cls: 0.1444, loss_bbox: 0.2945, d0.loss_cls: 0.2413, d0.loss_bbox: 0.4174, d1.loss_cls: 0.1784, d1.loss_bbox: 0.3224, d2.loss_cls: 0.1600, d2.loss_bbox: 0.2981, d3.loss_cls: 0.1493, d3.loss_bbox: 0.2967, d4.loss_cls: 0.1442, d4.loss_bbox: 0.2990, loss: 2.9456, grad_norm: 39.6482
2025-04-16 19:58:56,103 - mmdet - INFO - Epoch [2][1400/7033]	lr: 9.331e-05, eta: 8:14:12, time: 0.781, data_time: 0.020, memory: 4350, loss_cls: 0.1428, loss_bbox: 0.2999, d0.loss_cls: 0.2455, d0.loss_bbox: 0.4195, d1.loss_cls: 0.1831, d1.loss_bbox: 0.3265, d2.loss_cls: 0.1593, d2.loss_bbox: 0.3011, d3.loss_cls: 0.1495, d3.loss_bbox: 0.2998, d4.loss_cls: 0.1432, d4.loss_bbox: 0.3036, loss: 2.9738, grad_norm: 23.3376
2025-04-16 19:59:35,461 - mmdet - INFO - Epoch [2][1450/7033]	lr: 9.331e-05, eta: 8:13:10, time: 0.787, data_time: 0.021, memory: 4350, loss_cls: 0.1426, loss_bbox: 0.3038, d0.loss_cls: 0.2295, d0.loss_bbox: 0.4292, d1.loss_cls: 0.1754, d1.loss_bbox: 0.3322, d2.loss_cls: 0.1577, d2.loss_bbox: 0.3083, d3.loss_cls: 0.1477, d3.loss_bbox: 0.3047, d4.loss_cls: 0.1421, d4.loss_bbox: 0.3066, loss: 2.9799, grad_norm: 20.3879
2025-04-16 20:00:14,800 - mmdet - INFO - Epoch [2][1500/7033]	lr: 9.331e-05, eta: 8:12:08, time: 0.787, data_time: 0.023, memory: 4350, loss_cls: 0.1476, loss_bbox: 0.3027, d0.loss_cls: 0.2495, d0.loss_bbox: 0.4303, d1.loss_cls: 0.1868, d1.loss_bbox: 0.3326, d2.loss_cls: 0.1638, d2.loss_bbox: 0.3082, d3.loss_cls: 0.1533, d3.loss_bbox: 0.3062, d4.loss_cls: 0.1478, d4.loss_bbox: 0.3064, loss: 3.0350, grad_norm: 33.1612
