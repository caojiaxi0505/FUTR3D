{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a2383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jxcao_futr3d/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
    "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
    "\n",
    "def apply_rotary_emb_mamba(x: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    x_ = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, x_)\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis).flatten(3)\n",
    "    return x_out.type_as(x).to(x.device)\n",
    "\n",
    "class RoPEAttention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with RoPE.\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, freqs_cis):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "import math\n",
    "from einops import rearrange\n",
    "from mamba_ssm.ops.triton.ssd_combined import ssd_selective_scan\n",
    "\n",
    "def init_conv(conv):\n",
    "    \"\"\"Initialize convolution layers.\"\"\"\n",
    "    nn.init.kaiming_normal_(conv.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "    if conv.bias is not None:\n",
    "        nn.init.constant_(conv.bias, 0)\n",
    "\n",
    "class RoPEMamba(nn.Module):\n",
    "    \"\"\"RoPE Mamba model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 # --------------------------------\n",
    "                 use_conv=True,\n",
    "                 d_model=256,\n",
    "                 d_state=1,\n",
    "                 headdim=32,\n",
    "                 A_init_range=(1, 16),\n",
    "                 dt_min=0.001,\n",
    "                 dt_max=0.1,\n",
    "                 dt_init_floor=1e-4,\n",
    "                 dt_limit=(0.0, float(\"inf\")),\n",
    "                 bias=False,\n",
    "                 chunk_size=256,\n",
    "                 device=None,\n",
    "                 dtype=None,\n",
    "                 H=180,\n",
    "                 W=180,\n",
    "                 # --------------------------------\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.d_model = self.d_inner = self.d_ssm = d_model\n",
    "        self.d_state = d_state\n",
    "        self.headdim = headdim\n",
    "        assert self.d_ssm % self.headdim == 0\n",
    "        self.nheads = self.d_ssm // self.headdim\n",
    "        self.dt_limit = dt_limit\n",
    "        self.activation = \"silu\"\n",
    "        self.chunk_size = chunk_size\n",
    "        d_in_proj = (2 * self.d_inner + 2 * self.d_state + self.nheads) * 4\n",
    "        self.d_in_proj = d_in_proj\n",
    "        # self.in_proj_H = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
    "        # self.in_proj_V = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
    "        self.in_proj = nn.Linear(self.d_model, d_in_proj, bias=bias, **factory_kwargs)\n",
    "        self.act = nn.SiLU()\n",
    "        # --- HF Path ---\n",
    "        dt_HF = torch.clamp(\n",
    "            torch.exp(\n",
    "                torch.rand(self.nheads, **factory_kwargs)\n",
    "                * (math.log(dt_max) - math.log(dt_min))\n",
    "                + math.log(dt_min)\n",
    "            ),\n",
    "            min=dt_init_floor,\n",
    "        )\n",
    "        inv_dt_HF = dt_HF + torch.log(-torch.expm1(-dt_HF))\n",
    "        self.dt_bias_HF = nn.Parameter(inv_dt_HF)\n",
    "        self.dt_bias_HF._no_weight_decay = True\n",
    "\n",
    "        # --- HB Path ---\n",
    "        dt_HB = torch.clamp(\n",
    "            torch.exp(\n",
    "                torch.rand(self.nheads, **factory_kwargs)\n",
    "                * (math.log(dt_max) - math.log(dt_min))\n",
    "                + math.log(dt_min)\n",
    "            ),\n",
    "            min=dt_init_floor,\n",
    "        )\n",
    "        inv_dt_HB = dt_HB + torch.log(-torch.expm1(-dt_HB))\n",
    "        self.dt_bias_HB = nn.Parameter(inv_dt_HB)\n",
    "        self.dt_bias_HB._no_weight_decay = True\n",
    "\n",
    "        # --- VH Path ---\n",
    "        dt_VH = torch.clamp(\n",
    "            torch.exp(\n",
    "                torch.rand(self.nheads, **factory_kwargs)\n",
    "                * (math.log(dt_max) - math.log(dt_min))\n",
    "                + math.log(dt_min)\n",
    "            ),\n",
    "            min=dt_init_floor,\n",
    "        )\n",
    "        inv_dt_VH = dt_VH + torch.log(-torch.expm1(-dt_VH))\n",
    "        self.dt_bias_VH = nn.Parameter(inv_dt_VH)\n",
    "        self.dt_bias_VH._no_weight_decay = True\n",
    "\n",
    "        # --- VB Path ---\n",
    "        dt_VB = torch.clamp(\n",
    "            torch.exp(\n",
    "                torch.rand(self.nheads, **factory_kwargs)\n",
    "                * (math.log(dt_max) - math.log(dt_min))\n",
    "                + math.log(dt_min)\n",
    "            ),\n",
    "            min=dt_init_floor,\n",
    "        )\n",
    "        inv_dt_VB = dt_VB + torch.log(-torch.expm1(-dt_VB))\n",
    "        self.dt_bias_VB = nn.Parameter(inv_dt_VB)\n",
    "        self.dt_bias_VB._no_weight_decay = True\n",
    "\n",
    "        assert A_init_range[0] > 0 and A_init_range[1] >= A_init_range[0]\n",
    "\n",
    "        # --- A_log for HF Path ---\n",
    "        A_HF = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(\n",
    "            *A_init_range\n",
    "        )\n",
    "        A_log_HF = torch.log(A_HF).to(dtype=dtype)\n",
    "        self.A_log_HF = nn.Parameter(A_log_HF)\n",
    "        self.A_log_HF._no_weight_decay = True\n",
    "\n",
    "        # --- A_log for HB Path ---\n",
    "        A_HB = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(\n",
    "            *A_init_range\n",
    "        )\n",
    "        A_log_HB = torch.log(A_HB).to(dtype=dtype)\n",
    "        self.A_log_HB = nn.Parameter(A_log_HB)\n",
    "        self.A_log_HB._no_weight_decay = True\n",
    "\n",
    "        # --- A_log for VH Path ---\n",
    "        A_VH = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(\n",
    "            *A_init_range\n",
    "        )\n",
    "        A_log_VH = torch.log(A_VH).to(dtype=dtype)\n",
    "        self.A_log_VH = nn.Parameter(A_log_VH)\n",
    "        self.A_log_VH._no_weight_decay = True\n",
    "\n",
    "        # --- A_log for VB Path ---\n",
    "        A_VB = torch.empty(self.nheads, dtype=torch.float32, device=device).uniform_(\n",
    "            *A_init_range\n",
    "        )\n",
    "        A_log_VB = torch.log(A_VB).to(dtype=dtype)\n",
    "        self.A_log_VB = nn.Parameter(A_log_VB)\n",
    "        self.A_log_VB._no_weight_decay = True\n",
    "\n",
    "        # --- D for HF Path ---\n",
    "        self.D_HF = nn.Parameter(torch.ones(self.nheads, device=device)) # Consider adding dtype=dtype if needed\n",
    "        self.D_HF._no_weight_decay = True\n",
    "\n",
    "        # --- D for HB Path ---\n",
    "        self.D_HB = nn.Parameter(torch.ones(self.nheads, device=device)) # Consider adding dtype=dtype if needed\n",
    "        self.D_HB._no_weight_decay = True\n",
    "\n",
    "        # --- D for VH Path ---\n",
    "        self.D_VH = nn.Parameter(torch.ones(self.nheads, device=device)) # Consider adding dtype=dtype if needed\n",
    "        self.D_VH._no_weight_decay = True\n",
    "\n",
    "        # --- D for VB Path ---\n",
    "        self.D_VB = nn.Parameter(torch.ones(self.nheads, device=device)) # Consider adding dtype=dtype if needed\n",
    "        self.D_VB._no_weight_decay = True\n",
    "\n",
    "        self.out_proj = nn.Linear(self.d_model * 4, self.d_model, bias=bias, **factory_kwargs)\n",
    "        self.out_norm = nn.LayerNorm(self.d_model, eps=1e-6, **factory_kwargs)\n",
    "        self.out_drop = nn.Dropout(proj_drop)\n",
    "        self.use_conv = use_conv\n",
    "        if self.use_conv:\n",
    "            self.conv1d_HF = nn.Conv1d(\n",
    "                in_channels=d_model,\n",
    "                out_channels=d_model,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.conv1d_HB = nn.Conv1d(\n",
    "                in_channels=d_model,\n",
    "                out_channels=d_model,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.conv1d_VF = nn.Conv1d(\n",
    "                in_channels=d_model,\n",
    "                out_channels=d_model,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            )\n",
    "            self.conv1d_VB = nn.Conv1d(\n",
    "                in_channels=d_model,\n",
    "                out_channels=d_model,\n",
    "                kernel_size=4,\n",
    "                stride=4,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            )\n",
    "            init_conv(self.conv1d_HF)\n",
    "            init_conv(self.conv1d_HB)\n",
    "            init_conv(self.conv1d_VF)\n",
    "            init_conv(self.conv1d_VB)\n",
    "            mask_low_resolution = torch.ones(\n",
    "                1, 1, H // 2, W // 2, device=device if device is not None else \"cpu\"\n",
    "            )\n",
    "            morton_H_indices_low, morton_V_indices_low = self.morton_code_extraction(mask_low_resolution)\n",
    "            inverse_H_indices_low = torch.empty_like(morton_H_indices_low)\n",
    "            inverse_H_indices_low[morton_H_indices_low] = torch.arange(morton_H_indices_low.size(0), device=morton_H_indices_low.device)\n",
    "            inverse_V_indices_low = torch.empty_like(morton_V_indices_low)\n",
    "            inverse_V_indices_low[morton_V_indices_low] = torch.arange(morton_V_indices_low.size(0), device=morton_V_indices_low.device)\n",
    "            self.register_buffer('morton_H_indices_low', morton_H_indices_low)\n",
    "            self.register_buffer('morton_V_indices_low', morton_V_indices_low)\n",
    "            self.register_buffer('inverse_H_indices_low', inverse_H_indices_low)\n",
    "            self.register_buffer('inverse_V_indices_low', inverse_V_indices_low)\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        mask = torch.ones(1, 1, H, W, device=device if device is not None else \"cpu\")\n",
    "        morton_H_indices, morton_V_indices = self.morton_code_extraction(mask)\n",
    "        inverse_H_indices = torch.empty_like(morton_H_indices)\n",
    "        inverse_H_indices[morton_H_indices] = torch.arange(morton_H_indices.size(0), device=morton_H_indices.device)\n",
    "        inverse_V_indices = torch.empty_like(morton_V_indices)\n",
    "        inverse_V_indices[morton_V_indices] = torch.arange(morton_V_indices.size(0), device=morton_V_indices.device)\n",
    "        self.register_buffer('morton_H_indices', morton_H_indices)\n",
    "        self.register_buffer('morton_V_indices', morton_V_indices)\n",
    "        self.register_buffer('inverse_H_indices', inverse_H_indices)\n",
    "        self.register_buffer('inverse_V_indices', inverse_V_indices)\n",
    "        conv_dim = self.d_inner + 2 * self.d_state\n",
    "        self.conv_dim = conv_dim\n",
    "        # --- Conv1d for x component ---\n",
    "        # Horizontal Forward for x\n",
    "        # self.conv1d_hf_x = nn.Conv1d(\n",
    "        #     in_channels=conv_dim,\n",
    "        #     out_channels=conv_dim,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=conv_dim,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Horizontal Backward for x\n",
    "        # self.conv1d_hb_x = nn.Conv1d(\n",
    "        #     in_channels=conv_dim,\n",
    "        #     out_channels=conv_dim,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=conv_dim,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Vertical Forward (Height-wise) for x\n",
    "        # self.conv1d_vh_x = nn.Conv1d(\n",
    "        #     in_channels=conv_dim,\n",
    "        #     out_channels=conv_dim,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=conv_dim,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Vertical Backward (Width-wise) for x\n",
    "        # self.conv1d_vb_x = nn.Conv1d(\n",
    "        #     in_channels=conv_dim,\n",
    "        #     out_channels=conv_dim,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=conv_dim,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "\n",
    "        # # --- Conv1d for z component ---\n",
    "        # # Horizontal Forward for z\n",
    "        # self.conv1d_hf_z = nn.Conv1d(\n",
    "        #     in_channels=self.d_inner,\n",
    "        #     out_channels=self.d_inner,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=self.d_inner,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Horizontal Backward for z\n",
    "        # self.conv1d_hb_z = nn.Conv1d(\n",
    "        #     in_channels=self.d_inner,\n",
    "        #     out_channels=self.d_inner,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=self.d_inner,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Vertical Forward (Height-wise) for z\n",
    "        # self.conv1d_vh_z = nn.Conv1d(\n",
    "        #     in_channels=self.d_inner,\n",
    "        #     out_channels=self.d_inner,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=self.d_inner,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # # Vertical Backward (Width-wise) for z\n",
    "        # self.conv1d_vb_z = nn.Conv1d(\n",
    "        #     in_channels=self.d_inner,\n",
    "        #     out_channels=self.d_inner,\n",
    "        #     bias=False,\n",
    "        #     kernel_size=4,\n",
    "        #     groups=self.d_inner,\n",
    "        #     **factory_kwargs,\n",
    "        # )\n",
    "        # ------------------------\n",
    "        d_conv2d = d_model * 2\n",
    "        self.conv2d_hf = nn.Sequential(\n",
    "            nn.Conv2d(d_conv2d,d_conv2d,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(d_conv2d, eps=1e-3, momentum=0.01))\n",
    "        self.conv2d_hb = nn.Sequential(\n",
    "            nn.Conv2d(d_conv2d,d_conv2d,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(d_conv2d, eps=1e-3, momentum=0.01))\n",
    "        self.conv2d_vf = nn.Sequential(\n",
    "            nn.Conv2d(d_conv2d,d_conv2d,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(d_conv2d, eps=1e-3, momentum=0.01))\n",
    "        self.conv2d_vb = nn.Sequential(\n",
    "            nn.Conv2d(d_conv2d,d_conv2d,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm2d(d_conv2d, eps=1e-3, momentum=0.01))\n",
    "        \n",
    "    def forward(self, x, freqs_cis):\n",
    "        B, C, H, W = x.shape\n",
    "        morton_H_indices = self.morton_H_indices.to(x.device)\n",
    "        morton_V_indices = self.morton_V_indices.to(x.device)\n",
    "        inverse_H_indices = self.inverse_H_indices.to(x.device)\n",
    "        inverse_V_indices = self.inverse_V_indices.to(x.device)\n",
    "        if self.use_conv:\n",
    "            inverse_H_indices_low = self.inverse_H_indices_low.to(x.device)\n",
    "            inverse_V_indices_low = self.inverse_V_indices_low.to(x.device)\n",
    "        x_flat = x.view(B, C, -1).permute(0, 2, 1)\n",
    "        # ---- 重排在in_proj后实现\n",
    "        # x_morton_H = x_flat[:, :, morton_H_indices].permute(0, 2, 1)\n",
    "        # x_morton_V = x_flat[:, :, morton_V_indices].permute(0, 2, 1)\n",
    "        zxbcdt = self.in_proj(x_flat)\n",
    "\n",
    "        A_HF = -torch.exp(self.A_log_HF.float())\n",
    "        A_HB = -torch.exp(self.A_log_HB.float())\n",
    "        A_VF = -torch.exp(self.A_log_VF.float())\n",
    "        A_VB = -torch.exp(self.A_log_VB.float())\n",
    "\n",
    "        dim = self.d_ssm\n",
    "\n",
    "        z_HF, xBC_HF, dt_HF, z_HB, xBC_HB, dt_HB, z_VF, xBC_VF, dt_VF, z_VB, xBC_VB, dt_VB = torch.split(\n",
    "            zxbcdt, [dim, dim + 2 * self.d_state, self.nheads, dim, dim + 2 * self.d_state, self.nheads, dim, dim + 2 * self.d_state, self.nheads, dim, dim + 2 * self.d_state, self.nheads], dim=-1\n",
    "        )\n",
    "        # x_shape: (B, N, C)\n",
    "        x_HF, B_HF, C_HF = torch.split(xBC_HF, [dim, self.d_state, self.d_state], dim=-1)\n",
    "        x_HB, B_HB, C_HB = torch.split(xBC_HB, [dim, self.d_state, self.d_state], dim=-1)\n",
    "        x_VF, B_VF, C_VF = torch.split(xBC_VF, [dim, self.d_state, self.d_state], dim=-1)\n",
    "        x_VB, B_VB, C_VB = torch.split(xBC_VB, [dim, self.d_state, self.d_state], dim=-1)\n",
    "\n",
    "        xz_HF = torch.cat([x_HF, z_HF], dim=-1).permute(0, 2, 1).view(B,C*2,H,W)\n",
    "        xz_HB = torch.cat([x_HB, z_HB], dim=-1).permute(0, 2, 1).view(B,C*2,H,W)\n",
    "        xz_VF = torch.cat([x_VF, z_VF], dim=-1).permute(0, 2, 1).view(B,C*2,H,W)\n",
    "        xz_VB = torch.cat([x_VB, z_VB], dim=-1).permute(0, 2, 1).view(B,C*2,H,W)\n",
    "\n",
    "        xz_HF = self.conv2d_hf(xz_HF)\n",
    "        xz_HB = self.conv2d_hb(xz_HB)\n",
    "        xz_VF = self.conv2d_vf(xz_VF)\n",
    "        xz_VB = self.conv2d_vb(xz_VB)\n",
    "\n",
    "        x_HF, z_HF = torch.split(xz_HF.view(B, C*2, -1).permute(0, 2, 1),[C,C], dim=-1)\n",
    "        x_HB, z_HB = torch.split(xz_HB.view(B, C*2, -1).permute(0, 2, 1),[C,C], dim=-1)\n",
    "        x_VF, z_VF = torch.split(xz_VF.view(B, C*2, -1).permute(0, 2, 1),[C,C], dim=-1)\n",
    "        x_VB, z_VB = torch.split(xz_VB.view(B, C*2, -1).permute(0, 2, 1),[C,C], dim=-1)\n",
    "\n",
    "        x_HF = x_HF[:, morton_H_indices, :]\n",
    "        x_HB = x_HB[:, morton_H_indices, :]\n",
    "        x_VF = x_VF[:, morton_V_indices, :]\n",
    "        x_VB = x_VB[:, morton_V_indices, :]\n",
    "        z_HF = z_HF[:, morton_H_indices, :]\n",
    "        z_HB = z_HB[:, morton_H_indices, :]\n",
    "        z_VF = z_VF[:, morton_V_indices, :]\n",
    "        z_VB = z_VB[:, morton_V_indices, :]\n",
    "        B_HF = B_HF[:, morton_H_indices, :]\n",
    "        B_HB = B_HB[:, morton_H_indices, :]\n",
    "        B_VF = B_VF[:, morton_V_indices, :]\n",
    "        B_VB = B_VB[:, morton_V_indices, :]\n",
    "        C_HF = C_HF[:, morton_H_indices, :]\n",
    "        C_HB = C_HB[:, morton_H_indices, :]\n",
    "        C_VF = C_VF[:, morton_V_indices, :]\n",
    "        C_VB = C_VB[:, morton_V_indices, :]\n",
    "        dt_HF = dt_HF[:, morton_H_indices]\n",
    "        dt_HB = dt_HB[:, morton_H_indices]\n",
    "        dt_VF = dt_VF[:, morton_V_indices]\n",
    "        dt_VB = dt_VB[:, morton_V_indices]\n",
    "\n",
    "        x_HF = apply_rotary_emb_mamba(x_HF, freqs_cis=freqs_cis)\n",
    "        x_HB = apply_rotary_emb_mamba(x_HB, freqs_cis=freqs_cis)\n",
    "        x_VF = apply_rotary_emb_mamba(x_VF, freqs_cis=freqs_cis)\n",
    "        x_VB = apply_rotary_emb_mamba(x_VB, freqs_cis=freqs_cis)\n",
    "\n",
    "        x_HF = rearrange(x_HF, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "        B_HF = rearrange(B_HF, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        C_HF = rearrange(C_HF, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        z_HF = rearrange(z_HF, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "\n",
    "        x_HB = rearrange(x_HB, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "        B_HB = rearrange(B_HB, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        C_HB = rearrange(C_HB, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        z_HB = rearrange(z_HB, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "\n",
    "        x_VF = rearrange(x_VF, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "        B_VF = rearrange(B_VF, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        C_VF = rearrange(C_VF, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        z_VF = rearrange(z_VF, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "\n",
    "        x_VB = rearrange(x_VB, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "        B_VB = rearrange(B_VB, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        C_VB = rearrange(C_VB, \"b l (g n) -> b l g n\", g=1).contiguous()\n",
    "        z_VB = rearrange(z_VB, \"b l (h p) -> b l h p\", h=self.nheads).contiguous()\n",
    "\n",
    "        out_HF = ssd_selective_scan(\n",
    "            x_HF,\n",
    "            dt_HF.to(x_HF.dtype),\n",
    "            A_HF,\n",
    "            B_HF,\n",
    "            C_HF,\n",
    "            D=self.D_HF.float(),\n",
    "            z=z_HF,\n",
    "            dt_bias=self.dt_bias_HF,\n",
    "            dt_softplus=True,\n",
    "            dt_limit=self.dt_limit,\n",
    "        )\n",
    "        out_HB = ssd_selective_scan(\n",
    "            x_HB.flip(1),\n",
    "            dt_HB.to(x_HB.dtype).flip(1),\n",
    "            A_HB,\n",
    "            B_HB.flip(1),\n",
    "            C_HB.flip(1),\n",
    "            D=self.D_HB.float(),\n",
    "            z=z_HB.flip(1),\n",
    "            dt_bias=self.dt_bias_HB,\n",
    "            dt_softplus=True,\n",
    "            dt_limit=self.dt_limit,\n",
    "        ).flip(1)\n",
    "        out_VF = ssd_selective_scan(\n",
    "            x_VF,\n",
    "            dt_VF.to(x_VF.dtype),\n",
    "            A_VF,\n",
    "            B_VF,\n",
    "            C_VF,\n",
    "            D=self.D_VF.float(),\n",
    "            z=z_VF,\n",
    "            dt_bias=self.dt_bias_VF,\n",
    "            dt_softplus=True,\n",
    "            dt_limit=self.dt_limit,\n",
    "        )\n",
    "        out_VB = ssd_selective_scan(\n",
    "            x_VB.flip(1),\n",
    "            dt_VB.to(x_VB.dtype).flip(1),\n",
    "            A_VB,\n",
    "            B_VB.flip(1),\n",
    "            C_VB.flip(1),\n",
    "            D=self.D_VB.float(),\n",
    "            z=z_VB.flip(1),\n",
    "            dt_bias=self.dt_bias_VB,\n",
    "            dt_softplus=True,\n",
    "            dt_limit=self.dt_limit,\n",
    "        ).flip(1)\n",
    "\n",
    "        out_HF = rearrange(out_HF, \"b s h p -> b s (h p)\")\n",
    "        out_HB = rearrange(out_HB, \"b s h p -> b s (h p)\")\n",
    "        out_VF = rearrange(out_VF, \"b s h p -> b s (h p)\")\n",
    "        out_VB = rearrange(out_VB, \"b s h p -> b s (h p)\")\n",
    "\n",
    "        if self.use_conv:\n",
    "            out_HF = self.conv1d_HF(out_HF.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "            out_HB = self.conv1d_HB(out_HB.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "            out_VF = self.conv1d_VF(out_VF.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "            out_VB = self.conv1d_VB(out_VB.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "            \n",
    "            out_HF_inverse = out_HF[:, inverse_H_indices_low, :]\n",
    "            out_HB_inverse = out_HB[:, inverse_H_indices_low, :]\n",
    "            out_VF_inverse = out_VF[:, inverse_V_indices_low, :]\n",
    "            out_VB_inverse = out_VB[:, inverse_V_indices_low, :]\n",
    "        else:\n",
    "            out_HF_inverse = out_HF[:, inverse_H_indices, :]\n",
    "            out_HB_inverse = out_HB[:, inverse_H_indices, :]\n",
    "            out_VF_inverse = out_VF[:, inverse_V_indices, :]\n",
    "            out_VB_inverse = out_VB[:, inverse_V_indices, :]\n",
    "\n",
    "        out = self.out_proj(\n",
    "            torch.cat([out_HF_inverse, out_HB_inverse, out_VF_inverse, out_VB_inverse], dim=-1).contiguous()\n",
    "        )\n",
    "\n",
    "        out = self.out_norm(out)\n",
    "        out = self.out_drop(out)\n",
    "        if self.use_conv:\n",
    "            out = out.view(B, C, H // 2, W // 2)\n",
    "        else:\n",
    "            out = out.view(B, C, H, W)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def morton_code_extraction(self, mask):\n",
    "        device = mask.device\n",
    "        h, w = mask[0][0].shape\n",
    "        \"\"\"\n",
    "        说明：\n",
    "            row_indices[2,3]的值为2，表示该位置的行索引为2\n",
    "            col_indices[2,3]的值为3，表示该位置的列索引为3\n",
    "        \"\"\"\n",
    "        row_indices, col_indices = torch.meshgrid(\n",
    "            torch.arange(h, device=device),\n",
    "            torch.arange(w, device=device),\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "        row_indices = row_indices.flatten()\n",
    "        col_indices = col_indices.flatten()\n",
    "        valid_indices = mask[0][0].flatten() != 0\n",
    "        row_indices = row_indices[valid_indices]\n",
    "        col_indices = col_indices[valid_indices]\n",
    "        morton_codes_1 = self.interleave_bits(col_indices, row_indices)\n",
    "        morton_codes_2 = self.interleave_bits_x_last(col_indices, row_indices)\n",
    "        sorted_indices_1 = torch.argsort(morton_codes_1)\n",
    "        sorted_indices_2 = torch.argsort(morton_codes_2)\n",
    "        linear_indices_1 = (\n",
    "            row_indices[sorted_indices_1] * w + col_indices[sorted_indices_1]\n",
    "        )\n",
    "        linear_indices_2 = (\n",
    "            row_indices[sorted_indices_2] * w + col_indices[sorted_indices_2]\n",
    "        )\n",
    "        return linear_indices_1, linear_indices_2\n",
    "\n",
    "    def interleave_bits(self, x, y):\n",
    "        x = (x | (x << 8)) & 0x00FF00FF\n",
    "        x = (x | (x << 4)) & 0x0F0F0F0F\n",
    "        x = (x | (x << 2)) & 0x33333333\n",
    "        x = (x | (x << 1)) & 0x55555555\n",
    "        y = (y | (y << 8)) & 0x00FF00FF\n",
    "        y = (y | (y << 4)) & 0x0F0F0F0F\n",
    "        y = (y | (y << 2)) & 0x33333333\n",
    "        y = (y | (y << 1)) & 0x55555555\n",
    "        z = (x << 1) | y\n",
    "        return z\n",
    "\n",
    "    def interleave_bits_x_last(self, x, y):\n",
    "        x = (x | (x << 8)) & 0x00FF00FF\n",
    "        x = (x | (x << 4)) & 0x0F0F0F0F\n",
    "        x = (x | (x << 2)) & 0x33333333\n",
    "        x = (x | (x << 1)) & 0x55555555\n",
    "        y = (y | (y << 8)) & 0x00FF00FF\n",
    "        y = (y | (y << 4)) & 0x0F0F0F0F\n",
    "        y = (y | (y << 2)) & 0x33333333\n",
    "        y = (y | (y << 1)) & 0x55555555\n",
    "        z = (y << 1) | x\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q[:, :, 1:], k[:, :, 1:] = apply_rotary_emb(q[:, :, 1:], k[:, :, 1:], freqs_cis=freqs_cis)\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.layers import DropPath\n",
    "from timm.models.vision_transformer import Mlp\n",
    "\n",
    "class RoPE_Layer_scale_init_Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, Attention_block=RoPEAttention, Mlp_block=Mlp ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_block(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "\n",
    "    def forward(self, x, freqs_cis):\n",
    "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "class RoPE_Layer_scale_init_Mamba_Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, Attention_block=RoPEAttention, Mlp_block=Mlp ,init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_block(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, freqs_cis):\n",
    "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), freqs_cis=freqs_cis))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import PatchEmbed\n",
    "from timm.models.layers import trunc_normal_\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
    "    N = t_x.shape[0]\n",
    "    depth = freqs.shape[1]\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
    "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)        \n",
    "    return freqs_cis\n",
    "\n",
    "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "    freqs_x = []\n",
    "    freqs_y = []\n",
    "    mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    for i in range(num_heads):\n",
    "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)        \n",
    "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
    "        fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
    "        freqs_x.append(fx)\n",
    "        freqs_y.append(fy)\n",
    "    freqs_x = torch.stack(freqs_x, dim=0)\n",
    "    freqs_y = torch.stack(freqs_y, dim=0)\n",
    "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
    "    return freqs\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
    "    return t_x, t_y\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
    "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    t_x, t_y = init_t_xy(end_x, end_y)\n",
    "    freqs_x = torch.outer(t_x, freqs_x)\n",
    "    freqs_y = torch.outer(t_y, freqs_y)\n",
    "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "class rope_vit_models(nn.Module):\n",
    "    def __init__(self,\n",
    "                 rope_theta=100.0,\n",
    "                 rope_mixed=False,\n",
    "                 use_ape=False,\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 global_pool=None,\n",
    "                 block_layers=None,\n",
    "                 Patch_layer=PatchEmbed,\n",
    "                 act_layer=nn.GELU,\n",
    "                 Attention_block=None,\n",
    "                 Mlp_block=Mlp,\n",
    "                 dpr_constant=True,\n",
    "                 init_scale=1e-4,\n",
    "                 mlp_ratio_clstk=4.0,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = drop_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.patch_embed = Patch_layer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        dpr = [drop_path_rate for i in range(depth)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            block_layers(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=0.0,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                Attention_block=Attention_block,\n",
    "                Mlp_block=Mlp_block,\n",
    "                init_values=init_scale\n",
    "            ) for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.use_ape = use_ape\n",
    "        if not self.use_ape:\n",
    "            self.pos_embed = None\n",
    "        self.rope_mixed = rope_mixed\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        if self.rope_mixed:\n",
    "            self.compute_cis = partial(compute_mixed_cis, num_heads=self.num_heads)\n",
    "            freqs = []\n",
    "            for i, _ in enumerate(self.blocks):\n",
    "                freqs.append(init_random_2d_freqs(dim=embed_dim // num_heads, num_heads=num_heads, theta=rope_theta))\n",
    "            freqs = torch.stack(freqs, dim=1).view(2, len(self.blocks), -1)\n",
    "            self.freqs = nn.Parameter(freqs.clone(), requires_grad=True)\n",
    "            t_x, t_y = init_t_xy(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
    "            self.register_buffer('freqs_t_x', t_x)\n",
    "            self.register_buffer('freqs_t_y', t_y)\n",
    "        else:\n",
    "            self.compute_cis = partial(compute_axial_cis, dim=embed_dim//num_heads, theta=rope_theta)\n",
    "            freqs_cis = self.compute_cis(end_x = img_size // patch_size, end_y = img_size // patch_size)\n",
    "            self.freqs_cis = freqs_cis\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'freqs'}\n",
    "    \n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "    def forward_features(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.use_ape:\n",
    "            pos_embed = self.pos_embed\n",
    "            if pos_embed.shape[-2] != x.shape[-2]:\n",
    "                img_size = self.patch_embed.img_size\n",
    "                patch_size = self.patch_embed.patch_size\n",
    "                pos_embed = pos_embed.view(1, (img_size[1] // patch_size[1]), (img_size[0] // patch_size[0]), self.embed_dim).permute(0, 3, 1, 2)\n",
    "                pos_embed = F.interpolate(pos_embed, size=(H // patch_size[1], W // patch_size[0]), mode='bicubic', align_corners=False)\n",
    "                pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            x = x + pos_embed\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.rope_mixed:\n",
    "            if self.freqs_t_x.shape[0] != x.shape[1] - 1:\n",
    "                t_x, t_y = init_t_xy(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
    "                t_x, t_y = t_x.to(x.device), t_y.to(x.device)\n",
    "            else:\n",
    "                t_x, t_y = self.freqs_t_x, self.freqs_t_y\n",
    "            freqs_cis = self.compute_cis(self.freqs, t_x, t_y)\n",
    "            for i, blk in enumerate(self.blocks):\n",
    "                x = blk(x, freqs_cis=freqs_cis[i])\n",
    "        else:\n",
    "            if self.freqs_cis.shape[0] != x.shape[1] - 1:\n",
    "                freqs_cis = self.compute_cis(end_x = W // self.patch_size, end_y = H // self.patch_size)\n",
    "            else:\n",
    "                freqs_cis = self.freqs_cis\n",
    "            freqs_cis = freqs_cis.to(x.device)\n",
    "            for i , blk in enumerate(self.blocks):\n",
    "                x = blk(x, freqs_cis=freqs_cis)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.dropout_rate:\n",
    "            x = F.dropout(x, p=float(self.dropout_rate), training=self.training)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = rope_vit_models(\n",
    "#         img_size=224,\n",
    "#         patch_size=16,\n",
    "#         embed_dim=384,\n",
    "#         depth=12,\n",
    "#         num_heads=6,\n",
    "#         mlp_ratio=4,\n",
    "#         qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "#         block_layers=RoPE_Layer_scale_init_Block,\n",
    "#         Attention_block=RoPEAttention,\n",
    "#         rope_theta=100.0,\n",
    "#         rope_mixed=False)\n",
    "model = rope_vit_models(\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        block_layers=RoPE_Layer_scale_init_Block,\n",
    "        Attention_block=RoPEAttention,\n",
    "        rope_theta=10.0,\n",
    "        rope_mixed=True,\n",
    "        use_ape=True)\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "output = model(input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jxcao_futr3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
